{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DARTS nextgen","text":"<p>Panarctic Database of Active Layer Detatchment Slides and Retrogressive Thaw Slumps from Deep Learning on High Resolution Satellite Imagery. This is te successor of the thaw-slump-segmentation (pipeline), with which the first version of the DARTS dataset was created.</p>"},{"location":"#contribute","title":"Contribute","text":"<p>Before contributing please contact one of the authors and make sure to read the Contribution Guidelines.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>This page is also meant for internal documentation.</p>"},{"location":"contribute/#editor-setup","title":"Editor setup","text":"<p>There is only setup files provided for VSCode and no other editor (yet). A list of extensions and some settings can be found in the <code>.vscode</code>. At the first start, VSCode should ask you if you want to install the recommended extension. The settings should be automaticly used by VSCode. Both should provide the developers with a better experience and enforce code-style.</p>"},{"location":"contribute/#environment-setup","title":"Environment setup","text":"<p>Prereq:</p> <ul> <li>Rye: <code>curl -sSf https://rye.astral.sh/get | bash</code></li> <li>GDAL: <code>sudo apt update &amp;&amp; sudo apt install libpq-dev gdal-bin libgdal-dev</code> or for HPC <code>conda install conda-forge::gdal</code></li> <li>Clang: <code>sudo apt update &amp;&amp; sudo apt install clang</code> or for HPC <code>conda install conda-forge::clang_linux-64</code></li> </ul> <p>If you install GDAL via apt for linux you can view the supported versions here: https://pkgs.org/search/?q=libgdal-dev. For a finer controll over the versions please use conda.</p> <p>Now first check your gdal-version:</p> <pre><code>$ gdal-config --version\n3.9.2\n</code></pre> <p>And your CUDA version (if you want to use CUDA):</p> <pre><code>$ nvidia-smi\n# Now look on the top right of the table\n</code></pre> <p>The GDAL version is relevant, since the version of the python bindings needs to match the installed GDAL version</p> <p>Now, to sync with a specific <code>gdal</code> version, add <code>darts-preprocessing/gdalXX</code> to the <code>--features</code> flag. To sync with a specific <code>cuda</code> version, add <code>darts-ensemble/cuda1X</code> or without cuda <code>darts-ensemble/cpu</code>. E.g.:</p> <pre><code>rye sync -f --features darts-preprocessing/gdal39,darts-ensemble/cuda12 # For CUDA 12 and GDAL 3.9.2\n</code></pre> <p>As of right now, the supported <code>gdal</code> versions are: 3.9.2 (<code>gdal39</code>), 3.8.5 (<code>gdal38</code>), 3.8.4 (<code>gdal384</code>), 3.7.3 (<code>gdal37</code>) and 3.6.4 (<code>gdal36</code>). If your GDAL version is not supported (yet) please sync without GDAL and then install GDAL to an new optional group. For example, if your GDAL version is 3.8.4:</p> <pre><code>rye sync -f\nrye add --optional=gdal384 \"gdal==3.8.4\"\n</code></pre> <p>IMPORTANT! If you installed any of clang or gdal with conda, please ensure that while installing dependencies and working on the project to have the conda environment activated in which you installed clang and or gdal.</p>"},{"location":"contribute/#troubleshoot-rye-cant-find-the-right-versions","title":"Troubleshoot: Rye can't find the right versions","text":"<p>Because the <code>pyproject.toml</code> specifies additional sources, e.g. <code>https://download.pytorch.org/whl/cpu</code>, it can happen that the a package with an older version is found in these package-indexes. If such a version is found, <code>uv</code> (the installer behind <code>Rye</code>) currently stops searching other sources for the right version and stops with an <code>Version not found</code> error. This can look something like this:</p> <pre><code>No solution found when resolving dependencies:\n  \u2570\u2500\u25b6 Because only torchmetrics==1.0.3 is available and you require torchmetrics&gt;=1.4.1, we can conclude that your requirements are unsatisfiable.\n</code></pre> <p>To fix this you can set an environment variable to tell <code>uv</code> to search all package-indicies:</p> <pre><code>UV_INDEX_STRATEGY=\"unsafe-best-match\" rye sync ...\n</code></pre> <p>I recommend adding the following to your <code>.zshrc</code> or <code>.bashrc</code>:</p> <pre><code># Change the behaviour of uv package resolution to enable additional sources without breaking existing version-requirements\nexport UV_INDEX_STRATEGY=\"unsafe-best-match\"\n</code></pre> <p>Please see these issues:</p> <ul> <li>Rye: Can't specify per-dependency package index / can't specify uv behavior in config file</li> <li>UV: Add support for pinning a package to a specific index</li> </ul>"},{"location":"ref/","title":"Combined Reference","text":"<p>All references on one page</p> Table of Contents<ul> <li>Combined Reference<ul> <li>\u00a0darts_acquisition<ul> <li>Functions<ul> <li>\u00a0hello</li> </ul> </li> </ul> </li> <li>\u00a0darts_ensemble<ul> <li>Functions<ul> <li>\u00a0hello</li> </ul> </li> </ul> </li> <li>\u00a0darts_export</li> <li>\u00a0darts_postprocessing<ul> <li>Functions<ul> <li>\u00a0hello</li> </ul> </li> </ul> </li> <li>\u00a0darts_preprocessing<ul> <li>Functions<ul> <li>\u00a0hello</li> </ul> </li> </ul> </li> <li>\u00a0darts_segmentation<ul> <li>Functions<ul> <li>\u00a0hello</li> </ul> </li> </ul> </li> <li>\u00a0darts_superresolution<ul> <li>Functions<ul> <li>\u00a0hello</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"ref/#darts_acquisition","title":"<code>darts_acquisition</code>","text":"<p>Acquisition of data from various sources for the DARTS dataset.</p>"},{"location":"ref/#darts_acquisition-functions","title":"Functions","text":""},{"location":"ref/#darts_acquisition.hello","title":"<code>hello(name)</code>","text":"<p>Say hello to the user.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the user.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-acquisition/src/darts_acquisition/__init__.py</code> <pre><code>def hello(name: str) -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Args:\n        name (str): Name of the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return f\"Hello, {name}, from darts-acquisition!\"\n</code></pre>"},{"location":"ref/#darts_ensemble","title":"<code>darts_ensemble</code>","text":"<p>Inference and model ensembling for the DARTS dataset.</p>"},{"location":"ref/#darts_ensemble-functions","title":"Functions","text":""},{"location":"ref/#darts_ensemble.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-ensemble/src/darts_ensemble/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-ensemble!\"\n</code></pre>"},{"location":"ref/#darts_export","title":"<code>darts_export</code>","text":""},{"location":"ref/#darts_postprocessing","title":"<code>darts_postprocessing</code>","text":""},{"location":"ref/#darts_postprocessing-functions","title":"Functions","text":""},{"location":"ref/#darts_postprocessing.hello","title":"<code>hello()</code>","text":"Source code in <code>darts-postprocessing/src/darts_postprocessing/__init__.py</code> <pre><code>def hello() -&gt; str:\n    return \"Hello from darts-postprocessing!\"\n</code></pre>"},{"location":"ref/#darts_preprocessing","title":"<code>darts_preprocessing</code>","text":"<p>Data preprocessing and feature engineering for the DARTS dataset.</p>"},{"location":"ref/#darts_preprocessing-functions","title":"Functions","text":""},{"location":"ref/#darts_preprocessing.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-preprocessing!\"\n</code></pre>"},{"location":"ref/#darts_segmentation","title":"<code>darts_segmentation</code>","text":"<p>Image segmentation of thaw-slumps for the DARTS dataset.</p>"},{"location":"ref/#darts_segmentation-functions","title":"Functions","text":""},{"location":"ref/#darts_segmentation.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-segmentation/src/darts_segmentation/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-segmentation!\"\n</code></pre>"},{"location":"ref/#darts_superresolution","title":"<code>darts_superresolution</code>","text":"<p>Image superresolution of Sentinel 2 imagery for the DARTS dataset.</p>"},{"location":"ref/#darts_superresolution-functions","title":"Functions","text":""},{"location":"ref/#darts_superresolution.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-superresolution/src/darts_superresolution/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-superresolution!\"\n</code></pre>"},{"location":"dev/arch/","title":"Architecture describtion","text":"<p>This repository is a workspace repository, managed by Rye. Read more about workspaces at the Rye docs. Each workspace-member starts with <code>darts-*</code> and can be seen as an own package or module, exexpt the <code>darts-pipeline</code> member. Each package has it's own internal functions and it's public facing API. The public facing API of each package MUST follow the following section API paradigms. The <code>darts-pipeline</code> is a <code>virtual</code> project, hence it can't be installed as a package and has no public facing API.</p> Table of Contents<ul> <li>Architecture describtion<ul> <li>Package overview<ul> <li>Conceptual migration from thaw-slump-segmentation</li> <li>Create a new package</li> </ul> </li> <li>APIs between pipeline steps<ul> <li>Preprocessing Output</li> <li>Segmentation / Ensemble Output</li> <li>Postprocessing Output</li> <li>PyTorch Model checkpoints</li> </ul> </li> <li>API paradigms<ul> <li>Examples</li> <li>About the Xarray overhead with Ray</li> </ul> </li> </ul> </li> </ul>"},{"location":"dev/arch/#package-overview","title":"Package overview","text":"Package Name Type Description (Major) Dependencies - all need Xarray <code>darts-preprocessing</code> Data Loads data and combines the features to a Xarray Dataset GDAL <code>darts-superresolution</code> Train Trains a supper resolution model to scale Sentinel 2 images from 10m to 3m resolution PyTorch <code>darts-segmentation</code> Train Trains an segmentation model PyTorch, segmentation_models_pytorch <code>darts-ensemble</code> Ensemble Ensembles the different models and run the multi-stage inference pipeline. PyTorch <code>darts-postprocessing</code> Data Further refines the output from an ensemble or segmentaion and binarizes the probs PyTorch <code>darts-export</code> Data Saves the results from inference and combines the result to the final DARTS dataset GeoPandas, Scipy, Cucim <p>The following modules are planned or potential ideas for future expansion of the project:</p> Package Name Type Description (Major) Dependencies - all need Xarray <code>darts-acquisition</code> Data Fetches data from the data sources GEE, rasterio, ? <code>darts-detection</code> Train Trains an object detection model PyTorch <code>darts-?</code> Train Trains a ? model for more complex multi-stage ensembles ? <code>darts-evaluation</code> Test Evaluates the end-to-end process on a test dataset and external dataset GeoPandas <code>darts-utils</code> Data Shared utilities for data processing Scipy, Cucim, GeoPandas <code>darts-train-utils</code> Train Shared utilities for training PyTorch <p>The packages should follow this architecture: </p> <p>The <code>darts-pipeline</code> utilizes Ray to automaticly parallize the different computations. However, each package should be designed so that one could build their own pipeline without Ray. Hence, all Ray-related functions / transformations etc. should be defined in the <code>darts-pipeline</code> project.</p> <p>The packages can decide to wrap their public functions into a CLI with typer.</p> <p>The <code>Train</code> packages should also hold the code for training specific data preparation, model training and model evaluation. These packages should get their data from (already processed) data from the <code>darts-preprocessing</code> package. They should expose a statefull Model class with an <code>inference</code> function, which can be used by the <code>darts-ensemble</code> package.</p>"},{"location":"dev/arch/#conceptual-migration-from-thaw-slump-segmentation","title":"Conceptual migration from thaw-slump-segmentation","text":"<ul> <li>The <code>darts-ensemble</code> and <code>darts-postprocessing</code> packages is the successor of the <code>process-02-inference</code> and <code>process-03-ensemble</code> scripts.</li> <li>The <code>darts-preprocessing</code> and <code>darts-acquisition</code> packages are the successors of the <code>setup-raw-data</code> script and manual work of obtaining data.</li> <li>The <code>darts-export</code> package is splitted from the  <code>inference</code> script, should include the previous manual works of combining everything into the final dataset.</li> <li>The <code>darts-superresolution</code> package is the successor of the <code>superresolution</code> repository.</li> <li>The <code>darts-segmentation</code> package is the successor of the <code>train</code> and <code>prepare_data</code> script.</li> <li>The <code>darts-evaluation</code> package is the successor of the different manual evaluations.</li> </ul>"},{"location":"dev/arch/#create-a-new-package","title":"Create a new package","text":"<p>A new package can easily created with:</p> <pre><code>rye init darts-packagename\n</code></pre> <p>Rye creates a minimal project structure for us.</p> <p>The following things needs to be updates:</p> <ol> <li>The <code>pyproject.toml</code> file inside the new package.</li> </ol> <p>Add to the <code>pyproject.toml</code> file inside the new package is the following to enable Ruff:</p> <pre><code>```toml\n[tool.ruff]\n# Extend the `pyproject.toml` file in the parent directory...\nextend = \"../pyproject.toml\"\n```\n\nPlease also provide a description and a list of authors to the file.\n</code></pre> <ol> <li> <p>The <code>.github/workflows/update_version.yml</code> file, to include the package in the workflow.</p> <p>Under <code>package</code> and under step <code>Update version in pyproject.toml</code>.</p> </li> <li> <p>The docs by creating a <code>ref/name.md</code> file and add them to the nav inside the <code>mkdocs.yml</code>.</p> <p>To enable code detection, also add the package directory under <code>plugins</code> in the <code>mkdocs.yml</code>. Please also add the refs to the top-level <code>ref.md</code>.</p> </li> <li> <p>The Readme of the package</p> </li> </ol>"},{"location":"dev/arch/#apis-between-pipeline-steps","title":"APIs between pipeline steps","text":"<p>The following diagram visualizes the steps of the major <code>packages</code> of the pipeline: </p> <p>Each Tile should be represented as a single <code>xr.Dataset</code> with each feature / band as <code>DataVariable</code>. Each DataVariable should have their <code>source</code> documented in the <code>attrs</code>.</p>"},{"location":"dev/arch/#preprocessing-output","title":"Preprocessing Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype attrs <code>blue</code> (x, y) uint16 - source <code>green</code> (x, y) uint16 - source <code>red</code> (x, y) uint16 - source <code>nir</code> (x, y) uint16 - source <code>ndvi</code> (x, y) float32 - source <code>relative_elevation</code> (x, y) float32 - source <code>slope</code> (x, y) float32 - source <code>tc_brightness</code> (x, y) uint8 - source <code>tc_greenness</code> (x, y) uint8 - source <code>tc_wetness</code> (x, y) uint8 - source <code>valid_data_mask</code> (x, y) bool - source <code>quality_data_mask</code> (x, y) bool - source"},{"location":"dev/arch/#segmentation-ensemble-output","title":"Segmentation / Ensemble Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype attrs [Output from Preprocessing] <code>probabilities</code> (x, y) float32 <code>probabilities-model-X*</code> (x, y) bool <p>*: optional intermedia probabilities in an ensemble</p>"},{"location":"dev/arch/#postprocessing-output","title":"Postprocessing Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype attrs note [Output from Preprocessing] <code>probabilities_percent</code> (x, y) uint8 Values between 0-100, nodata:255 <code>binarized_segmentation</code> (x, y) uint8"},{"location":"dev/arch/#pytorch-model-checkpoints","title":"PyTorch Model checkpoints","text":"<p>Each checkpoint is stored as a torch <code>.pt</code> tensor file. The checkpoint MUST have the following structure:</p> <pre><code>{\n    \"config\": {\n        \"model_framework\": \"smp\", # Identifier which framework or model was used\n        \"model\": { ... }, # Model specific hyperparameter which are needed to create the model\n        \"input_combination\": [ ... ], # List of strings of the names with which the model was trained, order is important\n        \"patch_size\": 1024, # Patch size on which the model was trained\n        ... # More model-framework specific parameter, e.g. normalization method and factors\n    },\n    \"statedict\": model.module.state_dict(),\n}\n</code></pre>"},{"location":"dev/arch/#api-paradigms","title":"API paradigms","text":"<p>The packages should pass the data as Xarray Datasets between each other. Datasets can hold coordinate information aswell as other metadata (like CRS) in a single self-describing object. Since different <code>tiles</code> do not share the same coordinates or metadata, each <code>tile</code> should be represented by a single Xarray <code>Dataset</code>.</p> <ul> <li>Each public facing API function which in some way transforms data should accept a Xarray Dataset as input and return an Xarray Dataset.</li> <li>Data can also be accepted as a list of Xarray Dataset as input and returned as a list of Xarray Datasets for batched processing.     In this case, concattenation should happend internally and on <code>numpy</code> or <code>pytorch</code> level, NOT on <code>xarray</code> abstraction level.     The reason behind this it that the tiles don't share their coordinates, resulting in a lot of empty spaces between the tiles and high memory usage.     The name of the function should then be <code>function_batched</code>.</li> <li>Each public facing API function which loads data should return a single Xarray Dataset for each <code>tile</code>.</li> <li>Data should NOT be saved to file internally, with <code>darts-export</code> as the only exception. Instead, data should returned in-memory as a Xarray Dataset, so the user / pipeline can decide what to save and when.</li> <li>Function names should be verbs, e.g. <code>process</code>, <code>ensemble</code>, <code>do_inference</code>.</li> <li>If a function is stateless it should NOT be part of a class or wrapper</li> <li>If a function is stateful it should be part of a class or wrapper, this is important for Ray</li> </ul>"},{"location":"dev/arch/#examples","title":"Examples","text":"<p>Here are some examples, how these API paradigms should look like.</p> <ol> <li> <p>Single transformation</p> <pre><code>import darts-package\nimport xarray as xr\n\n# User loads / creates the dataset (a single tile) by themself\nds = xr.open_dataset(\"...\")\n\n# User calls the function to transform the dataset\nds = darts-package.transform(ds, **kwargs)\n\n# User can decide by themself what to do next, e.g. save\nds.to_netcdf(\"...\")\n</code></pre> </li> <li> <p>Batched transformation</p> <pre><code>import darts_package\nimport xarray as xr\n\n# User loads / creates multiple datasets (hence, multiple tiles) by themself\ndata = [xr.open_dataset(\"...\"), xr.open_dataset(\"...\"), ...]\n\n# User calls the function to transform the dataset\ndata = darts_package.transform_batched(data, **kwargs)\n\n# User can decide by themself what to do next\ndata[0].whatever()\n</code></pre> </li> <li> <p>Load &amp; preprocess some data</p> <pre><code>import darts_package\n\n# User calls the function to transform the dataset\nds = darts_package.load(\"path/to/data\", **kwargs)\n\n# User can decide by themself what to do next\nds.whatever()\n</code></pre> </li> <li> <p>Custom pipeline example</p> <pre><code>from pathlib import Path\nimport darts_preprocess\nimport darts_inference\n\nDATA_DIR = Path(\"./data/\")\nMODEL_DIR = Path(\"./models/\")\nOUT_DIR = Path(\"./out/\")\n\n# Inference is a stateful transformation, because it needs to load the model\n# Hence, the \nensemble = darts_inference.Ensemble.load(MODEL_DIR)\n\n# The data directory contains subfolders which then hold the input data\nfor dir in DATA_DIR:\n    name = dir.name\n\n    # Load the files from the processing directory\n    ds = darts_preprocess.load_and_preprocess(dir)\n\n    # Do the inferencce\n    ds = ensemble.inference(ds)\n\n    # Save the results\n    ds.to_netcdf(OUT_DIR / f\"{name}-result.nc\")\n</code></pre> </li> <li> <p>Pipeline with Ray</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\nimport ray\nimport darts_preprocess\nimport darts_inference\nimport darts_export\n\nDATA_DIR = Path(\"./data/\")\nMODEL_DIR = Path(\"./models/\")\nOUT_DIR = Path(\"./out/\")\n\nray.init()\n\n# We need to wrap the Xarray dataset in a class, so that Ray can serialize it\n@dataclass\nclass Tile:\n    ds: xr.Dataset\n\n# Wrapper for ray\ndef open_dataset_ray(row: dict[str, Any]) -&gt; dict[str, Any]:\n    data = xr.open_dataset(row[\"path\"])\n    tile = Tile(data)\n    return {\n        \"input\": tile,\n    }\n\n# Wrapper for the preprocessing -&gt; Stateless\ndef preprocess_tile_ray(row: dict[str, Tile]) -&gt; dict[str, Tile]:\n    ds = darts_preprocess.preprocess(row[\"input\"].ds)\n    return {\n        \"preprocessed\": Tile(ds),\n        \"input\": row[\"input\"]\n    }\n\n# Wrapper for the inference -&gt; Statefull\nclass EnsembleRay:\n    def __init__(self):\n        self.ensemble = darts_inference.Ensemble.load(MODEL_DIR)\n\n    def __call__(self, row: dict[str, Tile]) -&gt; dict[str, Tile]:\n        ds = self.ensemble.inference(row[\"preprocessed\"].ds)\n        return {\n            \"output\": Tile(ds),\n            \"preprocessed\": row[\"preprocessed\"],\n            \"input\": row[\"input\"],\n        }\n\n# We need to add 'local:///' to tell ray that we want to use the local filesystem\nfiles = data.glob(\"*.nc\")\nfile_list = [f\"local:////{file.resolve().absolute()}\" for file in files]\n\nds = ray.data.read_binary_files(file_list, include_paths=True)\nds = ds.map(open_dataset_ray) # Lazy open\nds = ds.map(preprocess_tile_ray) # Lazy preprocess\nds = ds.map(EnsembleRay) # Lazy inference\n\n# Save the results\nfor row in ds.iter_rows():\n    darts_export.save(row[\"output\"].ds, OUT_DIR / f\"{row['input'].ds.name}-result.nc\")\n</code></pre> </li> </ol>"},{"location":"dev/arch/#about-the-xarray-overhead-with-ray","title":"About the Xarray overhead with Ray","text":"<p>Ray expects batched data to be in either numpy or pandas format and can't work with Xarray datasets directly. Hence, a wrapper with custom stacking functions is needed. This tradeoff is not small, however, the benefits in terms of maintainability and readability are worth it.</p> <p></p>"},{"location":"dev/logging/","title":"Logging","text":"<p>We want to use the python logging module as much as possible to traceback errors and document the pipeline processes. Furthermore, we want to configure each logger with the <code>RichHandler</code>, which prettyfies the output with rich.</p>"},{"location":"dev/logging/#usage-guide","title":"Usage Guide","text":"<p>For logging inside a package should be done without any further configuration:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__) # don't replace __name__\n</code></pre> <p>Logging at a top-level can and should be further configured:</p> <p>Code is untested!</p> <pre><code>import logging\n\nfrom rich.logging import RichHandler\n\nconsole_handler = RichHandler(rich_tracebacks=True)\nconsole_handler.setLevel(logging.INFO)\nconsole_handler.setFormatter(logging.Formatter(\"%(message)s\", datefmt=\"[%X]\"))\n\nfile_handler = logging.FileHandler(\"app.log\")\nfile_handler.setLevel(logging.DEBUG)\nfile_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n\nlogging.basicConfig(handlers=[console_handler, file_handler])\n</code></pre>"},{"location":"dev/logging/#supressing-arrays","title":"Supressing Arrays","text":"<p>When printing or logging large numpy arrays a lot of numbers get truncated, however the array still takes a lot of space. Using <code>lovely_numpy</code> and <code>lovely_tensor</code> can help here:</p> <pre><code>import numyp as np\nimport torch\nimport xarray as xr\nfrom lovely_numpy import lo\nfrom lovely_tensors import monkey_patch\n\nmonkey_patch()\nxr.set_options(display_expand_data=False)\n\na = np.zeros((8, 1024, 1024))\nla = lo(a)\nda = xr.DataArray(a)\nt = torch.tensor(a)\n\nlogger.warning(la)\nlogger.warning(da)\nlogger.warning(t)\n</code></pre>"},{"location":"ref/acquisition/","title":"Acquisition Reference","text":""},{"location":"ref/acquisition/#darts_acquisition","title":"<code>darts_acquisition</code>","text":"<p>Acquisition of data from various sources for the DARTS dataset.</p>"},{"location":"ref/acquisition/#darts_acquisition-functions","title":"Functions","text":""},{"location":"ref/acquisition/#darts_acquisition.hello","title":"<code>hello(name)</code>","text":"<p>Say hello to the user.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the user.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-acquisition/src/darts_acquisition/__init__.py</code> <pre><code>def hello(name: str) -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Args:\n        name (str): Name of the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return f\"Hello, {name}, from darts-acquisition!\"\n</code></pre>"},{"location":"ref/ensemble/","title":"Ensemble Reference","text":""},{"location":"ref/ensemble/#darts_ensemble","title":"<code>darts_ensemble</code>","text":"<p>Inference and model ensembling for the DARTS dataset.</p>"},{"location":"ref/ensemble/#darts_ensemble-functions","title":"Functions","text":""},{"location":"ref/ensemble/#darts_ensemble.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-ensemble/src/darts_ensemble/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-ensemble!\"\n</code></pre>"},{"location":"ref/export/","title":"Export Reference","text":""},{"location":"ref/export/#darts_export","title":"<code>darts_export</code>","text":""},{"location":"ref/preprocessing/","title":"Preprocessing Reference","text":""},{"location":"ref/preprocessing/#darts_preprocessing","title":"<code>darts_preprocessing</code>","text":"<p>Data preprocessing and feature engineering for the DARTS dataset.</p>"},{"location":"ref/preprocessing/#darts_preprocessing-functions","title":"Functions","text":""},{"location":"ref/preprocessing/#darts_preprocessing.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-preprocessing!\"\n</code></pre>"},{"location":"ref/segmentation/","title":"Export Reference","text":""},{"location":"ref/segmentation/#darts_export","title":"<code>darts_export</code>","text":""},{"location":"ref/superresolution/","title":"Superresolution Reference","text":""},{"location":"ref/superresolution/#darts_superresolution","title":"<code>darts_superresolution</code>","text":"<p>Image superresolution of Sentinel 2 imagery for the DARTS dataset.</p>"},{"location":"ref/superresolution/#darts_superresolution-functions","title":"Functions","text":""},{"location":"ref/superresolution/#darts_superresolution.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-superresolution/src/darts_superresolution/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-superresolution!\"\n</code></pre>"}]}