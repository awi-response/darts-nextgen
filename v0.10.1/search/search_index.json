{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#darts-nextgen","title":"DARTS nextgen","text":"<p>Panarctic Database of Active Layer Detachment Slides and Retrogressive Thaw Slumps from Deep Learning on High Resolution Satellite Imagery. This is te successor of the thaw-slump-segmentation (pipeline), with which the first version of the DARTS dataset was created.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li> <p>Download source code from the GitHub repository:</p> <pre><code>git clone git@github.com:awi-response/darts-nextgen.git\ncd darts-nextgen\n</code></pre> </li> <li> <p>Install the required dependencies:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> </li> <li> <p>Run the Sentinel 2 based pipeline on an area of interest:</p> <pre><code>uv run darts inference sentinel2-sequential \\\n  --aoi-shapefile path/to/your/aoi.geojson \\\n  --model-files path/to/your/model/checkpoint \\\n  --start-date 2024-07 \\\n  --end-date 2024-09\n</code></pre> </li> </ol> <ul> <li> <p> Overview</p> <p>Get an overview on how this project works and how to run different pipelines.</p> <p> Get Started</p> </li> <li> <p> Install</p> <p>View detailed instructions on how to install the project for different environments and setup, e.g. with CUDA or conda.</p> <p> Install</p> </li> <li> <p> Pipeline Components</p> <p>Learn about the different components of the pipeline and how they work together.</p> <p> Components</p> </li> <li> <p> API Reference</p> <p>View the API reference of the components.</p> <p> Reference</p> </li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>Before contributing please contact one of the authors and make sure to read the Contribution Guidelines.</p>"},{"location":"contribute/","title":"Contribute","text":""},{"location":"contribute/#contribute","title":"Contribute","text":"<p>This page is also meant for internal documentation.</p>"},{"location":"contribute/#editor-setup","title":"Editor setup","text":"<p>There is only setup files provided for VSCode and no other editor (yet). A list of extensions and some settings can be found in the <code>.vscode</code>. At the first start, VSCode should ask you if you want to install the recommended extension. The settings should be automaticly used by VSCode. Both should provide the developers with a better experience and enforce code-style.</p>"},{"location":"contribute/#environment-setup","title":"Environment setup","text":"<p>Please read and follow the installation guide to setup the environment.</p>"},{"location":"contribute/#writing-docs","title":"Writing docs","text":"<p>The documentation is managed with Material for MkDocs. The documentation related dependencies are separated from the main dependencies and can be installed with:</p> <pre><code>uv sync --group docs\n</code></pre> <p>Note</p> <p>You should combine the <code>--group docs</code> with the extras you previously used, e.g. <code>uv sync --extra training --extra cuda126 --group docs</code>.</p> <p>To start the documentation server for live-update, run:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>In general all mkdocs commands can be run with <code>uv run mkdocs ...</code>.</p>"},{"location":"contribute/#recommended-notebook-header","title":"Recommended Notebook header","text":"<p>The following code snipped can be put in the very first cell of a notebook to already to add logging and initialize earth engine.</p> <pre><code>import logging\n\nfrom rich.logging import RichHandler\nfrom rich import traceback\n\nfrom darts.utils.earthengine import init_ee\nfrom darts.utils.logging import LoggingManager\n\nLoggingManager.setup_logging()\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\ntraceback.install(show_locals=True)  # Change to False if you encounter too large tracebacks\ninit_ee(\"ee-project\")  # Replace with your project\n</code></pre>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#overview","title":"Overview","text":"<p>This is a guide to help you, as a user / data engineer, get started with the project.</p> Table of Contents<ul> <li>Overview<ul> <li>Installation</li> <li>Running stuff via the CLI<ul> <li>Device selection</li> <li>Config files</li> <li>Log files</li> </ul> </li> <li>Running a pipeline based on Sentinel 2 data</li> <li>Running a pipeline based on PLANET data<ul> <li>Create a config file</li> <li>Run a the pipeline</li> </ul> </li> <li>Creating your own pipeline</li> </ul> </li> </ul>"},{"location":"overview/#installation","title":"Installation","text":"<p>To setup the environment for the project, you need to install uv and run the following command, assuming CUDA 12.6 is installed:</p> <pre><code>uv sync --extra cuda126\n</code></pre> <p>For other CUDA versions, see the installation guide.</p> <p>Training specific dependencies are optional and therefore not installed by default. To install them, add <code>--extra training</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> <p>To see if the installation was successful, you can run the following command:</p> <pre><code>uv run darts env-info\n</code></pre>"},{"location":"overview/#running-stuff-via-the-cli","title":"Running stuff via the CLI","text":"<p>The project provides a CLI to run different pipelines, training and other utility functions. Because the environment is setup with <code>uv</code>, you can run the CLI commands with <code>uv run darts ...</code>. If you manually active the environment with <code>source .venv/bin/activate</code>, you can run the CLI commands just via <code>darts ...</code> without <code>uv run</code>.</p> <p>To see a list of all available commands, run:</p> <pre><code>uv run darts --help\n</code></pre> <p>To get help for a specific command, run:</p> <pre><code>uv run darts the-specific-command --help\n</code></pre>"},{"location":"overview/#device-selection","title":"Device selection","text":"<p>By default, the CLI will automatic select the device for functions that support it. To force a specific device, you can use the <code>--device</code> parameter. Read more about the device selection in the device guide.</p>"},{"location":"overview/#config-files","title":"Config files","text":"<p>The CLI supports config files in TOML format to reduce the amount of parameters you need to pass or to safe different configurations. By default the CLI tries to load a <code>config.toml</code> file from the current directory. However, you can specify a different file with the <code>--config-file</code> parameter.</p> <p>As of right now, the CLI tries to match all parameters under the <code>darts</code> key of the config file, skipping not needed ones. For more information about the config file,  see the config guide..</p>"},{"location":"overview/#log-files","title":"Log files","text":"<p>By default the CLI sets up a logging handler at <code>INFO</code> level for the <code>darts</code> specific packages found in this workspace. The log-level can be changed via the <code>--verbose</code> flag of the CLI to set it to <code>DEBUG</code>. Running any command will output a logging file at the logging directory, which can be specified via the <code>--log-dir</code> parameter. The logging file will be named after the command and the current timestamp. If you want to change the logging behavior in python code, you can check out the logging guide.</p>"},{"location":"overview/#running-a-pipeline-based-on-sentinel-2-data","title":"Running a pipeline based on Sentinel 2 data","text":"<p>The <code>run-sequential-aoi-sentinel2-pipeline</code> automatically downloads and processes Sentinel 2 data based on an Area of Interest (AOI) in GeoJSON format. Before running you need access to a trained model. Note, that only special checkpoints can be used, as described in the architecture guide. In future versions, downloading of the model via huggingface will be supported, but for now you need to ask the developers for a valid model checkpoint.</p> <p>To run the pipeline run:</p> <pre><code>uv run darts inference sentinel2-sequential --aoi-shapefile path/to/your/aoi.geojson --model-files path/to/your/model/checkpoint --start-date 2024-07 --end-date 2024-09\n</code></pre> <p>Run <code>uv run darts inference sentinel2-sequential --help</code> for more configuration options.</p> <p>Pipeline v2</p> <p>The Pipeline v2 Guide provides a more in-depth explanation of the pipeline and its components.</p>"},{"location":"overview/#running-a-pipeline-based-on-planet-data","title":"Running a pipeline based on PLANET data","text":"<p>PLANET data cannot be downloaded automatically. Hence, you need to download the data manually and place it a directory of you choice.</p> <p>Example directory structure of a PLANET Orthotile:</p> <pre><code>    data/input/planet/PSOrthoTile/\n    \u251c\u2500\u2500 4372514/\n    \u2502  \u2514\u2500\u2500 5790392_4372514_2022-07-16_2459/\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_Analytic_metadata.xml\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_DN_udm.tif\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_SR.tif\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_metadata.json\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_udm2.tif\n    \u2502      \u2514\u2500\u2500 Thumbs.db\n    \u2514\u2500\u2500 4974017/\n        \u2514\u2500\u2500 5854937_4974017_2022-08-14_2475/\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_Analytic_metadata.xml\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_DN_udm.tif\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_SR.tif\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_metadata.json\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_udm2.tif\n            \u2514\u2500\u2500 Thumbs.db\n</code></pre> <p>Example directory structure of a PLANET Scene:</p> <pre><code>    data/input/planet/PSScene/\n    \u251c\u2500\u2500 20230703_194241_43_2427/\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427.json\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_metadata.xml\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_SR.tif\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_udm2.tif\n    \u2502  \u2514\u2500\u2500 20230703_194241_43_2427_metadata.json\n    \u2514\u2500\u2500 20230703_194243_54_2427/\n       \u251c\u2500\u2500 20230703_194243_54_2427.json\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_AnalyticMS_metadata.xml\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_AnalyticMS_SR.tif\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_udm2.tif\n       \u2514\u2500\u2500 20230703_194243_54_2427_metadata.json\n</code></pre> <p>Backcompatability of Sentinel 2 data</p> <p>For historical reasons, it is possible to run similar pipelines with Sentinel 2 data. For this, the Sentinel 2 data is expected to be in the same directory structure as the PLANET data. Hence, data from Google EarthEngine or from the Copernicus Cloud needs to be adjusted and scaled by the factor of <code>0.0001</code>.</p> <pre><code>data/input/sentinel2/\n\u251c\u2500\u2500 20210818T223529_20210818T223531_T03WXP/\n\u2502  \u251c\u2500\u2500 20210818T223529_20210818T223531_T03WXP_SCL_clip.tif\n\u2502  \u2514\u2500\u2500 20210818T223529_20210818T223531_T03WXP_SR_clip.tif\n\u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ/\n\u251c\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SCL_clip.tif\n\u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SR_clip.tif\n</code></pre>"},{"location":"overview/#create-a-config-file","title":"Create a config file","text":"<p>Because the minimal amount of parameters to pass for the PLANET pipeline, it is recommended to use a config file.</p> <p>An example config file can be found in the root of this repository called <code>config.toml.example</code>. You can copy this file to either <code>configs/</code> or copy and rename it to <code>config.toml</code>, so that you personal config will be ignored by git.</p> <p>Please change  <code>orthotiles-dir</code> and <code>scenes-dir</code> according to your PLANET download directory.</p> <p>You also need to specify the paths the model checkpoints (<code>model-dir</code>, <code>tcvis-model-name</code> and <code>notcvis-model-name</code>) you want to use. Note, that only special checkpoints can be used, as described in the architecture guide By setting <code>notcvis-model-name</code> to <code>None</code>, the pipeline will only use the TCVIS model.</p> <p>Auxiliary data (TCVIS and ArcticDEM) will be downloaded on demand into a datacube, which paths needs to be specified as well (<code>arcticdem-dir</code> and <code>tcvis-dir</code>).</p> <p>Finally, specify an output directory (<code>output-dir</code>), where you want to save the results of the pipeline.</p> <p>Of course you can tweak all other options aswell, also via the CLI. A list of all options can be found in the config guide or by running a command with the <code>--help</code> parameter.</p> Example config file <p>This is how an example config file could look like for the automated Sentinel 2 pipeline:</p> config.toml<pre><code>[darts]\nee-project = \"ee-tobias-hoelzer\"\n\n[darts.aoi]\naoi-shapefile = \"./data/banks_island.shp\"\nstart-date = \"2024-07\"\nend-date = \"2024-10\"\nmax-cloud-cover = 1 # %\n\n[darts.paths]\ninput-cache = \"./data/cache/s2gee\"\noutput-data-dir = \"./data/out\"\narcticdem-dir = \"./data/download/arcticdem\"\ntcvis-dir = \"./data/download/tcvis\"\nmodel-file = \"./models/s2-tcvis-final-large_2025-02-12.ckpt\"\n\n[darts.tiling]\nbatch-size = 8  # Reduce incase of memory issues\npatch-size = 512  # Reduce incase of memory issues\noverlap = 128  # Recommended to be 1/4 of patch-size\n</code></pre>"},{"location":"overview/#run-a-the-pipeline","title":"Run a the pipeline","text":"<p>Finally run the pipeline with the following command. Additional parameters can be passed via the CLI, which will overwrite the config file.</p> <pre><code>uv run darts inference planet-sequential --config-file path/to/your/config.toml\n</code></pre>"},{"location":"overview/#creating-your-own-pipeline","title":"Creating your own pipeline","text":"<p>The project was build with the idea in mind, that it is easy to create a new pipeline, with e.g. different parallelisation techniques. The architecture guide provides an overview of the project structure and the key components. A good starting point to understand the components is the intro to components. The build-in pipelines are a good example how the components can be used and put together to create a new pipeline.</p>"},{"location":"dev/arch/","title":"Architecture","text":""},{"location":"dev/arch/#architecture-describtion","title":"Architecture describtion","text":"<p>This repository is a workspace repository, managed by uv. Read more about workspaces at the uv docs. Each workspace-member starts with <code>darts-*</code> and can be seen as an own package or module, except the <code>darts</code> directory which is the top-level package. Each package has it's own internal functions and it's public facing API. The public facing API of each package MUST follow the API paradigms.</p> Table of Contents<ul> <li>Architecture describtion<ul> <li>Package overview<ul> <li>Conceptual migration from thaw-slump-segmentation</li> <li>Create a new package</li> <li>Versioning</li> </ul> </li> <li>PyTorch Model checkpoints</li> <li>API paradigms<ul> <li>Examples</li> <li>About the Xarray overhead with Ray</li> </ul> </li> </ul> </li> </ul>"},{"location":"dev/arch/#package-overview","title":"Package overview","text":"<p>Main design priciple</p> <p>Each package should provide components - stateless functions or stateful classes - which should be then combined either by the top-level <code>darts</code> package or by the user. Each component should take a Xarray Dataset as input and return a Xarray Dataset as output, with the exception of components of the <code>darts-aquisition</code> and <code>darts-export</code> packages. This way it should be easy to combine different components to build a custom pipeline for different parallelization frameworks and workflows.</p> Package Name Type Description (Major) Dependencies - all need Xarray <code>darts-acquisition</code> Data Fetches data from the data sources and created Xarray Datasets GEE, rasterio, ODC-Geo <code>darts-preprocessing</code> Data Combines Xarray Datasets from different acquisition sources and do some preprocessing on the data Cupy, Xarray-Spatial <code>darts-superresolution</code> Train Run a super-resolution model to scale Sentinel 2 images from 10m to 3m resolution PyTorch <code>darts-segmentation</code> Train Run the segmentation model PyTorch, segmentation_models_pytorch <code>darts-ensemble</code> Ensemble Ensembles the different models and run the multi-stage inference pipeline. PyTorch <code>darts-postprocessing</code> Data Further refines the output from an ensemble or segmentaion and binarizes the probs Scipy, Cucim <code>darts-export</code> Data Saves the results from inference and combines the result to the final DARTS dataset GeoPandas, rasterio <code>darts-utils</code> Data Shared utilities for data processing <p>The packages are currently designed around the v2 Pipeline.</p>"},{"location":"dev/arch/#conceptual-migration-from-thaw-slump-segmentation","title":"Conceptual migration from thaw-slump-segmentation","text":"<ul> <li>The <code>darts-ensemble</code> and <code>darts-postprocessing</code> packages is the successor of the <code>process-02-inference</code> and <code>process-03-ensemble</code> scripts.</li> <li>The <code>darts-preprocessing</code> and <code>darts-acquisition</code> packages are the successors of the <code>setup-raw-data</code> script and manual work of obtaining data.</li> <li>The <code>darts-export</code> package is splitted from the  <code>inference</code> script, should include the previous manual works of combining everything into the final dataset.</li> <li>The <code>darts-superresolution</code> package is the successor of the <code>superresolution</code> repository.</li> <li>The <code>darts-segmentation</code> package is the successor of the <code>train</code> and <code>prepare_data</code> script.</li> </ul> <p>The following diagram visualizes how the new packages are meant to work together. </p> <p>This is a mock</p> <p>This diagram is not realised in any form. It just exists for demonstrational purposes. To see an example of a realized pipeline based on this architecture please see the Pipeline v2 Guide</p>"},{"location":"dev/arch/#create-a-new-package","title":"Create a new package","text":"<p>A new package can easily created with:</p> <pre><code>uv init darts-packagename\n</code></pre> <p>uv creates a minimal project structure for us.</p> <p>The following things needs to be done updated and created:</p> <ol> <li>The <code>pyproject.toml</code> file inside the new package:</li> </ol> <p>Add to the <code>pyproject.toml</code> file inside the new package is the following to enable Ruff:</p> <pre><code>```toml\n[tool.ruff]\n# Extend the `pyproject.toml` file in the parent directory...\nextend = \"../pyproject.toml\"\n```\n\nPlease also provide a description and a list of authors to the file.\n</code></pre> <ol> <li> <p>The docs:     By updating the <code>notebooks/create_api_docs.ipynb</code>, running it and updating the <code>nav</code> section of the <code>mkdocs.yml</code> with the generated text.     To enable code detection, also add the package directory under <code>plugins</code> in the <code>mkdocs.yml</code>.</p> </li> <li> <p>The Readme of the package</p> </li> </ol>"},{"location":"dev/arch/#versioning","title":"Versioning","text":"<p>All packages have at all time the same version. The versioning is done via git-tags and the uv dynamic versioning tool. Hence, the version of the <code>pyproject.toml</code> of each subpackage is ignored and has no meaning.</p>"},{"location":"dev/arch/#pytorch-model-checkpoints","title":"PyTorch Model checkpoints","text":"<p>Each checkpoint is stored as a torch <code>.pt</code> tensor file. The checkpoint MUST have the following structure:</p> <pre><code>{\n    \"config\": {\n        \"model_framework\": \"smp\", # Identifier which framework or model was used\n        \"model\": { ... }, # Model specific hyperparameter which are needed to create the model\n        \"input_combination\": [ ... ], # List of strings of the names with which the model was trained, order is important\n        \"patch_size\": 1024, # Patch size on which the model was trained\n        ... # More model-framework specific parameter, e.g. normalization method and factors\n    },\n    \"statedict\": model.module.state_dict(),\n}\n</code></pre> <p>Pre-Deprecation warning</p> <p>It is planned to switch from our custom structure to huggingface model accessors.</p>"},{"location":"dev/arch/#api-paradigms","title":"API paradigms","text":"<p>The packages should pass the data as Xarray Datasets between each other. Datasets can hold coordinate information aswell as other metadata (like CRS) in a single self-describing object. Since different <code>tiles</code> do not share the same coordinates or metadata, each <code>tile</code> should be represented by a single Xarray <code>Dataset</code>.</p> <ul> <li>Each public facing API function which in some way transforms data should accept a Xarray Dataset as input and return an Xarray Dataset.</li> <li>Data can also be accepted as a list of Xarray Dataset as input and returned as a list of Xarray Datasets for batched processing.     In this case, concattenation should happend internally and on <code>numpy</code> or <code>pytorch</code> level, NOT on <code>xarray</code> abstraction level.     The reason behind this it that the tiles don't share their coordinates, resulting in a lot of empty spaces between the tiles and high memory usage.     The name of the function should then be <code>function_batched</code>.</li> <li>Each public facing API function which loads data should return a single Xarray Dataset for each <code>tile</code>.</li> <li>Data should NOT be saved to file internally, with <code>darts-export</code> as the only exception. Instead, data should returned in-memory as a Xarray Dataset, so the user / pipeline can decide what to save and when.</li> <li>Function names should be verbs, e.g. <code>process</code>, <code>ensemble</code>, <code>do_inference</code>.</li> <li>If a function is stateless it should NOT be part of a class or wrapper</li> <li>If a function is stateful it should be part of a class or wrapper, this is important for Ray</li> <li>Each Tile should be represented as a single <code>xr.Dataset</code> with each feature / band as <code>DataVariable</code>.</li> <li>Each DataVariable should have their <code>data_source</code> documented in the <code>attrs</code>, aswell as <code>long_name</code> and <code>units</code> if any for plotting.</li> <li>A <code>_FillValue</code> should also be set for no-data with <code>.rio.write_nodata(\"no-data-value\")</code>.</li> </ul> <p>Components</p> <p>The goal of these paradigms is to write functions which work as Components. Potential users can then later pick their components and put them together in their custom pipeline, utilizing their own parallelization framework.</p>"},{"location":"dev/arch/#examples","title":"Examples","text":"<p>Here are some examples, how these API paradigms should look like.</p> <p>This is a mock</p> <p>Even if some real packages are shown, these examples use mock-functions / non-existing functions and will not work .</p> <ol> <li> <p>Single transformation</p> <pre><code>import darts-package\nimport xarray as xr\n\n# User loads / creates the dataset (a single tile) by themself\nds = xr.open_dataset(\"...\")\n\n# User calls the function to transform the dataset\nds = darts-package.transform(ds, **kwargs)\n\n# User can decide by themself what to do next, e.g. save\nds.to_netcdf(\"...\")\n</code></pre> </li> <li> <p>Batched transformation</p> <pre><code>import darts_package\nimport xarray as xr\n\n# User loads / creates multiple datasets (hence, multiple tiles) by themself\ndata = [xr.open_dataset(\"...\"), xr.open_dataset(\"...\"), ...]\n\n# User calls the function to transform the dataset\ndata = darts_package.transform_batched(data, **kwargs)\n\n# User can decide by themself what to do next\ndata[0].whatever()\n</code></pre> </li> <li> <p>Load &amp; preprocess some data</p> <pre><code>import darts_package\n\n# User calls the function to transform the dataset\nds = darts_package.load(\"path/to/data\", **kwargs)\n\n# User can decide by themself what to do next\nds.whatever()\n</code></pre> </li> <li> <p>Custom pipeline example</p> <pre><code>from pathlib import Path\nimport darts_preprocessing\nimport darts_ensemble\n\nDATA_DIR = Path(\"./data/\")\nMODEL_FILE = Path(\"./models/model.pt\")\nOUT_DIR = Path(\"./out/\")\n\n# Inference is a stateful transformation, because it needs to load the model\n# Hence, the \nensemble = darts_ensemble.EnsembleV1(MODEL_FILE)\n\n# The data directory contains subfolders which then hold the input data\nfor dir in DATA_DIR:\n    name = dir.name\n\n    # Load the files from the processing directory\n    ds = darts_preprocessing.load_and_preprocess(dir)\n\n    # Do the inferencce\n    ds = ensemble.inference(ds)\n\n    # Save the results\n    ds.to_netcdf(OUT_DIR / f\"{name}-result.nc\")\n</code></pre> </li> <li> <p>Pipeline with Ray</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\nimport ray\nimport darts_preprocess\nimport darts_inference\nimport darts_export\n\nDATA_DIR = Path(\"./data/\")\nMODEL_DIR = Path(\"./models/\")\nOUT_DIR = Path(\"./out/\")\n\nray.init()\n\n# We need to wrap the Xarray dataset in a class, so that Ray can serialize it\n@dataclass\nclass Tile:\n    ds: xr.Dataset\n\n# Wrapper for ray\ndef open_dataset_ray(row: dict[str, Any]) -&gt; dict[str, Any]:\n    data = xr.open_dataset(row[\"path\"])\n    tile = Tile(data)\n    return {\n        \"input\": tile,\n    }\n\n# Wrapper for the preprocessing -&gt; Stateless\ndef preprocess_tile_ray(row: dict[str, Tile]) -&gt; dict[str, Tile]:\n    ds = darts_preprocess.preprocess(row[\"input\"].ds)\n    return {\n        \"preprocessed\": Tile(ds),\n        \"input\": row[\"input\"]\n    }\n\n# Wrapper for the inference -&gt; Statefull\nclass EnsembleRay:\n    def __init__(self):\n        self.ensemble = darts_inference.Ensemble.load(MODEL_DIR)\n\n    def __call__(self, row: dict[str, Tile]) -&gt; dict[str, Tile]:\n        ds = self.ensemble.inference(row[\"preprocessed\"].ds)\n        return {\n            \"output\": Tile(ds),\n            \"preprocessed\": row[\"preprocessed\"],\n            \"input\": row[\"input\"],\n        }\n\n# We need to add 'local:///' to tell ray that we want to use the local filesystem\nfiles = data.glob(\"*.nc\")\nfile_list = [f\"local:////{file.resolve().absolute()}\" for file in files]\n\nds = ray.data.read_binary_files(file_list, include_paths=True)\nds = ds.map(open_dataset_ray) # Lazy open\nds = ds.map(preprocess_tile_ray) # Lazy preprocess\nds = ds.map(EnsembleRay) # Lazy inference\n\n# Save the results\nfor row in ds.iter_rows():\n    darts_export.save(row[\"output\"].ds, OUT_DIR / f\"{row['input'].ds.name}-result.nc\")\n</code></pre> </li> </ol>"},{"location":"dev/arch/#about-the-xarray-overhead-with-ray","title":"About the Xarray overhead with Ray","text":"<p>Ray expects batched data to be in either numpy or pandas format and can't work with Xarray datasets directly. Hence, a wrapper with custom stacking functions is needed. This tradeoff is not small, however, the benefits in terms of maintainability and readability are worth it.</p> <p></p>"},{"location":"dev/auxiliary/","title":"Datacubes","text":""},{"location":"dev/auxiliary/#auxiliary-data-and-datacubes","title":"Auxiliary Data and Datacubes","text":"<p>DARTS uses several auxiliary data - data which does not change between different scenes and / or time steps. Raster auxiliary data is stored in Zarr Datacubes.</p> <p>Currently, the following auxiliary data is used:</p> <ul> <li>ArcticDEM</li> <li>Tasseled Cap indices (Brightness, Greenness, Wetness)</li> </ul> <p>with more to come.</p>"},{"location":"dev/auxiliary/#arcticdem","title":"ArcticDEM","text":"<p>The ArcticDEM is downloaded via their STAC server using these extend files.</p> <p>The user can specify the download directory, where the ArcticDEM will be procedurally stored in a Zarr Datacube. The user can also specify the resolution of the ArcticDEM, which is either 2m, 10m or 32m. Each resolution is stored in their own Zarr Datacube.</p>"},{"location":"dev/auxiliary/#tasseled-cap-indices-tcvis","title":"Tasseled Cap indices (TCVIS)","text":"<p>The TCVIS data is downloaded from Google Earth-Engine (GEE) using the TCVIS collection from Ingmar Nitze: <code>\"users/ingmarnitze/TCTrend_SR_2000-2019_TCVIS\"</code>.</p>"},{"location":"dev/auxiliary/#why-zarr-datacubes","title":"Why Zarr Datacubes?","text":"<p>Zarr is a file format for storing chunked, compressed, N-dimensional arrays. It is designed to store large arrays of data, and to facilitate fast and efficient IO. Zarr works well integrated with Dask and Xarray.</p> <p>By storing the auxiliary data in Zarr Datacubes, it is much easier and faster to access the data of interest. If we would use GeoTiffs, we would have to first create a Cloud-Optimized GeoTiff (COG), which is basically an ensemble (mosaic) of multiple GeoTiffs. Then we would have to read from the COG, which behind the scenes would open multiple GeoTiffs and crops them to fit the region of interest. E.g. Opening a specific region of interest 10km x 10km from a 2m resolution COG would take up to 2 minutes, if the COGs extend is panarctic. Opening the same region from a Zarr Datacube takes less than 1 second.</p> <p>Inspiration</p> <p>This implementation and concept is heavily inspired by EarthMovers implementation of serverless datacube generation.</p>"},{"location":"dev/auxiliary/#procedural-download","title":"Procedural download","text":"<p>Info</p> <p>The currently used auxiliary data is downloaded on demand, only data actually used is downloaded and stored on your local machine. Hence, the stored datacubes can be thought of as a cache, which is filled with data as needed.</p> <p>There are currently two implementations of the procedural download used: a cloud based STAC download and a download via Google Earth-Engine.</p> <p>Because the single tiles of the STAC mosaic can be overlapping and intersect with multiple Zarr chunks, the STAC download is slightly more complicated. Since Google Earth-Engine allows for exact geoboxes, download of the exact chunks is possible. This reduces the complexity of the download.</p> STAC GEE 1. ROI 2. ROI <p>The above graphics shows the difference between loading data from STAC (left) and Google Earth-Engine (right). With the STAC download, the data is downloaded from a mosaic of tiles, which can be overlapping with each other and cover multiple Zarr chunks. It may occur that a chunk is not fully covered by the STAC mosaic, which results in only partial loaded chunks. In such cases, the missing data in these chunks will be updated if the other intersecting tile is downloaded, which may occur to a later time if a connected ROI is requested. The download process is much easier for GEE, since one can request the exact geoboxes of the Zarr chunks and GEE will handle the rest. Hence, chunks will always be fully covered by the downloaded data.</p> <p>Regarding the open ROI process, both implementations follow the same principle:</p> <ol> <li>Check which Tiles / Chunks intersect with the region of interest</li> <li>Dowload all new Tiles / Chunks</li> <li>Store the new Tiles / Chunks in their specific Zarr chunks</li> <li>Return the region of interest of the Zarr Datacube</li> </ol>"},{"location":"dev/auxiliary/#stac-download","title":"STAC download","text":""},{"location":"dev/auxiliary/#google-earth-engine-download","title":"Google Earth-Engine download","text":""},{"location":"dev/bands/","title":"Bands","text":""},{"location":"dev/bands/#band-modalities-and-normalisation","title":"Band / Modalities and Normalisation","text":"<p>In the training dataset preparation, all bands available from the preprocessing will be included into the train dataset. They are normalised and clipped to the range [0, 1], by hard-coded normalisation factors and offsets. When training, a subset of available bands can be selected, on which the model should be trained. This enables the possibility to quickly test different band combinations and their influence on the model performance without the need to preprocess the data again.</p> <p>The information about which bands used for training is the written into the model checkpoint. This information is then used to select the bands for inference.</p>"},{"location":"dev/bands/#representations-states","title":"Representations states","text":"<p>Split up the data representation into three different representations:</p> <ul> <li>Disk: Data is stored in the most efficient way, e.g. uint16 for Sentinel 2, uint8 for TCVIS</li> <li>Memory: Data is stored in the most convenient and correct way for visualisation purposes. This should be equal to the original data representation. E.g. [-1, 1] float for NDVI</li> <li>Model: Data ist normalised to [0, 1] for training and inference and is always <code>float32</code></li> </ul> <p>Memory representation exeptions</p> <p>The memory representation is not always the same as the original data representation. For example, Satellite data like Sentinel 2 is originally stored as <code>uint16</code>, however, we also want to account for NaN values in the data. Therefore, the memory representation of Sentinel 2 data is <code>float32</code> instead, but with the same range as the original data and 0 replaced with NaN.</p>"},{"location":"dev/bands/#specification-details","title":"Specification Details","text":"<p>The data convertion happens in three different places of the pipeline:</p> <ul> <li>Disk -&gt; Memory:</li> <li>Cache-Manager where data is loaded from a cache NetCDF file into memory.     This should be handled via xarray, which follows the CF conventions.</li> <li>Training-Preprocessors</li> <li>NOT in the export -&gt; The export is handled manually</li> <li>Memory -&gt; Model: In the segmentation module, right before the data is transformed into PyTorch tensors for inference.   For training, this is done before writing the data into the training dataset to save compute power and enable better caching.</li> <li>Model -&gt; Memory: Never happens, the output propabilities are exported as is.   Further, the probabilities are only appended to the data in memory-representation.   Therefore the model-representation only ever exists in the inference or training code.</li> <li>Memory -&gt; Disk: At the export module and with the Cache-Manager when writing cache files.   This is done via xarray, which follows the CF conventions.</li> </ul> <p>Terminology</p> <p>Because the data can have 3 different representations, it becomes unclear what is meant by \"encoded\" and \"decoded\". In general, the \"Memory\" representation is always the \"true\" and therefore \"decoded\" representation. However, outside of the context of convertions, we may use a following terminology:</p> <ul> <li>Encoded: The data in the representation that is used for caching and exports, i.e. disk-representation.</li> <li>Decoded: The data in the representation that is used for working and visualisation, i.e. memory-representation.</li> <li>Normalised: The data in the representation that is used for training and inference, i.e. model-representation.</li> </ul> DataVariable usage shape dtype (memory) dtype(disk) valid-range disk-range no-data (disk) attrs source note <code>blue</code> inp (x, y) float32 uint16 [-0.1, 0.5] [0, 65535] 0 data_source, long_name, units PLANET / S2 <code>green</code> inp (x, y) float32 uint16 [-0.1, 0.5] [0, 65535] 0 data_source, long_name, units PLANET / S2 <code>red</code> inp (x, y) float32 uint16 [-0.1, 0.5] [0, 65535] 0 data_source, long_name, units PLANET / S2 <code>nir</code> inp (x, y) float32 uint16 [-0.1, 0.5] [0, 65535] 0 data_source, long_name, units PLANET / S2 <code>s2_scl</code> qal (x, y) uint8 uint8 [0, 11] [0, 11] - data_source, long_name S2 https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/scene-classification/ <code>planet_udm</code> qal (x, y) uint8 uint8 [0, 8] [0, 8] - PLANET https://docs.planet.com/data/imagery/udm/ <code>quality_data_mask</code> qal (x, y) uint8 uint8 {0, 1, 2} {0, 1, 2} - data_source, long_name, description Acquisition 0 = Invalid, 1 = Low Quality, 2 = High Quality <code>dem</code> inp (x, y) float32 int16 [-100, 3000] [0, 31000] -1 data_source, long_name, units SmartGeocubes <code>arcticdem_data_mask</code> qal (x, y) uint8 bool {0, 1} {False, True} - data_source, long_name, units SmartGeocubes <code>tc_brightness</code> inp (x, y) uint8 uint8 [0, 255] [0, 255] - data_source, long_name EarthEngine <code>tc_greenness</code> inp (x, y) uint8 uint8 [0, 255] [0, 255] - data_source, long_name EarthEngine <code>tc_wetness</code> inp (x, y) uint8 uint8 [0, 255] [0, 255] - data_source, long_name EarthEngine <code>ndvi</code> inp (x, y) float32 int16 [-1, 1] [0, 20000] -1 long_name Preprocessing <code>relative_elevation</code> inp (x, y) float32 int16 [-50, 50] [0, 30000] -1 data_source, long_name, units Preprocessing <code>slope</code> inp (x, y) float32 int16 [0, 90] [0, 9000] -1 data_source, long_name Preprocessing <code>aspect</code> inp (x, y) float32 int16 [0, 360] [0, 3600] -1 data_source, long_name Preprocessing <code>hillshade</code> inp (x, y) float32 int16 [0, 1] [0, 10000] -1 data_source, long_name Preprocessing <code>curvature</code> inp (x, y) float32 int16 [-1, 1] [0, 20000] -1 data_source, long_name Preprocessing <code>probabilities</code> dbg (x, y) float32 uint8 [0, 1] [0, 100] 255 long_name Ensemble / Segmentation <code>probabilities-X*</code> dbg (x, y) float32 uint8 [0, 1] [0, 100] 255 long_name Ensemble / Segmentation <code>binarized_segmentation</code> out (x, y) bool bool {False, True} {False, True} - long_name Postprocessing <code>binarized_segmentation-X*</code> dbg (x, y) bool bool {False, True} {False, True} - long_name Postprocessing <code>extent</code> out (x, y) bool bool {False, True} {False, True} - long_name Postprocessing <p>Notes:</p> <ul> <li><code>X*</code> = Model name, e.g. <code>probabilities-tcvis</code>, <code>probabilities-notcvis</code>, etc.</li> <li>The <code>no-data</code> value in memory for <code>float32</code> is always <code>nan</code>.</li> <li><code>bool</code> types before postprocessing must be represented as uint8 in memory for easy reprojection etc.</li> <li>Modes of usage:</li> <li><code>inp</code>: (Potential) Input to the model</li> <li><code>qal</code>: Quality Assurance Layer, not used as input to the model, but for masking or filtering</li> <li><code>dbg</code>: Only exported for debugging purposes</li> <li><code>out</code>: Output of the model</li> </ul> <p>Incompleteness</p> <p><code>attrs</code> is outdated.</p> <p>Missing:</p> <ul> <li>New DEM Engineered - VRM DI etc.</li> <li>New Indices - TGI, EXG, GLI etc.</li> </ul> <p>Loss of Information</p> <p>Because we encode almost every variable we work with into a smaller sized representation or into a smaller range, information get's lost. E.g. when writing the DEM to disk, values larger than 3000m will be clipped to 3000m and the minimum step size between values reduces to 0.1m. This is enough for our purposes, but may not be suitable for other applications.</p>"},{"location":"dev/bands/#optical-bands-planet-vs-s2-harmonized-gee-vs-sentinel-2-l2a-copernicus","title":"Optical bands: PLANET vs. S2-Harmonized (GEE) vs. Sentinel-2 L2A (Copernicus)","text":"<p>This is complicated: in theory the range of this data is between 0 and 1 and measured as surface reflectance. However, the values can be negative (e.g. due to atmospheric correction) and larger than 1 (e.g. due to bright surfaces like snow). Thus, Copernicus applies a shift of -0.1 to allow for negative values in the Sentinel-2 L2A product. This was introduced in 2022 - Google Earth Engine just reverts the shift for all data after 2022 in the S2-Harmonized collection, because they never re-upload the data. Once uploaded, the data is fixed and will never change. Thus, GEE S2-Harmonized data is lossy. The unharmonized dataset in GEE, which is officially deprecated, is not lossy, but spectral values are not comparable between years before and after 2022. Further, because data is never re-uploaded, the processing of older imagery is different than newer imagery. The data in GEE and in Copernicus is always stored as uint16 values between 0 and 65535 (maximum of uint16) with a scale factor of 10000, just with different offsets.</p> <p>In our pipeline we want to be able to utilize negative values in the model. Because most viable (non-snow, non-cloud) values are not larger than 0.5, we decided to use the range [-0.1, 0.5] for the memory representation. Of course, this only applies to the normalization before the data is fed into the model, thus calculating indices like NDVI are not limited to this range.</p> <p>Data which is directly downloaded from either Copernicus or GEE is directly stored in the cache with their own representation. This is not documented in the table above and is specific to the acquisition module. All data which is output from the acquisition module is always converted to the memory representation.</p>"},{"location":"dev/bands/#dem","title":"DEM","text":"<p>The highest point in the arctic is approx. 3000m. The lowest depends on the geoid used, for arcticdem there are very few values below -10. (i guess) Hence, the valid-range scaling is similar to the optical data arbitrary.</p> <p>For TPI (relative_elevation), the valid-range strongly depends on the kernel used. The range increases with larger kernel sizes. E.g. some tests with Sentinel 2:</p> <ul> <li>2px (20m) kernel: [-3, 3]</li> <li>10px (100m) kernel: [-40, 20]</li> <li>100px (1000m) kernel: [-60, 40]</li> </ul> <p>Since we use mostly a kernel between 10px and 100px, we can expect the valid range to be between [-50, 50].</p>"},{"location":"dev/bands/#implementation-details","title":"Implementation Details","text":"<p>All Disk &lt;-&gt; Memory convertion are be done via xarray through their CF convention layer (<code>decode_cf=True</code>) For that, the attributes <code>_FillValue</code>, <code>scale_factor</code>, and <code>add_offset</code> are set by a helper module <code>darts_utils.bands.BandManager</code>. This helper is used by the Cache-Manager and the export module to ensure that the data is always in the correct representation.</p> <p>_FillValue</p> <p>With rioxarray it is possible to assign a <code>_FillValue</code> attribute to the data variables with <code>.rio.write_nodata()</code>. This can lead to weird behaviour when writing and reading the data:</p> <pre><code>&gt;&gt;&gt; \"Before writing with _FillValue=0.0: dtype=uint16, attrs={'_FillValue': 0.0, 'data_source': 'planet', 'long_name': 'NDVI'}\"\n&gt;&gt;&gt; \"After reading with _FillValue=0.0: dtype=float32, attrs={'data_source': 'planet', 'long_name': 'NDVI'}\"\n&gt;&gt;&gt; \"Before writing with _FillValue=0: dtype=uint16, attrs={'_FillValue': 0, 'data_source': 'planet', 'long_name': 'NDVI'}\"\n&gt;&gt;&gt; \"After reading with _FillValue=0: dtype=float32, attrs={'data_source': 'planet', 'long_name': 'NDVI'}\"\n&gt;&gt;&gt; \"Before writing wihtout _FillValue: dtype=uint16, attrs={'data_source': 'planet', 'long_name': 'NDVI'}\"\n&gt;&gt;&gt; \"After reading without _FillValue: dtype=uint16, attrs={'data_source': 'planet', 'long_name': 'NDVI'}\"\n</code></pre>"},{"location":"dev/bands/#scale-and-offset","title":"Scale and Offset","text":"<p>The scale and offset for normalization is automatically derived from the <code>valid-range</code> parameter of a BandCodec.</p> <p>For disk encoding the following formula can be used to derive the scale and offset manually based on the <code>valid-range</code> and the <code>disk-range</code>:</p> <pre><code>offset = valid_range.min\nscale = (valid_range.max - valid_range.min) / (disk_range.max - disk_range.min)\n</code></pre> <p>E.g. for NDVI with a <code>valid_range=(-1.0, 1.0)</code> and <code>disk_range=(0, 20000)</code>:</p> <pre><code>&gt; offset = valid_range.min\n&gt; scale = (valid_range.max - valid_range.min) / (disk_range.max - disk_range.min)\n&gt; offset, scale\n-1., (2 / 20000) -&gt; -1., 0.0001\n</code></pre>"},{"location":"dev/bands/#legacy-support","title":"Legacy support","text":"<p>In order to support legacy models, it is necessary to check which model version was used. For this, from now on all checkpoints get a new field <code>model_version</code> in their metadata. Fortunatly, all previous normalizations are equal to the new ones, hence to remapping needs to be done.</p>"},{"location":"dev/cuda_fixes/","title":"Problems with CUDA","text":""},{"location":"dev/cuda_fixes/#problems-with-cuda","title":"Problems with CUDA","text":"<p>This is a collection of known issues and potential fixes for CUDA-related problems in the codebase.</p> <p>CUDA Version</p> <p>There exist multiple CUDA versions at a time: the driver version installed on the system, and the runtime version which is set by the environment. Read more here</p> <p>If using the CLI, there is a command to output information about the current environment setup:</p> <pre><code>uv run darts env-info\n</code></pre>"},{"location":"dev/cuda_fixes/#cucim-import","title":"CUCIM import","text":"<pre><code>python3: cufile_worker_thread.h:57: virtual void CUFileThreadPoolWorker::run(): Assertion `0' failed.\n</code></pre> <p><code>cufile.log</code>:</p> <pre><code> 07-05-2025 20:29:31:49 [pid=747532 tid=748005] ERROR  cufio_core:55 Threadpool Thread ID:  139915782243904 cuDevicePrimaryCtxRetain failed with error 2\n</code></pre> <p>What happend? Probably, another user on the cluster is using a GPU in exclusive mode, which prevents other users from using it. CUCIM is trying to access the GPU, but fails because the GPU is not available.</p> <p>Please read this related community issue:</p> <p>From the snippet pasted here, it looks like the cuda device is not available at this time and as a result the cuDevicePrimaryCtxRetain cuda call fails. CUDA_ERROR_DEVICE_UNAVAILABLE = 46 This indicates that requested CUDA device is unavailable at the current time. Devices are often unavailable due to use of CU_COMPUTEMODE_EXCLUSIVE_PROCESS or CU_COMPUTEMODE_PROHIBITED. My suspicion is that one of the compute mode settings are effected in this environment preventing the thread to retain the context on the same device.</p> <p>How to fix? Just set the <code>CUDA_VISIBLE_DEVICES</code> environment variable to the device you want to use. Don't forget to set the <code>--device</code> argument of the CLI to <code>cuda</code> is such case, since our auto detection will not work in this case.</p> <pre><code>export CUDA_VISIBLE_DEVICES=0\n</code></pre>"},{"location":"dev/cuda_fixes/#nvrtc","title":"NVRTC","text":"<pre><code>Unable to replicate\n</code></pre> <p>What happend? The LD_LIBRARY_PATH is either not set correctly or the <code>cuda-nvrtc</code> package is not installed in the conda environment.</p> <p>How to fix?</p> <p>Install and use a conda environment created by Pixi. Please refer to the Installation Guide.</p> <p>How to fix? (Legacy version) Create a conda environment with the <code>cuda-nvrtc</code> package installed. You can do this by running the following command:</p> <pre><code>conda create -n cuda_nvrtc -c nvidia cuda-nvrtc\n</code></pre> <p>This environment must NOT be activated, it is just to install the library files somewhere. Check:</p> <pre><code>$ conda list -n cuda120\n# packages in environment at /path/to/.conda/envs/cuda_nvrtc:\n#\n# Name                    Version                   Build  Channel\ncuda-nvrtc                12.1.105                      0    nvidia\n</code></pre> <p>Now you can set the <code>LD_LIBRARY_PATH</code> to include the path to the <code>cuda-nvrtc</code> library:</p> <pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/.conda/envs/cuda120/lib/\n</code></pre> <p>This will allow numba to find the <code>libnvrtc.so</code> library when it is needed:</p> <pre><code>$ ls -l /path/to/.conda/envs/cuda120/lib/\ntotal 62234\nlrwxrwxrwx 1 jokuep001 hpc_user       29 Mar 24 16:25 libnvrtc-builtins.so.12.1 -&gt; libnvrtc-builtins.so.12.1.105\n-rwxrwxr-x 3 jokuep001 hpc_user  6846016 Apr  4  2023 libnvrtc-builtins.so.12.1.105\nlrwxrwxrwx 1 jokuep001 hpc_user       20 Mar 24 16:25 libnvrtc.so.12 -&gt; libnvrtc.so.12.1.105\n-rwxrwxr-x 3 jokuep001 hpc_user 56875328 Apr  4  2023 libnvrtc.so.12.1.105\n</code></pre>"},{"location":"dev/cuda_fixes/#ptxas","title":"PTXAS","text":"<pre><code>LinkerError: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION                                                                                                                                                                                                 \nptxas application ptx input, line 9; fatal   : Unsupported .version 8.7; current version is '8.2'\n</code></pre> <p>What happend? There is a mismatch between the CUDA version used by the projects code and the CUDA version used by the system. This error is caused by a mismatch between the OS and the CUDA version of the system. In our case, the default CUDA version of the system was 12.6 and Ubuntu 22.04. CUDA 12.6 expects for our use case a PTX version of 8.7, but Ubutnu 22.04 only supports PTX version 8.2.</p> <p>How to fix?</p> <p>Write a script to set all the environment variables (these may differ on your system):</p> <pre><code>export CUDA_PATH=\"/usr/local/cuda-12.2\"\nexport CUDA_HOME=\"/usr/local/cuda-12.2\"\nexport PATH=\"/usr/local/cuda-12.2/bin:$PATH\"\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.2/extras/CUPTI/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.2/compat:$LD_LIBRARY_PATH\n\nexport NUMBA_CUDA_DRIVER=\"/usr/local/cuda-12.2/compat/libcuda.so\"\nexport NUMBA_CUDA_INCLUDE_PATH=\"/usr/local/cuda-12.2/include\"\n</code></pre> <p>and then source it:</p> <pre><code>source /path/to/your/script.sh\n</code></pre> <p>Just running <code>sh script.sh</code> will not work, since it will create a new shell and the environment variables will not be set in the current shell. You can also add these lines to your <code>.bashrc</code> file, so they are set automatically when you open a new terminal. However, this may cause problems for other CUDA applications and projects, so be careful with this approach. Of course, you can also set these variables manually in the terminal before running the code.</p>"},{"location":"dev/cuda_fixes/#template","title":"Template","text":"<p>\"Error message\"</p> <p>What happend? ...</p> <p>How to fix? ...</p>"},{"location":"dev/docs/","title":"Docs","text":""},{"location":"dev/docs/#documentation-organization","title":"Documentation organization","text":"<p>Guides should help users to get familar on how to use the pipelines or how they could implement certain parts of our project / components.</p> <p>The dev section should help us developers to understand the codebase and decision making.</p>"},{"location":"dev/docs/#todos","title":"ToDos","text":"<ul> <li> Add components guide</li> <li> Further document them</li> <li> Document the required inputs and output data_vars and attrs<ul> <li> In API ref</li> <li> In Components Guide</li> </ul> </li> </ul>"},{"location":"guides/components/","title":"DARTS Components","text":""},{"location":"guides/components/#darts-components","title":"DARTS components","text":"<p>Components</p> <p>The idea behind the Architecture of <code>darts-nextgen</code> is to provide <code>Components</code> to the user. Users should pick their components and put them together in their custom pipeline, utilizing their own parallelization framework. Pipeline v2 shows how this could look like for a simple sequential pipeline - hence without parallelization framework.</p> <p>There are many different components implemented, all within the different <code>darts packages</code>. Here is an overview over the currently implemented components and their hardware requirements / bounds.</p> <p>Note: the following table was generated from the public re-exports in each package's <code>__init__.py</code>. I inferred resource requirements conservatively from each component's role (data acquisition -&gt; network/disk; preprocessing/postprocessing/metrics -&gt; CPU/disk; training/ensembling -&gt; heavy compute and often GPU). Where unclear I made reasonable assumptions and marked them in the table. If you want tighter/verified bounds I can inspect individual implementations and tests to refine the entries.</p> Component Stateful? and why? Network <sup>1</sup> Disk <sup>2</sup> Compute <sup>3</sup> GPU <sup>4</sup> Acquisition darts_acquisition.download_arcticdem Stateless darts_acquisition.load_arcticdem Stateless darts_acquisition.load_planet_masks Stateless darts_acquisition.load_planet_scene Stateless darts_acquisition.download_cdse_s2_sr_scene Stateless darts_acquisition.load_cdse_s2_sr_scene Stateless darts_acquisition.download_gee_s2_sr_scene Stateless darts_acquisition.load_gee_s2_sr_scene Stateless darts_acquisition.download_tcvis Stateless darts_acquisition.load_tcvis Stateless Preprocessing darts_preprocessing.calculate_aspect Stateless darts_preprocessing.calculate_curvature Stateless darts_preprocessing.calculate_dissection_index Stateless darts_preprocessing.calculate_hillshade Stateless darts_preprocessing.calculate_slope Stateless darts_preprocessing.calculate_terrain_ruggedness_index Stateless darts_preprocessing.calculate_topographic_position_index Stateless darts_preprocessing.calculate_vector_ruggedness_measure Stateless darts_preprocessing.calculate_ctvi Stateless darts_preprocessing.calculate_evi Stateless darts_preprocessing.calculate_exg Stateless darts_preprocessing.calculate_gli Stateless darts_preprocessing.calculate_gndvi Stateless darts_preprocessing.calculate_grvi Stateless darts_preprocessing.calculate_ndvi Stateless darts_preprocessing.calculate_nrvi Stateless darts_preprocessing.calculate_rvi Stateless darts_preprocessing.calculate_savi Stateless darts_preprocessing.calculate_tgi Stateless darts_preprocessing.calculate_ttvi Stateless darts_preprocessing.calculate_tvi Stateless darts_preprocessing.calculate_vari Stateless darts_preprocessing.calculate_vdvi Stateless darts_preprocessing.calculate_vigreen Stateless darts_preprocessing.calculate_spyndex Stateless darts_preprocessing.preprocess_legacy_fast <sup>5</sup> Stateless darts_preprocessing.preprocess_v2 <sup>5</sup> Stateless Segmentation darts_segmentation.segment.SMPSegmenter Stateful \u2014 holds model state (weights/config) Ensemble darts_ensemble.EnsembleV1 Stateful \u2014 holds ensemble state (weights/config) Postprocessing darts_postprocessing.binarize Stateless darts_postprocessing.erode_mask Stateless darts_postprocessing.prepare_export <sup>6</sup> Stateless Export darts_export.export_tile Stateless <p>Next to the components, there exist several helper functions for e.g. searching Sentinel-2 scenes or metrics for the training. Have a look at the Reference for a list of all functions and a describtion of what they do.</p> <ol> <li> <p>Network: Requires network access if\u00a0\u21a9</p> </li> <li> <p>Disk: Reads from or writes to disk\u00a0\u21a9</p> </li> <li> <p>Compute: Does some heavy compute, utilizing the CPU\u00a0\u21a9</p> </li> <li> <p>GPU: Supports offloading compute to the GPU\u00a0\u21a9</p> </li> <li> <p>Wrapper for multiple preprocessing steps, handling compatibility with each other\u00a0\u21a9\u21a9</p> </li> <li> <p>Wrapper for multiple postprocessing steps\u00a0\u21a9</p> </li> </ol>"},{"location":"guides/config/","title":"Config Files","text":""},{"location":"guides/config/#config-files","title":"Config Files","text":"<p>The <code>darts</code> CLI support passing parameters via a config file in TOML format. This can be useful to reduce the amount of parameters you need to pass or to safe different configurations. In general, the CLI tries to match all parameters under the <code>darts</code> key of the config file, skipping not needed ones.</p>"},{"location":"guides/config/#example-usage","title":"Example usage","text":"<p>Let's take a closer look with the example command <code>darts hello</code>. This command has the following function signature:</p> <pre><code>def hello(name: str, n: int = 1):\n    \"\"\"Say hello to someone.\n\n    Args:\n        name (str): The name of the person to say hello to\n        n (int, optional): The number of times to say hello. Defaults to 1.\n\n    Raises:\n        ValueError: If n is 3.\n\n    \"\"\"\n    for i in range(n):\n        logger.debug(f\"Currently at {i=}\")\n        if n == 3:\n            raise ValueError(\"I don't like 3\")\n        logger.info(f\"Hello {name}\")\n</code></pre> <p>Let's run the command without making a config file:</p> <pre><code>$ uv run darts hello Alice\nDEBUG Currently at i=0\nINFO Hello Alice\n</code></pre> <p>Now specify a config file <code>config.toml</code>:</p> <pre><code>[darts]\nname = \"Not Alice\"\nn = 2\n</code></pre> <p>And run the same command:</p> <pre><code>$ uv run darts hello Alice\nDEBUG Currently at i=0\nINFO Hello Alice\nDEBUG Currently at i=1\nINFO Hello Alice\n</code></pre> <p>The <code>name</code> parameter is still taken from the CLI, while the <code>n</code> parameter is taken from the config file.</p> <p>Because the CLI utilized a custom TOML parser to parse the config file and pass it to the CLI tool cyclopts, only parameters under the <code>darts</code> key are considered. Subheading keys are not considered, but can be used to structure the config file:</p> <pre><code>[darts]\nname = \"Not Alice\"\n\n[darts.numbers]\nn = 2\n</code></pre> <p>The <code>numbers</code> key is ignored by the CLI, hence <code>n</code> will be add to the command as before.</p> <p>Warning</p> <p>The only parameters not passed from the config file are the <code>--config-file</code>, <code>--log-dir</code>, <code>--log-plain</code> and the verbosity parameters. These parameters are evaluated before the config file is parsed, hence it is not possible to specify the logging directory via the config file.</p>"},{"location":"guides/config/#real-world-example-with-sentinel-2-processing","title":"Real world example with Sentinel 2 processing","text":"<p>Sentinel 2 processing via. Area of Interest file:</p> <pre><code>[darts]\nee-project = \"your-ee-project\"\ndask-worker = 4\n\n[darts.paths]\ninput-cache = \"./data/cache/s2gee\"\noutput-data-dir = \"./data/out\"\narcticdem-dir = \"./data/datacubes/arcticdem\"\ntcvis-dir = \"./data/datacubes/tcvis\"\nmodel-file = \"./models/s2-tcvis-final-large_2025-02-12.ckpt\"\n</code></pre> <p>Running the command:</p> <pre><code>uv run darts inference sentinel2-sequential --aoi-shapefile path/to/your/aoi.geojson --start-date 2024-07 --end-date 2024-09\n</code></pre>"},{"location":"guides/debugging/","title":"Debugging","text":""},{"location":"guides/debugging/#debugging","title":"Debugging","text":"<p>CUDA related</p> <p>There is a complete page about CUDA related errors.</p> <p>To produce more (hopefully) helpful output from the CLI, the verbosity level can be set via the verbosity flags:</p> <ul> <li><code>-v</code> This will set the log-level of the all DARTS modules to DEBUG. The output becomes much more noisy, but contains a lot of useful information.</li> <li><code>-vv</code> This will do the same as <code>-v</code>, but in addition also show local in tracebacks via <code>rich.tracebacks.install(show_locals=True)</code> and also set the log-level of some third-party libraries (like PyTorch) to INFO as well. This, however, is much slower and can potentially output to much information for the terminal to handle.</li> <li><code>-vvv</code> This will do the same as <code>-vv</code>, but instead of setting third-party libraries to INFO, it will set them to DEBUG. This is mainly useful for debugging issues with third-party libraries.</li> </ul> <p>By using the <code>--log-plain</code> flag the loggers will not use <code>rich</code> to print to the console, this should help working on machines with poor / no TTY support, like SLURM applications.</p>"},{"location":"guides/devices/","title":"Devices","text":""},{"location":"guides/devices/#devices","title":"Devices","text":"<p>Supported devices</p> <p>As of right now, only CUDA and CPU devices are supported. How to install a working Python environment for either case please refer to the installation guide.</p> <p>Some functions can be accelerated by a GPU if a CUDA device is available and the python environment is properly installed with CUDA enabled. These functions will automatically detect if a CUDA device is available and will use it if so and not specified otherwise.</p> <p>Debuggin CUDA</p> <p>There is a complete page about CUDA related errors.</p>"},{"location":"guides/devices/#device-selection-with-the-cli","title":"Device selection with the CLI","text":"<p>The CLIs pipeline commands, e.g. <code>darts inference sentinel2-sequential</code> allow for specifying a device with the <code>--device</code> flag:</p> <pre><code>uv run darts inference sentinel2-sequential --device cuda\n</code></pre> <p>One can pass either</p> <ul> <li><code>auto</code>: This will automatically select a free GPU based on memory usage (&lt;50%). If no GPU is available will use the CPU instead.</li> <li><code>cuda</code>: This will use the first (0) device, fails if no GPU is available.</li> <li><code>cpu</code>: This will use the CPU.</li> <li>an integer: This will use the specified GPU, e.g. <code>2</code> will take the third availble GPU.</li> </ul> <p>Of course the device can also be set by setting the <code>CUDA_VISIBLE_DEVICES</code> environment variable.</p>"},{"location":"guides/devices/#device-selection-within-specific-functions","title":"Device selection within specific functions","text":"<p>It is possible to force the use of a specific device through the <code>device</code> parameter of the respective function. For most GPU-capable functions it is possible to pass either <code>cpu</code> or <code>cuda</code> as a string to the <code>device</code> parameter. In a multi-GPU setup, the device can be specified by passing the device index as an integer (e.g. <code>0</code> for the first GPU, <code>1</code> for the second GPU, etc.). However, functions which use PyTorch expect the device to be a PyTorch device object, so you need to pass <code>torch.device(\"cuda:0\")</code> instead of just <code>0</code>. Which type of device is expected is documented in the respective function documentation.</p> <p>As of now, the following functions can be accelerated by the GPU:</p> <ul> <li>darts_acquisition.load_cdse_s2_sr_scene - <code>device</code>: <code>\"cpu\" | \"cuda\" | int</code></li> <li>darts_acquisition.load_gee_s2_sr_scene - <code>device</code>: <code>\"cpu\" | \"cuda\" | int</code></li> <li>darts_preprocessing.preprocess_v2 - <code>device</code>: <code>\"cpu\" | \"cuda\" | int</code></li> <li>darts_segmentation.segment.SMPSegmenter - <code>device</code>: <code>torch.device</code></li> <li>darts_ensemble.EnsembleV1 - <code>device</code>: <code>torch.device</code></li> <li>darts_postprocessing.prepare_export - <code>device</code>: <code>\"cpu\" | \"cuda\" | int</code></li> </ul> <p>Compute backends</p> <p>All preprocessing-engineering functions are written without the need of a specific compute backend, thanks to xarrays widely compatibility. This allows for passing xarray Datasets with e.g. a <code>dask</code> or <code>cupy</code> backend and the computations will happens with this backend automatically.</p>"},{"location":"guides/installation/","title":"Installation","text":""},{"location":"guides/installation/#advanced-installation","title":"Advanced Installation","text":"<p>A lot of caveats and side effects</p> <p>Please read this complete guide before doing anything on your machine! Especially handling CUDA stuff can be tricky and lead to problems. This project tries to make the installation as easy as possible while still being flexible about the OS and GPU setup. Furthermore, we try to not infer with existing setups done on your machine - that is why we use <code>uv</code> and <code>pixi</code> in the first place. But this is only possible if you follow the instructions carefully.</p> <p>Prereq:</p> <ul> <li>uv: <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code></li> <li>gcloud CLI</li> <li>cuda (optional for GPU support)</li> </ul> <p>This project uses <code>uv</code> to manage the python environment. If you are not familiar with <code>uv</code> yet, please read their documentation first. Please don't use <code>pip</code> or <code>conda</code> to install the dependencies, as this often leads to problems. We have spend a lot of time making sure that the install process is easy and quick, but this is only possible with <code>uv</code>. So please use it.</p> <p>Using conda</p> <p>Sometimes, it is not possible to only use <code>uv</code>, e.g. if CUDA is not installed correctly and one does not have access to fix this. In these scenarios, one would use <code>conda</code> or <code>mamba</code> to create a new environment and install the required packages. However, we strongly discourage this, since it requires a lot of manual work and knowledge about what to install. E.g. did you know, that installing the <code>cuda-toolkit</code> from conda-forge will always install version 11.8? To install the correct version, one would need to use the <code>nvidia</code> channel and install the <code>cuda-toolkit</code> from there. Or even better, the <code>cuda</code> package, which installs all CUDA-related stuff. Instead use <code>pixi</code>*, we provide a ready to use environment setup in our project. Read more about this in the respective section below.</p> <p>In general the environment can be installed with <code>uv sync</code>. However, this project depends on some libraries (torch and torchvision) which don't get installed per default. Therefore you need to specify an extra flag to install the correct dependencies, e.g.</p> <pre><code>uv sync --extra cuda126\n</code></pre> <p>This will install the environment with the correct dependencies for CUDA 12.6. The following sections will explain the different extra flags and groups which can be used to install the environment for different purposes and systems.</p>"},{"location":"guides/installation/#cuda-and-cpu-only-installations","title":"CUDA and CPU-only installations","text":"<p>Several CUDA versions can be used, but it may happen that some problems occur on different systems. Currently CUDA 12.1, and 12.6 are supported, but sometimes other versions work as well. We use python extra dependencies, so it is possible to specify the CUDA version via an <code>--extra</code> flag in the <code>uv sync</code> command.</p> <p>You can check the currently installed CUDA version via:</p> <pre><code>nvidia-smi\n# Look at the top right corner for the CUDA version\n</code></pre> <p>Warning</p> <p>If the <code>nvidia-smi</code> command is not found, you might need to install the nvidia drivers. Be very cautious with the installation of the driver, rather read the documentation with care.</p> <p>To install the python environment for a specific CUDA version use one of the following commands respectively:</p> <pre><code>uv sync --extra cuda118 # Not well tested\nuv sync --extra cuda121\nuv sync --extra cuda124 # Not well tested\nuv sync --extra cuda126 \nuv sync --extra cuda128 # Not well tested\n</code></pre> <p>CUDA version missmatch</p> <p>Sometimes it is possible to use a different CUDA version for the python packages than the one installed. E.g. we tested our code on a system with CUDA 12.2 installed, but used the python packages for CUDA 12.1. This is not recommended, but sometimes it works. In general, one should NOT use a higher CUDA version for the python packages than the one installed on the system.</p> <p>Install the python environment for CPU-only use:</p> <pre><code>uv sync --extra cpu\n</code></pre> <p>Danger</p> <p>Either <code>--extra cpu</code> or <code>--extra cudaXXX</code> must be specified. Without important libraries like PyTorch will not be installed and the environment will not work.</p>"},{"location":"guides/installation/#training-specific-dependencies","title":"Training specific dependencies","text":"<p>Training specific dependencies are optional and therefore not installed by default. To install them, add <code>--extra training</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> <p>psycopg2</p> <p>The training dependencies depend on psycopg2, which requires postgresql installed on your system.</p>"},{"location":"guides/installation/#packages-for-the-documentation","title":"Packages for the documentation","text":"<p>Packages which are used to create this documentation are not installed by default and are not available via as an extra. Instead they are installed as part of an optional <code>dependency-group</code>, or <code>group</code> for short. To install the documentation dependencies, add <code>--group docs</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training --group docs\n</code></pre>"},{"location":"guides/installation/#installation-with-pixi","title":"Installation with pixi","text":"<p>Experimental</p> <p>The following section is experimental and not yet fully tested.</p> <p>Warning</p> <p>Using <code>pixi</code> instead of <code>uv</code> is not as ergonomic, because one needs to further wrap the commands or work in an shell.</p> <p>Prereq:</p> <ul> <li>pixi: <code>curl -fsSL https://pixi.sh/install.sh | sh</code></li> </ul> <p>Pixi is a tool to manage system-environment using conda packages. Hence, a lot of the functionality is similar to normal <code>conda</code>, however, <code>pixi</code> follows a project related approach.</p> <p>In case you need to install <code>cuda</code> in an isolated environment, this is the way to go. The idea is to use <code>pixi</code> to create a conda environment and within this environment use <code>uv</code> to manage the python packages in an virtual environment. The conda environment will contain the necessary system packages, like <code>cuda-nvcc</code> or <code>cuda-toolkit</code>. To ensure that the python packages all refereing (linking) to these packages, all the installation done with <code>uv</code> will be done inside the <code>pixi</code> environment. The following graphic should illustrate this:</p> <p></p> <p>We provide different conda environments over pixi: <code>cuda121</code>, <code>cuda124</code>, <code>cuda126</code>, <code>cuda128</code>, and <code>default</code> (no cuda). To run the installation in one of these environments use the <code>pixi run</code> command:</p> <pre><code>pixi run -e cuda126 uv sync --extra cuda126 --extra training\npixi run -e cuda126 uv run darts --help\n</code></pre> <p>However, this results in a lot of chaining of commands, which beccome very verbose when using multiple commands. Instead, it is possible to create a pixi shell, in which the uv commands can be run directly, as described in the other sections:</p> <pre><code>pixi shell -e cuda126\nuv sync --extra cuda126 --extra training\nuv run darts --help\n</code></pre> <p>This is similar to <code>conda activate ...</code>, but in a better and more encapsulated way. It is also possible to exit this environment any time:</p> <pre><code>exit\n</code></pre> <p>Another way is to run the <code>darts</code> command directly via pixi, since we enable a passthrough, which will run <code>uv run darts</code> for you:</p> <pre><code>pixi run darts --help\n</code></pre> <p>However, with this method, it is still required that the venv is installed inside the conda environment of pixi like described above.</p>"},{"location":"guides/logging/","title":"Logging","text":""},{"location":"guides/logging/#logging-guide","title":"Logging Guide","text":"<p>We want to use the python logging module as much as possible to traceback errors and document the pipeline processes. Furthermore, we want to configure each logger with the <code>RichHandler</code>, which prettyfies the output with rich.</p>"},{"location":"guides/logging/#setup-guide","title":"Setup Guide","text":"<p>Currently, all setup related to logging is found in the <code>darts.utils.logging.py</code> file. It contains two functions:</p> <ol> <li>A setup function which sets the log-level for all <code>darts.*</code> logger and add default options to xarray and pytorch to supress arrays. See how to supress arrays.</li> <li>A function which adds a file and a rich log handler.</li> </ol> <p>Both functions are used in the CLI setup but can also be called from e.g. a notebook. The recommended approach for handling logging within a notebook is the following:</p> <pre><code>import logging\nfrom rich.logging import RichHandler\nfrom darts.utils.logging import LoggingManager\n\nLoggingManager.setup_logging(verbose=True)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\n</code></pre> <p>This way the notebook won't spam logfiles everywhere and we still have control over our rich handler.</p>"},{"location":"guides/logging/#usage-guide","title":"Usage Guide","text":"<p>For logging inside a darts-package should be done without any further configuration:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__.replace(\"darts_\", \"darts.\")) # don't replace __name__\n</code></pre> <p>Logging at the top-level <code>darts</code> package can just use a <code>__name__</code> logger:</p> <pre><code>import loggin\n\nlogger = logging.getLogger(__name__) # don't replace __name__\n</code></pre>"},{"location":"guides/logging/#supressing-arrays","title":"Supressing Arrays","text":"<p>When printing or logging large numpy arrays a lot of numbers get truncated, however the array still takes a lot of space. Using <code>lovely_numpy</code> and <code>lovely_tensor</code> can help here:</p> <pre><code>import numyp as np\nimport torch\nimport xarray as xr\nfrom lovely_numpy import lo\nimport lovely_tensors\n\nlovely_tensors.monkey_patch()\nlovely_tensors.set_config(color=False)  # Disable colored output, which is better for logging\nxr.set_options(display_expand_data=False)\n\na = np.zeros((8, 1024, 1024))\nla = lo(a)\nda = xr.DataArray(a)\nt = torch.tensor(a)\n\nlogger.warning(la)\nlogger.warning(da)\nlogger.warning(t)\n</code></pre>"},{"location":"guides/logging/#dev-guide","title":"Dev Guide","text":"<p>This section should cover best practices developing inside the darts package.</p> <ul> <li>Fail fast</li> <li>Logging exceptions to debug</li> </ul>"},{"location":"guides/logging/#exceptions","title":"Exceptions","text":"<p>When logging exceptions from within an component, the complete exception should be logged with the traceback by setting <code>exc_info=True</code> to debug and then re-raised:</p> <pre><code>try:\n    # some code\nexcept Exception as e:\n    logger.debug(e, exc_info=True)\n    raise\n</code></pre>"},{"location":"guides/offline/","title":"Downloads and Offline mode","text":""},{"location":"guides/offline/#downloads-and-offline-mode","title":"Downloads and Offline Mode","text":"<p>DARTS is designed to work with large geospatial datasets from various sources. To ensure efficient workflows, especially when working on HPC systems or in environments with limited internet connectivity, DARTS implements automatic data downloading with intelligent caching and a dedicated offline mode.</p>"},{"location":"guides/offline/#overview","title":"Overview","text":"<p>DARTS handles two types of data downloads:</p> <ol> <li>Optical data (Sentinel-2): Downloaded from remote STAC servers or Google Earth Engine</li> <li>Auxiliary data (ArcticDEM, TCVIS): Downloaded procedurally into local datacubes as needed</li> </ol> <p>All downloaded data is cached locally by default, so subsequent runs can reuse previously downloaded data without internet access. For optical data it is possible to deactivate caching, which may be useful for very large runs where the each image is only processed once.</p>"},{"location":"guides/offline/#sentinel-2-scene-downloads","title":"Sentinel-2 Scene Downloads","text":"<p>When running a Sentinel-2 pipeline, DARTS automatically downloads scenes from either:</p> <ul> <li>CDSE (Copernicus Data Space Ecosystem): The default source, accessed via STAC API</li> <li>GEE (Google Earth Engine): Alternative source, useful when working already on Google Cloud</li> </ul> <p>!!! \"danger\" Different processing pipelines</p> <pre><code>Sentinel 2 data from Google Earth Engine comes in different processing levels, since they loaded data only once.\nThus, the spectral values are not super comparable across the years and can reduce model performance.\n</code></pre> <p>The default download process works as follows:</p> <ol> <li>Scene Discovery: Based on your input (scene IDs, tile IDs, or AOI), DARTS queries the respective service to find matching scenes</li> <li>Local Caching: Scenes are downloaded and stored in a compressed zarr format in a local \"raw data store\"</li> <li>Automatic Reuse: On subsequent runs, if a scene is already cached, it's loaded from the local store instead of being re-downloaded</li> </ol> <p>By default, Sentinel-2 raw data is stored in:</p> <pre><code>&lt;DARTS_DATA_DIR&gt;/sentinel2/&lt;source&gt;/\n</code></pre> <p>Where <code>&lt;source&gt;</code> is either <code>cdse</code> or <code>gee</code>. You can customize this location using the <code>raw_data_store</code> parameter in your pipeline configuration.</p> <p>The caching functionality can be disabled by passing <code>--no-raw-data-store</code> to the CLI when running <code>uv run darts inference sentinel2-sequential</code>.</p>"},{"location":"guides/offline/#auxiliary-data-downloads","title":"Auxiliary Data Downloads","text":"<p>DARTS uses auxiliary datasets to enhance segmentation performance:</p> <ul> <li>ArcticDEM: High-resolution elevation data (2m, 10m, or 32m resolution)</li> <li>TCVIS: Tasseled Cap vegetation trends from Landsat</li> </ul> <p>These datasets are stored in Zarr datacubes powered by Icechunk and downloaded procedurally - only the tiles you actually need are downloaded and stored. This approach is much more efficient than downloading entire continental datasets.</p> <p>For detailed information about how procedural downloading works, see the Auxiliary Data documentation.</p> <p>By default auxiliary data is stored under:</p> <ul> <li>ArcticDEM: <code>&lt;DARTS_DATA_DIR&gt;/auxiliary/arcticdem_&lt;resolution&gt;m.icechunk</code></li> <li>TCVIS: <code>&lt;DARTS_DATA_DIR&gt;/auxiliary/tcvis.icechunk</code></li> </ul> <p>You can customize these using the <code>arcticdem_dir</code> and <code>tcvis_dir</code> parameters.</p>"},{"location":"guides/offline/#using-darts-on-machines-without-internet","title":"Using DARTS on machines without internet","text":"<p>When you need to run DARTS on a system without internet access (e.g., an HPC cluster), you can pre-download all necessary data using the <code>prep-data</code> command. The <code>prep-data</code> command allows you to download optical data and/or auxiliary data before running your pipeline offline. E.g.</p> <pre><code>darts inference prep-data sentinel2 \\\n    --pipeline.scene-ids S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678 \\\n    --pipeline.raw-data-source cdse \\\n    --pipeline.raw-data-store /data/s2_raw_data \\\n    --pipeline.arcticdem-dir /data/arcticdem_10m.icechunk \\\n    --pipeline.tcvis-dir /data/tcvis.icechunk \\\n    --optical # Don't forget this\n    --aux # Don't forget this\n</code></pre> <p>When using <code>tile-ids</code> or <code>aoi-file</code> for scene discovery, DARTS generates a list of scene IDs. You can save this list for offline use:</p> <pre><code># Online: Discover scenes and save IDs\ndarts inference prep-data sentinel2 \\\n    --pipeline.tile-ids 33UUP \\\n    --pipeline.prep-data-scene-id-file /data/scene_ids.json \\\n    ...\n\n# This creates a file with the discovered scene IDs that can be used offline\n</code></pre> <p>To actually run the pipeline in offline-mode, set <code>offline=True</code> in your pipeline configuration or use the <code>--pipeline.offline</code> flag:</p> <pre><code>darts inference sentinel2-sequential \\\n    --pipeline.scene-ids S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678 \\\n    --pipeline.offline \\\n    # ... other parameters\n</code></pre> <p>This of course expects that all necessary data is present at the specified paths.</p>"},{"location":"guides/offline/#storage-considerations","title":"Storage Considerations","text":"<ul> <li>Sentinel-2 Raw Data: ~500-800 MB per scene (compressed)</li> <li>ArcticDEM Datacube: Grows as needed, ~1-2 GB per 10,000 km\u00b2 at 10m resolution</li> <li>TCVIS Datacube: ~50-100 MB per 10,000 km\u00b2</li> </ul> <p>Plan your storage accordingly, especially for large-scale processing.</p>"},{"location":"guides/offline/#see-also","title":"See Also","text":"<ul> <li>Auxiliary Data Documentation - Detailed information about datacube downloads</li> <li>Pipeline Configuration - Complete pipeline configuration options</li> <li>DARTS Paths - Understanding DARTS directory structure</li> </ul>"},{"location":"guides/paths/","title":"Paths and Data Management","text":""},{"location":"guides/paths/#paths-and-data-management","title":"Paths and Data Management","text":"<p>DARTS uses a structured approach to manage data storage locations across different storage types (fast SSD storage vs. large/slow storage). This guide explains how the path system works and how to configure it.</p>"},{"location":"guides/paths/#overview","title":"Overview","text":"<p>DARTS organizes data into two main storage categories:</p> <ul> <li>Fast Storage: For data requiring quick access (training data, models)</li> <li>Large Storage: For large datasets and outputs (auxiliary data, artifacts, cache, logs, outputs)</li> </ul>"},{"location":"guides/paths/#directory-structure","title":"Directory Structure","text":""},{"location":"guides/paths/#fast-storage-directories","title":"Fast Storage Directories","text":"<ul> <li><code>training/</code>: Training datasets</li> <li><code>models/</code>: Trained model files (*.pt)</li> </ul>"},{"location":"guides/paths/#large-storage-directories","title":"Large Storage Directories","text":"<ul> <li><code>aux/</code>: Auxiliary data (ArcticDEM, TCVis)</li> <li><code>artifacts/</code>: Training artifacts and checkpoints</li> <li><code>cache/</code>: Temporary cache files</li> <li><code>logs/</code>: Log files</li> <li><code>output/</code>: Pipeline output results</li> <li><code>input/</code>: Input data (Planet, Sentinel-2)</li> <li><code>archive/</code>: Archived data</li> </ul>"},{"location":"guides/paths/#auxiliary-data-subdirectories","title":"Auxiliary Data Subdirectories","text":"<p>Within the <code>aux/</code> directory:</p> <ul> <li><code>admin_boundaries/</code>: Administrative boundaries and regions</li> <li><code>arcticdem_2m.icechunk/</code>: ArcticDEM at 2m resolution</li> <li><code>arcticdem_10m.icechunk/</code>: ArcticDEM at 10m resolution</li> <li><code>arcticdem_32m.icechunk/</code>: ArcticDEM at 32m resolution</li> <li><code>tcvis.icechunk/</code>: Temporal Change Visualization data</li> </ul>"},{"location":"guides/paths/#input-data-subdirectories","title":"Input Data Subdirectories","text":"<p>Within the <code>input/</code> directory:</p> <ul> <li><code>planet/tiles/</code>: PlanetScope orthotiles</li> <li><code>planet/scenes/</code>: PlanetScope scenes</li> <li><code>sentinel2/grid/</code>: Sentinel-2 grid data</li> <li><code>sentinel2/cdse-scenes/</code>: Sentinel-2 raw data from Copernicus Data Space Ecosystem</li> <li><code>sentinel2/gee-scenes/</code>: Sentinel-2 raw data from Google Earth Engine</li> </ul>"},{"location":"guides/paths/#configuration-methods","title":"Configuration Methods","text":"<p>DARTS provides three ways to configure paths, listed in priority order (highest to lowest):</p>"},{"location":"guides/paths/#1-cli-flags-highest-priority","title":"1. CLI Flags (Highest Priority)","text":"<p>You can specify custom paths using CLI flags when running any DARTS command. These flags override all other settings.</p>"},{"location":"guides/paths/#default-directory-flags","title":"Default Directory Flags","text":"<p>These flags set the base directories for fast and large storage:</p> <pre><code>darts inference planet-sequential \\\n  --default-dirs.darts-dir /path/to/data \\\n  --default-dirs.fast-dir /path/to/fast/storage \\\n  --default-dirs.large-dir /path/to/large/storage \\\n  ...\n</code></pre> <ul> <li><code>--default-dirs.darts-dir</code>: Sets both fast and large directories (if they're not specified separately)</li> <li><code>--default-dirs.fast-dir</code>: Overrides the fast storage location</li> <li><code>--default-dirs.large-dir</code>: Overrides the large storage location</li> </ul>"},{"location":"guides/paths/#specific-directory-flags","title":"Specific Directory Flags","text":"<p>You can also override specific directories:</p> <pre><code>darts inference sentinel2-sequential \\\n  --output-data-dir /custom/output \\\n  --arcticdem-dir /custom/arcticdem \\\n  --tcvis-dir /custom/tcvis \\\n  ...\n</code></pre> <p>Available specific directory flags:</p> <ul> <li><code>--output-data-dir</code>: Where to write pipeline outputs</li> <li><code>--arcticdem-dir</code>: Location of ArcticDEM data</li> <li><code>--tcvis-dir</code>: Location of TCVis data</li> </ul>"},{"location":"guides/paths/#2-environment-variables-medium-priority","title":"2. Environment Variables (Medium Priority)","text":"<p>Set environment variables before running DARTS:</p> <pre><code>export DARTS_DATA_DIR=/path/to/data\nexport DARTS_FAST_DATA_DIR=/path/to/fast/storage\nexport DARTS_LARGE_DATA_DIR=/path/to/large/storage\n\ndarts inference planet-sequential ...\n</code></pre> <p>Environment variables:</p> <ul> <li><code>DARTS_DATA_DIR</code>: Base directory (used if fast/large not specified)</li> <li><code>DARTS_FAST_DATA_DIR</code>: Fast storage location</li> <li><code>DARTS_LARGE_DATA_DIR</code>: Large storage location</li> </ul>"},{"location":"guides/paths/#3-configuration-files-lowest-priority","title":"3. Configuration Files (Lowest Priority)","text":"<p>Specify paths in your <code>config.toml</code> file:</p> <pre><code>[default_dirs]\ndarts_dir = \"/path/to/data\"\nfast_dir = \"/path/to/fast/storage\"\nlarge_dir = \"/path/to/large/storage\"\n\n# Or override specific directories\noutput_data_dir = \"/custom/output\"\narcticdem_dir = \"/custom/arcticdem\"\n</code></pre> <p>Then run with the config file:</p> <pre><code>darts --config-file config.toml inference planet-sequential ...\n</code></pre>"},{"location":"guides/paths/#4-default-behavior-no-configuration","title":"4. Default Behavior (No Configuration)","text":"<p>If no paths are configured, DARTS uses the current working directory for all storage locations.</p>"},{"location":"guides/paths/#path-resolution-priority","title":"Path Resolution Priority","text":"<p>When DARTS resolves paths, it follows this priority:</p> <ol> <li>Explicit CLI flag (e.g., <code>--output-data-dir</code>)</li> <li>Environment variable (e.g., <code>DARTS_LARGE_DATA_DIR</code>)</li> <li>Config file value</li> <li>Current working directory</li> </ol> <p>For <code>fast_dir</code> and <code>large_dir</code>:</p> <ul> <li>If not set, they default to <code>darts_dir</code></li> <li>If <code>darts_dir</code> is not set, they default to the current working directory</li> </ul>"},{"location":"guides/paths/#common-usage-examples","title":"Common Usage Examples","text":""},{"location":"guides/paths/#example-1-simple-setup-same-location","title":"Example 1: Simple Setup (Same Location)","text":"<p>Use the current directory for everything:</p> <pre><code>cd /data/darts-project\ndarts inference planet-sequential --image-ids IMG_001 IMG_002\n</code></pre>"},{"location":"guides/paths/#example-2-split-fastlarge-storage","title":"Example 2: Split Fast/Large Storage","text":"<p>Store training data and models on SSD, everything else on HDD:</p> <pre><code>darts inference sentinel2-sequential \\\n  --default-dirs.fast-dir /ssd/darts-fast \\\n  --default-dirs.large-dir /hdd/darts-large \\\n  --tile-ids 33UXP 33UYP\n</code></pre>"},{"location":"guides/paths/#example-3-custom-output-location","title":"Example 3: Custom Output Location","text":"<p>Override just the output directory:</p> <pre><code>darts inference planet-sequential \\\n  --output-data-dir /project/results \\\n  --image-ids IMG_001\n</code></pre>"},{"location":"guides/paths/#example-4-shared-auxiliary-data","title":"Example 4: Shared Auxiliary Data","text":"<p>Use shared auxiliary data with local outputs:</p> <pre><code>darts inference sentinel2-sequential \\\n  --default-dirs.large-dir /local/output \\\n  --arcticdem-dir /shared/arcticdem_10m.icechunk \\\n  --tcvis-dir /shared/tcvis.icechunk \\\n  --tile-ids 33UXP\n</code></pre>"},{"location":"guides/paths/#example-5-using-environment-variables","title":"Example 5: Using Environment Variables","text":"<p>Set up your environment once:</p> <pre><code>export DARTS_FAST_DATA_DIR=/ssd/darts\nexport DARTS_LARGE_DATA_DIR=/hdd/darts\n\n# All subsequent commands use these paths\ndarts inference planet-sequential --image-ids IMG_001\ndarts training train-smp --config train.toml\n</code></pre>"},{"location":"guides/paths/#example-6-configuration-file-for-projects","title":"Example 6: Configuration File for Projects","text":"<p>Create a project-specific <code>config.toml</code>:</p> <pre><code>[default_dirs]\nfast_dir = \"/project/fast-storage\"\nlarge_dir = \"/project/large-storage\"\n\n[pipeline]\noutput_data_dir = \"/project/results\"\noverwrite = false\noffline = true\n</code></pre> <p>Run with:</p> <pre><code>darts --config-file project-config.toml inference planet-sequential --image-ids IMG_001\n</code></pre>"},{"location":"guides/paths/#debugging-paths","title":"Debugging Paths","text":"<p>To see which paths DARTS is using, use the <code>debug-paths</code> command:</p> <pre><code># Check default paths\ndarts debug-paths\n\n# Check paths with custom settings\ndarts debug-paths --default-dirs.fast-dir /ssd/darts --default-dirs.large-dir /hdd/darts\n\n# See what environment variables would set\nexport DARTS_FAST_DATA_DIR=/ssd/darts\ndarts debug-paths\n</code></pre> <p>This will print all resolved paths:</p> <ul> <li>Fast Directory</li> <li>Large Directory  </li> <li>Auxiliary Directory</li> <li>Artifacts Directory</li> <li>Training Directory</li> <li>Cache Directory</li> <li>Logs Directory</li> <li>Output Directory</li> <li>Models Directory</li> <li>Input Directory</li> <li>Archive Directory</li> <li>And all subdirectories</li> </ul>"},{"location":"guides/paths/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Environment Variables for Persistent Settings: Set <code>DARTS_FAST_DATA_DIR</code> and <code>DARTS_LARGE_DATA_DIR</code> in your shell profile for consistent paths across sessions.</p> </li> <li> <p>Use Config Files for Projects: Create project-specific config files that team members can share.</p> </li> <li> <p>Use CLI Flags for One-Off Changes: Override paths temporarily without changing your environment or config files.</p> </li> <li> <p>Keep Auxiliary Data Centralized: Store ArcticDEM and TCVis in a shared location to avoid duplicating large datasets.</p> </li> <li> <p>Organize by Project: Structure your large storage with project subdirectories:</p> </li> </ol> <pre><code>/large/darts/\n\u251c\u2500\u2500 project-A/output/\n\u251c\u2500\u2500 project-B/output/\n\u2514\u2500\u2500 shared/aux/\n</code></pre> <ol> <li>Check Before Large Runs: Use <code>darts debug-paths</code> to verify your configuration before starting large processing jobs.</li> </ol>"},{"location":"guides/paths/#pipeline-specific-considerations","title":"Pipeline-Specific Considerations","text":""},{"location":"guides/paths/#training-pipelines","title":"Training Pipelines","text":"<p>Training data is automatically organized by pipeline and patch size:</p> <ul> <li><code>{fast_dir}/training/{pipeline}_{patch_size}/</code></li> </ul> <p>Example: <code>training/planet_256/</code> for Planet pipeline with 256x256 patches.</p>"},{"location":"guides/paths/#inference-pipelines","title":"Inference Pipelines","text":"<ul> <li>Input Data: Automatically discovered in <code>{large_dir}/input/</code></li> <li>Models: Automatically discovered in <code>{fast_dir}/models/</code> (all *.pt files)</li> <li>Output: Written to <code>{large_dir}/output/</code> by default</li> </ul>"},{"location":"guides/paths/#data-preparation","title":"Data Preparation","text":"<p>Use the <code>prep-data</code> commands to download data for offline use:</p> <pre><code># Prepare optical and auxiliary data\ndarts inference prep-data planet \\\n  --default-dirs.large-dir /hdd/darts \\\n  --pipeline.image-ids IMG_001 IMG_002 \\\n  --aux\n\n# Prepare only optical data\ndarts inference prep-data sentinel2 \\\n  --pipeline.tile-ids 33UXP \\\n  --optical\n</code></pre>"},{"location":"guides/paths/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/paths/#issue-cannot-find-models","title":"Issue: \"Cannot find models\"","text":"<p>Solution: Ensure model files (*.pt) are in <code>{fast_dir}/models/</code> or specify explicitly:</p> <pre><code>--model-files /path/to/model.pt\n</code></pre>"},{"location":"guides/paths/#issue-permission-denied","title":"Issue: \"Permission denied\"","text":"<p>Solution: Check directory permissions and ensure the user has write access:</p> <pre><code>chmod -R u+w /path/to/darts-data\n</code></pre>"},{"location":"guides/paths/#issue-disk-full-during-training","title":"Issue: \"Disk full\" during training","text":"<p>Solution: Ensure fast storage has sufficient space, or redirect training data:</p> <pre><code>--default-dirs.fast-dir /larger/drive/darts-fast\n</code></pre>"},{"location":"guides/paths/#issue-path-not-found","title":"Issue: \"Path not found\"","text":"<p>Solution: DARTS creates directories automatically, but parent directories must exist:</p> <pre><code>mkdir -p /parent/path\ndarts inference ... --default-dirs.large-dir /parent/path/darts-large\n</code></pre>"},{"location":"guides/paths/#programmatic-usage","title":"Programmatic Usage","text":"<p>When using DARTS as a Python library:</p> <pre><code>from darts_utils.paths import paths, DefaultPaths\n\n# Set paths programmatically\npaths.set_defaults(DefaultPaths(\n    fast_dir=\"/ssd/darts\",\n    large_dir=\"/hdd/darts\"\n))\n\n# Access paths\nmodel_dir = paths.models\noutput_dir = paths.out\narcticdem_10m = paths.arcticdem(10)\n\n# Use in your code\nmy_output = output_dir / \"my_results\"\n</code></pre>"},{"location":"guides/pipeline-v2/","title":"Pipeline v2","text":""},{"location":"guides/pipeline-v2/#pipeline-v2","title":"Pipeline v2","text":"<p>The following document describes the pipeline which lead to the DARTS v1 dataset and will potentially lead to the DARTS v2 dataset. The orginial v1 dataset was not created with this repository, however, a newer, faster version of this pipeline is implemented here, which still uses the exact same pipeline-steps. Hence, it should be possible to re-create the DARTS v1 dataset with this repository. The implemented pipeline in this repository could potentially be used for future iterations and releases of the DARTS dataset.</p> <p>In addition to the PLANET version of the DARTS dataset, the pipeline also supports Sentinel 2 imagery as optical input, resulting in a lower spatial resolution (10m instead of 3m).</p> <p>Note</p> <p>The v1 / v2 pipeline is also aliased by <code>legacy</code> pipeline somewhere deep in the code.</p> <p>As of right now, four basic realizations of the v2 pipeline are implemented:</p> <ul> <li><code>darts inference sentinel2-sequential</code></li> <li><code>darts inference planet-sequential</code></li> <li><code>darts inference sentinel2-ray</code></li> <li><code>darts inference planet-ray</code></li> </ul> <p>The <code>sequential</code> variants run without any parallelization framework, while the <code>ray</code> variants use Ray for distributed computing.</p> <p>The pipeline currently consists of the following steps:</p> <ol> <li>Load the optical and auxiliary data     This step depends on the realization of the pipeline.     Either darts_acquisition.load_planet_scene, darts_acquisition.load_gee_s2_sr_scene or darts_acquisition.load_cdse_s2_sr_scene.     Also loads the masks if not loaded from GEE or CDSE: darts_acquisition.load_planet_masks, for the GEE and CDSE versions the masks are already included.     For the auxiliary data: darts_acquisition.load_arcticdem and darts_acquisition.load_tcvis</li> <li>Preprocess the optical data: darts_preprocessing.preprocess_v2.</li> <li>Segment the optical data: darts_ensemble.EnsembleV1.segment_tile.</li> <li>Postprocess the segmentation and make it ready for export: darts_postprocessing.prepare_export.</li> <li>Export the data: darts_export.export_tile.</li> </ol> <p></p> <p>A very simplified version of this implementation looks like this:</p> <pre><code>from darts_acquisition import load_arcticdem, load_tcvis\nfrom darts_acquisition.s2 import load_gee_s2_sr_scene\nfrom darts_ensemble import EnsembleV1\nfrom darts_export import export_tile\nfrom darts_postprocessing import prepare_export\nfrom darts_preprocessing import preprocess_v2\n\ns2id = \"20230701T194909_20230701T195350_T11XNA\"\narcticdem_dir = \"/path/to/arcticdem\"\ntcvis_dir = \"/path/to/tcvis\"\nmodel_files = {\n    \"model1\": \"/path/to/model1.pt\",\n    \"model2\": \"/path/to/model2.pt\",\n}\noutpath = \"/path/to/output\"\n\nensemble = EnsembleV1(model_files)\n\ntile = load_gee_s2_sr_scene(s2id)\n\narcticdem = load_arcticdem(\n    tile.odc.geobox,\n    arcticdem_dir,\n    resolution=10,\n    buffer=ceil(100 / 10 * sqrt(2)),\n)\n\ntcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n\ntile = preprocess_v2(tile, arcticdem, tcvis, tpi_outer_radius=100, tpi_inner_radius=0)\n\ntile = ensemble.segment_tile(tile, patch_size=1024, overlap=256, batch_size=8)\n\ntile = prepare_export(tile, bin_threshold=0.5, mask_erosion_size=10, min_object_size=32)\n\nexport_tile(tile, outpath)\n</code></pre> <p>Further reading</p> <p>To learn more about how the pipeline and their components steps work, please refer to the following materials:</p> <ul> <li>The Paper about the DARTS Dataset (No link yet)</li> <li>The Components Guide</li> <li>The API Reference</li> </ul> <p>There are further features implemented, which do not come from the components:</p> <ul> <li>Time tracking of processing steps (using <code>stopuhr</code> and <code>Chronometer</code>)</li> <li>Skipping of already processed tiles (via <code>missing_outputs</code> check)</li> <li>Environment debugging info (CUDA device selection and information)</li> <li>Result tracking (outputs saved to parquet files with status and errors)</li> <li>Ensemble support (using multiple models for improved predictions)</li> <li>Offline mode (for processing without internet connection)</li> <li>Configurable export bands (probabilities, binarized, polygonized, extent, thumbnail, optical, dem, tcvis, metadata)</li> </ul>"},{"location":"guides/pipeline-v2/#minimal-configuration-example","title":"Minimal configuration example","text":"<p>For Sentinel-2 processing:</p> <pre><code>[darts]\nee-project = \"ee-tobias-hoelzer\"\nmodel-files = [\"./models/s2-tcvis-final-large_2025-02-12.ckpt\"]\naoi-file = \"./data/myaoi.gpkg\"\nstart-date = \"2024-07-01\"\nend-date = \"2024-09-30\"\n</code></pre> <p>Or using scene IDs directly:</p> <pre><code>[darts.inference.sentinel2-sequential]\nmodel-files = [\"./models/model1.pt\", \"./models/model2.pt\"]\nscene-ids = [\"20230701T194909_20230701T195350_T11XNA\", \"20230704T195909_20230704T200350_T11XNA\"]\noutput-data-dir = \"./output\"\n</code></pre>"},{"location":"guides/pipeline-v2/#full-configuration-explanation","title":"Full configuration explanation","text":"<p>The pipeline can be configured via command-line arguments or a TOML configuration file. All parameters from the CLI help output are available as configuration options.</p>"},{"location":"guides/pipeline-v2/#core-parameters","title":"Core Parameters","text":"<ul> <li>model-files: List of model file paths for ensemble segmentation. If a single model is provided, ensemble features are disabled.</li> <li>output-data-dir: Directory where processed outputs will be saved.</li> <li>arcticdem-dir: Directory containing ArcticDEM datacube (auto-downloaded if missing).</li> <li>tcvis-dir: Directory containing TCVis data.</li> <li>device: Computing device (<code>\"cuda\"</code>, <code>\"cpu\"</code>, <code>\"auto\"</code>, or specific GPU index).</li> </ul>"},{"location":"guides/pipeline-v2/#scene-selection-sentinel-2","title":"Scene Selection (Sentinel-2)","text":"<p>Four mutually exclusive methods:</p> <ol> <li>scene-ids: Direct list of Sentinel-2 scene IDs</li> <li>scene-id-file: Path to file containing scene IDs (one per line)</li> <li>tile-ids: List of Sentinel-2 tile IDs + filtering parameters</li> <li>aoi-file: Shapefile with area of interest + filtering parameters</li> </ol>"},{"location":"guides/pipeline-v2/#filtering-parameters","title":"Filtering Parameters","text":"<ul> <li>start-date / end-date: Date range in YYYY-MM-DD format</li> <li>max-cloud-cover: Maximum cloud cover percentage (default: 10)</li> <li>max-snow-cover: Maximum snow cover percentage (default: 10)</li> <li>months: List of months (1-12) for filtering</li> <li>years: List of years for filtering</li> </ul>"},{"location":"guides/pipeline-v2/#processing-parameters","title":"Processing Parameters","text":"<ul> <li>tpi-outer-radius: Outer radius for TPI calculation in meters (default: 100)</li> <li>tpi-inner-radius: Inner radius for TPI calculation in meters (default: 0)</li> <li>patch-size: Tile size for inference (default: 1024)</li> <li>overlap: Overlap between patches (default: 256)</li> <li>batch-size: Batch size for inference (default: 8)</li> <li>reflection: Reflection padding for inference (default: 0)</li> </ul>"},{"location":"guides/pipeline-v2/#postprocessing-parameters","title":"Postprocessing Parameters","text":"<ul> <li>binarization-threshold: Threshold for converting probabilities to binary (default: 0.5)</li> <li>mask-erosion-size: Size of disk for mask erosion (default: 10)</li> <li>edge-erosion-size: Size for edge cropping, defaults to mask-erosion-size</li> <li>min-object-size: Minimum object size in pixels (default: 32)</li> <li>quality-level: Quality mask level: <code>\"high_quality\"</code>, <code>\"low_quality\"</code>, <code>\"none\"</code>, or int 0-2 (default: 1)</li> </ul>"},{"location":"guides/pipeline-v2/#export-parameters","title":"Export Parameters","text":"<ul> <li>export-bands: List of bands to export (default: <code>[\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"]</code>)</li> <li>Available: <code>\"probabilities\"</code>, <code>\"binarized\"</code>, <code>\"polygonized\"</code>, <code>\"extent\"</code>, <code>\"thumbnail\"</code>, <code>\"optical\"</code>, <code>\"dem\"</code>, <code>\"tcvis\"</code>, <code>\"metadata\"</code>, or specific band names</li> <li>write-model-outputs: Save individual model outputs in addition to ensemble (default: False)</li> </ul>"},{"location":"guides/pipeline-v2/#data-source-parameters-sentinel-2","title":"Data Source Parameters (Sentinel-2)","text":"<ul> <li>raw-data-source: Source for S2 data: <code>\"cdse\"</code> or <code>\"gee\"</code> (default: <code>\"cdse\"</code>)</li> <li>raw-data-store: Directory for storing raw S2 data locally</li> <li>no-raw-data-store: Disable local storage of raw data (default: False)</li> <li>ee-project: Earth Engine project ID (required for GEE source)</li> <li>ee-use-highvolume: Use EE high-volume server (default: True)</li> </ul>"},{"location":"guides/pipeline-v2/#operational-flags","title":"Operational Flags","text":"<ul> <li>overwrite: Overwrite existing outputs (default: False)</li> <li>offline: Run without downloading data (default: False)</li> <li>debug-data: Write intermediate debug data (default: False)</li> </ul>"},{"location":"guides/pipeline-v2/#usage-examples","title":"Usage Examples","text":""},{"location":"guides/pipeline-v2/#command-line","title":"Command Line","text":"<pre><code># Using an AOI file with date filtering\ndarts inference sentinel2-sequential \\\n    --aoi-file ./data/myaoi.gpkg \\\n    --start-date 2024-07-01 \\\n    --end-date 2024-09-30 \\\n    --model-files ./models/model1.pt ./models/model2.pt \\\n    --output-data-dir ./output\n\n# Using specific scene IDs\ndarts inference sentinel2-sequential \\\n    --scene-ids 20230701T194909_20230701T195350_T11XNA \\\n    --model-files ./models/model.pt\n\n# Planet pipeline\ndarts inference planet-sequential \\\n    --orthotiles-dir ./data/planet/orthotiles \\\n    --scenes-dir ./data/planet/scenes \\\n    --model-files ./models/planet_model.pt\n</code></pre>"},{"location":"guides/pipeline-v2/#configuration-file","title":"Configuration File","text":"<p>Create a <code>config.toml</code>:</p> <pre><code>[darts.inference.sentinel2-sequential]\naoi-file = \"./data/myaoi.gpkg\"\nstart-date = \"2024-07-01\"\nend-date = \"2024-09-30\"\nmax-cloud-cover = 15\nmax-snow-cover = 5\nmodel-files = [\"./models/model1.pt\", \"./models/model2.pt\"]\noutput-data-dir = \"./output\"\npatch-size = 1024\noverlap = 256\nbatch-size = 8\nexport-bands = [\"probabilities\", \"binarized\", \"polygonized\", \"thumbnail\"]\noverwrite = false\n</code></pre> <p>Run with:</p> <pre><code>darts --config-file config.toml inference sentinel2-sequential\n</code></pre>"},{"location":"guides/pipeline-v2/#offline-processing","title":"Offline Processing","text":"<p>First, prepare data:</p> <pre><code>darts inference prep-data sentinel2 \\\n    --aoi-file ./data/myaoi.gpkg \\\n    --start-date 2024-07-01 \\\n    --end-date 2024-09-30 \\\n    --raw-data-store ./raw_data \\\n    --sentinel2-grid-dir ./aux_data/s2_grid\n</code></pre> <p>Then run offline:</p> <pre><code>darts inference sentinel2-sequential \\\n    --offline \\\n    --raw-data-store ./raw_data \\\n    --prep-data-scene-id-file ./raw_data/scene_ids.txt \\\n    --model-files ./models/model.pt\n</code></pre>"},{"location":"guides/training/","title":"How a model is born","text":""},{"location":"guides/training/#how-a-model-is-born","title":"How a model is born","text":"<p>Recommendations &amp; best practices</p> <p>It is recommended to use a terminal multiplexer like <code>tmux</code>, <code>screen</code> or <code>zellij</code> to run multiple training runs in parallel. This way there is no need to have multiple terminal open over the span of multiple days.</p> <p>Using the folding method <code>\"region-stratified\"</code> enables more data-efficient training because the test split can then be set to random.</p> <p>Multi-scoring strategy should be <code>\"geometric\"</code> for combinations of recall, precision, f1 and jaccard index / iou, else <code>\"harmonic\"</code>.</p> <p>To do a super quick tutorial to get you started go the quickstart guide.</p> <p>The \"model creation process\" (training) is implemented as a three-level hierarchy, meaning the upper level does several calls to the level below it:</p> <ol> <li>Tuning (Hyperparameter-Sweeps)</li> <li>Cross-Validation</li> <li>(Training-) Run</li> </ol> <p></p>"},{"location":"guides/training/#artifacts-and-naming","title":"Artifacts and Naming","text":"<p>Training artefacts are stored in an organized way so that one does not get lost in 1000s of different directories. Especially when tuning hyperparameters, a lot of different runs are created, which can be difficult to track.</p> <p>For organisation, each tune, cv and training run has it's own name, which can be provided manually, but is usually generated automatically. The name can only be provided manually for the call-level - hence when tuning one can only provide the name for the tune, respective cross-validations and training runs are named automatically based on the provided name. If no name is provided, a random, but human-readable name is generated. Further, a random 8-character id is also generated for each run, primarily for tracking purposes with Weights &amp; Biases.</p> <p>The naming scheme is as follows:</p> <ul> <li><code>tune_name</code>: automatically generated or provided</li> <li><code>cv_name</code>: <code>{tune_name}-cv{hp_index}</code> if called by tune, else automatically generated or provided</li> <li><code>run_name</code>: <code>{cv_name}-run-f{fold_index}s{seed}</code> if called by cross-validation (or indirect tune), else automatically generated or provided</li> <li><code>run_id</code>: 8-character id</li> </ul> <p>Artifacts are stored in the following hierarchy:</p> <ul> <li>Created by runs of tunes: <code>{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}</code></li> <li>Created by runs of cross-validations: <code>{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}</code></li> <li>Created by single runs: <code>{artifact_dir}/_runs/{run_name}-{run_id}</code></li> </ul> <p>This way, the top-level <code>artifact_dir</code> is kept clean and organized.</p> <p>Local vs. WandB</p> <p>The training uses a local directory for storing the artifacts, such as metrics or the model. The final directory where these artifacts is always called <code>{run_name}-{run_id}</code>. This way, it should be easy to relate which artifacts belong to which run in wandb, where the url of a run is always <code>https://wandb.ai/{wandb_entity}/{wandb_project}/runs/{run_id}</code>.</p> <p>The cross-validation will not only contain the artifacts from the training runs but also a <code>run_infos.parquet</code> file with information about each run / experiment. This dataframe contains a <code>fold</code>, <code>seed</code>, <code>duration</code>, <code>checkpoint</code>, <code>is_unstable</code>, <code>is_unstable</code> and metrics columns, where the metrics are taken from the <code>trainer.callback_metrics</code>. The <code>is_unstable</code> column indicates whether the score-metrics of the run were unstable (not finite or zero). Further, it also contains a <code>score</code> and a <code>score_is_unstable</code> column, which contains the score and a boolean indicating whether any run of the cross-validation was unstable. These columns contain the same value for every row (run), since they are valid for the complete cross-validation.</p> <p>Weights &amp; Biases is optionally used for further tracking and logging. <code>wandb_project</code> and <code>wandb_entity</code> can be used to specify the project and entity for logging. Wandb will create a run <code>run_id</code> named <code>{run_name}</code>, meaning the id can be used to directly access the run via link and the name can be used for searching a run. For cross-validation and tuning <code>cv_name</code> and <code>tune_name</code> are set as <code>job_type</code> and <code>group</code> to emulate sweeps. This is a workaround and could potentially fixed if wandb will update their client library to allow the manual creation of sweeps.</p>"},{"location":"guides/training/#specifying-the-devices","title":"Specifying the devices","text":"<p>PyTorch Lightning supports different strategies for training on different devices and accelerators. These can be specified by the <code>--accelerator</code>, <code>--strategy</code>, <code>--devices</code> and <code>--num_nodes</code> parameters which are forwarded by the training scripts to the Lightning Trainer. The default values for these parameters are all <code>\"auto\"</code>, except for <code>--num_nodes</code>, which defaults to <code>1</code>. This means, that if no values are provided, Lightning will automatically detect the available devices and use one of them for training. Because of the limitation of the CLI regarding unions of lists and int/str, devices must always be a list.</p> <p>Here are some configurations for common scenarios:</p> Szenario <code>accelerator</code> <code>strategy</code> <code>devices</code> <code>num_nodes</code> Single GPU <code>\"gpu\"</code> <code>\"auto\"</code> (default) <code>[\"auto\"]</code> (default) <code>1</code> (default) Single GPU on Mac <code>\"mpu\"</code> <code>\"auto\"</code> (default) <code>[\"auto\"]</code> (default) <code>1</code> (default) DDP with 4 specified GPUs <code>\"gpu\"</code> <code>\"ddp_fork\"</code> <code>[0, 2, 5, 6]</code> <code>1</code> (default) <p>Please refer to the documentation of PyTorch Lightning:</p> <ul> <li>Trainer API</li> <li>Strategies</li> <li>DDP Example</li> </ul> <p>For the cross-validation and tuning script, two other <code>--strategy</code> options apart from the ones provided by PyTorch Lightning can be specified, which defines how training runs are executed in parallel. In this scenario, instead of running a single training run on multiple devices, multiple training runs are executed in parallel across multiple devices. Note that it is not possible to use any distribbuted strategy like DDP in this case.</p> <ul> <li><code>\"cv-parallel\"</code>: This strategy will run the training runs of a cross-validation in parallel.</li> <li><code>\"tune-parallel\"</code>: This strategy will run the cross-validations of a tune in parallel. In this scenario, the training runs of a cross-validation will be executed in series.</li> </ul> <p>DDP with parallel tuning or cross-validation</p> <p>When running multiple processes in parallel, normal Distributed Data Parallel (DDP) can not be used, since it will call the complete script multiple times. Thus, e.g. for tuning, multiple tunes would be created, which is not intended. Hence, the cross-validation and tuning script disable the DDP strategy by default and use instead the <code>\"ddp_fork\"</code> strategy if more than one <code>\"num_nodes\"</code> or <code>\"devices\"</code> is specified.</p>"},{"location":"guides/training/cv/","title":"Cross Validation","text":""},{"location":"guides/training/cv/#cross-validation","title":"Cross-Validation","text":"<pre><code>[uv run] darts training crossval-smp ...\n</code></pre>"},{"location":"guides/training/cv/#fold-strategies","title":"Fold strategies","text":"<p>While cross-validating, the data can further be split into a training and validation set. One can specify the fraction of the validation set by providing an integer to <code>total_folds</code>. Higher values will result in smaller, validation sets and therefore more fold-combinations. To reduce the number of folds actually run, one can provide the <code>n_folds</code> parameter to limit the number of folds actually run. Thus, some folds will be skipped. The \"folding\" is based on <code>scikit-learn</code> and currently supports the following folding methods, which can be specified by the <code>fold_method</code> parameter:</p> <ul> <li><code>\"kfold\"</code>: Split the data into <code>total_folds</code> folds, where each fold can be used as a validation set. Uses sklearn.model_selection.KFold.</li> <li><code>\"stratified\"</code>: Will use the <code>\"empty\"</code> column of the metadata to create <code>total_folds</code> shuffled folds where each fold contains the same amount of empty and non-empty samples. Uses sklearn.model_selection.StratifiedKFold.</li> <li><code>\"shuffle\"</code>: Similar to <code>\"stratified\"</code>, but the order of the data is shuffled before splitting. Uses sklearn.model_selection.StratifiedShuffleSplit.</li> <li><code>\"region\"</code>: Will use the <code>\"region\"</code> column of the metadata to create <code>total_folds</code> folds where each fold splits the data by one or multiple regions. Uses sklearn.model_selection.GroupShuffleSplit.</li> <li><code>\"region-stratified\"</code>: Merge of the <code>\"region\"</code> and <code>\"stratified\"</code> methods. Uses sklearn.model_selection.StratifiedGroupKFold.</li> </ul> <p>Even in normal training a single KFold split is used to split between training and validation. This can be disabled by setting <code>fold_method</code> to <code>None</code>. In such cases, the validation set becomes equal to the training set, meaning longer validation time and the metrics are always calculated on seen data. This is useful for e.g. the final training of a model before deployment.</p> Using DartsDataModule <p>The data splitting is implemented by the darts_segmentation.training.data.DartsDataModule and can therefore be used in other settings as well.</p>"},{"location":"guides/training/cv/#scoring-strategies","title":"Scoring strategies","text":"<p>To turn the information (metrics) gathered of a single cross-validation into a useful score, we need to somehow aggregate the metrics. In cases we are only interested in a single metric, this is easy: we can easily compute the mean. This metric can be specified by the <code>scoring_metric</code> parameter of the cross validation. It is also possible to use multiple metrics by specifying a list of metrics in the <code>scoring_metric</code> parameter. This, however, makes it a little more complicated.</p> <p>Multi-metric scoring is implemented as combine-then-reduce, meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. The combining strategy can be specified by the <code>multi_score_strategy</code> parameter. As of now, there are four strategies implemented: <code>\"arithmetic\"</code>, <code>\"geometric\"</code>, <code>\"harmonic\"</code> and <code>\"min\"</code>.</p> <p>The following visualization should help visualize how the different strategies work. Note that the loss is interpreted as \"lower is better\" and has also a broader range of possible values, exceeding 1. For the multi-metric scoring with IoU and Loss the arithmetic and geometric strategies are very instable. The scores for very low loss values where so high that the scores needed to be clipped to the range [0, 1] for the visualization to be able to show the behaviour of these strategies. However, especially the geometric mean shows a smoother curve than the harmonic mean for the multi-metric scoring with IoU and Recall. This should show that the strategy should be chosen carefully and in respect to the metrics used.</p> IoU &amp; Loss IoU &amp; Recall Code to reproduce the visualization <p>If you are unsure which strategy to use, you can use this code snippet to make a visualization based on your metrics:</p> <pre><code>import numpy as np\nimport xarray as xr\n\na = np.arange(0, 1, 0.01)\na = xr.DataArray(a, dims=[\"a\"], coords={\"a\": a})\n# 1 / ... indicates \"lower is better\" - replace it if needed\nb = np.arange(0, 2, 0.01)\nb = 1 / xr.DataArray(b, dims=[\"b\"], coords={\"b\": b})\n\ndef viz_strategies(a, b):\n    harmonic = 2 / (1 / a + 1 / b)\n    geometric = np.sqrt(a * b)\n    arithmetic = (a + b) / 2\n    minimum = np.minimum(a, b)\n\n    harmonic = harmonic.rename(\"harmonic mean\")\n    geometric = geometric.rename(\"geometric mean\")\n    arithmetic = arithmetic.rename(\"arithmetic mean\")\n    minimum = minimum.rename(\"minimum\")\n\n    fig, axs = plt.subplots(1, 4, figsize=(25, 5))\n    axs = axs.flatten()\n    harmonic.plot(ax=axs[0])\n    axs[0].set_title(\"Harmonic\")\n    geometric.plot(ax=axs[1], vmax=min(geometric.max(), 1))\n    axs[1].set_title(\"Geometric\")\n    arithmetic.plot(ax=axs[2], vmax=min(arithmetic.max(), 1))\n    axs[2].set_title(\"Arithmetic\")\n    minimum.plot(ax=axs[3])\n    axs[3].set_title(\"Minimum\")\n    return fig\n\nviz_strategies(a, b).show()\n</code></pre> <p>Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>Available metrics</p> <p>The following metrics are visible to the scoring function:</p> <ul> <li><code>'train/time'</code></li> <li><code>'train/device/batches_per_second'</code></li> <li><code>'train/device/samples_per_second'</code></li> <li><code>'train/device/flops_per_second'</code></li> <li><code>'train/device/mfu'</code></li> <li><code>'train/loss'</code></li> <li><code>'train/Accuracy'</code></li> <li><code>'train/CohenKappa'</code></li> <li><code>'train/F1Score'</code></li> <li><code>'train/HammingDistance'</code></li> <li><code>'train/JaccardIndex'</code></li> <li><code>'train/Precision'</code></li> <li><code>'train/Recall'</code></li> <li><code>'train/Specificity'</code></li> <li><code>'val/loss'</code></li> <li><code>'val/Accuracy'</code></li> <li><code>'val/CohenKappa'</code></li> <li><code>'val/F1Score'</code></li> <li><code>'val/HammingDistance'</code></li> <li><code>'val/JaccardIndex'</code></li> <li><code>'val/Precision'</code></li> <li><code>'val/Recall'</code></li> <li><code>'val/Specificity'</code></li> <li><code>'val/AUROC'</code></li> <li><code>'val/AveragePrecision'</code></li> </ul> <p>These are derived from <code>trainer.logged_metrics</code>.</p>"},{"location":"guides/training/cv/#random-state","title":"Random-state","text":"<p>All random state of the tuning and the cross-validation is seeded to 42. Random state of the training can be specified through a parameter. The cross-validation will not only cross-validates along different folds but also over different random seeds. Thus, for a single cross-validation with 5 folds and 3 seeds, 15 runs will be executed.</p>"},{"location":"guides/training/data/","title":"Preprocessing","text":""},{"location":"guides/training/data/#preprocessing","title":"Preprocessing","text":"<p>To preprocess planet data into the necessary structure, you can use the following command:</p> <pre><code>[uv run] darts training create-dataset planet --your-args-here ...\n</code></pre> <p>This will run the v2 preprocessing used by the v2 segmentation pipeline, but instead of passing the preprocessed data it creates patches of a fixed size and stores them into the <code>data.zarr</code> array. Further, it will also create the necessary metadata, config and labels files.</p> <p>The final train data is saved to disk in form of zarr arrays with dimensions <code>[n, c, h, w]</code> and <code>[n, h, w]</code> for the labels respectivly, with chunksizes of <code>n=1</code>. Hence, every sample is saved in a separate chunk and therefore in a seperate file on disk, but all managed by zarr.</p> <p>The preprocessing is done with the same components used in the v2 segmentation pipeline. Hence, the same configuration options are available.</p> You can also use the underlying function directly: <p>darts.training.preprocess_planet_train_data</p>"},{"location":"guides/training/data/#training-data-structure","title":"Training data structure","text":"<p>The <code>train_data_dir</code> must look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2514\u2500\u2500 metadata.parquet\n</code></pre> <p>The <code>data.zarr</code> is expected to be a zarr group containing a <code>x</code> and a <code>y</code> zarr array. This data contains the training data, hence images and labels - the shape of the <code>x</code> array must be <code>[n, c, h, w]</code> and of <code>y</code> <code>[n, h, w]</code> with <code>n</code> beeing the total number of samples, <code>c</code> the number of channels, <code>h</code> the height and <code>w</code> the width. The data must be complete, since splits and cross-validation folds are created on-the-fly by the training scripts.</p> <p><code>metadata.parquet</code> must contain at least the following columns:</p> <ul> <li>\"sample_id\": The id of the sample</li> <li>\"region\": The region the sample belongs to</li> <li>\"empty\": Whether the image has positive pixels The index must refer to the index of the sample in the zarr data.</li> </ul> <p><code>config.toml</code> must contain all configuration parameters necessary to preprocess the data from scratch. Further, it must contain information about the bands (channels) used, which will be used to setup the model. The configuration should lay under the <code>darts</code> key of the file.</p> <p>Ideally, use the preprocessing functions explained above to create this structure.</p>"},{"location":"guides/training/data/#own-preprocessing","title":"Own preprocessing","text":"<p>If you want to use your own preprocessing, you can do so by either creating the data yourself or by using the darts_segmentation.training.prepare_training.TrainDatasetBuilder helper class.</p> <p>The usage is quite simple:</p> <pre><code>from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_segmentation.utils import Bands\n\nbands = Bands.from_dict(\n    {\n        # \"name\": (scale, offset)\n        \"red\": (1 / 3000, 0.0),\n        \"green\": (1 / 3000, 0.0),\n        \"blue\": (1 / 3000, 0.0),\n    }\n)\n\nbuilder = TrainDatasetBuilder(\n    train_data_dir=train_data_dir,  # The directory where the data is stored\n    patch_size=patch_size, # The size of the patches to create\n    overlap=overlap, # The overlap of the patches\n    bands=bands, # The bands to use, expects the Bands class\n    exclude_nopositive=exclude_nopositive, # Whether to exclude samples without positive pixels\n    exclude_nan=exclude_nan, # Whether to exclude samples with NaN values\n    mask_erosion_size=mask_erosion_size, # How much the mask should be eroded (increase size of nan-holes)\n    device=device, # The device to use for preprocessing should be a torch.device\n)\n\nfor tile, labels, region in your_data:\n    # tile (your sample) should be an xarray Dataset with at least the bands specified above as variables\n    # labels should be an geopandas GeoDataFrame\n    builder.add_tile(\n        tile=tile,\n        labels=labels,\n        region=region, # Region should be a str describing the region of your sample, can be arbitrary\n        sample_id=sample_id, # A unique id for the sample\n        metadata={\n            ... # Any metadata you want to store with the sample, not mandatory\n        },\n    )\n\nbuilder.finalize(\n    {\n        ... # Any metadata you want to store with in the final config, not mandatory\n    }\n)\n</code></pre>"},{"location":"guides/training/data/#about-patching","title":"About patching","text":"<p>The preprocessing will create patches of the images and labels to increase performance. Read performance is actually pretty important for training smaller models, since the time it takes for an image to be read from disk is in relation to the training step of smaller models significant. That is why we store the patches in a zarr array beforehand, so we gain maximum read performance.</p> <p>The information about the patches, meaning region, id, geometry etc., are stored in the metadata file.</p> <p>The following image shows an examples of how these patches look like when visualized. The data of the example comes from a single year - hence all labels are applied to all images. Of course one can write a pipeline matches labels to specific images, and in case the default pipeline works like this. However, this is an easier example to visualize the patches. Further, this example only covers a single region. Of course this can also be done for multiple regions and extents. Both examples show the extent in red in the background together with the labels in orange and the colored patches. The first example shows which patch comes from which image. The second example shows which patch is empty, meaning it has no positive pixels in the labels.</p> <p></p> <p></p>"},{"location":"guides/training/quickstart/","title":"Training Quickstart","text":""},{"location":"guides/training/quickstart/#quickstart-training","title":"Quickstart Training","text":"<p>In this tutorial, you should be able to quickly setup the training of a segmentation model on the PLANET data.</p>"},{"location":"guides/training/quickstart/#0-prereq","title":"0. Prereq","text":"<ul> <li>Make sure you have installed the package and all dependencies. See the installation guide for more information.</li> <li>Clone this repository to obtain the labels for the training data.</li> <li>Ask a maintainer for access to the PLANET training data.</li> </ul>"},{"location":"guides/training/quickstart/#1-setup-the-configuration-file","title":"1. Setup the configuration file","text":"<p>Copy this configuration file to your local machine, e.g. under <code>configs/planet-training-quickstart.toml</code>, and adjust</p> <ul> <li>the paths to your needs</li> <li>the account settings of earth engine and wandb</li> </ul> Configuration file configs/planet-training-quickstart.toml<pre><code>[darts.wandb] # (5)\nwandb-project = \"...\"\nwandb-entity = \"...\"\nee-project = \"...\"\n\n[darts.paths] # (3)\ndata-dir = \"/path/to/planet_data\"\narcticdem-dir = \"/path/to/data/datacubes/arcticdem2m.icechunk\"\ntcvis-dir = \"/path/to/data/datacubes/tcvis.icechunk\"\nadmin-dir = \"/path/to/data/aux/admin\"\npreprocess-cache = \"/path/to/data/cache\"\nartifact-dir = \"/path/to/artifacts\"\n\n[darts.training.paths] # (4)\nlabels-dir = \"/path/to/ML_training_labels/retrogressive_thaw_slumps\" # (1)\ntrain-data-dir = \"/path/to/data/training/planet_quickstart\" # (2)\n\n[darts.preprocess]\ntpi-outer-radius = 100\ntpi-inner-radius = 0\nmask-erosion-size = 3\n\n[darts.training]\ndevice = \"auto\"\nnum-workers = 16\nmax-epochs = 6\nlog-every-n-steps = 50\ncheck-val-every-n-epoch = 5\nplot-every-n-val-epochs = 4 # == 20 epochs\nearly-stopping-patience = 0\nbands = [\n    'blue',\n    'green',\n    'red',\n    'nir',\n    'ndvi',\n    'tc_brightness',\n    'tc_greenness',\n    'tc_wetness',\n    'relative_elevation',\n    'slope',\n]\nfold = 0\n\n[darts.test]\ndata-split-method = \"region\"\ndata-split-by = ['Taymyrsky Dolgano-Nenetsky District']\n\n[darts.training.preprocessing]\npatch-size = 896\noverlap = 224 # increase to 64 if exclude-nan = True\nexclude-nopositive = false\nexclude-nan = false\nforce-preprocess = false\n\n# Only used in cross-validation and tuning\n[darts.cross-validation]\nfold-method = \"region-stratified\"\ntotal-folds = 5\nn-folds = 2\nn-randoms = 1\nscoring-metric = [\"val/JaccardIndex\", \"val/Recall\"]\nmulti-score-strategy = \"geometric\"\n\n# Only used in training or cross-validation, not tuning\n[darts.training.hyperparameters]\nlearning-rate = 4e-4\nbatch-size = 6\ngamma = 0.999\nfocal-loss-alpha = 0.92\nfocal-loss-gamma = 1.6\nmodel-arch = \"UPerNet\"\nmodel-encoder = \"tu-maxvit_tiny_rw_224\"\naugment = [\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\"\n]\n\n# Only used for tuning\n[darts.tuning]\nhpconfig = \"configs/planet-training-quickstart.toml\" # link to this file for convinience\nn-trials = 10\n\n# Only used for tuning\n[hyperparameters]\nlearning-rate = {distribution = \"loguniform\", low = 1.0e-5, high = 1.0e-3}\nbatch-size = 6\ngamma = 0.997\nfocal-loss-alpha = {low = 0.8, high = 0.99}\nfocal-loss-gamma = {low = 0.0, high = 2.0}\nmodel-arch = [\"Unet\", \"MAnet\", \"UPerNet\", \"Segformer\"]\nmodel-encoder = [\"resnet50\", \"resnext50_32x4d\", \"mit_b2\", \"tu-convnextv2_tiny\", \"tu-maxvit_tiny_rw_224\"]\naugment = {distribution = \"constant\", value = [\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\"\n]}\n</code></pre> <ol> <li>This should point to the directory of the repository you cloned in step 2.</li> <li>The <code>train-data-dir</code> should point to a fast read-access storage, like a local mounted SSD to speed up the training process.</li> <li>Change these paths to your needs. I recommend to just change the \"/path/to/\" part to have everything in one place.</li> <li>These paths aswell.</li> <li>Change these to your account settings.</li> </ol>"},{"location":"guides/training/quickstart/#2-preprocess-the-data","title":"2. Preprocess the data","text":"<pre><code>[uv run] darts training create-dataset planet --config-file configs/planet-training-quickstart.toml\n</code></pre> <p>This will create the training data in the <code>train-data-dir</code> specified in the configuration file.</p> Take a look at the data <p>If the <code>preprocess-cache</code> directory is specified, the preprocessing will automatically cache the preprocessed data before it is turned into the training data format. You can visualize the data with xarray:</p> <pre><code>import xarray as xr\nfrom pathlib import Path\n\nfpath = list(Path(\"/path/to/data/cache/planet_v2\").glob(\"*.nc\"))[0]\ntile = xr.open_zarr(fpath, decode_coords=\"all\")\ntile\n</code></pre> <pre><code># Visualize the data (reduce the resolution for faster plotting)\ntile.red[::10, ::10].plot.imshow(cmap=\"Reds\")\n</code></pre> <p>To have a look at how the training data looks like, you can use <code>zarr</code> and <code>geopandas</code>:</p> <pre><code>import zarr\n\nzroot = zarr.open(\"/path/to/data/training/planet_quickstart/data.zarr\")\nzroot.tree()\n</code></pre> <pre><code>print(zroot[\"x\"].shape)\n</code></pre> <pre><code>import geopandas as gpd\n\nmetadata = gpd.read_parquet(\"/path/to/data/training/planet_quickstart/metadata.parquet\")\nmetadata.head()\n</code></pre> <pre><code>metadata.explore()\n</code></pre>"},{"location":"guides/training/quickstart/#3-train-the-model","title":"3. Train the model","text":"<pre><code>[uv run] darts training train-smp --config-file configs/planet-training-quickstart.toml\n</code></pre>"},{"location":"guides/training/quickstart/#4-test-the-model","title":"4. Test the model","text":"<pre><code>[uv run] darts training test-smp --config-file configs/planet-training-quickstart.toml\n</code></pre>"},{"location":"guides/training/quickstart/#5-do-a-cross-validation","title":"5. Do a cross-validation","text":"<p>This will take a while</p> <pre><code>[uv run] darts training crossval-smp --config-file configs/planet-training-quickstart.toml\n</code></pre>"},{"location":"guides/training/quickstart/#6-hyperparameter-tuning","title":"6. Hyperparameter tuning","text":"<p>This will take a while</p> <pre><code>[uv run] darts training tune-smp --config-file configs/planet-training-quickstart.toml\n</code></pre>"},{"location":"guides/training/training/","title":"Training","text":""},{"location":"guides/training/training/#training-binary-segmentation","title":"Training (Binary Segmentation)","text":"<p>To train a simple SMP (Segmentation Model Pytorch) model you can use the command:</p> <pre><code>[uv run] darts training train-smp --your-args-here ...\n</code></pre> <p>Model Architecture</p> <p>Configurations for the architecture and encoder can be found in the SMP documentation for model configurations.</p> <p>Change defaults</p> <p>Even though the defaults from the CLI are somewhat useful, it is recommended to create a config file and change the behavior of the training there.</p> <p>This command will train a simple SMP model on the data in the <code>train-data-dir</code> directory. The training relies on PyTorch Lightning, which is a high-level interface for PyTorch. It is recommended to use Weights and Biases (wandb) for the logging, because the training script is heavily influenced by how the organization of wandb works.</p> <p>The training follows the data splitting, decribed in the Data Guide and Cross-Validation Guide To test the model on the test split, you can use the following command:</p> <pre><code>[uv run] darts training test-smp --your-args-here ...\n</code></pre> You can also use the underlying functions directly: <p>darts_segmentation.training.train_smp darts_segmentation.training.test_smp</p>"},{"location":"guides/training/training/#data-splits","title":"Data splits","text":"<p>The initial training/test data split is performed at train/test time by using the <code>data_split_method</code> and <code>data_split_by</code> parameters. <code>data_split_method</code> can be one of the following:</p> <ul> <li><code>\"random\"</code> will split the data randomly, the seed is always 42 and the size of the test set can be specified by providing a list with a single float between 0 and 1 to <code>data_split_by</code>.</li> <li><code>\"region\"</code> will split the data by one or multiple regions, which can be specified by providing a str or list of str to <code>data_split_by</code>.</li> <li><code>\"sample\"</code> will split the data by sample ids, which can be specified similar to <code>\"region\"</code>.</li> <li><code>None</code>, no split is done and the complete dataset is used for both training and testing.</li> </ul>"},{"location":"guides/training/tune/","title":"Hyperparameter tuning","text":""},{"location":"guides/training/tune/#hyperparameter-tuning","title":"Hyperparameter tuning","text":"<p>With the tuning script hyperparameters can be tuned by running a sweep. The sweep uses cross-validation to evaluate the performance of a single hyperparameter configuration.</p> <pre><code>[uv run] darts training tune-smp ...\n</code></pre> Use the function <p>darts_segmentation.training.tune.tune_smp</p> <p>How the hyperparameters should be sweeped can be configured in a YAML or Toml file, specified by the <code>hpconfig</code> parameter. This file must contain a key called <code>\"hyperparameters\"</code> containing a list of hyperparameters distributions. These distributions can either be explicit defined by another dictionary containing a <code>\"distribution\"</code> key, or they can be implicit defined by a single value, a list or a dictionary containing a <code>\"low\"</code> and <code>\"high\"</code> key.</p> <p>The following distributions are supported:</p> <ul> <li><code>\"uniform\"</code>: Uniform distribution - must have a <code>\"low\"</code> and <code>\"high\"</code> value</li> <li><code>\"loguniform\"</code>: Log-uniform distribution - must have a <code>\"low\"</code> and <code>\"high\"</code> value</li> <li><code>\"intuniform\"</code>: Integer uniform distribution - must have a <code>\"low\"</code> and <code>\"high\"</code> value (both are inclusive)</li> <li><code>\"choice\"</code>: Choice distribution - must have a list of <code>\"choices\"</code> for explicit case, else just pass a list</li> <li><code>\"value\"</code>: Fixed value distribution - must have a <code>\"value\"</code> key for explicit case, else just pass a value</li> </ul> <p>And the following hyperparameters can be configured:</p> Hyperparameter Type Default model_arch str \"Unet\" model_encoder str \"dpn107\" model_encoder_weights str or None None augment bool True learning_rate float 1e-3 gamma float 0.9 focal_loss_alpha float or None None focal_loss_gamma float 2.0 batch_size int 8 <p>Because the configuration file doesn't use the <code>darts</code> key, it can also be merged into the normal configuration file and specified by the <code>hpconfig</code> parameter to also use that file.</p> Why using a separate configuration file? <ul> <li>It makes creating different sweeps easier</li> <li>It separates the sweep configuration from the normal configuration</li> <li>It allows for using dicts in the config - this is not possible right now due to the way we handle the main configuration file.</li> </ul> <p>Per default, a random search is performed, where the number of samples can be specified by <code>n_trials</code>. If <code>n_trials</code> is set to \"grid\", a grid search is performed instead. However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.</p> <p>Optionally it is possible to retrain and test with the best hyperparameter configuration by setting <code>retrain_and_test</code> to <code>True</code>. This will retrain the model on the complete train split without folding and test the data on the test split.</p>"},{"location":"guides/training/tune/#parallel-execution-with-multiprocessing","title":"Parallel execution with multiprocessing","text":"<p>The tuning script supports parallel execution of cross-validation runs across multiple devices using multiprocessing. This can significantly speed up hyperparameter tuning when you have multiple GPUs available.</p> <p>To enable parallel execution, use the <code>--strategy tune-parallel</code> flag along with specifying multiple devices:</p> <pre><code>[uv run] darts training tune-smp \\\n    --strategy tune-parallel \\\n    --devices 0 1 2 3 \\\n    --hpconfig configs/hyperparameters.yaml \\\n    ...\n</code></pre>"},{"location":"guides/training/tune/#how-it-works","title":"How it works","text":"<p>When using <code>tune-parallel</code>:</p> <ul> <li>Multiple cross-validation runs (each with a different hyperparameter configuration) are executed in parallel</li> <li>Each cross-validation run is assigned to an available GPU from the device pool</li> <li>Within each cross-validation, the individual folds are executed sequentially (not in parallel)</li> <li>Once a cross-validation completes, the GPU is returned to the pool and assigned to the next pending run</li> </ul> <p>This approach maximizes GPU utilization when running many hyperparameter configurations, as the number of parallel workers equals the number of specified devices.</p>"},{"location":"guides/training/tune/#example","title":"Example","text":"<p>If you have 4 GPUs and want to tune 100 hyperparameter configurations with 5-fold cross-validation:</p> <pre><code>[uv run] darts training tune-smp \\\n    --strategy tune-parallel \\\n    --devices 0 1 2 3 \\\n    --n-trials 100 \\\n    --n-folds 5 \\\n    --hpconfig configs/hyperparameters.yaml \\\n    --train-data-dir data/preprocessed\n</code></pre> <p>This will run 4 cross-validations in parallel (one per GPU), and each cross-validation will sequentially train 5 models (one per fold). As cross-validations complete, new ones are started until all 100 hyperparameter configurations have been evaluated.</p> <p>Strategy comparison</p> <ul> <li>Serial execution (default): Cross-validations run one after another. Within each cross-validation, you can optionally use <code>--strategy cv-parallel</code> to parallelize the fold training.</li> <li><code>tune-parallel</code>: Multiple cross-validations run in parallel across GPUs. Within each cross-validation, folds are trained sequentially.</li> <li>You cannot combine <code>tune-parallel</code> with <code>cv-parallel</code> - choose one level of parallelization based on your workload.</li> </ul> <p>DDP compatibility</p> <p>When using <code>tune-parallel</code>, distributed data parallel (DDP) strategies are automatically disabled for the cross-validation runs to prevent conflicts with multiprocessing. Each training run will use a single device only.</p>"},{"location":"reference/darts/","title":"darts","text":""},{"location":"reference/darts/#darts","title":"darts","text":"<p>DARTS processing pipeline.</p>"},{"location":"reference/darts/#darts.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts/cli/","title":"cli","text":""},{"location":"reference/darts/cli/#darts.cli","title":"darts.cli","text":"<p>Entrypoint for the darts-pipeline CLI.</p>"},{"location":"reference/darts/cli/#darts.cli.LoggingManager","title":"LoggingManager  <code>module-attribute</code>","text":"<pre><code>LoggingManager = (\n    darts.utils.logging.LoggingManagerSingleton()\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = cyclopts.App(\n    version=darts.__version__,\n    console=rich.get_console(),\n    config=darts.cli.config_parser,\n    help_format=\"plaintext\",\n    version_format=\"plaintext\",\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.config_parser","title":"config_parser  <code>module-attribute</code>","text":"<pre><code>config_parser = darts.utils.config.ConfigParser()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.inference_app","title":"inference_app  <code>module-attribute</code>","text":"<pre><code>inference_app = cyclopts.App(\n    name=\"inference\",\n    group=darts.cli.subcommands_group,\n    help=\"Predefined inference pipelines\",\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.inference_data_app","title":"inference_data_app  <code>module-attribute</code>","text":"<pre><code>inference_data_app = cyclopts.App(\n    name=\"prep-data\",\n    group=darts.cli.utilities_group,\n    help=\"Data preparation for offline use\",\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ray_group","title":"ray_group  <code>module-attribute</code>","text":"<pre><code>ray_group = cyclopts.Group.create_ordered('Ray Pipelines')\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.root_file","title":"root_file  <code>module-attribute</code>","text":"<pre><code>root_file = pathlib.Path(__file__).resolve()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.sequential_group","title":"sequential_group  <code>module-attribute</code>","text":"<pre><code>sequential_group = cyclopts.Group.create_ordered(\n    \"Sequential Pipelines\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.subcommands_group","title":"subcommands_group  <code>module-attribute</code>","text":"<pre><code>subcommands_group = cyclopts.Group.create_ordered(\n    \"Pipelines &amp; Scripts\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.training_app","title":"training_app  <code>module-attribute</code>","text":"<pre><code>training_app = cyclopts.App(\n    name=\"training\",\n    group=darts.cli.subcommands_group,\n    help=\"Predefined training pipelines\",\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.training_data_app","title":"training_data_app  <code>module-attribute</code>","text":"<pre><code>training_data_app = cyclopts.App(\n    name=\"create-dataset\", help=\"Dataset creation\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.utilities_group","title":"utilities_group  <code>module-attribute</code>","text":"<pre><code>utilities_group = cyclopts.Group.create_ordered(\"Utilities\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser","title":"ConfigParser","text":"<pre><code>ConfigParser()\n</code></pre> <p>Parser for cyclopts config.</p> <p>An own implementation is needed to select our own toml structure and source. Implemented as a class to be able to provide the config-file as a parameter of the CLI.</p> <p>Initialize the ConfigParser (no-op).</p> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the ConfigParser (no-op).\"\"\"\n    self._config = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.__call__","title":"__call__","text":"<pre><code>__call__(\n    apps: list[cyclopts.App],\n    commands: tuple[str, ...],\n    arguments: cyclopts.ArgumentCollection,\n)\n</code></pre> <p>Parser for cyclopts config. An own implementation is needed to select our own toml structure.</p> <p>First, the configuration file at \"config.toml\" is loaded. Then, this config is flattened and then mapped to the input arguments of the called function. Hence parent keys are not considered.</p> <p>Parameters:</p> <ul> <li> <code>apps</code>               (<code>list[cyclopts.App]</code>)           \u2013            <p>The cyclopts apps. Unused, but must be provided for the cyclopts hook.</p> </li> <li> <code>commands</code>               (<code>tuple[str, ...]</code>)           \u2013            <p>The commands. Unused, but must be provided for the cyclopts hook.</p> </li> <li> <code>arguments</code>               (<code>cyclopts.ArgumentCollection</code>)           \u2013            <p>The arguments to apply the config to.</p> </li> </ul> <p>Examples:</p>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.__call__--setup-the-cyclopts-app","title":"Setup the cyclopts App","text":"<pre><code>import cyclopts\nfrom darts.utils.config import ConfigParser\n\nconfig_parser = ConfigParser()\napp = cyclopts.App(config=config_parser)\n\n# Intercept the logging behavior to add a file handler\n@app.meta.default\ndef launcher(\n    *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n    log_dir: Path = Path(\"logs\"),\n    config_file: Path = Path(\"config.toml\"),\n):\n    command, bound, _ = app.parse_args(tokens)\n    add_logging_handlers(command.__name__, console, log_dir)\n    return command(*bound.args, **bound.kwargs)\n\nif __name__ == \"__main__\":\n    app.meta()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.__call__--usage","title":"Usage","text":"<p>Config file <code>./config.toml</code>:</p> <pre><code>[darts.hello] # The parent key is completely ignored\nname = \"Tobias\"\n</code></pre> <p>Function signature which is called:</p> <pre><code># ... setup code for cyclopts\n@app.command()\ndef hello(name: str):\n    print(f\"Hello {name}\")\n</code></pre> <p>Calling the function from CLI:</p> <pre><code>$ darts hello\nHello Tobias\n\n$ darts hello --name=Max\nHello Max\n</code></pre> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def __call__(self, apps: list[cyclopts.App], commands: tuple[str, ...], arguments: cyclopts.ArgumentCollection):\n    \"\"\"Parser for cyclopts config. An own implementation is needed to select our own toml structure.\n\n    First, the configuration file at \"config.toml\" is loaded.\n    Then, this config is flattened and then mapped to the input arguments of the called function.\n    Hence parent keys are not considered.\n\n    Args:\n        apps (list[cyclopts.App]): The cyclopts apps. Unused, but must be provided for the cyclopts hook.\n        commands (tuple[str, ...]): The commands. Unused, but must be provided for the cyclopts hook.\n        arguments (cyclopts.ArgumentCollection): The arguments to apply the config to.\n\n    Examples:\n        ### Setup the cyclopts App\n\n        ```python\n        import cyclopts\n        from darts.utils.config import ConfigParser\n\n        config_parser = ConfigParser()\n        app = cyclopts.App(config=config_parser)\n\n        # Intercept the logging behavior to add a file handler\n        @app.meta.default\n        def launcher(\n            *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n            log_dir: Path = Path(\"logs\"),\n            config_file: Path = Path(\"config.toml\"),\n        ):\n            command, bound, _ = app.parse_args(tokens)\n            add_logging_handlers(command.__name__, console, log_dir)\n            return command(*bound.args, **bound.kwargs)\n\n        if __name__ == \"__main__\":\n            app.meta()\n        ```\n\n\n        ### Usage\n\n        Config file `./config.toml`:\n\n        ```toml\n        [darts.hello] # The parent key is completely ignored\n        name = \"Tobias\"\n        ```\n\n        Function signature which is called:\n\n        ```python\n        # ... setup code for cyclopts\n        @app.command()\n        def hello(name: str):\n            print(f\"Hello {name}\")\n        ```\n\n        Calling the function from CLI:\n\n        ```sh\n        $ darts hello\n        Hello Tobias\n\n        $ darts hello --name=Max\n        Hello Max\n        ```\n\n    \"\"\"\n    if self._config is None:\n        config_arg, _, _ = arguments.match(\"--config-file\")\n        config_file = config_arg.convert_and_validate()\n        # Use default config file if not specified\n        if not config_file:\n            config_file = config_arg.field_info.default\n        # else never happens\n        self.open_config(config_file)\n\n    self.apply_config(arguments)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.apply_config","title":"apply_config","text":"<pre><code>apply_config(arguments: cyclopts.ArgumentCollection)\n</code></pre> <p>Apply the loaded config to the cyclopts mapping.</p> <p>Parameters:</p> <ul> <li> <code>arguments</code>               (<code>cyclopts.ArgumentCollection</code>)           \u2013            <p>The arguments to apply the config to.</p> </li> </ul> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def apply_config(self, arguments: cyclopts.ArgumentCollection):\n    \"\"\"Apply the loaded config to the cyclopts mapping.\n\n    Args:\n        arguments (cyclopts.ArgumentCollection): The arguments to apply the config to.\n\n    \"\"\"\n    to_add = []\n    for k in self._config.keys():\n        value = self._config[k][\"value\"]\n\n        try:\n            argument, remaining_keys, _ = arguments.match(f\"--{k}\")\n        except ValueError:\n            # Config key not found in arguments - ignore\n            continue\n\n        # Skip if the argument is not bound to a parameter\n        if argument.tokens or argument.field_info.kind is argument.field_info.VAR_KEYWORD:\n            continue\n\n        # Skip if the argument is from the config file\n        if any(x.source != \"config-file\" for x in argument.tokens):\n            continue\n\n        # Parse value to tuple of strings\n        if not isinstance(value, list):\n            value = (value,)\n        value = tuple(str(x) for x in value)\n        # Add the new tokens to the list\n        for i, v in enumerate(value):\n            to_add.append(\n                (\n                    argument,\n                    cyclopts.Token(keyword=k, value=v, source=\"config-file\", index=i, keys=remaining_keys),\n                )\n            )\n    # Add here after all \"arguments.match\" calls, to avoid changing the list while iterating\n    for argument, token in to_add:\n        argument.append(token)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.open_config","title":"open_config","text":"<pre><code>open_config(file_path: str | pathlib.Path) -&gt; None\n</code></pre> <p>Open the config file, takes the 'darts' key, flattens the resulting dict and saves as config.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the config file.</p> </li> </ul> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def open_config(self, file_path: str | Path) -&gt; None:\n    \"\"\"Open the config file, takes the 'darts' key, flattens the resulting dict and saves as config.\n\n    Args:\n        file_path (str | Path): The path to the config file.\n\n    \"\"\"\n    file_path = file_path if isinstance(file_path, Path) else Path(file_path)\n\n    if not file_path.exists():\n        logger.warning(f\"No config file found at {file_path.resolve()}\")\n        self._config = {}\n        return\n\n    with file_path.open(\"rb\") as f:\n        config = tomllib.load(f)[\"darts\"]\n\n    # Flatten the config data ()\n    self._config = flatten_dict(config)\n    logger.info(f\"loaded config from '{file_path.resolve()}'\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths","title":"PipelineV2Paths  <code>dataclass</code>","text":"<pre><code>PipelineV2Paths(\n    model_files: list[pathlib.Path] = None,\n    default_dirs: darts_utils.paths.DefaultPaths = (\n        lambda: darts_utils.paths.DefaultPaths()\n    )(),\n    output_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    orthotiles_dir: pathlib.Path | None = None,\n    scenes_dir: pathlib.Path | None = None,\n    sentinel2_grid_dir: pathlib.Path | None = None,\n    raw_data_store: pathlib.Path | None = None,\n    raw_data_source: typing.Literal[\"cdse\", \"gee\"] = \"cdse\",\n    no_raw_data_store: bool = False,\n)\n</code></pre> <p>Default paths for v2 pipelines.</p>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.default_dirs","title":"default_dirs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_dirs: darts_utils.paths.DefaultPaths = dataclasses.field(\n    default_factory=lambda: darts_utils.paths.DefaultPaths()\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.no_raw_data_store","title":"no_raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>no_raw_data_store: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.raw_data_source","title":"raw_data_source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_source: typing.Literal['cdse', 'gee'] = 'cdse'\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.raw_data_store","title":"raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_store: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.sentinel2_grid_dir","title":"sentinel2_grid_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_grid_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def __post_init__(self):  # noqa: D105\n    paths.set_defaults(self.default_dirs)\n    # The defaults will be overwritten in the respective realizations\n    self.output_data_dir = self.output_data_dir or paths.output_data(\"base_pipeline\")\n    self.model_files = self.model_files or paths.ensemble_models()\n    self.arcticdem_dir = self.arcticdem_dir or paths.arcticdem(2)\n    self.tcvis_dir = self.tcvis_dir or paths.tcvis()\n    self.output_data_dir = self.output_data_dir or paths.output_data(\"planet\")\n    self.orthotiles_dir = self.orthotiles_dir or paths.planet_orthotiles()\n    self.scenes_dir = self.scenes_dir or paths.planet_scenes()\n    self.output_data_dir = self.output_data_dir or paths.output_data(f\"sentinel2-{self.raw_data_source}\")\n    self.raw_data_store = self.raw_data_store or paths.sentinel2_raw_data(self.raw_data_source)\n    if self.no_raw_data_store:\n        self.raw_data_store = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PipelineV2Paths.log","title":"log","text":"<pre><code>log(level: int = logging.DEBUG)\n</code></pre> <p>Log all paths managed.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def log(self, level: int = logging.DEBUG):\n    \"\"\"Log all paths managed.\"\"\"\n    label_width = 47\n    logmsg = textwrap.dedent(f\"\"\"\n        === Pipeline (Sequential V2) Paths ===\n        {\"Output Data Directory:\":&lt;{label_width}} {self.output_data_dir}\n        {\"ArcticDEM Directory:\":&lt;{label_width}} {self.arcticdem_dir}\n        {\"TCVis Directory:\":&lt;{label_width}} {self.tcvis_dir}\n        {\"Planet Orthotiles Directory:\":&lt;{label_width}} {self.orthotiles_dir}\n        {\"Planet Scenes Directory:\":&lt;{label_width}} {self.scenes_dir}\n        {\"Sentinel-2 Grid Directory:\":&lt;{label_width}} {self.sentinel2_grid_dir}\n        {\"Sentinel-2 Raw Data Directory:\":&lt;{label_width}} {self.raw_data_store}\n    \"\"\").strip()\n    logger.log(level, logmsg)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline","title":"PlanetPipeline  <code>dataclass</code>","text":"<pre><code>PlanetPipeline(\n    model_files: list[pathlib.Path] = None,\n    default_dirs: darts_utils.paths.DefaultPaths = (\n        lambda: darts_utils.paths.DefaultPaths()\n    )(),\n    output_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    edge_erosion_size: int | None = None,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    offline: bool = False,\n    debug_data: bool = False,\n    orthotiles_dir: pathlib.Path | None = None,\n    scenes_dir: pathlib.Path | None = None,\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for processing PlanetScope data.</p> <p>Processes PlanetScope imagery (both orthotiles and scenes) for RTS segmentation. Supports both offline and online processing modes.</p> Data Structure <p>Expects PlanetScope data organized as: - Orthotiles: <code>orthotiles_dir/tile_id/scene_id/</code> - Scenes: <code>scenes_dir/scene_id/</code></p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory containing PlanetScope orthotiles. If None, uses default path from DARTS paths. Defaults to None.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory containing PlanetScope scenes. If None, uses default path from DARTS paths. Defaults to None.</p> </li> <li> <code>image_ids</code>               (<code>list | None</code>, default:                   <code>None</code> )           \u2013            <p>List of image/scene IDs to process. If None, processes all images found in orthotiles_dir and scenes_dir. Defaults to None.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path] | None</code>, default:                   <code>None</code> )           \u2013            <p>Path(s) to model file(s) for segmentation. Single Path implies <code>write_model_outputs=False</code>. If None, searches default model directory for all .pt files. Defaults to None.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Output directory for results. If None, uses <code>{default_out}/planet</code>. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for ArcticDEM datacube. Will be created/downloaded if needed. If None, uses default path. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for TCVis data. If None, uses default path. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>, default:                   <code>None</code> )           \u2013            <p>Computation device. \"cuda\" uses GPU 0, int specifies GPU index, \"auto\" selects free GPU. Defaults to None.</p> </li> <li> <code>ee_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Earth Engine project ID. May be omitted if defined in persistent credentials. Defaults to None.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use EE high-volume server. Defaults to True.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Outer radius (m) for TPI calculation. Defaults to 100.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Inner radius (m) for TPI calculation. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Patch size for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Overlap between patches. Defaults to 256.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Disk size for mask erosion and inner edge cropping. Defaults to 10.</p> </li> <li> <code>edge_erosion_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Size for outer edge cropping. If None, uses <code>mask_erosion_size</code>. Defaults to None.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Minimum object size (pixels) to keep. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>Quality filtering level. 0=\"none\", 1=\"low_quality\", 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>Bands to export. Can include \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\", \"metadata\", or specific band names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Save individual model outputs (not just ensemble). Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Overwrite existing output files. Defaults to False.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Skip downloading missing data. Defaults to False.</p> </li> <li> <code>debug_data</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Write intermediate debugging data. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.debug_data","title":"debug_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug_data: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.default_dirs","title":"default_dirs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_dirs: darts_utils.paths.DefaultPaths = dataclasses.field(\n    default_factory=lambda: darts_utils.paths.DefaultPaths()\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.edge_erosion_size","title":"edge_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>edge_erosion_size: int | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.offline","title":"offline  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offline: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def __post_init__(self):  # noqa: D105\n    super().__post_init__()\n    self.output_data_dir = self.output_data_dir or paths.output_data(\"planet\")\n    self.orthotiles_dir = self.orthotiles_dir or paths.planet_orthotiles()\n    self.scenes_dir = self.scenes_dir or paths.planet_scenes()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n)\n</code></pre> <p>Run the sequential pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.PlanetPipeline</code>)           \u2013            <p>Configured PlanetPipeline instance.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetPipeline\"):\n    \"\"\"Run the sequential pipeline for PlanetScope data.\n\n    Args:\n        pipeline: Configured PlanetPipeline instance.\n\n    \"\"\"\n    pipeline.__post_init__()\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.cli_prepare_data","title":"cli_prepare_data  <code>staticmethod</code>","text":"<pre><code>cli_prepare_data(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download all necessary data for offline processing.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.PlanetPipeline</code>)           \u2013            <p>Configured PlanetPipeline instance.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of the <code>aux</code> flag or model needs. Defaults to False.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli_prepare_data(*, pipeline: \"PlanetPipeline\", aux: bool = False, force: bool = False):\n    \"\"\"Download all necessary data for offline processing.\n\n    Args:\n        pipeline: Configured PlanetPipeline instance.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.\n        force: If True, downloads all possible data, independent of the `aux` flag or model needs.\n            Defaults to False.\n\n    \"\"\"\n    assert not pipeline.offline, \"Pipeline must be online to prepare data for offline usage.\"\n    pipeline.__post_init__()\n    pipeline.prepare_data(optical=False, aux=aux, force=force)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download and prepare data for offline processing.</p> <p>Validates configuration, determines data requirements from models, and downloads requested data (optical imagery and/or auxiliary data).</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> <li> <code>SystemExit</code>             \u2013            <p>If the process is terminated.</p> </li> <li> <code>SystemError</code>             \u2013            <p>If a system error occurs.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def prepare_data(self, optical: bool = False, aux: bool = False, force: bool = False):\n    \"\"\"Download and prepare data for offline processing.\n\n    Validates configuration, determines data requirements from models,\n    and downloads requested data (optical imagery and/or auxiliary data).\n\n    Args:\n        optical: If True, downloads optical imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n        SystemExit: If the process is terminated.\n        SystemError: If a system error occurs.\n\n    \"\"\"\n    assert optical or aux, \"Nothing to prepare. Please set optical and/or aux to True.\"\n\n    # ? We only want to download stuff - no need for using the GPU here\n    self.device = \"cpu\"\n    self._dump_config()\n\n    from darts_acquisition import download_arcticdem, download_tcvis\n    from stopuhr import Chronometer\n\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n\n    if aux or force:\n        # Get the ensemble to check which auxiliary data is necessary\n        if force:\n            needs_arcticdem, needs_tcvis = True, True\n        else:\n            ensemble = self._load_ensemble()\n            needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n        if not needs_arcticdem and not needs_tcvis:\n            logger.warning(\"No auxiliary data required by the models. Skipping download of auxiliary data...\")\n        else:\n            logger.info(f\"Models {needs_tcvis=} {needs_arcticdem=}.\")\n            self._create_auxiliary_datacubes(arcticdem=needs_arcticdem, tcvis=needs_tcvis)\n\n            # Predownload auxiliary\n            aoi = self._tile_aoi()\n            if needs_arcticdem:\n                logger.info(\"start download ArcticDEM\")\n                with timer(\"Downloading ArcticDEM\"):\n                    download_arcticdem(aoi, self.arcticdem_dir, resolution=self._arcticdem_resolution())\n            if needs_tcvis:\n                logger.info(\"start download TCVIS\")\n                init_ee(self.ee_project, self.ee_use_highvolume)\n                with timer(\"Downloading TCVis\"):\n                    download_tcvis(aoi, self.tcvis_dir)\n\n    # Predownload tiles if optical flag is set\n    if not optical and not force:\n        return\n\n    # Iterate over all the data\n    with timer(\"Loading Optical\"):\n        tileinfo = self._tileinfos()\n        n_tiles = 0\n        logger.info(f\"Found {len(tileinfo)} tiles to download.\")\n        for i, (tilekey, _) in enumerate(tileinfo):\n            tile_id = self._get_tile_id(tilekey)\n            try:\n                self._download_tile(tilekey)\n                n_tiles += 1\n                logger.info(f\"Downloaded sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n            except (KeyboardInterrupt, SystemError, SystemExit) as e:\n                logger.warning(f\"{type(e).__name__} detected.\\nExiting...\")\n                raise e\n            except Exception as e:\n                logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n                logger.exception(e)\n        else:\n            logger.info(f\"Downloaded {n_tiles} tiles.\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> <p>Run the complete segmentation pipeline.</p> <p>Executes the full pipeline including: 1. Configuration validation and dumping 2. Loading ensemble models 3. Creating/loading auxiliary datacubes 4. Processing each tile:    - Loading optical data    - Loading auxiliary data (ArcticDEM, TCVis) as needed    - Preprocessing    - Segmentation    - Postprocessing    - Exporting results 5. Saving results and timing information</p> <p>Results are saved to the output directory with timestamped configuration, results parquet file, and timing information.</p> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    \"\"\"Run the complete segmentation pipeline.\n\n    Executes the full pipeline including:\n    1. Configuration validation and dumping\n    2. Loading ensemble models\n    3. Creating/loading auxiliary datacubes\n    4. Processing each tile:\n       - Loading optical data\n       - Loading auxiliary data (ArcticDEM, TCVis) as needed\n       - Preprocessing\n       - Segmentation\n       - Postprocessing\n       - Exporting results\n    5. Saving results and timing information\n\n    Results are saved to the output directory with timestamped configuration,\n    results parquet file, and timing information.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n\n    \"\"\"\n    self._validate()\n    current_time = self._dump_config()\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    import pandas as pd\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_v2\n    from stopuhr import Chronometer, stopwatch\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n    self.device = decide_device(self.device)\n\n    if not self.offline:\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    self._create_auxiliary_datacubes()\n\n    # determine models to use\n    ensemble = self._load_ensemble()\n    ensemble_subsets = ensemble.model_names\n    needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=ensemble_subsets)\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} seems to be already processed, \"\n                        \"but some of the requested outputs are missing. \"\n                        \"Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading Optical\", log=False):\n                tile = self._load_tile(tilekey)\n\n            if needs_arcticdem:\n                with timer(\"Loading ArcticDEM\", log=False):\n                    arcticdem_resolution = self._arcticdem_resolution()\n                    arcticdem = load_arcticdem(\n                        tile.odc.geobox,\n                        self.arcticdem_dir,\n                        resolution=arcticdem_resolution,\n                        buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                        offline=self.offline,\n                    )\n            else:\n                arcticdem = None\n\n            if needs_tcvis:\n                with timer(\"Loading TCVis\", log=False):\n                    tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir, offline=self.offline)\n            else:\n                tcvis = None\n\n            with timer(\"Preprocessing\", log=False):\n                tile = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n\n            with timer(\"Postprocessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    device=self.device,\n                    edge_erosion_size=self.edge_erosion_size,\n                )\n\n            export_metadata = self._result_metadata(tilekey)\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    metadata=export_metadata,\n                    debug=self.debug_data,\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.timer.parquet\")\n            if len(stopwatch.durations) &gt; 0:\n                stopwatch.export().to_parquet(self.output_data_dir / f\"{current_time}.stopwatch.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary(printer=logger.info)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline","title":"PlanetRayPipeline  <code>dataclass</code>","text":"<pre><code>PlanetRayPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    num_cpus: int = 1,\n    devices: list[int] | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    orthotiles_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSOrthoTile\"\n    ),\n    scenes_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSScene\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.ray_v2._BaseRayPipeline</code></p> <p>Pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSOrthoTile')</code> )           \u2013            <p>The directory containing the PlanetScope orthotiles.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSScene')</code> )           \u2013            <p>The directory containing the PlanetScope scenes.</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.num_cpus","title":"num_cpus  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_cpus: int = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSOrthoTile\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSScene\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(*, pipeline: darts.pipelines.ray_v2.PlanetRayPipeline)\n</code></pre> <p>Run the sequential pipeline for Planet data.</p> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetRayPipeline\"):\n    \"\"\"Run the sequential pipeline for Planet data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetRayPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    if self.devices is not None:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(d) for d in self.devices)\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import ray\n\n    ray_context = ray.init(\n        num_cpus=self.num_cpus,  # We use one CPU per Ray task\n        num_gpus=len(self.devices) if self.devices is not None else None,\n    )\n    logger.debug(f\"Ray initialized with context: {ray_context}\")\n    logger.info(f\"Ray Dashboard URL: {ray_context.dashboard_url}\")\n    logger.debug(f\"Ray cluster resources: {ray.cluster_resources()}\")\n    logger.debug(f\"Ray available resources: {ray.available_resources()}\")\n\n    # Initlize ee in every worker\n    @ray.remote\n    def init_worker():\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    num_workers = int(ray.cluster_resources().get(\"CPU\", 1))\n    logger.info(f\"Initializing {num_workers} Ray workers with Earth Engine.\")\n    ray.get([init_worker.remote() for _ in range(num_workers)])\n\n    import smart_geocubes\n    from darts_export import missing_outputs\n\n    from darts.pipelines._ray_wrapper import (\n        _export_tile_ray,\n        _load_aux,\n        _prepare_export_ray,\n        _preprocess_ray,\n        _RayEnsembleV1,\n    )\n    from darts.utils.logging import LoggingManager\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    # ray_ensemble = _RayEnsembleV1.remote(models)\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    adem_buffer = ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2))\n\n    # Get files to process\n    tileinfo: list[RayInputDict] = []\n    for i, (tilekey, outpath) in enumerate(self._tileinfos()):\n        tile_id = self._get_tile_id(tilekey)\n        if not self.overwrite:\n            mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n            if mo == \"none\":\n                logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                continue\n            if mo == \"some\":\n                logger.warning(\n                    f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                    \" Skipping because overwrite=False...\"\n                )\n                continue\n        tileinfo.append({\"tilekey\": tilekey, \"outpath\": str(outpath.resolve()), \"tile_id\": tile_id})\n    tileinfo = tileinfo[:10]\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n\n    # Ray data pipeline\n    # TODO: setup device stuff correctly\n    ds = ray.data.from_items(tileinfo)\n    ds = ds.map(self._load_tile, num_cpus=1)\n    ds = ds.map(\n        _load_aux,\n        fn_kwargs={\n            \"arcticdem_dir\": self.arcticdem_dir,\n            \"arcticdem_resolution\": arcticdem_resolution,\n            \"buffer\": adem_buffer,\n            \"tcvis_dir\": self.tcvis_dir,\n        },\n        num_cpus=1,\n    )\n    ds = ds.map(\n        _preprocess_ray,\n        fn_kwargs={\n            \"tpi_outer_radius\": self.tpi_outer_radius,\n            \"tpi_inner_radius\": self.tpi_inner_radius,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n        concurrency=4,\n    )\n    ds = ds.map(\n        _RayEnsembleV1,\n        fn_constructor_kwargs={\"model_dict\": models},\n        fn_kwargs={\n            \"patch_size\": self.patch_size,\n            \"overlap\": self.overlap,\n            \"batch_size\": self.batch_size,\n            \"reflection\": self.reflection,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n        num_gpus=0.8,\n        concurrency=1,\n    )\n    ds = ds.map(\n        _prepare_export_ray,\n        fn_kwargs={\n            \"binarization_threshold\": self.binarization_threshold,\n            \"mask_erosion_size\": self.mask_erosion_size,\n            \"min_object_size\": self.min_object_size,\n            \"quality_level\": self.quality_level,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n    )\n    ds = ds.map(\n        _export_tile_ray,\n        fn_kwargs={\n            \"export_bands\": self.export_bands,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n    )\n    logger.debug(f\"Ray dataset: {ds}\")\n    logger.info(\"Ray pipeline created. Starting execution...\")\n    # This should trigger the execution\n    ds.write_parquet(f\"local://{self.output_data_dir.resolve()!s}/ray_output.parquet\")\n    logger.info(f\"Ray pipeline finished. Output written to {self.output_data_dir.resolve()!s}/ray_output.parquet\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline","title":"Sentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    default_dirs: darts_utils.paths.DefaultPaths = (\n        lambda: darts_utils.paths.DefaultPaths()\n    )(),\n    output_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    edge_erosion_size: int | None = None,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    offline: bool = False,\n    debug_data: bool = False,\n    scene_ids: list[str] | None = None,\n    scene_id_file: pathlib.Path | None = None,\n    tile_ids: list[str] | None = None,\n    aoi_file: pathlib.Path | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n    prep_data_scene_id_file: pathlib.Path | None = None,\n    sentinel2_grid_dir: pathlib.Path | None = None,\n    raw_data_store: pathlib.Path | None = None,\n    no_raw_data_store: bool = False,\n    raw_data_source: typing.Literal[\"gee\", \"cdse\"] = \"cdse\",\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for processing Sentinel-2 data.</p> <p>Processes Sentinel-2 Surface Reflectance (SR) imagery from either CDSE or Google Earth Engine. Supports multiple scene selection methods and flexible filtering options.</p> Source Selection <p>The data source is specified via the <code>raw_data_source</code> parameter: - \"cdse\": Copernicus Data Space Ecosystem (CDSE) - \"gee\": Google Earth Engine (GEE)</p> <p>Both sources require accounts and proper credential setup on the system.</p> Scene Selection <p>Scenes can be selected using one of four mutually exclusive methods (priority order):</p> <ol> <li><code>scene_ids</code>: Direct list of Sentinel-2 scene IDs</li> <li><code>scene_id_file</code>: JSON file containing scene IDs</li> <li><code>tile_ids</code>: List of Sentinel-2 tile IDs (e.g., \"33UVP\") with optional filters</li> <li><code>aoi_file</code>: Shapefile defining area of interest with optional filters</li> </ol> Filtering Options <p>When using <code>tile_ids</code> or <code>aoi_file</code>, scenes can be filtered by: - Cloud/snow cover: <code>max_cloud_cover</code>, <code>max_snow_cover</code> - Date range: <code>start_date</code> and <code>end_date</code> (YYYY-MM-DD format) - OR specific months/years: <code>months</code> (1-12) and <code>years</code></p> <p>Note: Date range takes priority over month/year filtering. Warning: No temporal filtering may cause rate-limit errors. Note: Month/year filtering is experimental and only implemented for CDSE.</p> Offline Processing <p>Use <code>cli_prepare_data</code> to download data for offline use. The <code>prep_data_scene_id_file</code> stores scene IDs from queries for offline reuse.</p> <p>Parameters:</p> <ul> <li> <code>scene_ids</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Direct list of Sentinel-2 scene IDs to process. Defaults to None.</p> </li> <li> <code>scene_id_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>JSON file containing scene IDs to process. Defaults to None.</p> </li> <li> <code>tile_ids</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Sentinel-2 tile IDs (requires filtering params). Defaults to None.</p> </li> <li> <code>aoi_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Shapefile with area of interest (requires filtering params). Defaults to None.</p> </li> <li> <code>start_date</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Start date for filtering (YYYY-MM-DD format). Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>End date for filtering (YYYY-MM-DD format). Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int | None</code>, default:                   <code>10</code> )           \u2013            <p>Maximum cloud cover percentage (0-100). Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int | None</code>, default:                   <code>10</code> )           \u2013            <p>Maximum snow cover percentage (0-100). Defaults to 10.</p> </li> <li> <code>months</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by months (1-12). Defaults to None.</p> </li> <li> <code>years</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by years. Defaults to None.</p> </li> <li> <code>prep_data_scene_id_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>File to store/load scene IDs for offline processing. Written during <code>prepare_data</code>, read during offline <code>run</code>. Defaults to None.</p> </li> <li> <code>sentinel2_grid_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for Sentinel-2 grid shapefiles. Used only in <code>prepare_data</code> with <code>tile_ids</code>. If None, uses default path. Defaults to None.</p> </li> <li> <code>raw_data_store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for storing raw Sentinel-2 data locally. If None, uses default path based on <code>raw_data_source</code>. Defaults to None.</p> </li> <li> <code>no_raw_data_store</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, processes data in-memory without local storage. Overrides <code>raw_data_store</code>. Defaults to False.</p> </li> <li> <code>raw_data_source</code>               (<code>typing.Literal['gee', 'cdse']</code>, default:                   <code>'cdse'</code> )           \u2013            <p>Data source to use. Defaults to \"cdse\".</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path] | None</code>, default:                   <code>None</code> )           \u2013            <p>Path(s) to model file(s) for segmentation. Single Path implies <code>write_model_outputs=False</code>. If None, searches default model directory for all .pt files. Defaults to None.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Output directory for results. If None, uses <code>{default_out}/sentinel2-{raw_data_source}</code>. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for ArcticDEM datacube. Will be created/downloaded if needed. If None, uses default path. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for TCVis data. If None, uses default path. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>, default:                   <code>None</code> )           \u2013            <p>Computation device. \"cuda\" uses GPU 0, int specifies GPU index, \"auto\" selects free GPU. Defaults to None.</p> </li> <li> <code>ee_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Earth Engine project ID. May be omitted if defined in persistent credentials. Defaults to None.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use EE high-volume server. Defaults to True.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Outer radius (m) for TPI calculation. Defaults to 100.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Inner radius (m) for TPI calculation. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Patch size for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Overlap between patches. Defaults to 256.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Disk size for mask erosion and inner edge cropping. Defaults to 10.</p> </li> <li> <code>edge_erosion_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Size for outer edge cropping. If None, uses <code>mask_erosion_size</code>. Defaults to None.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Minimum object size (pixels) to keep. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>Quality filtering level. 0=\"none\", 1=\"low_quality\", 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>Bands to export. Can include \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\", \"metadata\", or specific band names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Save individual model outputs (not just ensemble). Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Overwrite existing output files. Defaults to False.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Skip downloading missing data. Requires pre-downloaded data. Defaults to False.</p> </li> <li> <code>debug_data</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Write intermediate debugging data to output directory. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.aoi_file","title":"aoi_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_file: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.debug_data","title":"debug_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug_data: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.default_dirs","title":"default_dirs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_dirs: darts_utils.paths.DefaultPaths = dataclasses.field(\n    default_factory=lambda: darts_utils.paths.DefaultPaths()\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.edge_erosion_size","title":"edge_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>edge_erosion_size: int | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int | None = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.max_snow_cover","title":"max_snow_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_snow_cover: int | None = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.months","title":"months  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>months: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.no_raw_data_store","title":"no_raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>no_raw_data_store: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.offline","title":"offline  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offline: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.prep_data_scene_id_file","title":"prep_data_scene_id_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prep_data_scene_id_file: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.raw_data_source","title":"raw_data_source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_source: typing.Literal['gee', 'cdse'] = 'cdse'\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.raw_data_store","title":"raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_store: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.scene_id_file","title":"scene_id_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scene_id_file: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.scene_ids","title":"scene_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scene_ids: list[str] | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.sentinel2_grid_dir","title":"sentinel2_grid_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_grid_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.tile_ids","title":"tile_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tile_ids: list[str] | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.years","title":"years  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>years: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def __post_init__(self):  # noqa: D105\n    logger.debug(\"Before super\")\n    super().__post_init__()\n    logger.debug(\"After super\")\n    self.output_data_dir = self.output_data_dir or paths.output_data(f\"sentinel2-{self.raw_data_source}\")\n    self.raw_data_store = self.raw_data_store or paths.sentinel2_raw_data(self.raw_data_source)\n    if self.no_raw_data_store:\n        self.raw_data_store = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Sentinel-2 data.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.Sentinel2Pipeline</code>)           \u2013            <p>Configured Sentinel2Pipeline instance.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for Sentinel-2 data.\n\n    Args:\n        pipeline: Configured Sentinel2Pipeline instance.\n\n    \"\"\"\n    pipeline.__post_init__()\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.cli_prepare_data","title":"cli_prepare_data  <code>staticmethod</code>","text":"<pre><code>cli_prepare_data(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download all necessary data for offline processing.</p> <p>Queries the data source (CDSE or GEE) for scene IDs and downloads optical and/or auxiliary data. Stores scene IDs in <code>prep_data_scene_id_file</code> if specified for later offline use.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.Sentinel2Pipeline</code>)           \u2013            <p>Configured Sentinel2Pipeline instance.</p> </li> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical (Sentinel-2) imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli_prepare_data(\n    *, pipeline: \"Sentinel2Pipeline\", optical: bool = False, aux: bool = False, force: bool = False\n):\n    \"\"\"Download all necessary data for offline processing.\n\n    Queries the data source (CDSE or GEE) for scene IDs and downloads optical and/or auxiliary data.\n    Stores scene IDs in `prep_data_scene_id_file` if specified for later offline use.\n\n    Args:\n        pipeline: Configured Sentinel2Pipeline instance.\n        optical: If True, downloads optical (Sentinel-2) imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    \"\"\"\n    assert not pipeline.offline, \"Pipeline must be online to prepare data for offline usage.\"\n\n    # !: Because of an unknown bug, __post_init__ is not initialized automatically\n    pipeline.__post_init__()\n\n    logger.debug(f\"Preparing data with {optical=}, {aux=}.\")\n\n    if pipeline.prep_data_scene_id_file is not None:\n        if pipeline.prep_data_scene_id_file.exists():\n            logger.warning(\n                f\"Prep-data scene id file {pipeline.prep_data_scene_id_file=} already exists. \"\n                \"It will be overwritten.\"\n            )\n            pipeline.prep_data_scene_id_file.unlink()\n    pipeline.prepare_data(optical=optical, aux=aux, force=force)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download and prepare data for offline processing.</p> <p>Validates configuration, determines data requirements from models, and downloads requested data (optical imagery and/or auxiliary data).</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> <li> <code>SystemExit</code>             \u2013            <p>If the process is terminated.</p> </li> <li> <code>SystemError</code>             \u2013            <p>If a system error occurs.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def prepare_data(self, optical: bool = False, aux: bool = False, force: bool = False):\n    \"\"\"Download and prepare data for offline processing.\n\n    Validates configuration, determines data requirements from models,\n    and downloads requested data (optical imagery and/or auxiliary data).\n\n    Args:\n        optical: If True, downloads optical imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n        SystemExit: If the process is terminated.\n        SystemError: If a system error occurs.\n\n    \"\"\"\n    assert optical or aux, \"Nothing to prepare. Please set optical and/or aux to True.\"\n\n    # ? We only want to download stuff - no need for using the GPU here\n    self.device = \"cpu\"\n    self._dump_config()\n\n    from darts_acquisition import download_arcticdem, download_tcvis\n    from stopuhr import Chronometer\n\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n\n    if aux or force:\n        # Get the ensemble to check which auxiliary data is necessary\n        if force:\n            needs_arcticdem, needs_tcvis = True, True\n        else:\n            ensemble = self._load_ensemble()\n            needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n        if not needs_arcticdem and not needs_tcvis:\n            logger.warning(\"No auxiliary data required by the models. Skipping download of auxiliary data...\")\n        else:\n            logger.info(f\"Models {needs_tcvis=} {needs_arcticdem=}.\")\n            self._create_auxiliary_datacubes(arcticdem=needs_arcticdem, tcvis=needs_tcvis)\n\n            # Predownload auxiliary\n            aoi = self._tile_aoi()\n            if needs_arcticdem:\n                logger.info(\"start download ArcticDEM\")\n                with timer(\"Downloading ArcticDEM\"):\n                    download_arcticdem(aoi, self.arcticdem_dir, resolution=self._arcticdem_resolution())\n            if needs_tcvis:\n                logger.info(\"start download TCVIS\")\n                init_ee(self.ee_project, self.ee_use_highvolume)\n                with timer(\"Downloading TCVis\"):\n                    download_tcvis(aoi, self.tcvis_dir)\n\n    # Predownload tiles if optical flag is set\n    if not optical and not force:\n        return\n\n    # Iterate over all the data\n    with timer(\"Loading Optical\"):\n        tileinfo = self._tileinfos()\n        n_tiles = 0\n        logger.info(f\"Found {len(tileinfo)} tiles to download.\")\n        for i, (tilekey, _) in enumerate(tileinfo):\n            tile_id = self._get_tile_id(tilekey)\n            try:\n                self._download_tile(tilekey)\n                n_tiles += 1\n                logger.info(f\"Downloaded sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n            except (KeyboardInterrupt, SystemError, SystemExit) as e:\n                logger.warning(f\"{type(e).__name__} detected.\\nExiting...\")\n                raise e\n            except Exception as e:\n                logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n                logger.exception(e)\n        else:\n            logger.info(f\"Downloaded {n_tiles} tiles.\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> <p>Run the complete segmentation pipeline.</p> <p>Executes the full pipeline including: 1. Configuration validation and dumping 2. Loading ensemble models 3. Creating/loading auxiliary datacubes 4. Processing each tile:    - Loading optical data    - Loading auxiliary data (ArcticDEM, TCVis) as needed    - Preprocessing    - Segmentation    - Postprocessing    - Exporting results 5. Saving results and timing information</p> <p>Results are saved to the output directory with timestamped configuration, results parquet file, and timing information.</p> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    \"\"\"Run the complete segmentation pipeline.\n\n    Executes the full pipeline including:\n    1. Configuration validation and dumping\n    2. Loading ensemble models\n    3. Creating/loading auxiliary datacubes\n    4. Processing each tile:\n       - Loading optical data\n       - Loading auxiliary data (ArcticDEM, TCVis) as needed\n       - Preprocessing\n       - Segmentation\n       - Postprocessing\n       - Exporting results\n    5. Saving results and timing information\n\n    Results are saved to the output directory with timestamped configuration,\n    results parquet file, and timing information.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n\n    \"\"\"\n    self._validate()\n    current_time = self._dump_config()\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    import pandas as pd\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_v2\n    from stopuhr import Chronometer, stopwatch\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n    self.device = decide_device(self.device)\n\n    if not self.offline:\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    self._create_auxiliary_datacubes()\n\n    # determine models to use\n    ensemble = self._load_ensemble()\n    ensemble_subsets = ensemble.model_names\n    needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=ensemble_subsets)\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} seems to be already processed, \"\n                        \"but some of the requested outputs are missing. \"\n                        \"Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading Optical\", log=False):\n                tile = self._load_tile(tilekey)\n\n            if needs_arcticdem:\n                with timer(\"Loading ArcticDEM\", log=False):\n                    arcticdem_resolution = self._arcticdem_resolution()\n                    arcticdem = load_arcticdem(\n                        tile.odc.geobox,\n                        self.arcticdem_dir,\n                        resolution=arcticdem_resolution,\n                        buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                        offline=self.offline,\n                    )\n            else:\n                arcticdem = None\n\n            if needs_tcvis:\n                with timer(\"Loading TCVis\", log=False):\n                    tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir, offline=self.offline)\n            else:\n                tcvis = None\n\n            with timer(\"Preprocessing\", log=False):\n                tile = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n\n            with timer(\"Postprocessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    device=self.device,\n                    edge_erosion_size=self.edge_erosion_size,\n                )\n\n            export_metadata = self._result_metadata(tilekey)\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    metadata=export_metadata,\n                    debug=self.debug_data,\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.timer.parquet\")\n            if len(stopwatch.durations) &gt; 0:\n                stopwatch.export().to_parquet(self.output_data_dir / f\"{current_time}.stopwatch.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary(printer=logger.info)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline","title":"Sentinel2RayPipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2RayPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    num_cpus: int = 1,\n    devices: list[int] | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    aoi_shapefile: pathlib.Path = None,\n    start_date: str = None,\n    end_date: str = None,\n    max_cloud_cover: int = 10,\n    input_cache: pathlib.Path = pathlib.Path(\n        \"data/cache/input\"\n    ),\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.ray_v2._BaseRayPipeline</code></p> <p>Pipeline for Sentinel 2 data based on an area of interest.</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The shapefile containing the area of interest.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for filtering the Sentinel 2 scenes. Defaults to 10.</p> </li> <li> <code>input_cache</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/cache/input')</code> )           \u2013            <p>The directory to use for caching the input data. Defaults to Path(\"data/cache/input\").</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.aoi_shapefile","title":"aoi_shapefile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_shapefile: pathlib.Path = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.input_cache","title":"input_cache  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_cache: pathlib.Path = pathlib.Path(\"data/cache/input\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.num_cpus","title":"num_cpus  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_cpus: int = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *, pipeline: darts.pipelines.ray_v2.Sentinel2RayPipeline\n)\n</code></pre> <p>Run the sequential pipeline for AOI Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2RayPipeline\"):\n    \"\"\"Run the sequential pipeline for AOI Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2RayPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    if self.devices is not None:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(d) for d in self.devices)\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import ray\n\n    ray_context = ray.init(\n        num_cpus=self.num_cpus,  # We use one CPU per Ray task\n        num_gpus=len(self.devices) if self.devices is not None else None,\n    )\n    logger.debug(f\"Ray initialized with context: {ray_context}\")\n    logger.info(f\"Ray Dashboard URL: {ray_context.dashboard_url}\")\n    logger.debug(f\"Ray cluster resources: {ray.cluster_resources()}\")\n    logger.debug(f\"Ray available resources: {ray.available_resources()}\")\n\n    # Initlize ee in every worker\n    @ray.remote\n    def init_worker():\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    num_workers = int(ray.cluster_resources().get(\"CPU\", 1))\n    logger.info(f\"Initializing {num_workers} Ray workers with Earth Engine.\")\n    ray.get([init_worker.remote() for _ in range(num_workers)])\n\n    import smart_geocubes\n    from darts_export import missing_outputs\n\n    from darts.pipelines._ray_wrapper import (\n        _export_tile_ray,\n        _load_aux,\n        _prepare_export_ray,\n        _preprocess_ray,\n        _RayEnsembleV1,\n    )\n    from darts.utils.logging import LoggingManager\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    # ray_ensemble = _RayEnsembleV1.remote(models)\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    adem_buffer = ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2))\n\n    # Get files to process\n    tileinfo: list[RayInputDict] = []\n    for i, (tilekey, outpath) in enumerate(self._tileinfos()):\n        tile_id = self._get_tile_id(tilekey)\n        if not self.overwrite:\n            mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n            if mo == \"none\":\n                logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                continue\n            if mo == \"some\":\n                logger.warning(\n                    f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                    \" Skipping because overwrite=False...\"\n                )\n                continue\n        tileinfo.append({\"tilekey\": tilekey, \"outpath\": str(outpath.resolve()), \"tile_id\": tile_id})\n    tileinfo = tileinfo[:10]\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n\n    # Ray data pipeline\n    # TODO: setup device stuff correctly\n    ds = ray.data.from_items(tileinfo)\n    ds = ds.map(self._load_tile, num_cpus=1)\n    ds = ds.map(\n        _load_aux,\n        fn_kwargs={\n            \"arcticdem_dir\": self.arcticdem_dir,\n            \"arcticdem_resolution\": arcticdem_resolution,\n            \"buffer\": adem_buffer,\n            \"tcvis_dir\": self.tcvis_dir,\n        },\n        num_cpus=1,\n    )\n    ds = ds.map(\n        _preprocess_ray,\n        fn_kwargs={\n            \"tpi_outer_radius\": self.tpi_outer_radius,\n            \"tpi_inner_radius\": self.tpi_inner_radius,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n        concurrency=4,\n    )\n    ds = ds.map(\n        _RayEnsembleV1,\n        fn_constructor_kwargs={\"model_dict\": models},\n        fn_kwargs={\n            \"patch_size\": self.patch_size,\n            \"overlap\": self.overlap,\n            \"batch_size\": self.batch_size,\n            \"reflection\": self.reflection,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n        num_gpus=0.8,\n        concurrency=1,\n    )\n    ds = ds.map(\n        _prepare_export_ray,\n        fn_kwargs={\n            \"binarization_threshold\": self.binarization_threshold,\n            \"mask_erosion_size\": self.mask_erosion_size,\n            \"min_object_size\": self.min_object_size,\n            \"quality_level\": self.quality_level,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n    )\n    ds = ds.map(\n        _export_tile_ray,\n        fn_kwargs={\n            \"export_bands\": self.export_bands,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n    )\n    logger.debug(f\"Ray dataset: {ds}\")\n    logger.info(\"Ray pipeline created. Starting execution...\")\n    # This should trigger the execution\n    ds.write_parquet(f\"local://{self.output_data_dir.resolve()!s}/ray_output.parquet\")\n    logger.info(f\"Ray pipeline finished. Output written to {self.output_data_dir.resolve()!s}/ray_output.parquet\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.VerbosityLevel","title":"VerbosityLevel","text":"<p>               Bases: <code>enum.IntEnum</code></p> <p>Enum for verbosity levels.</p>"},{"location":"reference/darts/cli/#darts.cli.VerbosityLevel.DEBUG","title":"DEBUG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEBUG = 3\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.VerbosityLevel.NORMAL","title":"NORMAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NORMAL = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.VerbosityLevel.VERBOSE","title":"VERBOSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VERBOSE = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.VerbosityLevel.VERY_VERBOSE","title":"VERY_VERBOSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VERY_VERBOSE = 2\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.VerbosityLevel.from_cli","title":"from_cli  <code>classmethod</code>","text":"<pre><code>from_cli(\n    verbose: bool, very_verbose: bool, debug: bool\n) -&gt; darts.utils.logging.VerbosityLevel\n</code></pre> <p>Get the verbosity level from CLI flags.</p> <p>Parameters:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Whether the verbose flag is set.</p> </li> <li> <code>very_verbose</code>               (<code>bool</code>)           \u2013            <p>Whether the very verbose flag is set.</p> </li> <li> <code>debug</code>               (<code>bool</code>)           \u2013            <p>Whether the debug flag is set.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>VerbosityLevel</code> (              <code>darts.utils.logging.VerbosityLevel</code> )          \u2013            <p>The corresponding verbosity level.</p> </li> </ul> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>@classmethod\ndef from_cli(cls, verbose: bool, very_verbose: bool, debug: bool) -&gt; \"VerbosityLevel\":\n    \"\"\"Get the verbosity level from CLI flags.\n\n    Args:\n        verbose (bool): Whether the verbose flag is set.\n        very_verbose (bool): Whether the very verbose flag is set.\n        debug (bool): Whether the debug flag is set.\n\n    Returns:\n        VerbosityLevel: The corresponding verbosity level.\n\n    \"\"\"\n    if debug:\n        return cls.DEBUG\n    if very_verbose:\n        return cls.VERY_VERBOSE\n    if verbose:\n        return cls.VERBOSE\n    return cls.NORMAL\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.benchviz","title":"benchviz","text":"<pre><code>benchviz(\n    stopuhr_data: pathlib.Path,\n    *,\n    viz_dir: pathlib.Path | None = None,\n)\n</code></pre> <p>Visulize benchmark based on a Stopuhr data file produced by a pipeline run.</p> <p>Note</p> <p>This function changes the seaborn theme to \"whitegrid\" for better visualization.</p> <p>Parameters:</p> <ul> <li> <code>stopuhr_data</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the Stopuhr data file.</p> </li> <li> <code>viz_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory where the visualization will be saved. If None, the defaults to the parent directory of the Stopuhr data file. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>plt.Figure: A matplotlib figure containing the benchmark visualization.</p> </li> </ul> Source code in <code>darts/src/darts/utils/bench.py</code> <pre><code>def benchviz(\n    stopuhr_data: Path,\n    *,\n    viz_dir: Path | None = None,\n):\n    \"\"\"Visulize benchmark based on a Stopuhr data file produced by a pipeline run.\n\n    !!! note\n        This function changes the seaborn theme to \"whitegrid\" for better visualization.\n\n    Args:\n        stopuhr_data (Path): Path to the Stopuhr data file.\n        viz_dir (Path | None): Path to the directory where the visualization will be saved.\n            If None, the defaults to the parent directory of the Stopuhr data file.\n            Defaults to None.\n\n    Returns:\n        plt.Figure: A matplotlib figure containing the benchmark visualization.\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    import seaborn as sns\n\n    # Visualize the results\n    sns.set_theme(style=\"whitegrid\")\n\n    assert stopuhr_data.suffix == \".parquet\", \"Stopuhr data file must be a parquet file.\"\n\n    times = pd.read_parquet(stopuhr_data)\n    times_long = times.melt(ignore_index=False, value_name=\"time\", var_name=\"step\").reset_index(drop=False)\n    times_desc = times.describe()\n    times_sum = times.sum()\n\n    # Pretty print the results\n    for col in times_desc.columns:\n        mean = times_desc[col][\"mean\"]\n        std = times_desc[col][\"std\"]\n        total = times_sum[col]\n        n = int(times_desc[col][\"count\"].item())\n        logger.info(f\"{col} took {mean:.2f} \u00b1 {std:.2f}s ({n=} -&gt; {total=:.2f}s)\")\n\n    # axs: hist, histlog, bar, heat\n    fig, axs = plt.subplot_mosaic(\n        [\n            [\"histlog\"] * 4,\n            [\"histlog\"] * 4,\n            [\"hist\", \"hist\", \"heat\", \"heat\"],\n            [\"hist\", \"hist\", \"heat\", \"heat\"],\n            [\"bar\", \"bar\", \"bar\", \"bar\"],\n        ],\n        layout=\"constrained\",\n        figsize=(20, 15),\n    )\n\n    sns.histplot(\n        data=times_long,\n        x=\"time\",\n        hue=\"step\",\n        bins=100,\n        # log_scale=True,\n        ax=axs[\"hist\"],\n    )\n    axs[\"hist\"].set_xlabel(\"Time in seconds\")\n    axs[\"hist\"].set_title(\"Histogram of time taken for each step\", fontdict={\"fontweight\": \"bold\"})\n\n    sns.histplot(\n        data=times_long,\n        x=\"time\",\n        hue=\"step\",\n        bins=100,\n        log_scale=True,\n        kde=True,\n        ax=axs[\"histlog\"],\n    )\n    axs[\"histlog\"].set_xlabel(\"Time in seconds\")\n    axs[\"histlog\"].set_title(\"Histogram of time taken for each step (log scale)\", fontdict={\"fontweight\": \"bold\"})\n\n    sns.heatmap(\n        times.T,\n        robust=True,\n        cbar_kws={\"label\": \"Time in seconds\"},\n        ax=axs[\"heat\"],\n    )\n    axs[\"heat\"].set_xlabel(\"Sample\")\n    axs[\"heat\"].set_title(\"Heatmap of time taken for each step and sample\", fontdict={\"fontweight\": \"bold\"})\n\n    bottom = np.array([0.0])\n    for i, (step, time_taken) in enumerate(times.mean().items()):\n        axs[\"bar\"].barh([\"Time taken\"], [time_taken], label=step, color=sns.color_palette()[i], left=bottom)\n        # Add a text label to the bar\n        axs[\"bar\"].text(\n            bottom[-1] + time_taken / 2,\n            0,\n            f\"{step}:\\n{time_taken:.1f} s\",\n            va=\"center\",\n            ha=\"center\",\n            fontsize=10,\n            color=\"white\",\n        )\n        bottom += time_taken\n    axs[\"bar\"].legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.05), ncol=3)\n    # Make the y-axis labels vertical\n    axs[\"bar\"].set_yticks([0.15], labels=[\"Time taken\"], rotation=90)\n    axs[\"bar\"].set_xlabel(\"Time in seconds\")\n    axs[\"bar\"].set_title(\"Avg. time taken for each step\", fontdict={\"fontweight\": \"bold\"})\n\n    # Save the figure\n    viz_dir = viz_dir or stopuhr_data.parent\n    viz_dir.mkdir(parents=True, exist_ok=True)\n    fpath = viz_dir / stopuhr_data.name.replace(\".parquet\", \".png\")\n    fig.savefig(fpath, dpi=300, bbox_inches=\"tight\")\n    logger.info(f\"Benchmark visualization saved to {fpath.resolve()}\")\n\n    return fig\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.debug_default_paths","title":"debug_default_paths","text":"<pre><code>debug_default_paths(\n    default_paths: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    pipeline_paths: darts.pipelines.sequential_v2.PipelineV2Paths = darts.pipelines.sequential_v2.PipelineV2Paths(),\n)\n</code></pre> <p>Debug and print the current DARTS paths.</p> <p>Parameters:</p> <ul> <li> <code>default_paths</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>Default paths to set before logging. Defaults to DefaultPaths().</p> </li> <li> <code>pipeline_paths</code>               (<code>darts.pipelines.sequential_v2.PipelineV2Paths</code>, default:                   <code>darts.pipelines.sequential_v2.PipelineV2Paths()</code> )           \u2013            <p>Pipeline paths to log. Defaults to PipelineV2Paths().</p> </li> </ul> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.command\ndef debug_default_paths(\n    default_paths: DefaultPaths = DefaultPaths(), pipeline_paths: PipelineV2Paths = PipelineV2Paths()\n):\n    \"\"\"Debug and print the current DARTS paths.\n\n    Args:\n        default_paths (DefaultPaths, optional): Default paths to set before logging.\n            Defaults to DefaultPaths().\n        pipeline_paths (PipelineV2Paths, optional): Pipeline paths to log.\n            Defaults to PipelineV2Paths().\n\n    \"\"\"\n    paths.set_defaults(default_paths)\n    paths.log_all_paths(level=logging.INFO)\n    # TODO: This is just temporary until we upgrade to cyclotps v4 and rework our pipeline structure\n    pipeline_paths.log(level=logging.INFO)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.env_info","title":"env_info","text":"<pre><code>env_info()\n</code></pre> <p>Print debug information about the environment.</p> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.command\ndef env_info():\n    \"\"\"Print debug information about the environment.\"\"\"\n    from darts.utils.cuda import debug_info\n\n    logger.debug(f\"PATH: {os.environ.get('PATH', 'UNSET')}\")\n    debug_info()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.hello","title":"hello","text":"<pre><code>hello(name: str, *, n: int = 1)\n</code></pre> <p>Say hello to someone.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the person to say hello to</p> </li> <li> <code>n</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of times to say hello. Defaults to 1.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If n is 3.</p> </li> </ul> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.command\ndef hello(name: str, *, n: int = 1):\n    \"\"\"Say hello to someone.\n\n    Args:\n        name (str): The name of the person to say hello to\n        n (int, optional): The number of times to say hello. Defaults to 1.\n\n    Raises:\n        ValueError: If n is 3.\n\n    \"\"\"\n    for i in range(n):\n        logger.debug(f\"Currently at {i=}\")\n        if n == 3:\n            raise ValueError(\"I don't like 3\")\n        logger.info(f\"Hello {name}\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.help","title":"help","text":"<pre><code>help()\n</code></pre> <p>Display the help screen.</p> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.command\ndef help():\n    \"\"\"Display the help screen.\"\"\"\n    app.help_print()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.launcher","title":"launcher","text":"<pre><code>launcher(\n    *tokens: str,\n    log_dir: pathlib.Path = pathlib.Path(\"logs\"),\n    config_file: pathlib.Path = pathlib.Path(\"config.toml\"),\n    verbose: bool = False,\n    very_verbose: bool = False,\n    debug: bool = False,\n    log_plain: bool = False,\n)\n</code></pre> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.meta.default\ndef launcher(  # noqa: D103\n    *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n    log_dir: Path = Path(\"logs\"),\n    config_file: Path = Path(\"config.toml\"),\n    verbose: Annotated[bool, cyclopts.Parameter(alias=\"-v\")] = False,\n    very_verbose: Annotated[bool, cyclopts.Parameter(alias=\"-vv\")] = False,\n    debug: Annotated[bool, cyclopts.Parameter(alias=\"-vvv\")] = False,\n    log_plain: bool = False,\n):\n    verbosity = VerbosityLevel.from_cli(verbose, very_verbose, debug)\n    command, bound, ignored = app.parse_args(tokens, verbose=verbosity == VerbosityLevel.VERBOSE)\n    # Set verbosity to 1 for debug stuff like env_info\n    if command.__name__ == \"env_info\" and verbosity == VerbosityLevel.NORMAL:\n        verbosity = VerbosityLevel.VERBOSE\n    LoggingManager.add_logging_handlers(command.__name__, log_dir, verbosity, log_plain=log_plain)\n    logger.debug(f\"Running on Python version {sys.version} from {__name__} ({root_file})\")\n    additional_args = {}\n    if \"config_file\" in ignored:\n        additional_args[\"config_file\"] = config_file\n    if \"log_dir\" in ignored:\n        additional_args[\"log_dir\"] = log_dir\n    if \"verbosity\" in ignored:\n        additional_args[\"verbosity\"] = verbosity\n    return command(*bound.args, **bound.kwargs, **additional_args)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.preprocess_planet_train_data","title":"preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    train_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    admin_dir: pathlib.Path | None = None,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches and storing them in a zarr array for efficient random access during training. All data is stored in a single zarr group with associated metadata.</p> <p>The preprocessing creates patches of the specified size from each Planet scene and stores them as: - A zarr group containing 'x' (input data) and 'y' (labels) arrays - A geopandas dataframe with metadata including region, position, and label statistics - A configuration file with preprocessing parameters</p> <p>The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in each patch being stored in a separate file for super fast random access.</p> <p>The metadata dataframe contains information about each patch including: - sample_id: Identifier for the source Planet scene - region: Administrative region name - geometry: Spatial extent of the patch - empty: Whether the patch contains positive labeled pixels - Additional metadata as specified</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>A <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n\u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n\u251c\u2500\u2500 metadata.parquet\n\u2514\u2500\u2500 {timestamp}.cli.toml\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The \"output\" directory where the tensors are written to. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the TCVis data. If None, will use the default TCVis directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the admin files. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. If None, will neither use nor store preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def preprocess_planet_train_data(  # noqa: C901\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    train_data_dir: Path | None = None,\n    arcticdem_dir: Path | None = None,\n    tcvis_dir: Path | None = None,\n    admin_dir: Path | None = None,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches\n    and storing them in a zarr array for efficient random access during training. All data is stored in\n    a single zarr group with associated metadata.\n\n    The preprocessing creates patches of the specified size from each Planet scene and stores them as:\n    - A zarr group containing 'x' (input data) and 'y' (labels) arrays\n    - A geopandas dataframe with metadata including region, position, and label statistics\n    - A configuration file with preprocessing parameters\n\n    The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in\n    each patch being stored in a separate file for super fast random access.\n\n    The metadata dataframe contains information about each patch including:\n    - sample_id: Identifier for the source Planet scene\n    - region: Administrative region name\n    - geometry: Spatial extent of the patch\n    - empty: Whether the patch contains positive labeled pixels\n    - Additional metadata as specified\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    A `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/\n    \u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n    \u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n    \u251c\u2500\u2500 metadata.parquet\n    \u2514\u2500\u2500 {timestamp}.cli.toml\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        train_data_dir (Path | None, optional): The \"output\" directory where the tensors are written to.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        arcticdem_dir (Path | None, optional): The directory containing the ArcticDEM data\n            (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        tcvis_dir (Path | None, optional): The directory containing the TCVis data.\n            If None, will use the default TCVis directory based on the DARTS paths.\n            Defaults to None.\n        admin_dir (Path | None, optional): The directory containing the admin files.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        preprocess_cache (Path | None, optional): The directory to store the preprocessed data.\n            If None, will neither use nor store preprocessed data.\n            Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    paths.set_defaults(default_dirs)\n    train_data_dir = train_data_dir or paths.train_data_dir(\"planet_v2_rts\", patch_size)\n    arcticdem_dir = arcticdem_dir or paths.arcticdem(2)\n    tcvis_dir = tcvis_dir or paths.tcvis()\n    admin_dir = admin_dir or paths.admin_boundaries()\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.toml\",\n        function=preprocess_planet_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import smart_geocubes\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import LoggingManager\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    accessor = smart_geocubes.ArcticDEM2m(arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    fpaths = {fpath.stem: fpath for fpath in _legacy_path_gen(data_dir)}\n    footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these since they depend on the preprocessing we use\n    bands = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"nir\",\n        \"ndvi\",\n        \"relative_elevation\",\n        \"slope\",\n        \"aspect\",\n        \"hillshade\",\n        \"curvature\",\n        \"tc_brightness\",\n        \"tc_greenness\",\n        \"tc_wetness\",\n    ]\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache)\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        info_id = f\"{planet_id=} ({i + 1} of {len(footprint)})\"\n        try:\n            logger.info(f\"Processing sample {info_id}\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(\n                    f\"Footprint image '{planet_id}' at {footprint.fpath} does not exist. Skipping {info_id}...\"\n                )\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {info_id}\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {info_id}. Skipping...\")\n            logger.exception(e)\n\n    timer.summary()\n\n    if len(builder) == 0:\n        logger.warning(\"No samples were processed. Exiting...\")\n        return\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.preprocess_planet_train_data_pingo","title":"preprocess_planet_train_data_pingo","text":"<pre><code>preprocess_planet_train_data_pingo(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    train_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    admin_dir: pathlib.Path | None = None,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n)\n</code></pre> <p>Preprocess Planet data for training (Pingo version).</p> <p>This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches and storing them in a zarr array for efficient random access during training. All data is stored in a single zarr group with associated metadata.</p> <p>The preprocessing creates patches of the specified size from each Planet scene and stores them as: - A zarr group containing 'x' (input data) and 'y' (labels) arrays - A geopandas dataframe with metadata including region, position, and label statistics - A configuration file with preprocessing parameters</p> <p>The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in each patch being stored in a separate file for super fast random access.</p> <p>The metadata dataframe contains information about each patch including: - sample_id: Identifier for the source Planet scene - region: Administrative region name - geometry: Spatial extent of the patch - empty: Whether the patch contains positive labeled pixels - Additional metadata as specified</p> <p>Through <code>exclude_nopositive</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>A <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n\u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n\u251c\u2500\u2500 metadata.parquet\n\u2514\u2500\u2500 {timestamp}.cli.json\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The \"output\" directory where the tensors are written to. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the TCVis data. If None, will use the default TCVis directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the admin files. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. If None, will neither use nor store preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def preprocess_planet_train_data_pingo(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    train_data_dir: Path | None = None,\n    arcticdem_dir: Path | None = None,\n    tcvis_dir: Path | None = None,\n    admin_dir: Path | None = None,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n):\n    \"\"\"Preprocess Planet data for training (Pingo version).\n\n    This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches\n    and storing them in a zarr array for efficient random access during training. All data is stored in\n    a single zarr group with associated metadata.\n\n    The preprocessing creates patches of the specified size from each Planet scene and stores them as:\n    - A zarr group containing 'x' (input data) and 'y' (labels) arrays\n    - A geopandas dataframe with metadata including region, position, and label statistics\n    - A configuration file with preprocessing parameters\n\n    The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in\n    each patch being stored in a separate file for super fast random access.\n\n    The metadata dataframe contains information about each patch including:\n    - sample_id: Identifier for the source Planet scene\n    - region: Administrative region name\n    - geometry: Spatial extent of the patch\n    - empty: Whether the patch contains positive labeled pixels\n    - Additional metadata as specified\n\n    Through `exclude_nopositive` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    A `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/\n    \u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n    \u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n    \u251c\u2500\u2500 metadata.parquet\n    \u2514\u2500\u2500 {timestamp}.cli.json\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        train_data_dir (Path | None, optional): The \"output\" directory where the tensors are written to.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        arcticdem_dir (Path | None, optional): The directory containing the ArcticDEM data\n            (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        tcvis_dir (Path | None, optional): The directory containing the TCVis data.\n            If None, will use the default TCVis directory based on the DARTS paths.\n            Defaults to None.\n        admin_dir (Path | None, optional): The directory containing the admin files.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        preprocess_cache (Path | None, optional): The directory to store the preprocessed data.\n            If None, will neither use nor store preprocessed data.\n            Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    paths.set_defaults(default_dirs)\n    train_data_dir = train_data_dir or paths.train_data_dir(\"planet_v2_pingo\", patch_size)\n    arcticdem_dir = arcticdem_dir or paths.arcticdem(2)\n    tcvis_dir = tcvis_dir or paths.tcvis()\n    admin_dir = admin_dir or paths.admin_boundaries()\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.toml\",\n        function=preprocess_planet_train_data_pingo,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import smart_geocubes\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import LoggingManager\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    accessor = smart_geocubes.ArcticDEM2m(arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    footprints[\"fpath\"] = footprints.image_id.map(_path_gen(data_dir))\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these since they depend on the preprocessing we use\n    bands = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"nir\",\n        \"ndvi\",\n        \"relative_elevation\",\n        \"slope\",\n        \"aspect\",\n        \"hillshade\",\n        \"curvature\",\n        \"tc_brightness\",\n        \"tc_greenness\",\n        \"tc_wetness\",\n    ]\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        device=device,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache)\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        info_id = f\"{planet_id=} ({i + 1} of {len(footprint)})\"\n        try:\n            logger.debug(f\"Processing sample {info_id}\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(\n                    f\"Footprint image '{planet_id}' at {footprint.fpath} does not exist. Skipping {info_id}...\"\n                )\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {info_id}\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {info_id} . Skipping...\")\n            logger.exception(e)\n\n    timer.summary()\n\n    if len(builder) == 0:\n        logger.warning(\"No samples were processed. Exiting...\")\n        return\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.preprocess_s2_train_data","title":"preprocess_s2_train_data","text":"<pre><code>preprocess_s2_train_data(\n    *,\n    labels_dir: pathlib.Path,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    train_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    admin_dir: pathlib.Path | None = None,\n    planet_data_dir: pathlib.Path | None = None,\n    raw_data_store: pathlib.Path | None = None,\n    no_raw_data_store: bool = False,\n    preprocess_cache: pathlib.Path | None = None,\n    matching_cache: pathlib.Path | None = None,\n    no_matching_cache: bool = False,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    matching_day_range: int = 7,\n    matching_max_cloud_cover: int = 10,\n    matching_min_intersects: float = 0.7,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    save_matching_scores: bool = False,\n)\n</code></pre> <p>Preprocess Sentinel-2 data for training.</p> <p>This function preprocesses Sentinel-2 scenes matched to Planet footprints into a training-ready format by creating fixed-size patches and storing them in a zarr array for efficient random access during training. All data is stored in a single zarr group with associated metadata.</p> <p>The preprocessing matches Sentinel-2 scenes to Planet footprints based on temporal and spatial criteria, optionally aligns them spatially to Planet data, and creates patches of the specified size. The data is stored as: - A zarr group containing 'x' (input data) and 'y' (labels) arrays - A geopandas dataframe with metadata including region, position, and label statistics - A configuration file with preprocessing parameters</p> <p>The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in each patch being stored in a separate file for super fast random access.</p> <p>The metadata dataframe contains information about each patch including: - sample_id: Combined identifier for the S2 scene and Planet footprint - region: Administrative region name - geometry: Spatial extent of the patch - empty: Whether the patch contains positive labeled pixels - planet_id: Original Planet scene identifier - s2_id: Sentinel-2 scene identifier - Additional alignment and matching metadata</p> <p>Through <code>exclude_nopositive</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>A <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n\u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n\u251c\u2500\u2500 metadata.parquet\n\u251c\u2500\u2500 matching-cache.json      # Optional matching cache\n\u251c\u2500\u2500 matching-scores.parquet  # Optional matching scores\n\u2514\u2500\u2500 {timestamp}.cli.toml\n</code></pre> <p>Parameters:</p> <ul> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The \"output\" directory where the tensors are written to. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the TCVis data. If None, will use the default TCVis directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the admin files. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>planet_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the Planet scenes and orthotiles. The planet data is used to align the Sentinel-2 data to the Planet data, spatially. Can be set to None if no alignment is wished. Defaults to None.</p> </li> <li> <code>raw_data_store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to use for storing the raw Sentinel 2 data locally. If None, will use the default raw data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>no_raw_data_store</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will not store any raw data locally. This overrides the <code>raw_data_store</code> parameter. Defaults to False.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. If None, will neither use nor store preprocessed data. Defaults to None.</p> </li> <li> <code>matching_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to a file where the matchings are stored. Note: this is different from the matching scores. If None, will query the sentinel 2 STAC and calculate the best match based on the criteria. Defaults to None.</p> </li> <li> <code>no_matching_cache</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will not use or store any matching cache. This overrides the <code>matching_cache</code> parameter. Defaults to False.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com). Defaults to True.</p> </li> <li> <code>matching_day_range</code>               (<code>int</code>, default:                   <code>7</code> )           \u2013            <p>The day range to use for matching S2 scenes to Planet footprints. Defaults to 7.</p> </li> <li> <code>matching_max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for matching S2 scenes to Planet footprints. Defaults to 10.</p> </li> <li> <code>matching_min_intersects</code>               (<code>float</code>, default:                   <code>0.7</code> )           \u2013            <p>The minimum intersection percentage to use for matching S2 scenes to Planet footprints. Defaults to 0.7.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>save_matching_scores</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the matching scores. Defaults to False.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_sentinel2_v2.py</code> <pre><code>def preprocess_s2_train_data(  # noqa: C901\n    *,\n    labels_dir: Path,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    train_data_dir: Path | None = None,\n    arcticdem_dir: Path | None = None,\n    tcvis_dir: Path | None = None,\n    admin_dir: Path | None = None,\n    planet_data_dir: Path | None = None,\n    raw_data_store: Path | None = None,\n    no_raw_data_store: bool = False,\n    preprocess_cache: Path | None = None,\n    matching_cache: Path | None = None,\n    no_matching_cache: bool = False,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    matching_day_range: int = 7,\n    matching_max_cloud_cover: int = 10,\n    matching_min_intersects: float = 0.7,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    save_matching_scores: bool = False,\n):\n    \"\"\"Preprocess Sentinel-2 data for training.\n\n    This function preprocesses Sentinel-2 scenes matched to Planet footprints into a training-ready format\n    by creating fixed-size patches and storing them in a zarr array for efficient random access during training.\n    All data is stored in a single zarr group with associated metadata.\n\n    The preprocessing matches Sentinel-2 scenes to Planet footprints based on temporal and spatial criteria,\n    optionally aligns them spatially to Planet data, and creates patches of the specified size. The data is stored as:\n    - A zarr group containing 'x' (input data) and 'y' (labels) arrays\n    - A geopandas dataframe with metadata including region, position, and label statistics\n    - A configuration file with preprocessing parameters\n\n    The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in\n    each patch being stored in a separate file for super fast random access.\n\n    The metadata dataframe contains information about each patch including:\n    - sample_id: Combined identifier for the S2 scene and Planet footprint\n    - region: Administrative region name\n    - geometry: Spatial extent of the patch\n    - empty: Whether the patch contains positive labeled pixels\n    - planet_id: Original Planet scene identifier\n    - s2_id: Sentinel-2 scene identifier\n    - Additional alignment and matching metadata\n\n    Through `exclude_nopositive` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    A `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/\n    \u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n    \u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n    \u251c\u2500\u2500 metadata.parquet\n    \u251c\u2500\u2500 matching-cache.json      # Optional matching cache\n    \u251c\u2500\u2500 matching-scores.parquet  # Optional matching scores\n    \u2514\u2500\u2500 {timestamp}.cli.toml\n    ```\n\n    Args:\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        train_data_dir (Path | None, optional): The \"output\" directory where the tensors are written to.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        arcticdem_dir (Path | None, optional): The directory containing the ArcticDEM data\n            (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        tcvis_dir (Path | None, optional): The directory containing the TCVis data.\n            If None, will use the default TCVis directory based on the DARTS paths.\n            Defaults to None.\n        admin_dir (Path | None, optional): The directory containing the admin files.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        planet_data_dir (Path, optional): The directory containing the Planet scenes and orthotiles.\n            The planet data is used to align the Sentinel-2 data to the Planet data, spatially.\n            Can be set to None if no alignment is wished.\n            Defaults to None.\n        raw_data_store (Path | None): The directory to use for storing the raw Sentinel 2 data locally.\n            If None, will use the default raw data directory based on the DARTS paths.\n            Defaults to None.\n        no_raw_data_store (bool, optional): If True, will not store any raw data locally.\n            This overrides the `raw_data_store` parameter.\n            Defaults to False.\n        preprocess_cache (Path | None, optional): The directory to store the preprocessed data.\n            If None, will neither use nor store preprocessed data.\n            Defaults to None.\n        matching_cache (Path | None, optional): The path to a file where the matchings are stored.\n            Note: this is different from the matching scores.\n            If None, will query the sentinel 2 STAC and calculate the best match based on the criteria.\n            Defaults to None.\n        no_matching_cache (bool, optional): If True, will not use or store any matching cache.\n            This overrides the `matching_cache` parameter.\n            Defaults to False.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n            Defaults to True.\n        matching_day_range (int, optional): The day range to use for matching S2 scenes to Planet footprints.\n            Defaults to 7.\n        matching_max_cloud_cover (int, optional): The maximum cloud cover percentage to use for matching S2 scenes\n            to Planet footprints. Defaults to 10.\n        matching_min_intersects (float, optional): The minimum intersection percentage to use for matching S2 scenes\n            to Planet footprints. Defaults to 0.7.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        save_matching_scores (bool, optional): Whether to save the matching scores. Defaults to False.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    paths.set_defaults(default_dirs)\n    train_data_dir = train_data_dir or paths.train_data_dir(\"sentinel2_v2_rts\", patch_size)\n    arcticdem_dir = arcticdem_dir or paths.arcticdem(10)\n    tcvis_dir = tcvis_dir or paths.tcvis()\n    admin_dir = admin_dir or paths.admin_boundaries()\n    raw_data_store = raw_data_store or paths.sentinel2_raw_data(\"cdse\")\n    if no_raw_data_store:\n        raw_data_store = None\n    matching_cache = matching_cache or train_data_dir / \"matching-cache.json\"\n    if no_matching_cache:\n        matching_cache = None\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.toml\",\n        function=preprocess_s2_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import smart_geocubes\n    import xarray as xr\n    from botocore.exceptions import ProfileNotFound\n    from darts_acquisition import (\n        load_arcticdem,\n        load_cdse_s2_sr_scene,\n        load_tcvis,\n        match_cdse_s2_sr_scene_ids_from_geodataframe,\n    )\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.geo.geom import Geometry\n    from pystac import Item\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import LoggingManager\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    logger.info(\"Configured Rasterio\")\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    accessor = smart_geocubes.ArcticDEM10m(arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    footprints[\"geometry\"] = footprints[\"geometry\"].simplify(0.001)  # Simplify to reduce compute\n    footprints[\"date\"] = footprints.apply(_parse_date, axis=1)\n    if planet_data_dir is not None:\n        fpaths = {fpath.stem: fpath for fpath in _planet_legacy_path_gen(planet_data_dir)}\n        footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    logger.info(f\"label directory contained {len(footprints)} footprints\")\n\n    # Find S2 scenes that intersect with the Planet footprints\n    if matching_cache is None or not matching_cache.exists():\n        logger.info(\"evaluating online CDSE catalogue for matching Sentinel-2 scenes\")\n        matches = match_cdse_s2_sr_scene_ids_from_geodataframe(\n            aoi=footprints,\n            day_range=matching_day_range,\n            max_cloud_cover=matching_max_cloud_cover,\n            min_intersects=matching_min_intersects,\n            simplify_geometry=0.001,\n            save_scores=train_data_dir / \"matching-scores.parquet\" if save_matching_scores else None,\n        )\n        if matching_cache is not None:\n            matches_serializable = {k: v.to_dict() if isinstance(v, Item) else \"None\" for k, v in matches.items()}\n            with matching_cache.open(\"w\") as f:\n                json.dump(matches_serializable, f)\n            logger.info(f\"Saved matching scores to {matching_cache}\")\n            del matches_serializable  # Free memory\n    else:\n        logger.info(f\"Loading matching scores from {matching_cache}\")\n        with matching_cache.open(\"r\") as f:\n            matches_serializable = json.load(f)\n        matches = {int(k): Item.from_dict(v) if v != \"None\" else None for k, v in matches_serializable.items()}\n        del matches_serializable  # Free memory\n    footprints[\"s2_item\"] = footprints.index.map(matches)\n\n    # Filter out footprints without a matching S2 item\n    logger.info(f\"Found {len(footprints)} footprints, {footprints.s2_item.notna().sum()} with matching S2 items.\")\n    footprints = footprints[footprints.s2_item.notna()]\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these since they depend on the preprocessing we use\n    bands = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"nir\",\n        \"ndvi\",\n        \"relative_elevation\",\n        \"slope\",\n        \"aspect\",\n        \"hillshade\",\n        \"curvature\",\n        \"tc_brightness\",\n        \"tc_greenness\",\n        \"tc_wetness\",\n    ]\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache)\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        s2_item = footprint.s2_item\n        # Convert to stac item if dictionary\n        if isinstance(s2_item, dict):\n            s2_item = Item.from_dict(s2_item)\n\n        s2_id = s2_item.id\n        planet_id = footprint.image_id\n        info_id = f\"{s2_id=} -&gt; {planet_id=} ({i + 1} of {len(footprints)})\"\n        try:\n            logger.info(f\"Processing sample {info_id}\")\n\n            if planet_data_dir is not None and (\n                not footprint.fpath or pd.isna(footprint.fpath) or (not footprint.fpath.exists())\n            ):\n                logger.warning(\n                    f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping sample {info_id}...\"\n                )\n                continue\n\n            def _get_tile():\n                s2ds = load_cdse_s2_sr_scene(s2_item, store=raw_data_store)\n\n                # Crop to footprint geometry\n                geom = Geometry(footprint.geometry, crs=footprints.crs)\n                s2ds = s2ds.odc.crop(geom, apply_mask=True)\n                # Crop above will change all dtypes to float32 -&gt; change them back for s2_scl and qa mask\n                s2ds[\"s2_scl\"] = s2ds[\"s2_scl\"].fillna(0.0).astype(\"uint8\")\n                s2ds[\"quality_data_mask\"] = s2ds[\"quality_data_mask\"].fillna(0.0).astype(\"uint8\")\n\n                # Preprocess as usual\n                arctidem_res = 10\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    s2ds.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(s2ds.odc.geobox, tcvis_dir)\n\n                s2ds: xr.Dataset = preprocess_v2(\n                    s2ds,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return s2ds\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=f\"preprocess-s2train-v2-{s2_id}_{planet_id}\",\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            # Skip if the size is too small\n            if tile.sizes[\"x\"] &lt; patch_size or tile.sizes[\"y\"] &lt; patch_size:\n                logger.info(f\"Skipping sample {info_id} due to small size {tile.sizes}.\")\n                continue\n\n            footprint_labels = labels[labels.image_id == planet_id].to_crs(tile.odc.crs)\n            region = _get_region_name(footprint, admin2)\n\n            if planet_data_dir is not None:\n                with timer(\"Align to PLANET\"):\n                    footprint_labels, offsets_info = _align_offsets(tile, footprint, footprint_labels)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=f\"{s2_id}_{planet_id}\",\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"s2_id\": s2_id,\n                        \"fpath\": footprint.fpath,\n                        **offsets_info,\n                    },\n                )\n\n            logger.info(f\"Processed sample {info_id}\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n        except ProfileNotFound:\n            logger.error(\"tried to download from CDSE@AWS but no CDSE credentials found. \")\n            return\n        except Exception as e:\n            logger.warning(f\"Could not process sample {info_id}. Skipping...\")\n            logger.exception(e)\n\n    timer.summary()\n\n    if len(builder) == 0:\n        logger.warning(\"No samples were processed. Exiting...\")\n        return\n\n    builder.finalize(\n        {\n            \"planet_data_dir\": planet_data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.shell","title":"shell","text":"<pre><code>shell()\n</code></pre> <p>Open an interactive shell.</p> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.command\ndef shell():\n    \"\"\"Open an interactive shell.\"\"\"\n    app.interactive_shell()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.start_app","title":"start_app","text":"<pre><code>start_app()\n</code></pre> <p>Wrapp to start the app.</p> Source code in <code>darts/src/darts/cli.py</code> <pre><code>def start_app():\n    \"\"\"Wrapp to start the app.\"\"\"\n    try:\n        # First time initialization of the logging manager\n        LoggingManager.setup_logging()\n        app.meta()\n    except KeyboardInterrupt:\n        logger.info(\"Interrupted by user. Closing...\")\n    except SystemExit:\n        logger.info(\"Closing...\")\n    except Exception as e:\n        logger.exception(e)\n</code></pre>"},{"location":"reference/darts/pipelines/","title":"pipelines","text":""},{"location":"reference/darts/pipelines/#darts.pipelines","title":"darts.pipelines","text":"<p>Predefined pipelines for DARTS.</p>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline","title":"PlanetPipeline  <code>dataclass</code>","text":"<pre><code>PlanetPipeline(\n    model_files: list[pathlib.Path] = None,\n    default_dirs: darts_utils.paths.DefaultPaths = (\n        lambda: darts_utils.paths.DefaultPaths()\n    )(),\n    output_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    edge_erosion_size: int | None = None,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    offline: bool = False,\n    debug_data: bool = False,\n    orthotiles_dir: pathlib.Path | None = None,\n    scenes_dir: pathlib.Path | None = None,\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for processing PlanetScope data.</p> <p>Processes PlanetScope imagery (both orthotiles and scenes) for RTS segmentation. Supports both offline and online processing modes.</p> Data Structure <p>Expects PlanetScope data organized as: - Orthotiles: <code>orthotiles_dir/tile_id/scene_id/</code> - Scenes: <code>scenes_dir/scene_id/</code></p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory containing PlanetScope orthotiles. If None, uses default path from DARTS paths. Defaults to None.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory containing PlanetScope scenes. If None, uses default path from DARTS paths. Defaults to None.</p> </li> <li> <code>image_ids</code>               (<code>list | None</code>, default:                   <code>None</code> )           \u2013            <p>List of image/scene IDs to process. If None, processes all images found in orthotiles_dir and scenes_dir. Defaults to None.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path] | None</code>, default:                   <code>None</code> )           \u2013            <p>Path(s) to model file(s) for segmentation. Single Path implies <code>write_model_outputs=False</code>. If None, searches default model directory for all .pt files. Defaults to None.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Output directory for results. If None, uses <code>{default_out}/planet</code>. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for ArcticDEM datacube. Will be created/downloaded if needed. If None, uses default path. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for TCVis data. If None, uses default path. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>, default:                   <code>None</code> )           \u2013            <p>Computation device. \"cuda\" uses GPU 0, int specifies GPU index, \"auto\" selects free GPU. Defaults to None.</p> </li> <li> <code>ee_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Earth Engine project ID. May be omitted if defined in persistent credentials. Defaults to None.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use EE high-volume server. Defaults to True.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Outer radius (m) for TPI calculation. Defaults to 100.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Inner radius (m) for TPI calculation. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Patch size for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Overlap between patches. Defaults to 256.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Disk size for mask erosion and inner edge cropping. Defaults to 10.</p> </li> <li> <code>edge_erosion_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Size for outer edge cropping. If None, uses <code>mask_erosion_size</code>. Defaults to None.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Minimum object size (pixels) to keep. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>Quality filtering level. 0=\"none\", 1=\"low_quality\", 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>Bands to export. Can include \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\", \"metadata\", or specific band names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Save individual model outputs (not just ensemble). Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Overwrite existing output files. Defaults to False.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Skip downloading missing data. Defaults to False.</p> </li> <li> <code>debug_data</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Write intermediate debugging data. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.debug_data","title":"debug_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug_data: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.default_dirs","title":"default_dirs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_dirs: darts_utils.paths.DefaultPaths = dataclasses.field(\n    default_factory=lambda: darts_utils.paths.DefaultPaths()\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.edge_erosion_size","title":"edge_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>edge_erosion_size: int | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.offline","title":"offline  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offline: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def __post_init__(self):  # noqa: D105\n    super().__post_init__()\n    self.output_data_dir = self.output_data_dir or paths.output_data(\"planet\")\n    self.orthotiles_dir = self.orthotiles_dir or paths.planet_orthotiles()\n    self.scenes_dir = self.scenes_dir or paths.planet_scenes()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n)\n</code></pre> <p>Run the sequential pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.PlanetPipeline</code>)           \u2013            <p>Configured PlanetPipeline instance.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetPipeline\"):\n    \"\"\"Run the sequential pipeline for PlanetScope data.\n\n    Args:\n        pipeline: Configured PlanetPipeline instance.\n\n    \"\"\"\n    pipeline.__post_init__()\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.cli_prepare_data","title":"cli_prepare_data  <code>staticmethod</code>","text":"<pre><code>cli_prepare_data(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download all necessary data for offline processing.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.PlanetPipeline</code>)           \u2013            <p>Configured PlanetPipeline instance.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of the <code>aux</code> flag or model needs. Defaults to False.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli_prepare_data(*, pipeline: \"PlanetPipeline\", aux: bool = False, force: bool = False):\n    \"\"\"Download all necessary data for offline processing.\n\n    Args:\n        pipeline: Configured PlanetPipeline instance.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.\n        force: If True, downloads all possible data, independent of the `aux` flag or model needs.\n            Defaults to False.\n\n    \"\"\"\n    assert not pipeline.offline, \"Pipeline must be online to prepare data for offline usage.\"\n    pipeline.__post_init__()\n    pipeline.prepare_data(optical=False, aux=aux, force=force)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download and prepare data for offline processing.</p> <p>Validates configuration, determines data requirements from models, and downloads requested data (optical imagery and/or auxiliary data).</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> <li> <code>SystemExit</code>             \u2013            <p>If the process is terminated.</p> </li> <li> <code>SystemError</code>             \u2013            <p>If a system error occurs.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def prepare_data(self, optical: bool = False, aux: bool = False, force: bool = False):\n    \"\"\"Download and prepare data for offline processing.\n\n    Validates configuration, determines data requirements from models,\n    and downloads requested data (optical imagery and/or auxiliary data).\n\n    Args:\n        optical: If True, downloads optical imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n        SystemExit: If the process is terminated.\n        SystemError: If a system error occurs.\n\n    \"\"\"\n    assert optical or aux, \"Nothing to prepare. Please set optical and/or aux to True.\"\n\n    # ? We only want to download stuff - no need for using the GPU here\n    self.device = \"cpu\"\n    self._dump_config()\n\n    from darts_acquisition import download_arcticdem, download_tcvis\n    from stopuhr import Chronometer\n\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n\n    if aux or force:\n        # Get the ensemble to check which auxiliary data is necessary\n        if force:\n            needs_arcticdem, needs_tcvis = True, True\n        else:\n            ensemble = self._load_ensemble()\n            needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n        if not needs_arcticdem and not needs_tcvis:\n            logger.warning(\"No auxiliary data required by the models. Skipping download of auxiliary data...\")\n        else:\n            logger.info(f\"Models {needs_tcvis=} {needs_arcticdem=}.\")\n            self._create_auxiliary_datacubes(arcticdem=needs_arcticdem, tcvis=needs_tcvis)\n\n            # Predownload auxiliary\n            aoi = self._tile_aoi()\n            if needs_arcticdem:\n                logger.info(\"start download ArcticDEM\")\n                with timer(\"Downloading ArcticDEM\"):\n                    download_arcticdem(aoi, self.arcticdem_dir, resolution=self._arcticdem_resolution())\n            if needs_tcvis:\n                logger.info(\"start download TCVIS\")\n                init_ee(self.ee_project, self.ee_use_highvolume)\n                with timer(\"Downloading TCVis\"):\n                    download_tcvis(aoi, self.tcvis_dir)\n\n    # Predownload tiles if optical flag is set\n    if not optical and not force:\n        return\n\n    # Iterate over all the data\n    with timer(\"Loading Optical\"):\n        tileinfo = self._tileinfos()\n        n_tiles = 0\n        logger.info(f\"Found {len(tileinfo)} tiles to download.\")\n        for i, (tilekey, _) in enumerate(tileinfo):\n            tile_id = self._get_tile_id(tilekey)\n            try:\n                self._download_tile(tilekey)\n                n_tiles += 1\n                logger.info(f\"Downloaded sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n            except (KeyboardInterrupt, SystemError, SystemExit) as e:\n                logger.warning(f\"{type(e).__name__} detected.\\nExiting...\")\n                raise e\n            except Exception as e:\n                logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n                logger.exception(e)\n        else:\n            logger.info(f\"Downloaded {n_tiles} tiles.\")\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> <p>Run the complete segmentation pipeline.</p> <p>Executes the full pipeline including: 1. Configuration validation and dumping 2. Loading ensemble models 3. Creating/loading auxiliary datacubes 4. Processing each tile:    - Loading optical data    - Loading auxiliary data (ArcticDEM, TCVis) as needed    - Preprocessing    - Segmentation    - Postprocessing    - Exporting results 5. Saving results and timing information</p> <p>Results are saved to the output directory with timestamped configuration, results parquet file, and timing information.</p> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    \"\"\"Run the complete segmentation pipeline.\n\n    Executes the full pipeline including:\n    1. Configuration validation and dumping\n    2. Loading ensemble models\n    3. Creating/loading auxiliary datacubes\n    4. Processing each tile:\n       - Loading optical data\n       - Loading auxiliary data (ArcticDEM, TCVis) as needed\n       - Preprocessing\n       - Segmentation\n       - Postprocessing\n       - Exporting results\n    5. Saving results and timing information\n\n    Results are saved to the output directory with timestamped configuration,\n    results parquet file, and timing information.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n\n    \"\"\"\n    self._validate()\n    current_time = self._dump_config()\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    import pandas as pd\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_v2\n    from stopuhr import Chronometer, stopwatch\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n    self.device = decide_device(self.device)\n\n    if not self.offline:\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    self._create_auxiliary_datacubes()\n\n    # determine models to use\n    ensemble = self._load_ensemble()\n    ensemble_subsets = ensemble.model_names\n    needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=ensemble_subsets)\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} seems to be already processed, \"\n                        \"but some of the requested outputs are missing. \"\n                        \"Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading Optical\", log=False):\n                tile = self._load_tile(tilekey)\n\n            if needs_arcticdem:\n                with timer(\"Loading ArcticDEM\", log=False):\n                    arcticdem_resolution = self._arcticdem_resolution()\n                    arcticdem = load_arcticdem(\n                        tile.odc.geobox,\n                        self.arcticdem_dir,\n                        resolution=arcticdem_resolution,\n                        buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                        offline=self.offline,\n                    )\n            else:\n                arcticdem = None\n\n            if needs_tcvis:\n                with timer(\"Loading TCVis\", log=False):\n                    tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir, offline=self.offline)\n            else:\n                tcvis = None\n\n            with timer(\"Preprocessing\", log=False):\n                tile = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n\n            with timer(\"Postprocessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    device=self.device,\n                    edge_erosion_size=self.edge_erosion_size,\n                )\n\n            export_metadata = self._result_metadata(tilekey)\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    metadata=export_metadata,\n                    debug=self.debug_data,\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.timer.parquet\")\n            if len(stopwatch.durations) &gt; 0:\n                stopwatch.export().to_parquet(self.output_data_dir / f\"{current_time}.stopwatch.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary(printer=logger.info)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline","title":"PlanetRayPipeline  <code>dataclass</code>","text":"<pre><code>PlanetRayPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    num_cpus: int = 1,\n    devices: list[int] | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    orthotiles_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSOrthoTile\"\n    ),\n    scenes_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSScene\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.ray_v2._BaseRayPipeline</code></p> <p>Pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSOrthoTile')</code> )           \u2013            <p>The directory containing the PlanetScope orthotiles.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSScene')</code> )           \u2013            <p>The directory containing the PlanetScope scenes.</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.num_cpus","title":"num_cpus  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_cpus: int = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSOrthoTile\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSScene\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(*, pipeline: darts.pipelines.ray_v2.PlanetRayPipeline)\n</code></pre> <p>Run the sequential pipeline for Planet data.</p> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetRayPipeline\"):\n    \"\"\"Run the sequential pipeline for Planet data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetRayPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    if self.devices is not None:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(d) for d in self.devices)\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import ray\n\n    ray_context = ray.init(\n        num_cpus=self.num_cpus,  # We use one CPU per Ray task\n        num_gpus=len(self.devices) if self.devices is not None else None,\n    )\n    logger.debug(f\"Ray initialized with context: {ray_context}\")\n    logger.info(f\"Ray Dashboard URL: {ray_context.dashboard_url}\")\n    logger.debug(f\"Ray cluster resources: {ray.cluster_resources()}\")\n    logger.debug(f\"Ray available resources: {ray.available_resources()}\")\n\n    # Initlize ee in every worker\n    @ray.remote\n    def init_worker():\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    num_workers = int(ray.cluster_resources().get(\"CPU\", 1))\n    logger.info(f\"Initializing {num_workers} Ray workers with Earth Engine.\")\n    ray.get([init_worker.remote() for _ in range(num_workers)])\n\n    import smart_geocubes\n    from darts_export import missing_outputs\n\n    from darts.pipelines._ray_wrapper import (\n        _export_tile_ray,\n        _load_aux,\n        _prepare_export_ray,\n        _preprocess_ray,\n        _RayEnsembleV1,\n    )\n    from darts.utils.logging import LoggingManager\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    # ray_ensemble = _RayEnsembleV1.remote(models)\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    adem_buffer = ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2))\n\n    # Get files to process\n    tileinfo: list[RayInputDict] = []\n    for i, (tilekey, outpath) in enumerate(self._tileinfos()):\n        tile_id = self._get_tile_id(tilekey)\n        if not self.overwrite:\n            mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n            if mo == \"none\":\n                logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                continue\n            if mo == \"some\":\n                logger.warning(\n                    f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                    \" Skipping because overwrite=False...\"\n                )\n                continue\n        tileinfo.append({\"tilekey\": tilekey, \"outpath\": str(outpath.resolve()), \"tile_id\": tile_id})\n    tileinfo = tileinfo[:10]\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n\n    # Ray data pipeline\n    # TODO: setup device stuff correctly\n    ds = ray.data.from_items(tileinfo)\n    ds = ds.map(self._load_tile, num_cpus=1)\n    ds = ds.map(\n        _load_aux,\n        fn_kwargs={\n            \"arcticdem_dir\": self.arcticdem_dir,\n            \"arcticdem_resolution\": arcticdem_resolution,\n            \"buffer\": adem_buffer,\n            \"tcvis_dir\": self.tcvis_dir,\n        },\n        num_cpus=1,\n    )\n    ds = ds.map(\n        _preprocess_ray,\n        fn_kwargs={\n            \"tpi_outer_radius\": self.tpi_outer_radius,\n            \"tpi_inner_radius\": self.tpi_inner_radius,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n        concurrency=4,\n    )\n    ds = ds.map(\n        _RayEnsembleV1,\n        fn_constructor_kwargs={\"model_dict\": models},\n        fn_kwargs={\n            \"patch_size\": self.patch_size,\n            \"overlap\": self.overlap,\n            \"batch_size\": self.batch_size,\n            \"reflection\": self.reflection,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n        num_gpus=0.8,\n        concurrency=1,\n    )\n    ds = ds.map(\n        _prepare_export_ray,\n        fn_kwargs={\n            \"binarization_threshold\": self.binarization_threshold,\n            \"mask_erosion_size\": self.mask_erosion_size,\n            \"min_object_size\": self.min_object_size,\n            \"quality_level\": self.quality_level,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n    )\n    ds = ds.map(\n        _export_tile_ray,\n        fn_kwargs={\n            \"export_bands\": self.export_bands,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n    )\n    logger.debug(f\"Ray dataset: {ds}\")\n    logger.info(\"Ray pipeline created. Starting execution...\")\n    # This should trigger the execution\n    ds.write_parquet(f\"local://{self.output_data_dir.resolve()!s}/ray_output.parquet\")\n    logger.info(f\"Ray pipeline finished. Output written to {self.output_data_dir.resolve()!s}/ray_output.parquet\")\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline","title":"Sentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    default_dirs: darts_utils.paths.DefaultPaths = (\n        lambda: darts_utils.paths.DefaultPaths()\n    )(),\n    output_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    edge_erosion_size: int | None = None,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    offline: bool = False,\n    debug_data: bool = False,\n    scene_ids: list[str] | None = None,\n    scene_id_file: pathlib.Path | None = None,\n    tile_ids: list[str] | None = None,\n    aoi_file: pathlib.Path | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n    prep_data_scene_id_file: pathlib.Path | None = None,\n    sentinel2_grid_dir: pathlib.Path | None = None,\n    raw_data_store: pathlib.Path | None = None,\n    no_raw_data_store: bool = False,\n    raw_data_source: typing.Literal[\"gee\", \"cdse\"] = \"cdse\",\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for processing Sentinel-2 data.</p> <p>Processes Sentinel-2 Surface Reflectance (SR) imagery from either CDSE or Google Earth Engine. Supports multiple scene selection methods and flexible filtering options.</p> Source Selection <p>The data source is specified via the <code>raw_data_source</code> parameter: - \"cdse\": Copernicus Data Space Ecosystem (CDSE) - \"gee\": Google Earth Engine (GEE)</p> <p>Both sources require accounts and proper credential setup on the system.</p> Scene Selection <p>Scenes can be selected using one of four mutually exclusive methods (priority order):</p> <ol> <li><code>scene_ids</code>: Direct list of Sentinel-2 scene IDs</li> <li><code>scene_id_file</code>: JSON file containing scene IDs</li> <li><code>tile_ids</code>: List of Sentinel-2 tile IDs (e.g., \"33UVP\") with optional filters</li> <li><code>aoi_file</code>: Shapefile defining area of interest with optional filters</li> </ol> Filtering Options <p>When using <code>tile_ids</code> or <code>aoi_file</code>, scenes can be filtered by: - Cloud/snow cover: <code>max_cloud_cover</code>, <code>max_snow_cover</code> - Date range: <code>start_date</code> and <code>end_date</code> (YYYY-MM-DD format) - OR specific months/years: <code>months</code> (1-12) and <code>years</code></p> <p>Note: Date range takes priority over month/year filtering. Warning: No temporal filtering may cause rate-limit errors. Note: Month/year filtering is experimental and only implemented for CDSE.</p> Offline Processing <p>Use <code>cli_prepare_data</code> to download data for offline use. The <code>prep_data_scene_id_file</code> stores scene IDs from queries for offline reuse.</p> <p>Parameters:</p> <ul> <li> <code>scene_ids</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Direct list of Sentinel-2 scene IDs to process. Defaults to None.</p> </li> <li> <code>scene_id_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>JSON file containing scene IDs to process. Defaults to None.</p> </li> <li> <code>tile_ids</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Sentinel-2 tile IDs (requires filtering params). Defaults to None.</p> </li> <li> <code>aoi_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Shapefile with area of interest (requires filtering params). Defaults to None.</p> </li> <li> <code>start_date</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Start date for filtering (YYYY-MM-DD format). Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>End date for filtering (YYYY-MM-DD format). Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int | None</code>, default:                   <code>10</code> )           \u2013            <p>Maximum cloud cover percentage (0-100). Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int | None</code>, default:                   <code>10</code> )           \u2013            <p>Maximum snow cover percentage (0-100). Defaults to 10.</p> </li> <li> <code>months</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by months (1-12). Defaults to None.</p> </li> <li> <code>years</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by years. Defaults to None.</p> </li> <li> <code>prep_data_scene_id_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>File to store/load scene IDs for offline processing. Written during <code>prepare_data</code>, read during offline <code>run</code>. Defaults to None.</p> </li> <li> <code>sentinel2_grid_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for Sentinel-2 grid shapefiles. Used only in <code>prepare_data</code> with <code>tile_ids</code>. If None, uses default path. Defaults to None.</p> </li> <li> <code>raw_data_store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for storing raw Sentinel-2 data locally. If None, uses default path based on <code>raw_data_source</code>. Defaults to None.</p> </li> <li> <code>no_raw_data_store</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, processes data in-memory without local storage. Overrides <code>raw_data_store</code>. Defaults to False.</p> </li> <li> <code>raw_data_source</code>               (<code>typing.Literal['gee', 'cdse']</code>, default:                   <code>'cdse'</code> )           \u2013            <p>Data source to use. Defaults to \"cdse\".</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path] | None</code>, default:                   <code>None</code> )           \u2013            <p>Path(s) to model file(s) for segmentation. Single Path implies <code>write_model_outputs=False</code>. If None, searches default model directory for all .pt files. Defaults to None.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Output directory for results. If None, uses <code>{default_out}/sentinel2-{raw_data_source}</code>. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for ArcticDEM datacube. Will be created/downloaded if needed. If None, uses default path. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for TCVis data. If None, uses default path. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>, default:                   <code>None</code> )           \u2013            <p>Computation device. \"cuda\" uses GPU 0, int specifies GPU index, \"auto\" selects free GPU. Defaults to None.</p> </li> <li> <code>ee_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Earth Engine project ID. May be omitted if defined in persistent credentials. Defaults to None.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use EE high-volume server. Defaults to True.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Outer radius (m) for TPI calculation. Defaults to 100.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Inner radius (m) for TPI calculation. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Patch size for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Overlap between patches. Defaults to 256.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Disk size for mask erosion and inner edge cropping. Defaults to 10.</p> </li> <li> <code>edge_erosion_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Size for outer edge cropping. If None, uses <code>mask_erosion_size</code>. Defaults to None.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Minimum object size (pixels) to keep. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>Quality filtering level. 0=\"none\", 1=\"low_quality\", 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>Bands to export. Can include \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\", \"metadata\", or specific band names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Save individual model outputs (not just ensemble). Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Overwrite existing output files. Defaults to False.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Skip downloading missing data. Requires pre-downloaded data. Defaults to False.</p> </li> <li> <code>debug_data</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Write intermediate debugging data to output directory. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.aoi_file","title":"aoi_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_file: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.debug_data","title":"debug_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug_data: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.default_dirs","title":"default_dirs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_dirs: darts_utils.paths.DefaultPaths = dataclasses.field(\n    default_factory=lambda: darts_utils.paths.DefaultPaths()\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.edge_erosion_size","title":"edge_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>edge_erosion_size: int | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int | None = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.max_snow_cover","title":"max_snow_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_snow_cover: int | None = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.months","title":"months  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>months: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.no_raw_data_store","title":"no_raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>no_raw_data_store: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.offline","title":"offline  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offline: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.prep_data_scene_id_file","title":"prep_data_scene_id_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prep_data_scene_id_file: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.raw_data_source","title":"raw_data_source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_source: typing.Literal['gee', 'cdse'] = 'cdse'\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.raw_data_store","title":"raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_store: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.scene_id_file","title":"scene_id_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scene_id_file: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.scene_ids","title":"scene_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scene_ids: list[str] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.sentinel2_grid_dir","title":"sentinel2_grid_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_grid_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tile_ids","title":"tile_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tile_ids: list[str] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.years","title":"years  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>years: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def __post_init__(self):  # noqa: D105\n    logger.debug(\"Before super\")\n    super().__post_init__()\n    logger.debug(\"After super\")\n    self.output_data_dir = self.output_data_dir or paths.output_data(f\"sentinel2-{self.raw_data_source}\")\n    self.raw_data_store = self.raw_data_store or paths.sentinel2_raw_data(self.raw_data_source)\n    if self.no_raw_data_store:\n        self.raw_data_store = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Sentinel-2 data.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.Sentinel2Pipeline</code>)           \u2013            <p>Configured Sentinel2Pipeline instance.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for Sentinel-2 data.\n\n    Args:\n        pipeline: Configured Sentinel2Pipeline instance.\n\n    \"\"\"\n    pipeline.__post_init__()\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.cli_prepare_data","title":"cli_prepare_data  <code>staticmethod</code>","text":"<pre><code>cli_prepare_data(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download all necessary data for offline processing.</p> <p>Queries the data source (CDSE or GEE) for scene IDs and downloads optical and/or auxiliary data. Stores scene IDs in <code>prep_data_scene_id_file</code> if specified for later offline use.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.Sentinel2Pipeline</code>)           \u2013            <p>Configured Sentinel2Pipeline instance.</p> </li> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical (Sentinel-2) imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli_prepare_data(\n    *, pipeline: \"Sentinel2Pipeline\", optical: bool = False, aux: bool = False, force: bool = False\n):\n    \"\"\"Download all necessary data for offline processing.\n\n    Queries the data source (CDSE or GEE) for scene IDs and downloads optical and/or auxiliary data.\n    Stores scene IDs in `prep_data_scene_id_file` if specified for later offline use.\n\n    Args:\n        pipeline: Configured Sentinel2Pipeline instance.\n        optical: If True, downloads optical (Sentinel-2) imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    \"\"\"\n    assert not pipeline.offline, \"Pipeline must be online to prepare data for offline usage.\"\n\n    # !: Because of an unknown bug, __post_init__ is not initialized automatically\n    pipeline.__post_init__()\n\n    logger.debug(f\"Preparing data with {optical=}, {aux=}.\")\n\n    if pipeline.prep_data_scene_id_file is not None:\n        if pipeline.prep_data_scene_id_file.exists():\n            logger.warning(\n                f\"Prep-data scene id file {pipeline.prep_data_scene_id_file=} already exists. \"\n                \"It will be overwritten.\"\n            )\n            pipeline.prep_data_scene_id_file.unlink()\n    pipeline.prepare_data(optical=optical, aux=aux, force=force)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download and prepare data for offline processing.</p> <p>Validates configuration, determines data requirements from models, and downloads requested data (optical imagery and/or auxiliary data).</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> <li> <code>SystemExit</code>             \u2013            <p>If the process is terminated.</p> </li> <li> <code>SystemError</code>             \u2013            <p>If a system error occurs.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def prepare_data(self, optical: bool = False, aux: bool = False, force: bool = False):\n    \"\"\"Download and prepare data for offline processing.\n\n    Validates configuration, determines data requirements from models,\n    and downloads requested data (optical imagery and/or auxiliary data).\n\n    Args:\n        optical: If True, downloads optical imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n        SystemExit: If the process is terminated.\n        SystemError: If a system error occurs.\n\n    \"\"\"\n    assert optical or aux, \"Nothing to prepare. Please set optical and/or aux to True.\"\n\n    # ? We only want to download stuff - no need for using the GPU here\n    self.device = \"cpu\"\n    self._dump_config()\n\n    from darts_acquisition import download_arcticdem, download_tcvis\n    from stopuhr import Chronometer\n\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n\n    if aux or force:\n        # Get the ensemble to check which auxiliary data is necessary\n        if force:\n            needs_arcticdem, needs_tcvis = True, True\n        else:\n            ensemble = self._load_ensemble()\n            needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n        if not needs_arcticdem and not needs_tcvis:\n            logger.warning(\"No auxiliary data required by the models. Skipping download of auxiliary data...\")\n        else:\n            logger.info(f\"Models {needs_tcvis=} {needs_arcticdem=}.\")\n            self._create_auxiliary_datacubes(arcticdem=needs_arcticdem, tcvis=needs_tcvis)\n\n            # Predownload auxiliary\n            aoi = self._tile_aoi()\n            if needs_arcticdem:\n                logger.info(\"start download ArcticDEM\")\n                with timer(\"Downloading ArcticDEM\"):\n                    download_arcticdem(aoi, self.arcticdem_dir, resolution=self._arcticdem_resolution())\n            if needs_tcvis:\n                logger.info(\"start download TCVIS\")\n                init_ee(self.ee_project, self.ee_use_highvolume)\n                with timer(\"Downloading TCVis\"):\n                    download_tcvis(aoi, self.tcvis_dir)\n\n    # Predownload tiles if optical flag is set\n    if not optical and not force:\n        return\n\n    # Iterate over all the data\n    with timer(\"Loading Optical\"):\n        tileinfo = self._tileinfos()\n        n_tiles = 0\n        logger.info(f\"Found {len(tileinfo)} tiles to download.\")\n        for i, (tilekey, _) in enumerate(tileinfo):\n            tile_id = self._get_tile_id(tilekey)\n            try:\n                self._download_tile(tilekey)\n                n_tiles += 1\n                logger.info(f\"Downloaded sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n            except (KeyboardInterrupt, SystemError, SystemExit) as e:\n                logger.warning(f\"{type(e).__name__} detected.\\nExiting...\")\n                raise e\n            except Exception as e:\n                logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n                logger.exception(e)\n        else:\n            logger.info(f\"Downloaded {n_tiles} tiles.\")\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> <p>Run the complete segmentation pipeline.</p> <p>Executes the full pipeline including: 1. Configuration validation and dumping 2. Loading ensemble models 3. Creating/loading auxiliary datacubes 4. Processing each tile:    - Loading optical data    - Loading auxiliary data (ArcticDEM, TCVis) as needed    - Preprocessing    - Segmentation    - Postprocessing    - Exporting results 5. Saving results and timing information</p> <p>Results are saved to the output directory with timestamped configuration, results parquet file, and timing information.</p> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    \"\"\"Run the complete segmentation pipeline.\n\n    Executes the full pipeline including:\n    1. Configuration validation and dumping\n    2. Loading ensemble models\n    3. Creating/loading auxiliary datacubes\n    4. Processing each tile:\n       - Loading optical data\n       - Loading auxiliary data (ArcticDEM, TCVis) as needed\n       - Preprocessing\n       - Segmentation\n       - Postprocessing\n       - Exporting results\n    5. Saving results and timing information\n\n    Results are saved to the output directory with timestamped configuration,\n    results parquet file, and timing information.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n\n    \"\"\"\n    self._validate()\n    current_time = self._dump_config()\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    import pandas as pd\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_v2\n    from stopuhr import Chronometer, stopwatch\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n    self.device = decide_device(self.device)\n\n    if not self.offline:\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    self._create_auxiliary_datacubes()\n\n    # determine models to use\n    ensemble = self._load_ensemble()\n    ensemble_subsets = ensemble.model_names\n    needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=ensemble_subsets)\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} seems to be already processed, \"\n                        \"but some of the requested outputs are missing. \"\n                        \"Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading Optical\", log=False):\n                tile = self._load_tile(tilekey)\n\n            if needs_arcticdem:\n                with timer(\"Loading ArcticDEM\", log=False):\n                    arcticdem_resolution = self._arcticdem_resolution()\n                    arcticdem = load_arcticdem(\n                        tile.odc.geobox,\n                        self.arcticdem_dir,\n                        resolution=arcticdem_resolution,\n                        buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                        offline=self.offline,\n                    )\n            else:\n                arcticdem = None\n\n            if needs_tcvis:\n                with timer(\"Loading TCVis\", log=False):\n                    tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir, offline=self.offline)\n            else:\n                tcvis = None\n\n            with timer(\"Preprocessing\", log=False):\n                tile = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n\n            with timer(\"Postprocessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    device=self.device,\n                    edge_erosion_size=self.edge_erosion_size,\n                )\n\n            export_metadata = self._result_metadata(tilekey)\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    metadata=export_metadata,\n                    debug=self.debug_data,\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.timer.parquet\")\n            if len(stopwatch.durations) &gt; 0:\n                stopwatch.export().to_parquet(self.output_data_dir / f\"{current_time}.stopwatch.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary(printer=logger.info)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline","title":"Sentinel2RayPipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2RayPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    num_cpus: int = 1,\n    devices: list[int] | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    aoi_shapefile: pathlib.Path = None,\n    start_date: str = None,\n    end_date: str = None,\n    max_cloud_cover: int = 10,\n    input_cache: pathlib.Path = pathlib.Path(\n        \"data/cache/input\"\n    ),\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.ray_v2._BaseRayPipeline</code></p> <p>Pipeline for Sentinel 2 data based on an area of interest.</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The shapefile containing the area of interest.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for filtering the Sentinel 2 scenes. Defaults to 10.</p> </li> <li> <code>input_cache</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/cache/input')</code> )           \u2013            <p>The directory to use for caching the input data. Defaults to Path(\"data/cache/input\").</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.aoi_shapefile","title":"aoi_shapefile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_shapefile: pathlib.Path = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.input_cache","title":"input_cache  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_cache: pathlib.Path = pathlib.Path(\"data/cache/input\")\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.num_cpus","title":"num_cpus  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_cpus: int = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *, pipeline: darts.pipelines.ray_v2.Sentinel2RayPipeline\n)\n</code></pre> <p>Run the sequential pipeline for AOI Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2RayPipeline\"):\n    \"\"\"Run the sequential pipeline for AOI Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2RayPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    if self.devices is not None:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(d) for d in self.devices)\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import ray\n\n    ray_context = ray.init(\n        num_cpus=self.num_cpus,  # We use one CPU per Ray task\n        num_gpus=len(self.devices) if self.devices is not None else None,\n    )\n    logger.debug(f\"Ray initialized with context: {ray_context}\")\n    logger.info(f\"Ray Dashboard URL: {ray_context.dashboard_url}\")\n    logger.debug(f\"Ray cluster resources: {ray.cluster_resources()}\")\n    logger.debug(f\"Ray available resources: {ray.available_resources()}\")\n\n    # Initlize ee in every worker\n    @ray.remote\n    def init_worker():\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    num_workers = int(ray.cluster_resources().get(\"CPU\", 1))\n    logger.info(f\"Initializing {num_workers} Ray workers with Earth Engine.\")\n    ray.get([init_worker.remote() for _ in range(num_workers)])\n\n    import smart_geocubes\n    from darts_export import missing_outputs\n\n    from darts.pipelines._ray_wrapper import (\n        _export_tile_ray,\n        _load_aux,\n        _prepare_export_ray,\n        _preprocess_ray,\n        _RayEnsembleV1,\n    )\n    from darts.utils.logging import LoggingManager\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    # ray_ensemble = _RayEnsembleV1.remote(models)\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    adem_buffer = ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2))\n\n    # Get files to process\n    tileinfo: list[RayInputDict] = []\n    for i, (tilekey, outpath) in enumerate(self._tileinfos()):\n        tile_id = self._get_tile_id(tilekey)\n        if not self.overwrite:\n            mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n            if mo == \"none\":\n                logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                continue\n            if mo == \"some\":\n                logger.warning(\n                    f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                    \" Skipping because overwrite=False...\"\n                )\n                continue\n        tileinfo.append({\"tilekey\": tilekey, \"outpath\": str(outpath.resolve()), \"tile_id\": tile_id})\n    tileinfo = tileinfo[:10]\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n\n    # Ray data pipeline\n    # TODO: setup device stuff correctly\n    ds = ray.data.from_items(tileinfo)\n    ds = ds.map(self._load_tile, num_cpus=1)\n    ds = ds.map(\n        _load_aux,\n        fn_kwargs={\n            \"arcticdem_dir\": self.arcticdem_dir,\n            \"arcticdem_resolution\": arcticdem_resolution,\n            \"buffer\": adem_buffer,\n            \"tcvis_dir\": self.tcvis_dir,\n        },\n        num_cpus=1,\n    )\n    ds = ds.map(\n        _preprocess_ray,\n        fn_kwargs={\n            \"tpi_outer_radius\": self.tpi_outer_radius,\n            \"tpi_inner_radius\": self.tpi_inner_radius,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n        concurrency=4,\n    )\n    ds = ds.map(\n        _RayEnsembleV1,\n        fn_constructor_kwargs={\"model_dict\": models},\n        fn_kwargs={\n            \"patch_size\": self.patch_size,\n            \"overlap\": self.overlap,\n            \"batch_size\": self.batch_size,\n            \"reflection\": self.reflection,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n        num_gpus=0.8,\n        concurrency=1,\n    )\n    ds = ds.map(\n        _prepare_export_ray,\n        fn_kwargs={\n            \"binarization_threshold\": self.binarization_threshold,\n            \"mask_erosion_size\": self.mask_erosion_size,\n            \"min_object_size\": self.min_object_size,\n            \"quality_level\": self.quality_level,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n    )\n    ds = ds.map(\n        _export_tile_ray,\n        fn_kwargs={\n            \"export_bands\": self.export_bands,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n    )\n    logger.debug(f\"Ray dataset: {ds}\")\n    logger.info(\"Ray pipeline created. Starting execution...\")\n    # This should trigger the execution\n    ds.write_parquet(f\"local://{self.output_data_dir.resolve()!s}/ray_output.parquet\")\n    logger.info(f\"Ray pipeline finished. Output written to {self.output_data_dir.resolve()!s}/ray_output.parquet\")\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/","title":"ray_v2","text":""},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2","title":"darts.pipelines.ray_v2","text":"<p>Ray implementation of the v2 pipelines.</p>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline","title":"PlanetRayPipeline  <code>dataclass</code>","text":"<pre><code>PlanetRayPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    num_cpus: int = 1,\n    devices: list[int] | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    orthotiles_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSOrthoTile\"\n    ),\n    scenes_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSScene\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.ray_v2._BaseRayPipeline</code></p> <p>Pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSOrthoTile')</code> )           \u2013            <p>The directory containing the PlanetScope orthotiles.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSScene')</code> )           \u2013            <p>The directory containing the PlanetScope scenes.</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.num_cpus","title":"num_cpus  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_cpus: int = 1\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSOrthoTile\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSScene\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(*, pipeline: darts.pipelines.ray_v2.PlanetRayPipeline)\n</code></pre> <p>Run the sequential pipeline for Planet data.</p> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetRayPipeline\"):\n    \"\"\"Run the sequential pipeline for Planet data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.PlanetRayPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    if self.devices is not None:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(d) for d in self.devices)\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import ray\n\n    ray_context = ray.init(\n        num_cpus=self.num_cpus,  # We use one CPU per Ray task\n        num_gpus=len(self.devices) if self.devices is not None else None,\n    )\n    logger.debug(f\"Ray initialized with context: {ray_context}\")\n    logger.info(f\"Ray Dashboard URL: {ray_context.dashboard_url}\")\n    logger.debug(f\"Ray cluster resources: {ray.cluster_resources()}\")\n    logger.debug(f\"Ray available resources: {ray.available_resources()}\")\n\n    # Initlize ee in every worker\n    @ray.remote\n    def init_worker():\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    num_workers = int(ray.cluster_resources().get(\"CPU\", 1))\n    logger.info(f\"Initializing {num_workers} Ray workers with Earth Engine.\")\n    ray.get([init_worker.remote() for _ in range(num_workers)])\n\n    import smart_geocubes\n    from darts_export import missing_outputs\n\n    from darts.pipelines._ray_wrapper import (\n        _export_tile_ray,\n        _load_aux,\n        _prepare_export_ray,\n        _preprocess_ray,\n        _RayEnsembleV1,\n    )\n    from darts.utils.logging import LoggingManager\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    # ray_ensemble = _RayEnsembleV1.remote(models)\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    adem_buffer = ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2))\n\n    # Get files to process\n    tileinfo: list[RayInputDict] = []\n    for i, (tilekey, outpath) in enumerate(self._tileinfos()):\n        tile_id = self._get_tile_id(tilekey)\n        if not self.overwrite:\n            mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n            if mo == \"none\":\n                logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                continue\n            if mo == \"some\":\n                logger.warning(\n                    f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                    \" Skipping because overwrite=False...\"\n                )\n                continue\n        tileinfo.append({\"tilekey\": tilekey, \"outpath\": str(outpath.resolve()), \"tile_id\": tile_id})\n    tileinfo = tileinfo[:10]\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n\n    # Ray data pipeline\n    # TODO: setup device stuff correctly\n    ds = ray.data.from_items(tileinfo)\n    ds = ds.map(self._load_tile, num_cpus=1)\n    ds = ds.map(\n        _load_aux,\n        fn_kwargs={\n            \"arcticdem_dir\": self.arcticdem_dir,\n            \"arcticdem_resolution\": arcticdem_resolution,\n            \"buffer\": adem_buffer,\n            \"tcvis_dir\": self.tcvis_dir,\n        },\n        num_cpus=1,\n    )\n    ds = ds.map(\n        _preprocess_ray,\n        fn_kwargs={\n            \"tpi_outer_radius\": self.tpi_outer_radius,\n            \"tpi_inner_radius\": self.tpi_inner_radius,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n        concurrency=4,\n    )\n    ds = ds.map(\n        _RayEnsembleV1,\n        fn_constructor_kwargs={\"model_dict\": models},\n        fn_kwargs={\n            \"patch_size\": self.patch_size,\n            \"overlap\": self.overlap,\n            \"batch_size\": self.batch_size,\n            \"reflection\": self.reflection,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n        num_gpus=0.8,\n        concurrency=1,\n    )\n    ds = ds.map(\n        _prepare_export_ray,\n        fn_kwargs={\n            \"binarization_threshold\": self.binarization_threshold,\n            \"mask_erosion_size\": self.mask_erosion_size,\n            \"min_object_size\": self.min_object_size,\n            \"quality_level\": self.quality_level,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n    )\n    ds = ds.map(\n        _export_tile_ray,\n        fn_kwargs={\n            \"export_bands\": self.export_bands,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n    )\n    logger.debug(f\"Ray dataset: {ds}\")\n    logger.info(\"Ray pipeline created. Starting execution...\")\n    # This should trigger the execution\n    ds.write_parquet(f\"local://{self.output_data_dir.resolve()!s}/ray_output.parquet\")\n    logger.info(f\"Ray pipeline finished. Output written to {self.output_data_dir.resolve()!s}/ray_output.parquet\")\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayDataDict","title":"RayDataDict","text":"<p>               Bases: <code>typing.TypedDict</code></p>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayDataDict.adem","title":"adem  <code>instance-attribute</code>","text":"<pre><code>adem: darts.pipelines._ray_wrapper.RayDataset | None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayDataDict.outpath","title":"outpath  <code>instance-attribute</code>","text":"<pre><code>outpath: str\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayDataDict.tcvis","title":"tcvis  <code>instance-attribute</code>","text":"<pre><code>tcvis: darts.pipelines._ray_wrapper.RayDataset | None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayDataDict.tile","title":"tile  <code>instance-attribute</code>","text":"<pre><code>tile: darts.pipelines._ray_wrapper.RayDataset | None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayDataDict.tile_id","title":"tile_id  <code>instance-attribute</code>","text":"<pre><code>tile_id: str\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayDataDict.tilekey","title":"tilekey  <code>instance-attribute</code>","text":"<pre><code>tilekey: typing.Any\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayInputDict","title":"RayInputDict","text":"<p>               Bases: <code>typing.TypedDict</code></p> <p>A dictionary to hold the input data for Ray tasks.</p> <p>This is used to ensure that the input data can be serialized and deserialized correctly.</p>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayInputDict.outpath","title":"outpath  <code>instance-attribute</code>","text":"<pre><code>outpath: str\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayInputDict.tile_id","title":"tile_id  <code>instance-attribute</code>","text":"<pre><code>tile_id: str\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.RayInputDict.tilekey","title":"tilekey  <code>instance-attribute</code>","text":"<pre><code>tilekey: typing.Any\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline","title":"Sentinel2RayPipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2RayPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    num_cpus: int = 1,\n    devices: list[int] | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    aoi_shapefile: pathlib.Path = None,\n    start_date: str = None,\n    end_date: str = None,\n    max_cloud_cover: int = 10,\n    input_cache: pathlib.Path = pathlib.Path(\n        \"data/cache/input\"\n    ),\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.ray_v2._BaseRayPipeline</code></p> <p>Pipeline for Sentinel 2 data based on an area of interest.</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The shapefile containing the area of interest.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for filtering the Sentinel 2 scenes. Defaults to 10.</p> </li> <li> <code>input_cache</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/cache/input')</code> )           \u2013            <p>The directory to use for caching the input data. Defaults to Path(\"data/cache/input\").</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.aoi_shapefile","title":"aoi_shapefile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_shapefile: pathlib.Path = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.input_cache","title":"input_cache  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_cache: pathlib.Path = pathlib.Path(\"data/cache/input\")\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.num_cpus","title":"num_cpus  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_cpus: int = 1\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *, pipeline: darts.pipelines.ray_v2.Sentinel2RayPipeline\n)\n</code></pre> <p>Run the sequential pipeline for AOI Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2RayPipeline\"):\n    \"\"\"Run the sequential pipeline for AOI Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2.Sentinel2RayPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    if self.devices is not None:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(d) for d in self.devices)\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import ray\n\n    ray_context = ray.init(\n        num_cpus=self.num_cpus,  # We use one CPU per Ray task\n        num_gpus=len(self.devices) if self.devices is not None else None,\n    )\n    logger.debug(f\"Ray initialized with context: {ray_context}\")\n    logger.info(f\"Ray Dashboard URL: {ray_context.dashboard_url}\")\n    logger.debug(f\"Ray cluster resources: {ray.cluster_resources()}\")\n    logger.debug(f\"Ray available resources: {ray.available_resources()}\")\n\n    # Initlize ee in every worker\n    @ray.remote\n    def init_worker():\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    num_workers = int(ray.cluster_resources().get(\"CPU\", 1))\n    logger.info(f\"Initializing {num_workers} Ray workers with Earth Engine.\")\n    ray.get([init_worker.remote() for _ in range(num_workers)])\n\n    import smart_geocubes\n    from darts_export import missing_outputs\n\n    from darts.pipelines._ray_wrapper import (\n        _export_tile_ray,\n        _load_aux,\n        _prepare_export_ray,\n        _preprocess_ray,\n        _RayEnsembleV1,\n    )\n    from darts.utils.logging import LoggingManager\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    # ray_ensemble = _RayEnsembleV1.remote(models)\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    adem_buffer = ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2))\n\n    # Get files to process\n    tileinfo: list[RayInputDict] = []\n    for i, (tilekey, outpath) in enumerate(self._tileinfos()):\n        tile_id = self._get_tile_id(tilekey)\n        if not self.overwrite:\n            mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n            if mo == \"none\":\n                logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                continue\n            if mo == \"some\":\n                logger.warning(\n                    f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                    \" Skipping because overwrite=False...\"\n                )\n                continue\n        tileinfo.append({\"tilekey\": tilekey, \"outpath\": str(outpath.resolve()), \"tile_id\": tile_id})\n    tileinfo = tileinfo[:10]\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n\n    # Ray data pipeline\n    # TODO: setup device stuff correctly\n    ds = ray.data.from_items(tileinfo)\n    ds = ds.map(self._load_tile, num_cpus=1)\n    ds = ds.map(\n        _load_aux,\n        fn_kwargs={\n            \"arcticdem_dir\": self.arcticdem_dir,\n            \"arcticdem_resolution\": arcticdem_resolution,\n            \"buffer\": adem_buffer,\n            \"tcvis_dir\": self.tcvis_dir,\n        },\n        num_cpus=1,\n    )\n    ds = ds.map(\n        _preprocess_ray,\n        fn_kwargs={\n            \"tpi_outer_radius\": self.tpi_outer_radius,\n            \"tpi_inner_radius\": self.tpi_inner_radius,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n        concurrency=4,\n    )\n    ds = ds.map(\n        _RayEnsembleV1,\n        fn_constructor_kwargs={\"model_dict\": models},\n        fn_kwargs={\n            \"patch_size\": self.patch_size,\n            \"overlap\": self.overlap,\n            \"batch_size\": self.batch_size,\n            \"reflection\": self.reflection,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n        num_gpus=0.8,\n        concurrency=1,\n    )\n    ds = ds.map(\n        _prepare_export_ray,\n        fn_kwargs={\n            \"binarization_threshold\": self.binarization_threshold,\n            \"mask_erosion_size\": self.mask_erosion_size,\n            \"min_object_size\": self.min_object_size,\n            \"quality_level\": self.quality_level,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n    )\n    ds = ds.map(\n        _export_tile_ray,\n        fn_kwargs={\n            \"export_bands\": self.export_bands,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n    )\n    logger.debug(f\"Ray dataset: {ds}\")\n    logger.info(\"Ray pipeline created. Starting execution...\")\n    # This should trigger the execution\n    ds.write_parquet(f\"local://{self.output_data_dir.resolve()!s}/ray_output.parquet\")\n    logger.info(f\"Ray pipeline finished. Output written to {self.output_data_dir.resolve()!s}/ray_output.parquet\")\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline","title":"_BaseRayPipeline  <code>dataclass</code>","text":"<pre><code>_BaseRayPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    num_cpus: int = 1,\n    devices: list[int] | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n)\n</code></pre> <p>               Bases: <code>abc.ABC</code></p> <p>Base class for all v2 pipelines.</p> <p>This class provides the run method which is the main entry point for all pipelines.</p> <p>This class is meant to be subclassed by the specific pipelines. These pipeliens must implement the _aqdata_generator method.</p> <p>The main class must be also a dataclass, to fully inherit all parameter of this class (and the mixins).</p> <p>Parameters:</p> <ul> <li> <code>- num_cpus</code>               (<code>int</code>)           \u2013            <p>The number of CPUs to use for the Ray tasks. Defaults to 1.</p> </li> </ul>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.num_cpus","title":"num_cpus  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_cpus: int = 1\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/ray_v2/#darts.pipelines.ray_v2._BaseRayPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/ray_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    if self.devices is not None:\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(d) for d in self.devices)\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import ray\n\n    ray_context = ray.init(\n        num_cpus=self.num_cpus,  # We use one CPU per Ray task\n        num_gpus=len(self.devices) if self.devices is not None else None,\n    )\n    logger.debug(f\"Ray initialized with context: {ray_context}\")\n    logger.info(f\"Ray Dashboard URL: {ray_context.dashboard_url}\")\n    logger.debug(f\"Ray cluster resources: {ray.cluster_resources()}\")\n    logger.debug(f\"Ray available resources: {ray.available_resources()}\")\n\n    # Initlize ee in every worker\n    @ray.remote\n    def init_worker():\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    num_workers = int(ray.cluster_resources().get(\"CPU\", 1))\n    logger.info(f\"Initializing {num_workers} Ray workers with Earth Engine.\")\n    ray.get([init_worker.remote() for _ in range(num_workers)])\n\n    import smart_geocubes\n    from darts_export import missing_outputs\n\n    from darts.pipelines._ray_wrapper import (\n        _export_tile_ray,\n        _load_aux,\n        _prepare_export_ray,\n        _preprocess_ray,\n        _RayEnsembleV1,\n    )\n    from darts.utils.logging import LoggingManager\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    # ray_ensemble = _RayEnsembleV1.remote(models)\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    adem_buffer = ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2))\n\n    # Get files to process\n    tileinfo: list[RayInputDict] = []\n    for i, (tilekey, outpath) in enumerate(self._tileinfos()):\n        tile_id = self._get_tile_id(tilekey)\n        if not self.overwrite:\n            mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n            if mo == \"none\":\n                logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                continue\n            if mo == \"some\":\n                logger.warning(\n                    f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                    \" Skipping because overwrite=False...\"\n                )\n                continue\n        tileinfo.append({\"tilekey\": tilekey, \"outpath\": str(outpath.resolve()), \"tile_id\": tile_id})\n    tileinfo = tileinfo[:10]\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n\n    # Ray data pipeline\n    # TODO: setup device stuff correctly\n    ds = ray.data.from_items(tileinfo)\n    ds = ds.map(self._load_tile, num_cpus=1)\n    ds = ds.map(\n        _load_aux,\n        fn_kwargs={\n            \"arcticdem_dir\": self.arcticdem_dir,\n            \"arcticdem_resolution\": arcticdem_resolution,\n            \"buffer\": adem_buffer,\n            \"tcvis_dir\": self.tcvis_dir,\n        },\n        num_cpus=1,\n    )\n    ds = ds.map(\n        _preprocess_ray,\n        fn_kwargs={\n            \"tpi_outer_radius\": self.tpi_outer_radius,\n            \"tpi_inner_radius\": self.tpi_inner_radius,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n        concurrency=4,\n    )\n    ds = ds.map(\n        _RayEnsembleV1,\n        fn_constructor_kwargs={\"model_dict\": models},\n        fn_kwargs={\n            \"patch_size\": self.patch_size,\n            \"overlap\": self.overlap,\n            \"batch_size\": self.batch_size,\n            \"reflection\": self.reflection,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n        num_gpus=0.8,\n        concurrency=1,\n    )\n    ds = ds.map(\n        _prepare_export_ray,\n        fn_kwargs={\n            \"binarization_threshold\": self.binarization_threshold,\n            \"mask_erosion_size\": self.mask_erosion_size,\n            \"min_object_size\": self.min_object_size,\n            \"quality_level\": self.quality_level,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n            \"device\": \"cuda\",  # Ray will handle the device allocation\n        },\n        num_cpus=1,\n        num_gpus=0.1,\n    )\n    ds = ds.map(\n        _export_tile_ray,\n        fn_kwargs={\n            \"export_bands\": self.export_bands,\n            \"models\": models,\n            \"write_model_outputs\": self.write_model_outputs,\n        },\n        num_cpus=1,\n    )\n    logger.debug(f\"Ray dataset: {ds}\")\n    logger.info(\"Ray pipeline created. Starting execution...\")\n    # This should trigger the execution\n    ds.write_parquet(f\"local://{self.output_data_dir.resolve()!s}/ray_output.parquet\")\n    logger.info(f\"Ray pipeline finished. Output written to {self.output_data_dir.resolve()!s}/ray_output.parquet\")\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/","title":"sequential_v2","text":""},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2","title":"darts.pipelines.sequential_v2","text":"<p>Sequential implementation of the v2 pipelines.</p>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths","title":"PipelineV2Paths  <code>dataclass</code>","text":"<pre><code>PipelineV2Paths(\n    model_files: list[pathlib.Path] = None,\n    default_dirs: darts_utils.paths.DefaultPaths = (\n        lambda: darts_utils.paths.DefaultPaths()\n    )(),\n    output_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    orthotiles_dir: pathlib.Path | None = None,\n    scenes_dir: pathlib.Path | None = None,\n    sentinel2_grid_dir: pathlib.Path | None = None,\n    raw_data_store: pathlib.Path | None = None,\n    raw_data_source: typing.Literal[\"cdse\", \"gee\"] = \"cdse\",\n    no_raw_data_store: bool = False,\n)\n</code></pre> <p>Default paths for v2 pipelines.</p>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.default_dirs","title":"default_dirs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_dirs: darts_utils.paths.DefaultPaths = dataclasses.field(\n    default_factory=lambda: darts_utils.paths.DefaultPaths()\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.no_raw_data_store","title":"no_raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>no_raw_data_store: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.raw_data_source","title":"raw_data_source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_source: typing.Literal['cdse', 'gee'] = 'cdse'\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.raw_data_store","title":"raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_store: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.sentinel2_grid_dir","title":"sentinel2_grid_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_grid_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def __post_init__(self):  # noqa: D105\n    paths.set_defaults(self.default_dirs)\n    # The defaults will be overwritten in the respective realizations\n    self.output_data_dir = self.output_data_dir or paths.output_data(\"base_pipeline\")\n    self.model_files = self.model_files or paths.ensemble_models()\n    self.arcticdem_dir = self.arcticdem_dir or paths.arcticdem(2)\n    self.tcvis_dir = self.tcvis_dir or paths.tcvis()\n    self.output_data_dir = self.output_data_dir or paths.output_data(\"planet\")\n    self.orthotiles_dir = self.orthotiles_dir or paths.planet_orthotiles()\n    self.scenes_dir = self.scenes_dir or paths.planet_scenes()\n    self.output_data_dir = self.output_data_dir or paths.output_data(f\"sentinel2-{self.raw_data_source}\")\n    self.raw_data_store = self.raw_data_store or paths.sentinel2_raw_data(self.raw_data_source)\n    if self.no_raw_data_store:\n        self.raw_data_store = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PipelineV2Paths.log","title":"log","text":"<pre><code>log(level: int = logging.DEBUG)\n</code></pre> <p>Log all paths managed.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def log(self, level: int = logging.DEBUG):\n    \"\"\"Log all paths managed.\"\"\"\n    label_width = 47\n    logmsg = textwrap.dedent(f\"\"\"\n        === Pipeline (Sequential V2) Paths ===\n        {\"Output Data Directory:\":&lt;{label_width}} {self.output_data_dir}\n        {\"ArcticDEM Directory:\":&lt;{label_width}} {self.arcticdem_dir}\n        {\"TCVis Directory:\":&lt;{label_width}} {self.tcvis_dir}\n        {\"Planet Orthotiles Directory:\":&lt;{label_width}} {self.orthotiles_dir}\n        {\"Planet Scenes Directory:\":&lt;{label_width}} {self.scenes_dir}\n        {\"Sentinel-2 Grid Directory:\":&lt;{label_width}} {self.sentinel2_grid_dir}\n        {\"Sentinel-2 Raw Data Directory:\":&lt;{label_width}} {self.raw_data_store}\n    \"\"\").strip()\n    logger.log(level, logmsg)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline","title":"PlanetPipeline  <code>dataclass</code>","text":"<pre><code>PlanetPipeline(\n    model_files: list[pathlib.Path] = None,\n    default_dirs: darts_utils.paths.DefaultPaths = (\n        lambda: darts_utils.paths.DefaultPaths()\n    )(),\n    output_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    edge_erosion_size: int | None = None,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    offline: bool = False,\n    debug_data: bool = False,\n    orthotiles_dir: pathlib.Path | None = None,\n    scenes_dir: pathlib.Path | None = None,\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for processing PlanetScope data.</p> <p>Processes PlanetScope imagery (both orthotiles and scenes) for RTS segmentation. Supports both offline and online processing modes.</p> Data Structure <p>Expects PlanetScope data organized as: - Orthotiles: <code>orthotiles_dir/tile_id/scene_id/</code> - Scenes: <code>scenes_dir/scene_id/</code></p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory containing PlanetScope orthotiles. If None, uses default path from DARTS paths. Defaults to None.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory containing PlanetScope scenes. If None, uses default path from DARTS paths. Defaults to None.</p> </li> <li> <code>image_ids</code>               (<code>list | None</code>, default:                   <code>None</code> )           \u2013            <p>List of image/scene IDs to process. If None, processes all images found in orthotiles_dir and scenes_dir. Defaults to None.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path] | None</code>, default:                   <code>None</code> )           \u2013            <p>Path(s) to model file(s) for segmentation. Single Path implies <code>write_model_outputs=False</code>. If None, searches default model directory for all .pt files. Defaults to None.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Output directory for results. If None, uses <code>{default_out}/planet</code>. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for ArcticDEM datacube. Will be created/downloaded if needed. If None, uses default path. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for TCVis data. If None, uses default path. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>, default:                   <code>None</code> )           \u2013            <p>Computation device. \"cuda\" uses GPU 0, int specifies GPU index, \"auto\" selects free GPU. Defaults to None.</p> </li> <li> <code>ee_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Earth Engine project ID. May be omitted if defined in persistent credentials. Defaults to None.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use EE high-volume server. Defaults to True.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Outer radius (m) for TPI calculation. Defaults to 100.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Inner radius (m) for TPI calculation. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Patch size for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Overlap between patches. Defaults to 256.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Disk size for mask erosion and inner edge cropping. Defaults to 10.</p> </li> <li> <code>edge_erosion_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Size for outer edge cropping. If None, uses <code>mask_erosion_size</code>. Defaults to None.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Minimum object size (pixels) to keep. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>Quality filtering level. 0=\"none\", 1=\"low_quality\", 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>Bands to export. Can include \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\", \"metadata\", or specific band names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Save individual model outputs (not just ensemble). Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Overwrite existing output files. Defaults to False.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Skip downloading missing data. Defaults to False.</p> </li> <li> <code>debug_data</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Write intermediate debugging data. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.debug_data","title":"debug_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug_data: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.default_dirs","title":"default_dirs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_dirs: darts_utils.paths.DefaultPaths = dataclasses.field(\n    default_factory=lambda: darts_utils.paths.DefaultPaths()\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.edge_erosion_size","title":"edge_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>edge_erosion_size: int | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.offline","title":"offline  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offline: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def __post_init__(self):  # noqa: D105\n    super().__post_init__()\n    self.output_data_dir = self.output_data_dir or paths.output_data(\"planet\")\n    self.orthotiles_dir = self.orthotiles_dir or paths.planet_orthotiles()\n    self.scenes_dir = self.scenes_dir or paths.planet_scenes()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n)\n</code></pre> <p>Run the sequential pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.PlanetPipeline</code>)           \u2013            <p>Configured PlanetPipeline instance.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetPipeline\"):\n    \"\"\"Run the sequential pipeline for PlanetScope data.\n\n    Args:\n        pipeline: Configured PlanetPipeline instance.\n\n    \"\"\"\n    pipeline.__post_init__()\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.cli_prepare_data","title":"cli_prepare_data  <code>staticmethod</code>","text":"<pre><code>cli_prepare_data(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download all necessary data for offline processing.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.PlanetPipeline</code>)           \u2013            <p>Configured PlanetPipeline instance.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of the <code>aux</code> flag or model needs. Defaults to False.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli_prepare_data(*, pipeline: \"PlanetPipeline\", aux: bool = False, force: bool = False):\n    \"\"\"Download all necessary data for offline processing.\n\n    Args:\n        pipeline: Configured PlanetPipeline instance.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.\n        force: If True, downloads all possible data, independent of the `aux` flag or model needs.\n            Defaults to False.\n\n    \"\"\"\n    assert not pipeline.offline, \"Pipeline must be online to prepare data for offline usage.\"\n    pipeline.__post_init__()\n    pipeline.prepare_data(optical=False, aux=aux, force=force)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download and prepare data for offline processing.</p> <p>Validates configuration, determines data requirements from models, and downloads requested data (optical imagery and/or auxiliary data).</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> <li> <code>SystemExit</code>             \u2013            <p>If the process is terminated.</p> </li> <li> <code>SystemError</code>             \u2013            <p>If a system error occurs.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def prepare_data(self, optical: bool = False, aux: bool = False, force: bool = False):\n    \"\"\"Download and prepare data for offline processing.\n\n    Validates configuration, determines data requirements from models,\n    and downloads requested data (optical imagery and/or auxiliary data).\n\n    Args:\n        optical: If True, downloads optical imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n        SystemExit: If the process is terminated.\n        SystemError: If a system error occurs.\n\n    \"\"\"\n    assert optical or aux, \"Nothing to prepare. Please set optical and/or aux to True.\"\n\n    # ? We only want to download stuff - no need for using the GPU here\n    self.device = \"cpu\"\n    self._dump_config()\n\n    from darts_acquisition import download_arcticdem, download_tcvis\n    from stopuhr import Chronometer\n\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n\n    if aux or force:\n        # Get the ensemble to check which auxiliary data is necessary\n        if force:\n            needs_arcticdem, needs_tcvis = True, True\n        else:\n            ensemble = self._load_ensemble()\n            needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n        if not needs_arcticdem and not needs_tcvis:\n            logger.warning(\"No auxiliary data required by the models. Skipping download of auxiliary data...\")\n        else:\n            logger.info(f\"Models {needs_tcvis=} {needs_arcticdem=}.\")\n            self._create_auxiliary_datacubes(arcticdem=needs_arcticdem, tcvis=needs_tcvis)\n\n            # Predownload auxiliary\n            aoi = self._tile_aoi()\n            if needs_arcticdem:\n                logger.info(\"start download ArcticDEM\")\n                with timer(\"Downloading ArcticDEM\"):\n                    download_arcticdem(aoi, self.arcticdem_dir, resolution=self._arcticdem_resolution())\n            if needs_tcvis:\n                logger.info(\"start download TCVIS\")\n                init_ee(self.ee_project, self.ee_use_highvolume)\n                with timer(\"Downloading TCVis\"):\n                    download_tcvis(aoi, self.tcvis_dir)\n\n    # Predownload tiles if optical flag is set\n    if not optical and not force:\n        return\n\n    # Iterate over all the data\n    with timer(\"Loading Optical\"):\n        tileinfo = self._tileinfos()\n        n_tiles = 0\n        logger.info(f\"Found {len(tileinfo)} tiles to download.\")\n        for i, (tilekey, _) in enumerate(tileinfo):\n            tile_id = self._get_tile_id(tilekey)\n            try:\n                self._download_tile(tilekey)\n                n_tiles += 1\n                logger.info(f\"Downloaded sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n            except (KeyboardInterrupt, SystemError, SystemExit) as e:\n                logger.warning(f\"{type(e).__name__} detected.\\nExiting...\")\n                raise e\n            except Exception as e:\n                logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n                logger.exception(e)\n        else:\n            logger.info(f\"Downloaded {n_tiles} tiles.\")\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> <p>Run the complete segmentation pipeline.</p> <p>Executes the full pipeline including: 1. Configuration validation and dumping 2. Loading ensemble models 3. Creating/loading auxiliary datacubes 4. Processing each tile:    - Loading optical data    - Loading auxiliary data (ArcticDEM, TCVis) as needed    - Preprocessing    - Segmentation    - Postprocessing    - Exporting results 5. Saving results and timing information</p> <p>Results are saved to the output directory with timestamped configuration, results parquet file, and timing information.</p> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    \"\"\"Run the complete segmentation pipeline.\n\n    Executes the full pipeline including:\n    1. Configuration validation and dumping\n    2. Loading ensemble models\n    3. Creating/loading auxiliary datacubes\n    4. Processing each tile:\n       - Loading optical data\n       - Loading auxiliary data (ArcticDEM, TCVis) as needed\n       - Preprocessing\n       - Segmentation\n       - Postprocessing\n       - Exporting results\n    5. Saving results and timing information\n\n    Results are saved to the output directory with timestamped configuration,\n    results parquet file, and timing information.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n\n    \"\"\"\n    self._validate()\n    current_time = self._dump_config()\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    import pandas as pd\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_v2\n    from stopuhr import Chronometer, stopwatch\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n    self.device = decide_device(self.device)\n\n    if not self.offline:\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    self._create_auxiliary_datacubes()\n\n    # determine models to use\n    ensemble = self._load_ensemble()\n    ensemble_subsets = ensemble.model_names\n    needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=ensemble_subsets)\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} seems to be already processed, \"\n                        \"but some of the requested outputs are missing. \"\n                        \"Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading Optical\", log=False):\n                tile = self._load_tile(tilekey)\n\n            if needs_arcticdem:\n                with timer(\"Loading ArcticDEM\", log=False):\n                    arcticdem_resolution = self._arcticdem_resolution()\n                    arcticdem = load_arcticdem(\n                        tile.odc.geobox,\n                        self.arcticdem_dir,\n                        resolution=arcticdem_resolution,\n                        buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                        offline=self.offline,\n                    )\n            else:\n                arcticdem = None\n\n            if needs_tcvis:\n                with timer(\"Loading TCVis\", log=False):\n                    tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir, offline=self.offline)\n            else:\n                tcvis = None\n\n            with timer(\"Preprocessing\", log=False):\n                tile = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n\n            with timer(\"Postprocessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    device=self.device,\n                    edge_erosion_size=self.edge_erosion_size,\n                )\n\n            export_metadata = self._result_metadata(tilekey)\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    metadata=export_metadata,\n                    debug=self.debug_data,\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.timer.parquet\")\n            if len(stopwatch.durations) &gt; 0:\n                stopwatch.export().to_parquet(self.output_data_dir / f\"{current_time}.stopwatch.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary(printer=logger.info)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline","title":"Sentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    default_dirs: darts_utils.paths.DefaultPaths = (\n        lambda: darts_utils.paths.DefaultPaths()\n    )(),\n    output_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    edge_erosion_size: int | None = None,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    offline: bool = False,\n    debug_data: bool = False,\n    scene_ids: list[str] | None = None,\n    scene_id_file: pathlib.Path | None = None,\n    tile_ids: list[str] | None = None,\n    aoi_file: pathlib.Path | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n    prep_data_scene_id_file: pathlib.Path | None = None,\n    sentinel2_grid_dir: pathlib.Path | None = None,\n    raw_data_store: pathlib.Path | None = None,\n    no_raw_data_store: bool = False,\n    raw_data_source: typing.Literal[\"gee\", \"cdse\"] = \"cdse\",\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for processing Sentinel-2 data.</p> <p>Processes Sentinel-2 Surface Reflectance (SR) imagery from either CDSE or Google Earth Engine. Supports multiple scene selection methods and flexible filtering options.</p> Source Selection <p>The data source is specified via the <code>raw_data_source</code> parameter: - \"cdse\": Copernicus Data Space Ecosystem (CDSE) - \"gee\": Google Earth Engine (GEE)</p> <p>Both sources require accounts and proper credential setup on the system.</p> Scene Selection <p>Scenes can be selected using one of four mutually exclusive methods (priority order):</p> <ol> <li><code>scene_ids</code>: Direct list of Sentinel-2 scene IDs</li> <li><code>scene_id_file</code>: JSON file containing scene IDs</li> <li><code>tile_ids</code>: List of Sentinel-2 tile IDs (e.g., \"33UVP\") with optional filters</li> <li><code>aoi_file</code>: Shapefile defining area of interest with optional filters</li> </ol> Filtering Options <p>When using <code>tile_ids</code> or <code>aoi_file</code>, scenes can be filtered by: - Cloud/snow cover: <code>max_cloud_cover</code>, <code>max_snow_cover</code> - Date range: <code>start_date</code> and <code>end_date</code> (YYYY-MM-DD format) - OR specific months/years: <code>months</code> (1-12) and <code>years</code></p> <p>Note: Date range takes priority over month/year filtering. Warning: No temporal filtering may cause rate-limit errors. Note: Month/year filtering is experimental and only implemented for CDSE.</p> Offline Processing <p>Use <code>cli_prepare_data</code> to download data for offline use. The <code>prep_data_scene_id_file</code> stores scene IDs from queries for offline reuse.</p> <p>Parameters:</p> <ul> <li> <code>scene_ids</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Direct list of Sentinel-2 scene IDs to process. Defaults to None.</p> </li> <li> <code>scene_id_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>JSON file containing scene IDs to process. Defaults to None.</p> </li> <li> <code>tile_ids</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of Sentinel-2 tile IDs (requires filtering params). Defaults to None.</p> </li> <li> <code>aoi_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Shapefile with area of interest (requires filtering params). Defaults to None.</p> </li> <li> <code>start_date</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Start date for filtering (YYYY-MM-DD format). Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>End date for filtering (YYYY-MM-DD format). Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int | None</code>, default:                   <code>10</code> )           \u2013            <p>Maximum cloud cover percentage (0-100). Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int | None</code>, default:                   <code>10</code> )           \u2013            <p>Maximum snow cover percentage (0-100). Defaults to 10.</p> </li> <li> <code>months</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by months (1-12). Defaults to None.</p> </li> <li> <code>years</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>Filter by years. Defaults to None.</p> </li> <li> <code>prep_data_scene_id_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>File to store/load scene IDs for offline processing. Written during <code>prepare_data</code>, read during offline <code>run</code>. Defaults to None.</p> </li> <li> <code>sentinel2_grid_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for Sentinel-2 grid shapefiles. Used only in <code>prepare_data</code> with <code>tile_ids</code>. If None, uses default path. Defaults to None.</p> </li> <li> <code>raw_data_store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for storing raw Sentinel-2 data locally. If None, uses default path based on <code>raw_data_source</code>. Defaults to None.</p> </li> <li> <code>no_raw_data_store</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, processes data in-memory without local storage. Overrides <code>raw_data_store</code>. Defaults to False.</p> </li> <li> <code>raw_data_source</code>               (<code>typing.Literal['gee', 'cdse']</code>, default:                   <code>'cdse'</code> )           \u2013            <p>Data source to use. Defaults to \"cdse\".</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path] | None</code>, default:                   <code>None</code> )           \u2013            <p>Path(s) to model file(s) for segmentation. Single Path implies <code>write_model_outputs=False</code>. If None, searches default model directory for all .pt files. Defaults to None.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Output directory for results. If None, uses <code>{default_out}/sentinel2-{raw_data_source}</code>. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for ArcticDEM datacube. Will be created/downloaded if needed. If None, uses default path. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory for TCVis data. If None, uses default path. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>, default:                   <code>None</code> )           \u2013            <p>Computation device. \"cuda\" uses GPU 0, int specifies GPU index, \"auto\" selects free GPU. Defaults to None.</p> </li> <li> <code>ee_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Earth Engine project ID. May be omitted if defined in persistent credentials. Defaults to None.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use EE high-volume server. Defaults to True.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Outer radius (m) for TPI calculation. Defaults to 100.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Inner radius (m) for TPI calculation. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Patch size for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Overlap between patches. Defaults to 256.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Disk size for mask erosion and inner edge cropping. Defaults to 10.</p> </li> <li> <code>edge_erosion_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Size for outer edge cropping. If None, uses <code>mask_erosion_size</code>. Defaults to None.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Minimum object size (pixels) to keep. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>Quality filtering level. 0=\"none\", 1=\"low_quality\", 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>Bands to export. Can include \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\", \"metadata\", or specific band names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Save individual model outputs (not just ensemble). Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Overwrite existing output files. Defaults to False.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Skip downloading missing data. Requires pre-downloaded data. Defaults to False.</p> </li> <li> <code>debug_data</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Write intermediate debugging data to output directory. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.aoi_file","title":"aoi_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_file: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.debug_data","title":"debug_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug_data: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.default_dirs","title":"default_dirs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_dirs: darts_utils.paths.DefaultPaths = dataclasses.field(\n    default_factory=lambda: darts_utils.paths.DefaultPaths()\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.edge_erosion_size","title":"edge_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>edge_erosion_size: int | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int | None = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.max_snow_cover","title":"max_snow_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_snow_cover: int | None = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.months","title":"months  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>months: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.no_raw_data_store","title":"no_raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>no_raw_data_store: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.offline","title":"offline  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offline: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.prep_data_scene_id_file","title":"prep_data_scene_id_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prep_data_scene_id_file: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.raw_data_source","title":"raw_data_source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_source: typing.Literal['gee', 'cdse'] = 'cdse'\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.raw_data_store","title":"raw_data_store  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_store: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.scene_id_file","title":"scene_id_file  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scene_id_file: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.scene_ids","title":"scene_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scene_ids: list[str] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.sentinel2_grid_dir","title":"sentinel2_grid_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_grid_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.tile_ids","title":"tile_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tile_ids: list[str] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.years","title":"years  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>years: list[int] | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def __post_init__(self):  # noqa: D105\n    logger.debug(\"Before super\")\n    super().__post_init__()\n    logger.debug(\"After super\")\n    self.output_data_dir = self.output_data_dir or paths.output_data(f\"sentinel2-{self.raw_data_source}\")\n    self.raw_data_store = self.raw_data_store or paths.sentinel2_raw_data(self.raw_data_source)\n    if self.no_raw_data_store:\n        self.raw_data_store = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Sentinel-2 data.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.Sentinel2Pipeline</code>)           \u2013            <p>Configured Sentinel2Pipeline instance.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for Sentinel-2 data.\n\n    Args:\n        pipeline: Configured Sentinel2Pipeline instance.\n\n    \"\"\"\n    pipeline.__post_init__()\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.cli_prepare_data","title":"cli_prepare_data  <code>staticmethod</code>","text":"<pre><code>cli_prepare_data(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download all necessary data for offline processing.</p> <p>Queries the data source (CDSE or GEE) for scene IDs and downloads optical and/or auxiliary data. Stores scene IDs in <code>prep_data_scene_id_file</code> if specified for later offline use.</p> <p>Parameters:</p> <ul> <li> <code>pipeline</code>               (<code>darts.pipelines.sequential_v2.Sentinel2Pipeline</code>)           \u2013            <p>Configured Sentinel2Pipeline instance.</p> </li> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical (Sentinel-2) imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli_prepare_data(\n    *, pipeline: \"Sentinel2Pipeline\", optical: bool = False, aux: bool = False, force: bool = False\n):\n    \"\"\"Download all necessary data for offline processing.\n\n    Queries the data source (CDSE or GEE) for scene IDs and downloads optical and/or auxiliary data.\n    Stores scene IDs in `prep_data_scene_id_file` if specified for later offline use.\n\n    Args:\n        pipeline: Configured Sentinel2Pipeline instance.\n        optical: If True, downloads optical (Sentinel-2) imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis). Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    \"\"\"\n    assert not pipeline.offline, \"Pipeline must be online to prepare data for offline usage.\"\n\n    # !: Because of an unknown bug, __post_init__ is not initialized automatically\n    pipeline.__post_init__()\n\n    logger.debug(f\"Preparing data with {optical=}, {aux=}.\")\n\n    if pipeline.prep_data_scene_id_file is not None:\n        if pipeline.prep_data_scene_id_file.exists():\n            logger.warning(\n                f\"Prep-data scene id file {pipeline.prep_data_scene_id_file=} already exists. \"\n                \"It will be overwritten.\"\n            )\n            pipeline.prep_data_scene_id_file.unlink()\n    pipeline.prepare_data(optical=optical, aux=aux, force=force)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download and prepare data for offline processing.</p> <p>Validates configuration, determines data requirements from models, and downloads requested data (optical imagery and/or auxiliary data).</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> <li> <code>SystemExit</code>             \u2013            <p>If the process is terminated.</p> </li> <li> <code>SystemError</code>             \u2013            <p>If a system error occurs.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def prepare_data(self, optical: bool = False, aux: bool = False, force: bool = False):\n    \"\"\"Download and prepare data for offline processing.\n\n    Validates configuration, determines data requirements from models,\n    and downloads requested data (optical imagery and/or auxiliary data).\n\n    Args:\n        optical: If True, downloads optical imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n        SystemExit: If the process is terminated.\n        SystemError: If a system error occurs.\n\n    \"\"\"\n    assert optical or aux, \"Nothing to prepare. Please set optical and/or aux to True.\"\n\n    # ? We only want to download stuff - no need for using the GPU here\n    self.device = \"cpu\"\n    self._dump_config()\n\n    from darts_acquisition import download_arcticdem, download_tcvis\n    from stopuhr import Chronometer\n\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n\n    if aux or force:\n        # Get the ensemble to check which auxiliary data is necessary\n        if force:\n            needs_arcticdem, needs_tcvis = True, True\n        else:\n            ensemble = self._load_ensemble()\n            needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n        if not needs_arcticdem and not needs_tcvis:\n            logger.warning(\"No auxiliary data required by the models. Skipping download of auxiliary data...\")\n        else:\n            logger.info(f\"Models {needs_tcvis=} {needs_arcticdem=}.\")\n            self._create_auxiliary_datacubes(arcticdem=needs_arcticdem, tcvis=needs_tcvis)\n\n            # Predownload auxiliary\n            aoi = self._tile_aoi()\n            if needs_arcticdem:\n                logger.info(\"start download ArcticDEM\")\n                with timer(\"Downloading ArcticDEM\"):\n                    download_arcticdem(aoi, self.arcticdem_dir, resolution=self._arcticdem_resolution())\n            if needs_tcvis:\n                logger.info(\"start download TCVIS\")\n                init_ee(self.ee_project, self.ee_use_highvolume)\n                with timer(\"Downloading TCVis\"):\n                    download_tcvis(aoi, self.tcvis_dir)\n\n    # Predownload tiles if optical flag is set\n    if not optical and not force:\n        return\n\n    # Iterate over all the data\n    with timer(\"Loading Optical\"):\n        tileinfo = self._tileinfos()\n        n_tiles = 0\n        logger.info(f\"Found {len(tileinfo)} tiles to download.\")\n        for i, (tilekey, _) in enumerate(tileinfo):\n            tile_id = self._get_tile_id(tilekey)\n            try:\n                self._download_tile(tilekey)\n                n_tiles += 1\n                logger.info(f\"Downloaded sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n            except (KeyboardInterrupt, SystemError, SystemExit) as e:\n                logger.warning(f\"{type(e).__name__} detected.\\nExiting...\")\n                raise e\n            except Exception as e:\n                logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n                logger.exception(e)\n        else:\n            logger.info(f\"Downloaded {n_tiles} tiles.\")\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> <p>Run the complete segmentation pipeline.</p> <p>Executes the full pipeline including: 1. Configuration validation and dumping 2. Loading ensemble models 3. Creating/loading auxiliary datacubes 4. Processing each tile:    - Loading optical data    - Loading auxiliary data (ArcticDEM, TCVis) as needed    - Preprocessing    - Segmentation    - Postprocessing    - Exporting results 5. Saving results and timing information</p> <p>Results are saved to the output directory with timestamped configuration, results parquet file, and timing information.</p> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    \"\"\"Run the complete segmentation pipeline.\n\n    Executes the full pipeline including:\n    1. Configuration validation and dumping\n    2. Loading ensemble models\n    3. Creating/loading auxiliary datacubes\n    4. Processing each tile:\n       - Loading optical data\n       - Loading auxiliary data (ArcticDEM, TCVis) as needed\n       - Preprocessing\n       - Segmentation\n       - Postprocessing\n       - Exporting results\n    5. Saving results and timing information\n\n    Results are saved to the output directory with timestamped configuration,\n    results parquet file, and timing information.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n\n    \"\"\"\n    self._validate()\n    current_time = self._dump_config()\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    import pandas as pd\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_v2\n    from stopuhr import Chronometer, stopwatch\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n    self.device = decide_device(self.device)\n\n    if not self.offline:\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    self._create_auxiliary_datacubes()\n\n    # determine models to use\n    ensemble = self._load_ensemble()\n    ensemble_subsets = ensemble.model_names\n    needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=ensemble_subsets)\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} seems to be already processed, \"\n                        \"but some of the requested outputs are missing. \"\n                        \"Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading Optical\", log=False):\n                tile = self._load_tile(tilekey)\n\n            if needs_arcticdem:\n                with timer(\"Loading ArcticDEM\", log=False):\n                    arcticdem_resolution = self._arcticdem_resolution()\n                    arcticdem = load_arcticdem(\n                        tile.odc.geobox,\n                        self.arcticdem_dir,\n                        resolution=arcticdem_resolution,\n                        buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                        offline=self.offline,\n                    )\n            else:\n                arcticdem = None\n\n            if needs_tcvis:\n                with timer(\"Loading TCVis\", log=False):\n                    tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir, offline=self.offline)\n            else:\n                tcvis = None\n\n            with timer(\"Preprocessing\", log=False):\n                tile = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n\n            with timer(\"Postprocessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    device=self.device,\n                    edge_erosion_size=self.edge_erosion_size,\n                )\n\n            export_metadata = self._result_metadata(tilekey)\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    metadata=export_metadata,\n                    debug=self.debug_data,\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.timer.parquet\")\n            if len(stopwatch.durations) &gt; 0:\n                stopwatch.export().to_parquet(self.output_data_dir / f\"{current_time}.stopwatch.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary(printer=logger.info)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline","title":"_BasePipeline  <code>dataclass</code>","text":"<pre><code>_BasePipeline(\n    model_files: list[pathlib.Path] = None,\n    default_dirs: darts_utils.paths.DefaultPaths = (\n        lambda: darts_utils.paths.DefaultPaths()\n    )(),\n    output_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    edge_erosion_size: int | None = None,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = (\n        lambda: [\n            \"probabilities\",\n            \"binarized\",\n            \"polygonized\",\n            \"extent\",\n            \"thumbnail\",\n        ]\n    )(),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    offline: bool = False,\n    debug_data: bool = False,\n)\n</code></pre> <p>               Bases: <code>abc.ABC</code></p> <p>Base class for all v2 pipelines.</p> <p>This class provides the <code>run</code> and <code>prepare_data</code> methods which are the main entry points for all pipelines.</p> <p>This class is meant to be subclassed by the specific pipelines (e.g., PlanetPipeline, Sentinel2Pipeline). Subclasses must implement the following abstract methods:     - <code>_arcticdem_resolution</code>: Return the ArcticDEM resolution to use (2, 10, or 32 meters).     - <code>_get_tile_id</code>: Extract a tile identifier from a tilekey.     - <code>_tileinfos</code>: Return a list of (tilekey, output_path) tuples for all tiles to process.     - <code>_load_tile</code>: Load optical data for a given tilekey.     - <code>_tile_aoi</code>: Return a GeoDataFrame representing the area of interest for all tiles.</p> <p>Optionally, subclasses can override <code>_download_tile</code> to implement data download functionality.</p> <p>The subclass must also be a dataclass to fully inherit all parameters of this class.</p> <p>Parameters:</p> <ul> <li> <code>model_files</code>               (<code>list[pathlib.Path] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of model file paths to use for segmentation. If None, will search the default model directory for all .pt files. Defaults to None.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>(lambda: darts_utils.paths.DefaultPaths())()</code> )           \u2013            <p>Default directory paths configuration. Defaults to DefaultPaths().</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The output directory for results. If None, will use the default output directory based on DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory containing ArcticDEM datacube and extent files. If None, will use the default directory based on DARTS paths and resolution. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory containing TCVis data. If None, will use the default TCVis directory. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>, default:                   <code>None</code> )           \u2013            <p>Device for computation. \"cuda\" uses GPU 0, int specifies GPU index, \"auto\" selects free GPU, \"cpu\" uses CPU. Defaults to None (auto-selected).</p> </li> <li> <code>ee_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Earth Engine project ID. May be omitted if defined in persistent credentials. Defaults to None.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use Earth Engine high-volume server. Defaults to True.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Outer radius in meters for TPI (Topographic Position Index) calculation. Defaults to 100.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Inner radius in meters for TPI calculation. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Patch size for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>Overlap between patches during inference. Defaults to 256.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Size of disk for mask erosion and inner edge cropping. Defaults to 10.</p> </li> <li> <code>edge_erosion_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Size for outer edge cropping. If None, defaults to <code>mask_erosion_size</code>. Defaults to None.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Minimum object size in pixels to keep. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>Quality filtering level. Can be 0=\"none\", 1=\"low_quality\", 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>(lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail'])()</code> )           \u2013            <p>Bands to export, e.g., \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\", \"metadata\", or specific band names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save individual model outputs (not just ensemble). Automatically set to False if only one model is used. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing output files. Defaults to False.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will not attempt to download any missing data. Defaults to False.</p> </li> <li> <code>debug_data</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, writes intermediate data for debugging purposes. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.debug_data","title":"debug_data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug_data: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.default_dirs","title":"default_dirs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>default_dirs: darts_utils.paths.DefaultPaths = dataclasses.field(\n    default_factory=lambda: darts_utils.paths.DefaultPaths()\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.edge_erosion_size","title":"edge_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>edge_erosion_size: int | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.offline","title":"offline  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offline: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def __post_init__(self):\n    paths.set_defaults(self.default_dirs)\n    # The defaults will be overwritten in the respective realizations\n    self.output_data_dir = self.output_data_dir or paths.output_data(\"base_pipeline\")\n    self.model_files = self.model_files or paths.ensemble_models()\n    self.arcticdem_dir = self.arcticdem_dir or paths.arcticdem(self._arcticdem_resolution())\n    self.tcvis_dir = self.tcvis_dir or paths.tcvis()\n    self.edge_erosion_size = self.edge_erosion_size or self.mask_erosion_size\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(\n    optical: bool = False,\n    aux: bool = False,\n    force: bool = False,\n)\n</code></pre> <p>Download and prepare data for offline processing.</p> <p>Validates configuration, determines data requirements from models, and downloads requested data (optical imagery and/or auxiliary data).</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads optical imagery. Defaults to False.</p> </li> <li> <code>aux</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, downloads all possible data, independent of <code>optical</code> and <code>aux</code> flags or model needs. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> <li> <code>SystemExit</code>             \u2013            <p>If the process is terminated.</p> </li> <li> <code>SystemError</code>             \u2013            <p>If a system error occurs.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def prepare_data(self, optical: bool = False, aux: bool = False, force: bool = False):\n    \"\"\"Download and prepare data for offline processing.\n\n    Validates configuration, determines data requirements from models,\n    and downloads requested data (optical imagery and/or auxiliary data).\n\n    Args:\n        optical: If True, downloads optical imagery. Defaults to False.\n        aux: If True, downloads auxiliary data (ArcticDEM, TCVis) as needed. Defaults to False.\n        force: If True, downloads all possible data, independent of `optical` and `aux` flags or model needs.\n            Defaults to False.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n        SystemExit: If the process is terminated.\n        SystemError: If a system error occurs.\n\n    \"\"\"\n    assert optical or aux, \"Nothing to prepare. Please set optical and/or aux to True.\"\n\n    # ? We only want to download stuff - no need for using the GPU here\n    self.device = \"cpu\"\n    self._dump_config()\n\n    from darts_acquisition import download_arcticdem, download_tcvis\n    from stopuhr import Chronometer\n\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n\n    if aux or force:\n        # Get the ensemble to check which auxiliary data is necessary\n        if force:\n            needs_arcticdem, needs_tcvis = True, True\n        else:\n            ensemble = self._load_ensemble()\n            needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n        if not needs_arcticdem and not needs_tcvis:\n            logger.warning(\"No auxiliary data required by the models. Skipping download of auxiliary data...\")\n        else:\n            logger.info(f\"Models {needs_tcvis=} {needs_arcticdem=}.\")\n            self._create_auxiliary_datacubes(arcticdem=needs_arcticdem, tcvis=needs_tcvis)\n\n            # Predownload auxiliary\n            aoi = self._tile_aoi()\n            if needs_arcticdem:\n                logger.info(\"start download ArcticDEM\")\n                with timer(\"Downloading ArcticDEM\"):\n                    download_arcticdem(aoi, self.arcticdem_dir, resolution=self._arcticdem_resolution())\n            if needs_tcvis:\n                logger.info(\"start download TCVIS\")\n                init_ee(self.ee_project, self.ee_use_highvolume)\n                with timer(\"Downloading TCVis\"):\n                    download_tcvis(aoi, self.tcvis_dir)\n\n    # Predownload tiles if optical flag is set\n    if not optical and not force:\n        return\n\n    # Iterate over all the data\n    with timer(\"Loading Optical\"):\n        tileinfo = self._tileinfos()\n        n_tiles = 0\n        logger.info(f\"Found {len(tileinfo)} tiles to download.\")\n        for i, (tilekey, _) in enumerate(tileinfo):\n            tile_id = self._get_tile_id(tilekey)\n            try:\n                self._download_tile(tilekey)\n                n_tiles += 1\n                logger.info(f\"Downloaded sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n            except (KeyboardInterrupt, SystemError, SystemExit) as e:\n                logger.warning(f\"{type(e).__name__} detected.\\nExiting...\")\n                raise e\n            except Exception as e:\n                logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n                logger.exception(e)\n        else:\n            logger.info(f\"Downloaded {n_tiles} tiles.\")\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> <p>Run the complete segmentation pipeline.</p> <p>Executes the full pipeline including: 1. Configuration validation and dumping 2. Loading ensemble models 3. Creating/loading auxiliary datacubes 4. Processing each tile:    - Loading optical data    - Loading auxiliary data (ArcticDEM, TCVis) as needed    - Preprocessing    - Segmentation    - Postprocessing    - Exporting results 5. Saving results and timing information</p> <p>Results are saved to the output directory with timestamped configuration, results parquet file, and timing information.</p> <p>Raises:</p> <ul> <li> <code>KeyboardInterrupt</code>             \u2013            <p>If user interrupts execution.</p> </li> </ul> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    \"\"\"Run the complete segmentation pipeline.\n\n    Executes the full pipeline including:\n    1. Configuration validation and dumping\n    2. Loading ensemble models\n    3. Creating/loading auxiliary datacubes\n    4. Processing each tile:\n       - Loading optical data\n       - Loading auxiliary data (ArcticDEM, TCVis) as needed\n       - Preprocessing\n       - Segmentation\n       - Postprocessing\n       - Exporting results\n    5. Saving results and timing information\n\n    Results are saved to the output directory with timestamped configuration,\n    results parquet file, and timing information.\n\n    Raises:\n        KeyboardInterrupt: If user interrupts execution.\n\n    \"\"\"\n    self._validate()\n    current_time = self._dump_config()\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    import pandas as pd\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_v2\n    from stopuhr import Chronometer, stopwatch\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    timer = Chronometer(printer=logger.debug)\n    self.device = decide_device(self.device)\n\n    if not self.offline:\n        init_ee(self.ee_project, self.ee_use_highvolume)\n\n    self._create_auxiliary_datacubes()\n\n    # determine models to use\n    ensemble = self._load_ensemble()\n    ensemble_subsets = ensemble.model_names\n    needs_arcticdem, needs_tcvis = self._check_aux_needs(ensemble)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=ensemble_subsets)\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} seems to be already processed, \"\n                        \"but some of the requested outputs are missing. \"\n                        \"Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading Optical\", log=False):\n                tile = self._load_tile(tilekey)\n\n            if needs_arcticdem:\n                with timer(\"Loading ArcticDEM\", log=False):\n                    arcticdem_resolution = self._arcticdem_resolution()\n                    arcticdem = load_arcticdem(\n                        tile.odc.geobox,\n                        self.arcticdem_dir,\n                        resolution=arcticdem_resolution,\n                        buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                        offline=self.offline,\n                    )\n            else:\n                arcticdem = None\n\n            if needs_tcvis:\n                with timer(\"Loading TCVis\", log=False):\n                    tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir, offline=self.offline)\n            else:\n                tcvis = None\n\n            with timer(\"Preprocessing\", log=False):\n                tile = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n\n            with timer(\"Postprocessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    device=self.device,\n                    edge_erosion_size=self.edge_erosion_size,\n                )\n\n            export_metadata = self._result_metadata(tilekey)\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=ensemble_subsets if self.write_model_outputs else [],\n                    metadata=export_metadata,\n                    debug=self.debug_data,\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.timer.parquet\")\n            if len(stopwatch.durations) &gt; 0:\n                stopwatch.export().to_parquet(self.output_data_dir / f\"{current_time}.stopwatch.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary(printer=logger.info)\n</code></pre>"},{"location":"reference/darts/training/","title":"training","text":""},{"location":"reference/darts/training/#darts.training","title":"darts.training","text":"<p>Pipeline-related training functions and scripts.</p>"},{"location":"reference/darts/training/#darts.training.preprocess_planet_train_data","title":"preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    train_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    admin_dir: pathlib.Path | None = None,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches and storing them in a zarr array for efficient random access during training. All data is stored in a single zarr group with associated metadata.</p> <p>The preprocessing creates patches of the specified size from each Planet scene and stores them as: - A zarr group containing 'x' (input data) and 'y' (labels) arrays - A geopandas dataframe with metadata including region, position, and label statistics - A configuration file with preprocessing parameters</p> <p>The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in each patch being stored in a separate file for super fast random access.</p> <p>The metadata dataframe contains information about each patch including: - sample_id: Identifier for the source Planet scene - region: Administrative region name - geometry: Spatial extent of the patch - empty: Whether the patch contains positive labeled pixels - Additional metadata as specified</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>A <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n\u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n\u251c\u2500\u2500 metadata.parquet\n\u2514\u2500\u2500 {timestamp}.cli.toml\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The \"output\" directory where the tensors are written to. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the TCVis data. If None, will use the default TCVis directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the admin files. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. If None, will neither use nor store preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def preprocess_planet_train_data(  # noqa: C901\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    train_data_dir: Path | None = None,\n    arcticdem_dir: Path | None = None,\n    tcvis_dir: Path | None = None,\n    admin_dir: Path | None = None,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches\n    and storing them in a zarr array for efficient random access during training. All data is stored in\n    a single zarr group with associated metadata.\n\n    The preprocessing creates patches of the specified size from each Planet scene and stores them as:\n    - A zarr group containing 'x' (input data) and 'y' (labels) arrays\n    - A geopandas dataframe with metadata including region, position, and label statistics\n    - A configuration file with preprocessing parameters\n\n    The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in\n    each patch being stored in a separate file for super fast random access.\n\n    The metadata dataframe contains information about each patch including:\n    - sample_id: Identifier for the source Planet scene\n    - region: Administrative region name\n    - geometry: Spatial extent of the patch\n    - empty: Whether the patch contains positive labeled pixels\n    - Additional metadata as specified\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    A `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/\n    \u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n    \u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n    \u251c\u2500\u2500 metadata.parquet\n    \u2514\u2500\u2500 {timestamp}.cli.toml\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        train_data_dir (Path | None, optional): The \"output\" directory where the tensors are written to.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        arcticdem_dir (Path | None, optional): The directory containing the ArcticDEM data\n            (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        tcvis_dir (Path | None, optional): The directory containing the TCVis data.\n            If None, will use the default TCVis directory based on the DARTS paths.\n            Defaults to None.\n        admin_dir (Path | None, optional): The directory containing the admin files.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        preprocess_cache (Path | None, optional): The directory to store the preprocessed data.\n            If None, will neither use nor store preprocessed data.\n            Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    paths.set_defaults(default_dirs)\n    train_data_dir = train_data_dir or paths.train_data_dir(\"planet_v2_rts\", patch_size)\n    arcticdem_dir = arcticdem_dir or paths.arcticdem(2)\n    tcvis_dir = tcvis_dir or paths.tcvis()\n    admin_dir = admin_dir or paths.admin_boundaries()\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.toml\",\n        function=preprocess_planet_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import smart_geocubes\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import LoggingManager\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    accessor = smart_geocubes.ArcticDEM2m(arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    fpaths = {fpath.stem: fpath for fpath in _legacy_path_gen(data_dir)}\n    footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these since they depend on the preprocessing we use\n    bands = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"nir\",\n        \"ndvi\",\n        \"relative_elevation\",\n        \"slope\",\n        \"aspect\",\n        \"hillshade\",\n        \"curvature\",\n        \"tc_brightness\",\n        \"tc_greenness\",\n        \"tc_wetness\",\n    ]\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache)\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        info_id = f\"{planet_id=} ({i + 1} of {len(footprint)})\"\n        try:\n            logger.info(f\"Processing sample {info_id}\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(\n                    f\"Footprint image '{planet_id}' at {footprint.fpath} does not exist. Skipping {info_id}...\"\n                )\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {info_id}\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {info_id}. Skipping...\")\n            logger.exception(e)\n\n    timer.summary()\n\n    if len(builder) == 0:\n        logger.warning(\"No samples were processed. Exiting...\")\n        return\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n</code></pre>"},{"location":"reference/darts/training/#darts.training.preprocess_planet_train_data_pingo","title":"preprocess_planet_train_data_pingo","text":"<pre><code>preprocess_planet_train_data_pingo(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    train_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    admin_dir: pathlib.Path | None = None,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n)\n</code></pre> <p>Preprocess Planet data for training (Pingo version).</p> <p>This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches and storing them in a zarr array for efficient random access during training. All data is stored in a single zarr group with associated metadata.</p> <p>The preprocessing creates patches of the specified size from each Planet scene and stores them as: - A zarr group containing 'x' (input data) and 'y' (labels) arrays - A geopandas dataframe with metadata including region, position, and label statistics - A configuration file with preprocessing parameters</p> <p>The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in each patch being stored in a separate file for super fast random access.</p> <p>The metadata dataframe contains information about each patch including: - sample_id: Identifier for the source Planet scene - region: Administrative region name - geometry: Spatial extent of the patch - empty: Whether the patch contains positive labeled pixels - Additional metadata as specified</p> <p>Through <code>exclude_nopositive</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>A <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n\u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n\u251c\u2500\u2500 metadata.parquet\n\u2514\u2500\u2500 {timestamp}.cli.json\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The \"output\" directory where the tensors are written to. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the TCVis data. If None, will use the default TCVis directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the admin files. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. If None, will neither use nor store preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def preprocess_planet_train_data_pingo(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    train_data_dir: Path | None = None,\n    arcticdem_dir: Path | None = None,\n    tcvis_dir: Path | None = None,\n    admin_dir: Path | None = None,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n):\n    \"\"\"Preprocess Planet data for training (Pingo version).\n\n    This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches\n    and storing them in a zarr array for efficient random access during training. All data is stored in\n    a single zarr group with associated metadata.\n\n    The preprocessing creates patches of the specified size from each Planet scene and stores them as:\n    - A zarr group containing 'x' (input data) and 'y' (labels) arrays\n    - A geopandas dataframe with metadata including region, position, and label statistics\n    - A configuration file with preprocessing parameters\n\n    The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in\n    each patch being stored in a separate file for super fast random access.\n\n    The metadata dataframe contains information about each patch including:\n    - sample_id: Identifier for the source Planet scene\n    - region: Administrative region name\n    - geometry: Spatial extent of the patch\n    - empty: Whether the patch contains positive labeled pixels\n    - Additional metadata as specified\n\n    Through `exclude_nopositive` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    A `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/\n    \u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n    \u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n    \u251c\u2500\u2500 metadata.parquet\n    \u2514\u2500\u2500 {timestamp}.cli.json\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        train_data_dir (Path | None, optional): The \"output\" directory where the tensors are written to.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        arcticdem_dir (Path | None, optional): The directory containing the ArcticDEM data\n            (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        tcvis_dir (Path | None, optional): The directory containing the TCVis data.\n            If None, will use the default TCVis directory based on the DARTS paths.\n            Defaults to None.\n        admin_dir (Path | None, optional): The directory containing the admin files.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        preprocess_cache (Path | None, optional): The directory to store the preprocessed data.\n            If None, will neither use nor store preprocessed data.\n            Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    paths.set_defaults(default_dirs)\n    train_data_dir = train_data_dir or paths.train_data_dir(\"planet_v2_pingo\", patch_size)\n    arcticdem_dir = arcticdem_dir or paths.arcticdem(2)\n    tcvis_dir = tcvis_dir or paths.tcvis()\n    admin_dir = admin_dir or paths.admin_boundaries()\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.toml\",\n        function=preprocess_planet_train_data_pingo,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import smart_geocubes\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import LoggingManager\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    accessor = smart_geocubes.ArcticDEM2m(arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    footprints[\"fpath\"] = footprints.image_id.map(_path_gen(data_dir))\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these since they depend on the preprocessing we use\n    bands = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"nir\",\n        \"ndvi\",\n        \"relative_elevation\",\n        \"slope\",\n        \"aspect\",\n        \"hillshade\",\n        \"curvature\",\n        \"tc_brightness\",\n        \"tc_greenness\",\n        \"tc_wetness\",\n    ]\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        device=device,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache)\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        info_id = f\"{planet_id=} ({i + 1} of {len(footprint)})\"\n        try:\n            logger.debug(f\"Processing sample {info_id}\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(\n                    f\"Footprint image '{planet_id}' at {footprint.fpath} does not exist. Skipping {info_id}...\"\n                )\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {info_id}\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {info_id} . Skipping...\")\n            logger.exception(e)\n\n    timer.summary()\n\n    if len(builder) == 0:\n        logger.warning(\"No samples were processed. Exiting...\")\n        return\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n</code></pre>"},{"location":"reference/darts/training/#darts.training.preprocess_s2_train_data","title":"preprocess_s2_train_data","text":"<pre><code>preprocess_s2_train_data(\n    *,\n    labels_dir: pathlib.Path,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    train_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    admin_dir: pathlib.Path | None = None,\n    planet_data_dir: pathlib.Path | None = None,\n    raw_data_store: pathlib.Path | None = None,\n    no_raw_data_store: bool = False,\n    preprocess_cache: pathlib.Path | None = None,\n    matching_cache: pathlib.Path | None = None,\n    no_matching_cache: bool = False,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    matching_day_range: int = 7,\n    matching_max_cloud_cover: int = 10,\n    matching_min_intersects: float = 0.7,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    save_matching_scores: bool = False,\n)\n</code></pre> <p>Preprocess Sentinel-2 data for training.</p> <p>This function preprocesses Sentinel-2 scenes matched to Planet footprints into a training-ready format by creating fixed-size patches and storing them in a zarr array for efficient random access during training. All data is stored in a single zarr group with associated metadata.</p> <p>The preprocessing matches Sentinel-2 scenes to Planet footprints based on temporal and spatial criteria, optionally aligns them spatially to Planet data, and creates patches of the specified size. The data is stored as: - A zarr group containing 'x' (input data) and 'y' (labels) arrays - A geopandas dataframe with metadata including region, position, and label statistics - A configuration file with preprocessing parameters</p> <p>The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in each patch being stored in a separate file for super fast random access.</p> <p>The metadata dataframe contains information about each patch including: - sample_id: Combined identifier for the S2 scene and Planet footprint - region: Administrative region name - geometry: Spatial extent of the patch - empty: Whether the patch contains positive labeled pixels - planet_id: Original Planet scene identifier - s2_id: Sentinel-2 scene identifier - Additional alignment and matching metadata</p> <p>Through <code>exclude_nopositive</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>A <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n\u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n\u251c\u2500\u2500 metadata.parquet\n\u251c\u2500\u2500 matching-cache.json      # Optional matching cache\n\u251c\u2500\u2500 matching-scores.parquet  # Optional matching scores\n\u2514\u2500\u2500 {timestamp}.cli.toml\n</code></pre> <p>Parameters:</p> <ul> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The \"output\" directory where the tensors are written to. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the TCVis data. If None, will use the default TCVis directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the admin files. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>planet_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the Planet scenes and orthotiles. The planet data is used to align the Sentinel-2 data to the Planet data, spatially. Can be set to None if no alignment is wished. Defaults to None.</p> </li> <li> <code>raw_data_store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to use for storing the raw Sentinel 2 data locally. If None, will use the default raw data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>no_raw_data_store</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will not store any raw data locally. This overrides the <code>raw_data_store</code> parameter. Defaults to False.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. If None, will neither use nor store preprocessed data. Defaults to None.</p> </li> <li> <code>matching_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to a file where the matchings are stored. Note: this is different from the matching scores. If None, will query the sentinel 2 STAC and calculate the best match based on the criteria. Defaults to None.</p> </li> <li> <code>no_matching_cache</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will not use or store any matching cache. This overrides the <code>matching_cache</code> parameter. Defaults to False.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com). Defaults to True.</p> </li> <li> <code>matching_day_range</code>               (<code>int</code>, default:                   <code>7</code> )           \u2013            <p>The day range to use for matching S2 scenes to Planet footprints. Defaults to 7.</p> </li> <li> <code>matching_max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for matching S2 scenes to Planet footprints. Defaults to 10.</p> </li> <li> <code>matching_min_intersects</code>               (<code>float</code>, default:                   <code>0.7</code> )           \u2013            <p>The minimum intersection percentage to use for matching S2 scenes to Planet footprints. Defaults to 0.7.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>save_matching_scores</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the matching scores. Defaults to False.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_sentinel2_v2.py</code> <pre><code>def preprocess_s2_train_data(  # noqa: C901\n    *,\n    labels_dir: Path,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    train_data_dir: Path | None = None,\n    arcticdem_dir: Path | None = None,\n    tcvis_dir: Path | None = None,\n    admin_dir: Path | None = None,\n    planet_data_dir: Path | None = None,\n    raw_data_store: Path | None = None,\n    no_raw_data_store: bool = False,\n    preprocess_cache: Path | None = None,\n    matching_cache: Path | None = None,\n    no_matching_cache: bool = False,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    matching_day_range: int = 7,\n    matching_max_cloud_cover: int = 10,\n    matching_min_intersects: float = 0.7,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    save_matching_scores: bool = False,\n):\n    \"\"\"Preprocess Sentinel-2 data for training.\n\n    This function preprocesses Sentinel-2 scenes matched to Planet footprints into a training-ready format\n    by creating fixed-size patches and storing them in a zarr array for efficient random access during training.\n    All data is stored in a single zarr group with associated metadata.\n\n    The preprocessing matches Sentinel-2 scenes to Planet footprints based on temporal and spatial criteria,\n    optionally aligns them spatially to Planet data, and creates patches of the specified size. The data is stored as:\n    - A zarr group containing 'x' (input data) and 'y' (labels) arrays\n    - A geopandas dataframe with metadata including region, position, and label statistics\n    - A configuration file with preprocessing parameters\n\n    The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in\n    each patch being stored in a separate file for super fast random access.\n\n    The metadata dataframe contains information about each patch including:\n    - sample_id: Combined identifier for the S2 scene and Planet footprint\n    - region: Administrative region name\n    - geometry: Spatial extent of the patch\n    - empty: Whether the patch contains positive labeled pixels\n    - planet_id: Original Planet scene identifier\n    - s2_id: Sentinel-2 scene identifier\n    - Additional alignment and matching metadata\n\n    Through `exclude_nopositive` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    A `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/\n    \u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n    \u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n    \u251c\u2500\u2500 metadata.parquet\n    \u251c\u2500\u2500 matching-cache.json      # Optional matching cache\n    \u251c\u2500\u2500 matching-scores.parquet  # Optional matching scores\n    \u2514\u2500\u2500 {timestamp}.cli.toml\n    ```\n\n    Args:\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        train_data_dir (Path | None, optional): The \"output\" directory where the tensors are written to.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        arcticdem_dir (Path | None, optional): The directory containing the ArcticDEM data\n            (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        tcvis_dir (Path | None, optional): The directory containing the TCVis data.\n            If None, will use the default TCVis directory based on the DARTS paths.\n            Defaults to None.\n        admin_dir (Path | None, optional): The directory containing the admin files.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        planet_data_dir (Path, optional): The directory containing the Planet scenes and orthotiles.\n            The planet data is used to align the Sentinel-2 data to the Planet data, spatially.\n            Can be set to None if no alignment is wished.\n            Defaults to None.\n        raw_data_store (Path | None): The directory to use for storing the raw Sentinel 2 data locally.\n            If None, will use the default raw data directory based on the DARTS paths.\n            Defaults to None.\n        no_raw_data_store (bool, optional): If True, will not store any raw data locally.\n            This overrides the `raw_data_store` parameter.\n            Defaults to False.\n        preprocess_cache (Path | None, optional): The directory to store the preprocessed data.\n            If None, will neither use nor store preprocessed data.\n            Defaults to None.\n        matching_cache (Path | None, optional): The path to a file where the matchings are stored.\n            Note: this is different from the matching scores.\n            If None, will query the sentinel 2 STAC and calculate the best match based on the criteria.\n            Defaults to None.\n        no_matching_cache (bool, optional): If True, will not use or store any matching cache.\n            This overrides the `matching_cache` parameter.\n            Defaults to False.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n            Defaults to True.\n        matching_day_range (int, optional): The day range to use for matching S2 scenes to Planet footprints.\n            Defaults to 7.\n        matching_max_cloud_cover (int, optional): The maximum cloud cover percentage to use for matching S2 scenes\n            to Planet footprints. Defaults to 10.\n        matching_min_intersects (float, optional): The minimum intersection percentage to use for matching S2 scenes\n            to Planet footprints. Defaults to 0.7.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        save_matching_scores (bool, optional): Whether to save the matching scores. Defaults to False.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    paths.set_defaults(default_dirs)\n    train_data_dir = train_data_dir or paths.train_data_dir(\"sentinel2_v2_rts\", patch_size)\n    arcticdem_dir = arcticdem_dir or paths.arcticdem(10)\n    tcvis_dir = tcvis_dir or paths.tcvis()\n    admin_dir = admin_dir or paths.admin_boundaries()\n    raw_data_store = raw_data_store or paths.sentinel2_raw_data(\"cdse\")\n    if no_raw_data_store:\n        raw_data_store = None\n    matching_cache = matching_cache or train_data_dir / \"matching-cache.json\"\n    if no_matching_cache:\n        matching_cache = None\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.toml\",\n        function=preprocess_s2_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import smart_geocubes\n    import xarray as xr\n    from botocore.exceptions import ProfileNotFound\n    from darts_acquisition import (\n        load_arcticdem,\n        load_cdse_s2_sr_scene,\n        load_tcvis,\n        match_cdse_s2_sr_scene_ids_from_geodataframe,\n    )\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.geo.geom import Geometry\n    from pystac import Item\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import LoggingManager\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    logger.info(\"Configured Rasterio\")\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    accessor = smart_geocubes.ArcticDEM10m(arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    footprints[\"geometry\"] = footprints[\"geometry\"].simplify(0.001)  # Simplify to reduce compute\n    footprints[\"date\"] = footprints.apply(_parse_date, axis=1)\n    if planet_data_dir is not None:\n        fpaths = {fpath.stem: fpath for fpath in _planet_legacy_path_gen(planet_data_dir)}\n        footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    logger.info(f\"label directory contained {len(footprints)} footprints\")\n\n    # Find S2 scenes that intersect with the Planet footprints\n    if matching_cache is None or not matching_cache.exists():\n        logger.info(\"evaluating online CDSE catalogue for matching Sentinel-2 scenes\")\n        matches = match_cdse_s2_sr_scene_ids_from_geodataframe(\n            aoi=footprints,\n            day_range=matching_day_range,\n            max_cloud_cover=matching_max_cloud_cover,\n            min_intersects=matching_min_intersects,\n            simplify_geometry=0.001,\n            save_scores=train_data_dir / \"matching-scores.parquet\" if save_matching_scores else None,\n        )\n        if matching_cache is not None:\n            matches_serializable = {k: v.to_dict() if isinstance(v, Item) else \"None\" for k, v in matches.items()}\n            with matching_cache.open(\"w\") as f:\n                json.dump(matches_serializable, f)\n            logger.info(f\"Saved matching scores to {matching_cache}\")\n            del matches_serializable  # Free memory\n    else:\n        logger.info(f\"Loading matching scores from {matching_cache}\")\n        with matching_cache.open(\"r\") as f:\n            matches_serializable = json.load(f)\n        matches = {int(k): Item.from_dict(v) if v != \"None\" else None for k, v in matches_serializable.items()}\n        del matches_serializable  # Free memory\n    footprints[\"s2_item\"] = footprints.index.map(matches)\n\n    # Filter out footprints without a matching S2 item\n    logger.info(f\"Found {len(footprints)} footprints, {footprints.s2_item.notna().sum()} with matching S2 items.\")\n    footprints = footprints[footprints.s2_item.notna()]\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these since they depend on the preprocessing we use\n    bands = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"nir\",\n        \"ndvi\",\n        \"relative_elevation\",\n        \"slope\",\n        \"aspect\",\n        \"hillshade\",\n        \"curvature\",\n        \"tc_brightness\",\n        \"tc_greenness\",\n        \"tc_wetness\",\n    ]\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache)\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        s2_item = footprint.s2_item\n        # Convert to stac item if dictionary\n        if isinstance(s2_item, dict):\n            s2_item = Item.from_dict(s2_item)\n\n        s2_id = s2_item.id\n        planet_id = footprint.image_id\n        info_id = f\"{s2_id=} -&gt; {planet_id=} ({i + 1} of {len(footprints)})\"\n        try:\n            logger.info(f\"Processing sample {info_id}\")\n\n            if planet_data_dir is not None and (\n                not footprint.fpath or pd.isna(footprint.fpath) or (not footprint.fpath.exists())\n            ):\n                logger.warning(\n                    f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping sample {info_id}...\"\n                )\n                continue\n\n            def _get_tile():\n                s2ds = load_cdse_s2_sr_scene(s2_item, store=raw_data_store)\n\n                # Crop to footprint geometry\n                geom = Geometry(footprint.geometry, crs=footprints.crs)\n                s2ds = s2ds.odc.crop(geom, apply_mask=True)\n                # Crop above will change all dtypes to float32 -&gt; change them back for s2_scl and qa mask\n                s2ds[\"s2_scl\"] = s2ds[\"s2_scl\"].fillna(0.0).astype(\"uint8\")\n                s2ds[\"quality_data_mask\"] = s2ds[\"quality_data_mask\"].fillna(0.0).astype(\"uint8\")\n\n                # Preprocess as usual\n                arctidem_res = 10\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    s2ds.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(s2ds.odc.geobox, tcvis_dir)\n\n                s2ds: xr.Dataset = preprocess_v2(\n                    s2ds,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return s2ds\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=f\"preprocess-s2train-v2-{s2_id}_{planet_id}\",\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            # Skip if the size is too small\n            if tile.sizes[\"x\"] &lt; patch_size or tile.sizes[\"y\"] &lt; patch_size:\n                logger.info(f\"Skipping sample {info_id} due to small size {tile.sizes}.\")\n                continue\n\n            footprint_labels = labels[labels.image_id == planet_id].to_crs(tile.odc.crs)\n            region = _get_region_name(footprint, admin2)\n\n            if planet_data_dir is not None:\n                with timer(\"Align to PLANET\"):\n                    footprint_labels, offsets_info = _align_offsets(tile, footprint, footprint_labels)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=f\"{s2_id}_{planet_id}\",\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"s2_id\": s2_id,\n                        \"fpath\": footprint.fpath,\n                        **offsets_info,\n                    },\n                )\n\n            logger.info(f\"Processed sample {info_id}\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n        except ProfileNotFound:\n            logger.error(\"tried to download from CDSE@AWS but no CDSE credentials found. \")\n            return\n        except Exception as e:\n            logger.warning(f\"Could not process sample {info_id}. Skipping...\")\n            logger.exception(e)\n\n    timer.summary()\n\n    if len(builder) == 0:\n        logger.warning(\"No samples were processed. Exiting...\")\n        return\n\n    builder.finalize(\n        {\n            \"planet_data_dir\": planet_data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2/","title":"preprocess_planet_v2","text":""},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2","title":"darts.training.preprocess_planet_v2","text":"<p>Planet preprocessing functions for training with the v2 data preprocessing.</p>"},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2._get_region_name","title":"_get_region_name","text":"<pre><code>_get_region_name(\n    footprint: geopandas.GeoSeries,\n    admin2: geopandas.GeoDataFrame,\n) -&gt; str\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def _get_region_name(footprint: \"gpd.GeoSeries\", admin2: \"gpd.GeoDataFrame\") -&gt; str:\n    # Check if any label is intersecting with the test regions\n    admin2_of_footprint = admin2[admin2.intersects(footprint.geometry)]\n\n    if admin2_of_footprint.empty:\n        raise ValueError(\"No intersection found between labels and admin2 regions\")\n\n    region_name = admin2_of_footprint.iloc[0][\"shapeName\"]\n\n    if len(admin2_of_footprint) &gt; 1:\n        logger.warning(\n            f\"Found multiple regions for footprint {footprint.image_id}: {admin2_of_footprint.shapeName.to_list()}.\"\n            f\" Using the first one ({region_name})\"\n        )\n    return region_name\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2._legacy_path_gen","title":"_legacy_path_gen","text":"<pre><code>_legacy_path_gen(data_dir: pathlib.Path)\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def _legacy_path_gen(data_dir: Path):\n    for iterdir in data_dir.iterdir():\n        if iterdir.stem == \"iteration001\":\n            for sitedir in (iterdir).iterdir():\n                for imgdir in (sitedir).iterdir():\n                    if not imgdir.is_dir():\n                        continue\n                    try:\n                        yield next(imgdir.glob(\"*_SR.tif\")).parent\n                    except StopIteration:\n                        yield next(imgdir.glob(\"*_SR_clip.tif\")).parent\n        else:\n            for imgdir in (iterdir).iterdir():\n                if not imgdir.is_dir():\n                    continue\n                try:\n                    yield next(imgdir.glob(\"*_SR.tif\")).parent\n                except StopIteration:\n                    yield next(imgdir.glob(\"*_SR_clip.tif\")).parent\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2.preprocess_planet_train_data","title":"preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    train_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    admin_dir: pathlib.Path | None = None,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches and storing them in a zarr array for efficient random access during training. All data is stored in a single zarr group with associated metadata.</p> <p>The preprocessing creates patches of the specified size from each Planet scene and stores them as: - A zarr group containing 'x' (input data) and 'y' (labels) arrays - A geopandas dataframe with metadata including region, position, and label statistics - A configuration file with preprocessing parameters</p> <p>The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in each patch being stored in a separate file for super fast random access.</p> <p>The metadata dataframe contains information about each patch including: - sample_id: Identifier for the source Planet scene - region: Administrative region name - geometry: Spatial extent of the patch - empty: Whether the patch contains positive labeled pixels - Additional metadata as specified</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>A <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n\u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n\u251c\u2500\u2500 metadata.parquet\n\u2514\u2500\u2500 {timestamp}.cli.toml\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The \"output\" directory where the tensors are written to. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the TCVis data. If None, will use the default TCVis directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the admin files. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. If None, will neither use nor store preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def preprocess_planet_train_data(  # noqa: C901\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    train_data_dir: Path | None = None,\n    arcticdem_dir: Path | None = None,\n    tcvis_dir: Path | None = None,\n    admin_dir: Path | None = None,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches\n    and storing them in a zarr array for efficient random access during training. All data is stored in\n    a single zarr group with associated metadata.\n\n    The preprocessing creates patches of the specified size from each Planet scene and stores them as:\n    - A zarr group containing 'x' (input data) and 'y' (labels) arrays\n    - A geopandas dataframe with metadata including region, position, and label statistics\n    - A configuration file with preprocessing parameters\n\n    The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in\n    each patch being stored in a separate file for super fast random access.\n\n    The metadata dataframe contains information about each patch including:\n    - sample_id: Identifier for the source Planet scene\n    - region: Administrative region name\n    - geometry: Spatial extent of the patch\n    - empty: Whether the patch contains positive labeled pixels\n    - Additional metadata as specified\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    A `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/\n    \u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n    \u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n    \u251c\u2500\u2500 metadata.parquet\n    \u2514\u2500\u2500 {timestamp}.cli.toml\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        train_data_dir (Path | None, optional): The \"output\" directory where the tensors are written to.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        arcticdem_dir (Path | None, optional): The directory containing the ArcticDEM data\n            (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        tcvis_dir (Path | None, optional): The directory containing the TCVis data.\n            If None, will use the default TCVis directory based on the DARTS paths.\n            Defaults to None.\n        admin_dir (Path | None, optional): The directory containing the admin files.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        preprocess_cache (Path | None, optional): The directory to store the preprocessed data.\n            If None, will neither use nor store preprocessed data.\n            Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    paths.set_defaults(default_dirs)\n    train_data_dir = train_data_dir or paths.train_data_dir(\"planet_v2_rts\", patch_size)\n    arcticdem_dir = arcticdem_dir or paths.arcticdem(2)\n    tcvis_dir = tcvis_dir or paths.tcvis()\n    admin_dir = admin_dir or paths.admin_boundaries()\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.toml\",\n        function=preprocess_planet_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import smart_geocubes\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import LoggingManager\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    accessor = smart_geocubes.ArcticDEM2m(arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    fpaths = {fpath.stem: fpath for fpath in _legacy_path_gen(data_dir)}\n    footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these since they depend on the preprocessing we use\n    bands = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"nir\",\n        \"ndvi\",\n        \"relative_elevation\",\n        \"slope\",\n        \"aspect\",\n        \"hillshade\",\n        \"curvature\",\n        \"tc_brightness\",\n        \"tc_greenness\",\n        \"tc_wetness\",\n    ]\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache)\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        info_id = f\"{planet_id=} ({i + 1} of {len(footprint)})\"\n        try:\n            logger.info(f\"Processing sample {info_id}\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(\n                    f\"Footprint image '{planet_id}' at {footprint.fpath} does not exist. Skipping {info_id}...\"\n                )\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {info_id}\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {info_id}. Skipping...\")\n            logger.exception(e)\n\n    timer.summary()\n\n    if len(builder) == 0:\n        logger.warning(\"No samples were processed. Exiting...\")\n        return\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/","title":"preprocess_planet_v2_pingo","text":""},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo","title":"darts.training.preprocess_planet_v2_pingo","text":"<p>PLANET preprocessing functions for training with the v2 data preprocessing.</p>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo._get_region_name","title":"_get_region_name","text":"<pre><code>_get_region_name(\n    footprint: geopandas.GeoSeries,\n    admin2: geopandas.GeoDataFrame,\n) -&gt; str\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def _get_region_name(footprint: \"gpd.GeoSeries\", admin2: \"gpd.GeoDataFrame\") -&gt; str:\n    # Check if any label is intersecting with the test regions\n    admin2_of_footprint = admin2[admin2.intersects(footprint.geometry)]\n\n    if admin2_of_footprint.empty:\n        raise ValueError(\"No intersection found between labels and admin2 regions\")\n\n    region_name = admin2_of_footprint.iloc[0][\"shapeName\"]\n\n    if len(admin2_of_footprint) &gt; 1:\n        logger.warning(\n            f\"Found multiple regions for footprint {footprint.image_id}: {admin2_of_footprint.shapeName.to_list()}.\"\n            f\" Using the first one ({region_name})\"\n        )\n    return region_name\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo._path_gen","title":"_path_gen","text":"<pre><code>_path_gen(data_dir: pathlib.Path)\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def _path_gen(data_dir: Path):\n    return {fpath.parent.name: fpath.parent for fpath in data_dir.rglob(\"*_SR.tif\")}\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo.preprocess_planet_train_data_pingo","title":"preprocess_planet_train_data_pingo","text":"<pre><code>preprocess_planet_train_data_pingo(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    train_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    admin_dir: pathlib.Path | None = None,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n)\n</code></pre> <p>Preprocess Planet data for training (Pingo version).</p> <p>This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches and storing them in a zarr array for efficient random access during training. All data is stored in a single zarr group with associated metadata.</p> <p>The preprocessing creates patches of the specified size from each Planet scene and stores them as: - A zarr group containing 'x' (input data) and 'y' (labels) arrays - A geopandas dataframe with metadata including region, position, and label statistics - A configuration file with preprocessing parameters</p> <p>The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in each patch being stored in a separate file for super fast random access.</p> <p>The metadata dataframe contains information about each patch including: - sample_id: Identifier for the source Planet scene - region: Administrative region name - geometry: Spatial extent of the patch - empty: Whether the patch contains positive labeled pixels - Additional metadata as specified</p> <p>Through <code>exclude_nopositive</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>A <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n\u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n\u251c\u2500\u2500 metadata.parquet\n\u2514\u2500\u2500 {timestamp}.cli.json\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The \"output\" directory where the tensors are written to. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the TCVis data. If None, will use the default TCVis directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the admin files. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. If None, will neither use nor store preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def preprocess_planet_train_data_pingo(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    train_data_dir: Path | None = None,\n    arcticdem_dir: Path | None = None,\n    tcvis_dir: Path | None = None,\n    admin_dir: Path | None = None,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n):\n    \"\"\"Preprocess Planet data for training (Pingo version).\n\n    This function preprocesses Planet scenes into a training-ready format by creating fixed-size patches\n    and storing them in a zarr array for efficient random access during training. All data is stored in\n    a single zarr group with associated metadata.\n\n    The preprocessing creates patches of the specified size from each Planet scene and stores them as:\n    - A zarr group containing 'x' (input data) and 'y' (labels) arrays\n    - A geopandas dataframe with metadata including region, position, and label statistics\n    - A configuration file with preprocessing parameters\n\n    The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in\n    each patch being stored in a separate file for super fast random access.\n\n    The metadata dataframe contains information about each patch including:\n    - sample_id: Identifier for the source Planet scene\n    - region: Administrative region name\n    - geometry: Spatial extent of the patch\n    - empty: Whether the patch contains positive labeled pixels\n    - Additional metadata as specified\n\n    Through `exclude_nopositive` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    A `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/\n    \u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n    \u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n    \u251c\u2500\u2500 metadata.parquet\n    \u2514\u2500\u2500 {timestamp}.cli.json\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        train_data_dir (Path | None, optional): The \"output\" directory where the tensors are written to.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        arcticdem_dir (Path | None, optional): The directory containing the ArcticDEM data\n            (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        tcvis_dir (Path | None, optional): The directory containing the TCVis data.\n            If None, will use the default TCVis directory based on the DARTS paths.\n            Defaults to None.\n        admin_dir (Path | None, optional): The directory containing the admin files.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        preprocess_cache (Path | None, optional): The directory to store the preprocessed data.\n            If None, will neither use nor store preprocessed data.\n            Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    paths.set_defaults(default_dirs)\n    train_data_dir = train_data_dir or paths.train_data_dir(\"planet_v2_pingo\", patch_size)\n    arcticdem_dir = arcticdem_dir or paths.arcticdem(2)\n    tcvis_dir = tcvis_dir or paths.tcvis()\n    admin_dir = admin_dir or paths.admin_boundaries()\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.toml\",\n        function=preprocess_planet_train_data_pingo,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import smart_geocubes\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import LoggingManager\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    accessor = smart_geocubes.ArcticDEM2m(arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    footprints[\"fpath\"] = footprints.image_id.map(_path_gen(data_dir))\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these since they depend on the preprocessing we use\n    bands = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"nir\",\n        \"ndvi\",\n        \"relative_elevation\",\n        \"slope\",\n        \"aspect\",\n        \"hillshade\",\n        \"curvature\",\n        \"tc_brightness\",\n        \"tc_greenness\",\n        \"tc_wetness\",\n    ]\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        device=device,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache)\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        info_id = f\"{planet_id=} ({i + 1} of {len(footprint)})\"\n        try:\n            logger.debug(f\"Processing sample {info_id}\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(\n                    f\"Footprint image '{planet_id}' at {footprint.fpath} does not exist. Skipping {info_id}...\"\n                )\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {info_id}\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {info_id} . Skipping...\")\n            logger.exception(e)\n\n    timer.summary()\n\n    if len(builder) == 0:\n        logger.warning(\"No samples were processed. Exiting...\")\n        return\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n</code></pre>"},{"location":"reference/darts/training/preprocess_sentinel2_v2/","title":"preprocess_sentinel2_v2","text":""},{"location":"reference/darts/training/preprocess_sentinel2_v2/#darts.training.preprocess_sentinel2_v2","title":"darts.training.preprocess_sentinel2_v2","text":"<p>Sentinel-2 preprocessing functions for training with the v2 data preprocessing.</p>"},{"location":"reference/darts/training/preprocess_sentinel2_v2/#darts.training.preprocess_sentinel2_v2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/training/preprocess_sentinel2_v2/#darts.training.preprocess_sentinel2_v2.__validate_dir","title":"__validate_dir","text":"<pre><code>__validate_dir(imgdir)\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_sentinel2_v2.py</code> <pre><code>def __validate_dir(imgdir):\n    if not imgdir.is_dir():\n        return None\n\n    with suppress(StopIteration):\n        return next(imgdir.glob(\"*_SR.tif\")).parent\n    with suppress(StopIteration):\n        return next(imgdir.glob(\"*_SR_clip.tif\")).parent\n\n    return None\n</code></pre>"},{"location":"reference/darts/training/preprocess_sentinel2_v2/#darts.training.preprocess_sentinel2_v2._align_offsets","title":"_align_offsets","text":"<pre><code>_align_offsets(\n    tile: xarray.Dataset,\n    footprint: geopandas.GeoSeries,\n    labels: geopandas.GeoDataFrame,\n) -&gt; tuple[geopandas.GeoDataFrame, dict[str, float]]\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_sentinel2_v2.py</code> <pre><code>def _align_offsets(\n    tile: \"xr.Dataset\", footprint: \"gpd.GeoSeries\", labels: \"gpd.GeoDataFrame\"\n) -&gt; tuple[\"gpd.GeoDataFrame\", dict[str, float]]:\n    from darts_acquisition import (\n        load_planet_masks,\n        load_planet_scene,\n    )\n    from darts_acquisition.utils.arosics import get_offsets\n\n    assert tile.odc.crs == labels.crs, \"Tile and labels must have the same CRS\"\n    # Align S2 data to Planet data if planet_data_dir is provided\n    try:\n        planetds = load_planet_scene(footprint.fpath)\n        planet_mask = load_planet_masks(footprint.fpath)\n        offsets_info = get_offsets(\n            tile,\n            planetds,\n            bands=[\"red\", \"green\", \"blue\", \"nir\"],\n            window_size=128,\n            target_mask=tile.quality_data_mask == 2,\n            reference_mask=planet_mask.quality_data_mask == 2,\n            resample_to=\"target\",\n        )\n        logger.debug(f\"Aligned S2 dataset to Planet dataset with offsets {offsets_info}.\")\n        if not offsets_info.is_valid():\n            return labels, {\"x_offset\": 0, \"y_offset\": 0}\n        x_offset = (offsets_info.x_offset or 0) * tile.odc.geobox.resolution.x\n        y_offset = (offsets_info.y_offset or 0) * tile.odc.geobox.resolution.y\n        labels[\"geometry\"] = labels.geometry.translate(xoff=-x_offset, yoff=-y_offset)\n        return labels, {\n            \"x_offset\": x_offset,\n            \"y_offset\": y_offset,\n            \"reliability\": offsets_info.avg_reliability,\n            \"ssim_improvement\": offsets_info.avg_ssim_improvement,\n        }\n\n    except Exception:\n        logger.error(\"Error while aligning S2 dataset to Planet dataset, continue without alignment\", exc_info=True)\n        return labels, {}\n</code></pre>"},{"location":"reference/darts/training/preprocess_sentinel2_v2/#darts.training.preprocess_sentinel2_v2._get_region_name","title":"_get_region_name","text":"<pre><code>_get_region_name(\n    footprint: geopandas.GeoSeries,\n    admin2: geopandas.GeoDataFrame,\n) -&gt; str\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_sentinel2_v2.py</code> <pre><code>def _get_region_name(footprint: \"gpd.GeoSeries\", admin2: \"gpd.GeoDataFrame\") -&gt; str:\n    # Check if any label is intersecting with the test regions\n    admin2_of_footprint = admin2[admin2.intersects(footprint.geometry)]\n\n    if admin2_of_footprint.empty:\n        raise ValueError(\"No intersection found between labels and admin2 regions\")\n\n    region_name = admin2_of_footprint.iloc[0][\"shapeName\"]\n\n    if len(admin2_of_footprint) &gt; 1:\n        logger.warning(\n            f\"Found multiple regions for footprint {footprint.image_id}: {admin2_of_footprint.shapeName.to_list()}.\"\n            f\" Using the first one ({region_name})\"\n        )\n    return region_name\n</code></pre>"},{"location":"reference/darts/training/preprocess_sentinel2_v2/#darts.training.preprocess_sentinel2_v2._parse_date","title":"_parse_date","text":"<pre><code>_parse_date(row)\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_sentinel2_v2.py</code> <pre><code>def _parse_date(row):\n    import pandas as pd\n\n    orthotile = row[\"datasource\"] == \"PlanetScope OrthoTile\"\n    if orthotile:\n        return pd.to_datetime(row[\"image_id\"].split(\"_\")[-2], format=\"%Y-%m-%d\", utc=True)\n    else:\n        return pd.to_datetime(row[\"image_id\"].split(\"_\")[0], format=\"%Y%m%d\", utc=True)\n</code></pre>"},{"location":"reference/darts/training/preprocess_sentinel2_v2/#darts.training.preprocess_sentinel2_v2._planet_legacy_path_gen","title":"_planet_legacy_path_gen","text":"<pre><code>_planet_legacy_path_gen(data_dir: pathlib.Path)\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_sentinel2_v2.py</code> <pre><code>def _planet_legacy_path_gen(data_dir: Path):\n    for iterdir in data_dir.iterdir():\n        if iterdir.stem == \"iteration001\":\n            for sitedir in (iterdir).iterdir():\n                for imgdir in (sitedir).iterdir():\n                    imgdir_valid = __validate_dir(imgdir)\n                    if imgdir_valid is None:\n                        continue\n                    yield imgdir_valid\n        else:\n            for imgdir in (iterdir).iterdir():\n                imgdir_valid = __validate_dir(imgdir)\n                if imgdir_valid is None:\n                    continue\n                yield imgdir_valid\n</code></pre>"},{"location":"reference/darts/training/preprocess_sentinel2_v2/#darts.training.preprocess_sentinel2_v2.preprocess_s2_train_data","title":"preprocess_s2_train_data","text":"<pre><code>preprocess_s2_train_data(\n    *,\n    labels_dir: pathlib.Path,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    train_data_dir: pathlib.Path | None = None,\n    arcticdem_dir: pathlib.Path | None = None,\n    tcvis_dir: pathlib.Path | None = None,\n    admin_dir: pathlib.Path | None = None,\n    planet_data_dir: pathlib.Path | None = None,\n    raw_data_store: pathlib.Path | None = None,\n    no_raw_data_store: bool = False,\n    preprocess_cache: pathlib.Path | None = None,\n    matching_cache: pathlib.Path | None = None,\n    no_matching_cache: bool = False,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    matching_day_range: int = 7,\n    matching_max_cloud_cover: int = 10,\n    matching_min_intersects: float = 0.7,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    save_matching_scores: bool = False,\n)\n</code></pre> <p>Preprocess Sentinel-2 data for training.</p> <p>This function preprocesses Sentinel-2 scenes matched to Planet footprints into a training-ready format by creating fixed-size patches and storing them in a zarr array for efficient random access during training. All data is stored in a single zarr group with associated metadata.</p> <p>The preprocessing matches Sentinel-2 scenes to Planet footprints based on temporal and spatial criteria, optionally aligns them spatially to Planet data, and creates patches of the specified size. The data is stored as: - A zarr group containing 'x' (input data) and 'y' (labels) arrays - A geopandas dataframe with metadata including region, position, and label statistics - A configuration file with preprocessing parameters</p> <p>The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in each patch being stored in a separate file for super fast random access.</p> <p>The metadata dataframe contains information about each patch including: - sample_id: Combined identifier for the S2 scene and Planet footprint - region: Administrative region name - geometry: Spatial extent of the patch - empty: Whether the patch contains positive labeled pixels - planet_id: Original Planet scene identifier - s2_id: Sentinel-2 scene identifier - Additional alignment and matching metadata</p> <p>Through <code>exclude_nopositive</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>A <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n\u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n\u251c\u2500\u2500 metadata.parquet\n\u251c\u2500\u2500 matching-cache.json      # Optional matching cache\n\u251c\u2500\u2500 matching-scores.parquet  # Optional matching scores\n\u2514\u2500\u2500 {timestamp}.cli.toml\n</code></pre> <p>Parameters:</p> <ul> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The \"output\" directory where the tensors are written to. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the TCVis data. If None, will use the default TCVis directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the admin files. If None, will use the default auxiliary directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>planet_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory containing the Planet scenes and orthotiles. The planet data is used to align the Sentinel-2 data to the Planet data, spatially. Can be set to None if no alignment is wished. Defaults to None.</p> </li> <li> <code>raw_data_store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to use for storing the raw Sentinel 2 data locally. If None, will use the default raw data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>no_raw_data_store</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will not store any raw data locally. This overrides the <code>raw_data_store</code> parameter. Defaults to False.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. If None, will neither use nor store preprocessed data. Defaults to None.</p> </li> <li> <code>matching_cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to a file where the matchings are stored. Note: this is different from the matching scores. If None, will query the sentinel 2 STAC and calculate the best match based on the criteria. Defaults to None.</p> </li> <li> <code>no_matching_cache</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will not use or store any matching cache. This overrides the <code>matching_cache</code> parameter. Defaults to False.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com). Defaults to True.</p> </li> <li> <code>matching_day_range</code>               (<code>int</code>, default:                   <code>7</code> )           \u2013            <p>The day range to use for matching S2 scenes to Planet footprints. Defaults to 7.</p> </li> <li> <code>matching_max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for matching S2 scenes to Planet footprints. Defaults to 10.</p> </li> <li> <code>matching_min_intersects</code>               (<code>float</code>, default:                   <code>0.7</code> )           \u2013            <p>The minimum intersection percentage to use for matching S2 scenes to Planet footprints. Defaults to 0.7.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>save_matching_scores</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to save the matching scores. Defaults to False.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_sentinel2_v2.py</code> <pre><code>def preprocess_s2_train_data(  # noqa: C901\n    *,\n    labels_dir: Path,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    train_data_dir: Path | None = None,\n    arcticdem_dir: Path | None = None,\n    tcvis_dir: Path | None = None,\n    admin_dir: Path | None = None,\n    planet_data_dir: Path | None = None,\n    raw_data_store: Path | None = None,\n    no_raw_data_store: bool = False,\n    preprocess_cache: Path | None = None,\n    matching_cache: Path | None = None,\n    no_matching_cache: bool = False,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    matching_day_range: int = 7,\n    matching_max_cloud_cover: int = 10,\n    matching_min_intersects: float = 0.7,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    save_matching_scores: bool = False,\n):\n    \"\"\"Preprocess Sentinel-2 data for training.\n\n    This function preprocesses Sentinel-2 scenes matched to Planet footprints into a training-ready format\n    by creating fixed-size patches and storing them in a zarr array for efficient random access during training.\n    All data is stored in a single zarr group with associated metadata.\n\n    The preprocessing matches Sentinel-2 scenes to Planet footprints based on temporal and spatial criteria,\n    optionally aligns them spatially to Planet data, and creates patches of the specified size. The data is stored as:\n    - A zarr group containing 'x' (input data) and 'y' (labels) arrays\n    - A geopandas dataframe with metadata including region, position, and label statistics\n    - A configuration file with preprocessing parameters\n\n    The x dataarray contains the input data with shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension with chunk size 1, resulting in\n    each patch being stored in a separate file for super fast random access.\n\n    The metadata dataframe contains information about each patch including:\n    - sample_id: Combined identifier for the S2 scene and Planet footprint\n    - region: Administrative region name\n    - geometry: Spatial extent of the patch\n    - empty: Whether the patch contains positive labeled pixels\n    - planet_id: Original Planet scene identifier\n    - s2_id: Sentinel-2 scene identifier\n    - Additional alignment and matching metadata\n\n    Through `exclude_nopositive` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    A `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing. Additionally, a timestamp-based CLI configuration file is saved for reproducibility.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/\n    \u2502   \u251c\u2500\u2500 x/          # Input patches [n_patches, n_bands, patch_size, patch_size]\n    \u2502   \u2514\u2500\u2500 y/          # Label patches [n_patches, patch_size, patch_size]\n    \u251c\u2500\u2500 metadata.parquet\n    \u251c\u2500\u2500 matching-cache.json      # Optional matching cache\n    \u251c\u2500\u2500 matching-scores.parquet  # Optional matching scores\n    \u2514\u2500\u2500 {timestamp}.cli.toml\n    ```\n\n    Args:\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        train_data_dir (Path | None, optional): The \"output\" directory where the tensors are written to.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        arcticdem_dir (Path | None, optional): The directory containing the ArcticDEM data\n            (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        tcvis_dir (Path | None, optional): The directory containing the TCVis data.\n            If None, will use the default TCVis directory based on the DARTS paths.\n            Defaults to None.\n        admin_dir (Path | None, optional): The directory containing the admin files.\n            If None, will use the default auxiliary directory based on the DARTS paths.\n            Defaults to None.\n        planet_data_dir (Path, optional): The directory containing the Planet scenes and orthotiles.\n            The planet data is used to align the Sentinel-2 data to the Planet data, spatially.\n            Can be set to None if no alignment is wished.\n            Defaults to None.\n        raw_data_store (Path | None): The directory to use for storing the raw Sentinel 2 data locally.\n            If None, will use the default raw data directory based on the DARTS paths.\n            Defaults to None.\n        no_raw_data_store (bool, optional): If True, will not store any raw data locally.\n            This overrides the `raw_data_store` parameter.\n            Defaults to False.\n        preprocess_cache (Path | None, optional): The directory to store the preprocessed data.\n            If None, will neither use nor store preprocessed data.\n            Defaults to None.\n        matching_cache (Path | None, optional): The path to a file where the matchings are stored.\n            Note: this is different from the matching scores.\n            If None, will query the sentinel 2 STAC and calculate the best match based on the criteria.\n            Defaults to None.\n        no_matching_cache (bool, optional): If True, will not use or store any matching cache.\n            This overrides the `matching_cache` parameter.\n            Defaults to False.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n            Defaults to True.\n        matching_day_range (int, optional): The day range to use for matching S2 scenes to Planet footprints.\n            Defaults to 7.\n        matching_max_cloud_cover (int, optional): The maximum cloud cover percentage to use for matching S2 scenes\n            to Planet footprints. Defaults to 10.\n        matching_min_intersects (float, optional): The minimum intersection percentage to use for matching S2 scenes\n            to Planet footprints. Defaults to 0.7.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        save_matching_scores (bool, optional): Whether to save the matching scores. Defaults to False.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    paths.set_defaults(default_dirs)\n    train_data_dir = train_data_dir or paths.train_data_dir(\"sentinel2_v2_rts\", patch_size)\n    arcticdem_dir = arcticdem_dir or paths.arcticdem(10)\n    tcvis_dir = tcvis_dir or paths.tcvis()\n    admin_dir = admin_dir or paths.admin_boundaries()\n    raw_data_store = raw_data_store or paths.sentinel2_raw_data(\"cdse\")\n    if no_raw_data_store:\n        raw_data_store = None\n    matching_cache = matching_cache or train_data_dir / \"matching-cache.json\"\n    if no_matching_cache:\n        matching_cache = None\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.toml\",\n        function=preprocess_s2_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import smart_geocubes\n    import xarray as xr\n    from botocore.exceptions import ProfileNotFound\n    from darts_acquisition import (\n        load_arcticdem,\n        load_cdse_s2_sr_scene,\n        load_tcvis,\n        match_cdse_s2_sr_scene_ids_from_geodataframe,\n    )\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.geo.geom import Geometry\n    from pystac import Item\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import LoggingManager\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    logger.info(\"Configured Rasterio\")\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    accessor = smart_geocubes.ArcticDEM10m(arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    footprints[\"geometry\"] = footprints[\"geometry\"].simplify(0.001)  # Simplify to reduce compute\n    footprints[\"date\"] = footprints.apply(_parse_date, axis=1)\n    if planet_data_dir is not None:\n        fpaths = {fpath.stem: fpath for fpath in _planet_legacy_path_gen(planet_data_dir)}\n        footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    logger.info(f\"label directory contained {len(footprints)} footprints\")\n\n    # Find S2 scenes that intersect with the Planet footprints\n    if matching_cache is None or not matching_cache.exists():\n        logger.info(\"evaluating online CDSE catalogue for matching Sentinel-2 scenes\")\n        matches = match_cdse_s2_sr_scene_ids_from_geodataframe(\n            aoi=footprints,\n            day_range=matching_day_range,\n            max_cloud_cover=matching_max_cloud_cover,\n            min_intersects=matching_min_intersects,\n            simplify_geometry=0.001,\n            save_scores=train_data_dir / \"matching-scores.parquet\" if save_matching_scores else None,\n        )\n        if matching_cache is not None:\n            matches_serializable = {k: v.to_dict() if isinstance(v, Item) else \"None\" for k, v in matches.items()}\n            with matching_cache.open(\"w\") as f:\n                json.dump(matches_serializable, f)\n            logger.info(f\"Saved matching scores to {matching_cache}\")\n            del matches_serializable  # Free memory\n    else:\n        logger.info(f\"Loading matching scores from {matching_cache}\")\n        with matching_cache.open(\"r\") as f:\n            matches_serializable = json.load(f)\n        matches = {int(k): Item.from_dict(v) if v != \"None\" else None for k, v in matches_serializable.items()}\n        del matches_serializable  # Free memory\n    footprints[\"s2_item\"] = footprints.index.map(matches)\n\n    # Filter out footprints without a matching S2 item\n    logger.info(f\"Found {len(footprints)} footprints, {footprints.s2_item.notna().sum()} with matching S2 items.\")\n    footprints = footprints[footprints.s2_item.notna()]\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these since they depend on the preprocessing we use\n    bands = [\n        \"red\",\n        \"green\",\n        \"blue\",\n        \"nir\",\n        \"ndvi\",\n        \"relative_elevation\",\n        \"slope\",\n        \"aspect\",\n        \"hillshade\",\n        \"curvature\",\n        \"tc_brightness\",\n        \"tc_greenness\",\n        \"tc_wetness\",\n    ]\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache)\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        s2_item = footprint.s2_item\n        # Convert to stac item if dictionary\n        if isinstance(s2_item, dict):\n            s2_item = Item.from_dict(s2_item)\n\n        s2_id = s2_item.id\n        planet_id = footprint.image_id\n        info_id = f\"{s2_id=} -&gt; {planet_id=} ({i + 1} of {len(footprints)})\"\n        try:\n            logger.info(f\"Processing sample {info_id}\")\n\n            if planet_data_dir is not None and (\n                not footprint.fpath or pd.isna(footprint.fpath) or (not footprint.fpath.exists())\n            ):\n                logger.warning(\n                    f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping sample {info_id}...\"\n                )\n                continue\n\n            def _get_tile():\n                s2ds = load_cdse_s2_sr_scene(s2_item, store=raw_data_store)\n\n                # Crop to footprint geometry\n                geom = Geometry(footprint.geometry, crs=footprints.crs)\n                s2ds = s2ds.odc.crop(geom, apply_mask=True)\n                # Crop above will change all dtypes to float32 -&gt; change them back for s2_scl and qa mask\n                s2ds[\"s2_scl\"] = s2ds[\"s2_scl\"].fillna(0.0).astype(\"uint8\")\n                s2ds[\"quality_data_mask\"] = s2ds[\"quality_data_mask\"].fillna(0.0).astype(\"uint8\")\n\n                # Preprocess as usual\n                arctidem_res = 10\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    s2ds.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(s2ds.odc.geobox, tcvis_dir)\n\n                s2ds: xr.Dataset = preprocess_v2(\n                    s2ds,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return s2ds\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=f\"preprocess-s2train-v2-{s2_id}_{planet_id}\",\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            # Skip if the size is too small\n            if tile.sizes[\"x\"] &lt; patch_size or tile.sizes[\"y\"] &lt; patch_size:\n                logger.info(f\"Skipping sample {info_id} due to small size {tile.sizes}.\")\n                continue\n\n            footprint_labels = labels[labels.image_id == planet_id].to_crs(tile.odc.crs)\n            region = _get_region_name(footprint, admin2)\n\n            if planet_data_dir is not None:\n                with timer(\"Align to PLANET\"):\n                    footprint_labels, offsets_info = _align_offsets(tile, footprint, footprint_labels)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=f\"{s2_id}_{planet_id}\",\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"s2_id\": s2_id,\n                        \"fpath\": footprint.fpath,\n                        **offsets_info,\n                    },\n                )\n\n            logger.info(f\"Processed sample {info_id}\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n        except ProfileNotFound:\n            logger.error(\"tried to download from CDSE@AWS but no CDSE credentials found. \")\n            return\n        except Exception as e:\n            logger.warning(f\"Could not process sample {info_id}. Skipping...\")\n            logger.exception(e)\n\n    timer.summary()\n\n    if len(builder) == 0:\n        logger.warning(\"No samples were processed. Exiting...\")\n        return\n\n    builder.finalize(\n        {\n            \"planet_data_dir\": planet_data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n</code></pre>"},{"location":"reference/darts/utils/","title":"utils","text":""},{"location":"reference/darts/utils/#darts.utils","title":"darts.utils","text":"<p>Utilities for the pipeline related parts of the DARTS library.</p>"},{"location":"reference/darts/utils/bench/","title":"bench","text":""},{"location":"reference/darts/utils/bench/#darts.utils.bench","title":"darts.utils.bench","text":"<p>Benchmark related utilities.</p>"},{"location":"reference/darts/utils/bench/#darts.utils.bench.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/bench/#darts.utils.bench.benchviz","title":"benchviz","text":"<pre><code>benchviz(\n    stopuhr_data: pathlib.Path,\n    *,\n    viz_dir: pathlib.Path | None = None,\n)\n</code></pre> <p>Visulize benchmark based on a Stopuhr data file produced by a pipeline run.</p> <p>Note</p> <p>This function changes the seaborn theme to \"whitegrid\" for better visualization.</p> <p>Parameters:</p> <ul> <li> <code>stopuhr_data</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the Stopuhr data file.</p> </li> <li> <code>viz_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the directory where the visualization will be saved. If None, the defaults to the parent directory of the Stopuhr data file. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>plt.Figure: A matplotlib figure containing the benchmark visualization.</p> </li> </ul> Source code in <code>darts/src/darts/utils/bench.py</code> <pre><code>def benchviz(\n    stopuhr_data: Path,\n    *,\n    viz_dir: Path | None = None,\n):\n    \"\"\"Visulize benchmark based on a Stopuhr data file produced by a pipeline run.\n\n    !!! note\n        This function changes the seaborn theme to \"whitegrid\" for better visualization.\n\n    Args:\n        stopuhr_data (Path): Path to the Stopuhr data file.\n        viz_dir (Path | None): Path to the directory where the visualization will be saved.\n            If None, the defaults to the parent directory of the Stopuhr data file.\n            Defaults to None.\n\n    Returns:\n        plt.Figure: A matplotlib figure containing the benchmark visualization.\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    import seaborn as sns\n\n    # Visualize the results\n    sns.set_theme(style=\"whitegrid\")\n\n    assert stopuhr_data.suffix == \".parquet\", \"Stopuhr data file must be a parquet file.\"\n\n    times = pd.read_parquet(stopuhr_data)\n    times_long = times.melt(ignore_index=False, value_name=\"time\", var_name=\"step\").reset_index(drop=False)\n    times_desc = times.describe()\n    times_sum = times.sum()\n\n    # Pretty print the results\n    for col in times_desc.columns:\n        mean = times_desc[col][\"mean\"]\n        std = times_desc[col][\"std\"]\n        total = times_sum[col]\n        n = int(times_desc[col][\"count\"].item())\n        logger.info(f\"{col} took {mean:.2f} \u00b1 {std:.2f}s ({n=} -&gt; {total=:.2f}s)\")\n\n    # axs: hist, histlog, bar, heat\n    fig, axs = plt.subplot_mosaic(\n        [\n            [\"histlog\"] * 4,\n            [\"histlog\"] * 4,\n            [\"hist\", \"hist\", \"heat\", \"heat\"],\n            [\"hist\", \"hist\", \"heat\", \"heat\"],\n            [\"bar\", \"bar\", \"bar\", \"bar\"],\n        ],\n        layout=\"constrained\",\n        figsize=(20, 15),\n    )\n\n    sns.histplot(\n        data=times_long,\n        x=\"time\",\n        hue=\"step\",\n        bins=100,\n        # log_scale=True,\n        ax=axs[\"hist\"],\n    )\n    axs[\"hist\"].set_xlabel(\"Time in seconds\")\n    axs[\"hist\"].set_title(\"Histogram of time taken for each step\", fontdict={\"fontweight\": \"bold\"})\n\n    sns.histplot(\n        data=times_long,\n        x=\"time\",\n        hue=\"step\",\n        bins=100,\n        log_scale=True,\n        kde=True,\n        ax=axs[\"histlog\"],\n    )\n    axs[\"histlog\"].set_xlabel(\"Time in seconds\")\n    axs[\"histlog\"].set_title(\"Histogram of time taken for each step (log scale)\", fontdict={\"fontweight\": \"bold\"})\n\n    sns.heatmap(\n        times.T,\n        robust=True,\n        cbar_kws={\"label\": \"Time in seconds\"},\n        ax=axs[\"heat\"],\n    )\n    axs[\"heat\"].set_xlabel(\"Sample\")\n    axs[\"heat\"].set_title(\"Heatmap of time taken for each step and sample\", fontdict={\"fontweight\": \"bold\"})\n\n    bottom = np.array([0.0])\n    for i, (step, time_taken) in enumerate(times.mean().items()):\n        axs[\"bar\"].barh([\"Time taken\"], [time_taken], label=step, color=sns.color_palette()[i], left=bottom)\n        # Add a text label to the bar\n        axs[\"bar\"].text(\n            bottom[-1] + time_taken / 2,\n            0,\n            f\"{step}:\\n{time_taken:.1f} s\",\n            va=\"center\",\n            ha=\"center\",\n            fontsize=10,\n            color=\"white\",\n        )\n        bottom += time_taken\n    axs[\"bar\"].legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.05), ncol=3)\n    # Make the y-axis labels vertical\n    axs[\"bar\"].set_yticks([0.15], labels=[\"Time taken\"], rotation=90)\n    axs[\"bar\"].set_xlabel(\"Time in seconds\")\n    axs[\"bar\"].set_title(\"Avg. time taken for each step\", fontdict={\"fontweight\": \"bold\"})\n\n    # Save the figure\n    viz_dir = viz_dir or stopuhr_data.parent\n    viz_dir.mkdir(parents=True, exist_ok=True)\n    fpath = viz_dir / stopuhr_data.name.replace(\".parquet\", \".png\")\n    fig.savefig(fpath, dpi=300, bbox_inches=\"tight\")\n    logger.info(f\"Benchmark visualization saved to {fpath.resolve()}\")\n\n    return fig\n</code></pre>"},{"location":"reference/darts/utils/config/","title":"config","text":""},{"location":"reference/darts/utils/config/#darts.utils.config","title":"darts.utils.config","text":"<p>Utility functions for parsing and handling configuration files.</p>"},{"location":"reference/darts/utils/config/#darts.utils.config.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser","title":"ConfigParser","text":"<pre><code>ConfigParser()\n</code></pre> <p>Parser for cyclopts config.</p> <p>An own implementation is needed to select our own toml structure and source. Implemented as a class to be able to provide the config-file as a parameter of the CLI.</p> <p>Initialize the ConfigParser (no-op).</p> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the ConfigParser (no-op).\"\"\"\n    self._config = None\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.__call__","title":"__call__","text":"<pre><code>__call__(\n    apps: list[cyclopts.App],\n    commands: tuple[str, ...],\n    arguments: cyclopts.ArgumentCollection,\n)\n</code></pre> <p>Parser for cyclopts config. An own implementation is needed to select our own toml structure.</p> <p>First, the configuration file at \"config.toml\" is loaded. Then, this config is flattened and then mapped to the input arguments of the called function. Hence parent keys are not considered.</p> <p>Parameters:</p> <ul> <li> <code>apps</code>               (<code>list[cyclopts.App]</code>)           \u2013            <p>The cyclopts apps. Unused, but must be provided for the cyclopts hook.</p> </li> <li> <code>commands</code>               (<code>tuple[str, ...]</code>)           \u2013            <p>The commands. Unused, but must be provided for the cyclopts hook.</p> </li> <li> <code>arguments</code>               (<code>cyclopts.ArgumentCollection</code>)           \u2013            <p>The arguments to apply the config to.</p> </li> </ul> <p>Examples:</p>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.__call__--setup-the-cyclopts-app","title":"Setup the cyclopts App","text":"<pre><code>import cyclopts\nfrom darts.utils.config import ConfigParser\n\nconfig_parser = ConfigParser()\napp = cyclopts.App(config=config_parser)\n\n# Intercept the logging behavior to add a file handler\n@app.meta.default\ndef launcher(\n    *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n    log_dir: Path = Path(\"logs\"),\n    config_file: Path = Path(\"config.toml\"),\n):\n    command, bound, _ = app.parse_args(tokens)\n    add_logging_handlers(command.__name__, console, log_dir)\n    return command(*bound.args, **bound.kwargs)\n\nif __name__ == \"__main__\":\n    app.meta()\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.__call__--usage","title":"Usage","text":"<p>Config file <code>./config.toml</code>:</p> <pre><code>[darts.hello] # The parent key is completely ignored\nname = \"Tobias\"\n</code></pre> <p>Function signature which is called:</p> <pre><code># ... setup code for cyclopts\n@app.command()\ndef hello(name: str):\n    print(f\"Hello {name}\")\n</code></pre> <p>Calling the function from CLI:</p> <pre><code>$ darts hello\nHello Tobias\n\n$ darts hello --name=Max\nHello Max\n</code></pre> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def __call__(self, apps: list[cyclopts.App], commands: tuple[str, ...], arguments: cyclopts.ArgumentCollection):\n    \"\"\"Parser for cyclopts config. An own implementation is needed to select our own toml structure.\n\n    First, the configuration file at \"config.toml\" is loaded.\n    Then, this config is flattened and then mapped to the input arguments of the called function.\n    Hence parent keys are not considered.\n\n    Args:\n        apps (list[cyclopts.App]): The cyclopts apps. Unused, but must be provided for the cyclopts hook.\n        commands (tuple[str, ...]): The commands. Unused, but must be provided for the cyclopts hook.\n        arguments (cyclopts.ArgumentCollection): The arguments to apply the config to.\n\n    Examples:\n        ### Setup the cyclopts App\n\n        ```python\n        import cyclopts\n        from darts.utils.config import ConfigParser\n\n        config_parser = ConfigParser()\n        app = cyclopts.App(config=config_parser)\n\n        # Intercept the logging behavior to add a file handler\n        @app.meta.default\n        def launcher(\n            *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n            log_dir: Path = Path(\"logs\"),\n            config_file: Path = Path(\"config.toml\"),\n        ):\n            command, bound, _ = app.parse_args(tokens)\n            add_logging_handlers(command.__name__, console, log_dir)\n            return command(*bound.args, **bound.kwargs)\n\n        if __name__ == \"__main__\":\n            app.meta()\n        ```\n\n\n        ### Usage\n\n        Config file `./config.toml`:\n\n        ```toml\n        [darts.hello] # The parent key is completely ignored\n        name = \"Tobias\"\n        ```\n\n        Function signature which is called:\n\n        ```python\n        # ... setup code for cyclopts\n        @app.command()\n        def hello(name: str):\n            print(f\"Hello {name}\")\n        ```\n\n        Calling the function from CLI:\n\n        ```sh\n        $ darts hello\n        Hello Tobias\n\n        $ darts hello --name=Max\n        Hello Max\n        ```\n\n    \"\"\"\n    if self._config is None:\n        config_arg, _, _ = arguments.match(\"--config-file\")\n        config_file = config_arg.convert_and_validate()\n        # Use default config file if not specified\n        if not config_file:\n            config_file = config_arg.field_info.default\n        # else never happens\n        self.open_config(config_file)\n\n    self.apply_config(arguments)\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.apply_config","title":"apply_config","text":"<pre><code>apply_config(arguments: cyclopts.ArgumentCollection)\n</code></pre> <p>Apply the loaded config to the cyclopts mapping.</p> <p>Parameters:</p> <ul> <li> <code>arguments</code>               (<code>cyclopts.ArgumentCollection</code>)           \u2013            <p>The arguments to apply the config to.</p> </li> </ul> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def apply_config(self, arguments: cyclopts.ArgumentCollection):\n    \"\"\"Apply the loaded config to the cyclopts mapping.\n\n    Args:\n        arguments (cyclopts.ArgumentCollection): The arguments to apply the config to.\n\n    \"\"\"\n    to_add = []\n    for k in self._config.keys():\n        value = self._config[k][\"value\"]\n\n        try:\n            argument, remaining_keys, _ = arguments.match(f\"--{k}\")\n        except ValueError:\n            # Config key not found in arguments - ignore\n            continue\n\n        # Skip if the argument is not bound to a parameter\n        if argument.tokens or argument.field_info.kind is argument.field_info.VAR_KEYWORD:\n            continue\n\n        # Skip if the argument is from the config file\n        if any(x.source != \"config-file\" for x in argument.tokens):\n            continue\n\n        # Parse value to tuple of strings\n        if not isinstance(value, list):\n            value = (value,)\n        value = tuple(str(x) for x in value)\n        # Add the new tokens to the list\n        for i, v in enumerate(value):\n            to_add.append(\n                (\n                    argument,\n                    cyclopts.Token(keyword=k, value=v, source=\"config-file\", index=i, keys=remaining_keys),\n                )\n            )\n    # Add here after all \"arguments.match\" calls, to avoid changing the list while iterating\n    for argument, token in to_add:\n        argument.append(token)\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.open_config","title":"open_config","text":"<pre><code>open_config(file_path: str | pathlib.Path) -&gt; None\n</code></pre> <p>Open the config file, takes the 'darts' key, flattens the resulting dict and saves as config.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the config file.</p> </li> </ul> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def open_config(self, file_path: str | Path) -&gt; None:\n    \"\"\"Open the config file, takes the 'darts' key, flattens the resulting dict and saves as config.\n\n    Args:\n        file_path (str | Path): The path to the config file.\n\n    \"\"\"\n    file_path = file_path if isinstance(file_path, Path) else Path(file_path)\n\n    if not file_path.exists():\n        logger.warning(f\"No config file found at {file_path.resolve()}\")\n        self._config = {}\n        return\n\n    with file_path.open(\"rb\") as f:\n        config = tomllib.load(f)[\"darts\"]\n\n    # Flatten the config data ()\n    self._config = flatten_dict(config)\n    logger.info(f\"loaded config from '{file_path.resolve()}'\")\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.flatten_dict","title":"flatten_dict","text":"<pre><code>flatten_dict(\n    d: dict, parent_key: str = \"\", sep: str = \".\"\n) -&gt; dict[str, dict[str, str]]\n</code></pre> <p>Flatten a nested dictionary.</p> <p>Parameters:</p> <ul> <li> <code>d</code>               (<code>dict</code>)           \u2013            <p>The dictionary to flatten.</p> </li> <li> <code>parent_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The parent key. Defaults to \"\".</p> </li> <li> <code>sep</code>               (<code>str</code>, default:                   <code>'.'</code> )           \u2013            <p>The separator. Defaults to \".\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, dict[str, str]]</code>           \u2013            <p>dict[str, dict[str, str]]: The flattened dictionary. Key is the original key, value is a dictionary with the value and a concatenated key to save parents.</p> </li> </ul> <p>Examples: </p><pre><code>&gt;&gt;&gt; d = {\n&gt;&gt;&gt;     \"a\": 1,\n&gt;&gt;&gt;     \"b\": {\n&gt;&gt;&gt;         \"c\": 2,\n&gt;&gt;&gt;     },\n&gt;&gt;&gt; }\n&gt;&gt;&gt; print(flatten_dict(d))\n{\n    \"a\": {\"value\": 1, \"key\": \"a\"},\n    \"c\": {\"value\": 2, \"key\": \"b.c\"},\n}\n</code></pre><p></p> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def flatten_dict(d: dict, parent_key: str = \"\", sep: str = \".\") -&gt; dict[str, dict[str, str]]:\n    \"\"\"Flatten a nested dictionary.\n\n    Args:\n        d (dict): The dictionary to flatten.\n        parent_key (str, optional): The parent key. Defaults to \"\".\n        sep (str, optional): The separator. Defaults to \".\".\n\n    Returns:\n        dict[str, dict[str, str]]: The flattened dictionary.\n            Key is the original key, value is a dictionary with the value and a concatenated key to save parents.\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; d = {\n    &gt;&gt;&gt;     \"a\": 1,\n    &gt;&gt;&gt;     \"b\": {\n    &gt;&gt;&gt;         \"c\": 2,\n    &gt;&gt;&gt;     },\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt; print(flatten_dict(d))\n    {\n        \"a\": {\"value\": 1, \"key\": \"a\"},\n        \"c\": {\"value\": 2, \"key\": \"b.c\"},\n    }\n    ```\n\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((k, {\"value\": v, \"key\": new_key}))\n    return dict(items)\n</code></pre>"},{"location":"reference/darts/utils/cuda/","title":"cuda","text":""},{"location":"reference/darts/utils/cuda/#darts.utils.cuda","title":"darts.utils.cuda","text":"<p>Utility functions for working with CUDA devices.</p>"},{"location":"reference/darts/utils/cuda/#darts.utils.cuda.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/cuda/#darts.utils.cuda.debug_info","title":"debug_info","text":"<pre><code>debug_info()\n</code></pre> <p>Print debug information about the CUDA devices and library installations.</p> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If pynvml cannot be imported properly.</p> </li> </ul> Source code in <code>darts/src/darts/utils/cuda.py</code> <pre><code>def debug_info():  # noqa: C901\n    \"\"\"Print debug information about the CUDA devices and library installations.\n\n    Raises:\n        ImportError: If pynvml cannot be imported properly.\n\n    \"\"\"\n    logger.debug(\"===vvv CUDA DEBUG INFO vvv===\")\n    important_env_vars = [\n        \"CUDA_HOME\",\n        \"CUDA_PATH\",\n        \"CUDA_VISIBLE_DEVICES\",\n        \"LD_LIBRARY_PATH\",\n        \"NUMBA_CUDA_DRIVER\",\n        \"NUMBA_CUDA_INCLUDE_PATH\",\n    ]\n    for v in important_env_vars:\n        value = os.environ.get(v, \"UNSET\")\n        logger.debug(f\"{v}: {value}\")\n\n    logger.debug(\"Quicknote: CUDA driver is something different than CUDA runtime, hence versions can mismatch\")\n    try:\n        from pynvml import (  # type: ignore\n            NVMLError,\n            nvmlDeviceGetCount,\n            nvmlDeviceGetHandleByIndex,\n            nvmlDeviceGetMemoryInfo,\n            nvmlDeviceGetName,\n            nvmlInit,\n            nvmlShutdown,\n            nvmlSystemGetCudaDriverVersion_v2,\n            nvmlSystemGetDriverVersion,\n        )\n\n        try:\n            nvmlInit()\n            cuda_driver_version_legacy = nvmlSystemGetDriverVersion().decode()\n            cuda_driver_version = nvmlSystemGetCudaDriverVersion_v2()\n            logger.debug(f\"CUDA driver version: {cuda_driver_version} ({cuda_driver_version_legacy})\")\n            ndevices = nvmlDeviceGetCount()\n            logger.debug(f\"Number of CUDA devices: {ndevices}\")\n\n            for i in range(ndevices):\n                handle = nvmlDeviceGetHandleByIndex(i)\n                device_name = nvmlDeviceGetName(handle).decode()\n                meminfo = nvmlDeviceGetMemoryInfo(handle)\n                logger.debug(f\"Device {i} ({device_name}): {meminfo.used / meminfo.total:.2%} memory usage.\")\n            nvmlShutdown()\n        except NVMLError:\n            raise ImportError\n\n    except ImportError:\n        logger.debug(\"Module 'pynvml' could not be imported. darts is probably installed without CUDA support.\")\n    except Exception as e:\n        logger.error(\n            \"Error while trying to get CUDA driver version or device info.\"\n            \" Is is possible that this device has no CUDA device?\"\n        )\n        logger.exception(e, exc_info=True)\n\n    try:\n        import torch\n\n        logger.debug(f\"PyTorch version: {torch.__version__}\")\n        logger.debug(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n        logger.debug(f\"PyTorch CUDA runtime version: {torch.version.cuda}\")\n    except ImportError as e:\n        logger.error(\"Module 'torch' could not be imported:\")\n        logger.exception(e, exc_info=True)\n\n    try:\n        import cupy  # type: ignore\n        import cupyx\n\n        logger.debug(f\"Cupy version: {cupy.__version__}\")\n        cupy_driver_version = cupy.cuda.runtime.driverGetVersion()\n        logger.debug(f\"Cupy CUDA driver version: {cupy_driver_version}\")\n        # This is the version which is installed (dynamically linked via PATH or LD_LIBRARY_PATH) in the environment\n        env_runtime_version = cupy.cuda.get_local_runtime_version()\n        logger.debug(f\"Cupy CUDA runtime version: {env_runtime_version}\")\n        if cupy_driver_version &lt; env_runtime_version:\n            logger.warning(\n                \"CUDA runtime version is newer than CUDA driver version!\"\n                \" The CUDA environment is probably not setup correctly!\"\n                \" Consider linking CUDA to an older version with CUDA_HOME and LD_LIBRARY_PATH environment variables,\"\n                \" or in case of a setup done by pixi choose a different environment with the -e flag.\"\n            )\n        # This is the version which is was used when cupy was compiled (statically linked)\n        cupy_runtime_version = cupy.cuda.runtime.runtimeGetVersion()\n        if env_runtime_version != cupy_runtime_version:\n            logger.debug(\n                \"Cupy CUDA runtime versions don't match!\\n\"\n                f\"Got {env_runtime_version} as local (dynamically linked) runtime version.\\n\"\n                f\"Got {cupy_runtime_version} as by cupy statically linked runtime version.\\n\"\n                \"This can happen if cupy was compiled using a different CUDA runtime version. \"\n                \"Things should still work, note that Cupy will use the dynamically linked version.\"\n            )\n\n        logger.debug(f\"Cupy config:\\n{cupyx.get_runtime_info(full=True)}\")\n    except ImportError:\n        logger.debug(\"Module 'cupy' not found, darts is probably installed without CUDA support.\")\n\n    try:\n        import numba.cuda\n\n        cuda_available = numba.cuda.is_available()\n        logger.debug(f\"Numba CUDA is available: {cuda_available}\")\n        if cuda_available:\n            logger.debug(f\"Numba CUDA runtime version: {numba.cuda.runtime.get_version()}\")\n            # logger.debug(f\"Numba CUDA has supported devices: {numba.cuda.detect()}\")\n    except ImportError:\n        logger.debug(\"Module 'numba.cuda' not found, darts is probably installed without CUDA support.\")\n\n    from xrspatial.utils import has_cuda_and_cupy\n\n    logger.debug(f\"Cupy+Numba CUDA available: {has_cuda_and_cupy()}\")\n\n    try:\n        import cucim  # type: ignore\n\n        logger.debug(f\"Cucim version: {cucim.__version__}\")\n    except ImportError:\n        logger.debug(\"Module 'cucim' not found, darts is probably installed without CUDA support.\")\n\n    logger.debug(\"===^^^ CUDA DEBUG INFO ^^^===\")\n</code></pre>"},{"location":"reference/darts/utils/cuda/#darts.utils.cuda.decide_device","title":"decide_device","text":"<pre><code>decide_device(\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None,\n) -&gt; typing.Literal[\"cuda\", \"cpu\"] | int\n</code></pre> <p>Decide the device based on the input.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int</code>)           \u2013            <p>The device to run the model on.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['cuda', 'cpu'] | int</code>           \u2013            <p>Literal[\"cuda\", \"cpu\"] | int: The device to run the model on.</p> </li> </ul> Source code in <code>darts/src/darts/utils/cuda.py</code> <pre><code>def decide_device(device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None) -&gt; Literal[\"cuda\", \"cpu\"] | int:\n    \"\"\"Decide the device based on the input.\n\n    Args:\n        device (Literal[\"cuda\", \"cpu\", \"auto\"] | int): The device to run the model on.\n\n    Returns:\n        Literal[\"cuda\", \"cpu\"] | int: The device to run the model on.\n\n    \"\"\"\n    import torch\n    from xrspatial.utils import has_cuda_and_cupy\n\n    # We can't provide a default value for device in the parameter list because then we would need to import torch at\n    # top-level, which would make the CLI slow.\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() and has_cuda_and_cupy() else \"cpu\"\n        logger.info(f\"Device not provided. Using {device}.\")\n        return device\n\n    # Automatically select a free GPU (&lt;50% memory usage)\n    if device == \"auto\":\n        logger.info(f\"{device=}. Trying to automatically select a free GPU. (&lt;50% memory usage)\")\n\n        # Check if torch and cupy are available\n        if not has_cuda_and_cupy() or not torch.cuda.is_available():\n            logger.info(\"CUDA not available. Using CPU.\")\n            return \"cpu\"\n\n        try:\n            from pynvml import (  # type: ignore\n                nvmlDeviceGetCount,\n                nvmlDeviceGetHandleByIndex,\n                nvmlDeviceGetMemoryInfo,\n                nvmlInit,\n                nvmlShutdown,\n            )\n        except ImportError:\n            logger.warning(\"Module 'pynvml' not found. Using CPU.\")\n            return \"cpu\"\n\n        nvmlInit()\n        ndevices = nvmlDeviceGetCount()\n\n        # If there are multiple devices, we need to check which one is free\n        for i in range(ndevices):\n            handle = nvmlDeviceGetHandleByIndex(i)\n            meminfo = nvmlDeviceGetMemoryInfo(handle)\n            perc_used = meminfo.used / meminfo.total\n            logger.debug(f\"Device {i}: {perc_used:.2%} memory usage.\")\n            # If the device is less than 50% used, we skip it\n            if perc_used &gt; 0.5:\n                continue\n            else:\n                nvmlShutdown()\n                logger.info(f\"Using free GPU {i} ({perc_used:.2%} memory usage).\")\n                return i\n        else:\n            nvmlShutdown()\n            logger.warning(\n                \"No free GPU found (&lt;50% memory usage). Using CPU. \"\n                \"If you want to use a GPU, please select a device manually with the 'device' parameter.\"\n            )\n            return \"cpu\"\n\n    # If device is int or \"cuda\" or \"cpu\", we just return it\n    logger.info(f\"Using {device=}.\")\n    return device\n</code></pre>"},{"location":"reference/darts/utils/cuda/#darts.utils.cuda.set_pixi_cuda_env","title":"set_pixi_cuda_env","text":"<pre><code>set_pixi_cuda_env(\n    version: typing.Literal[\n        \"cuda121\", \"cuda124\", \"cuda126\", \"cuda128\"\n    ],\n    prefix: str = \"../\",\n)\n</code></pre> <p>Set the CUDA environment variables.</p> <p>This is useful when working with a notebook wich does not load the Pixi environment.</p> <p>Warnign</p> <p>This does not work currently as expected!</p> <p>Parameters:</p> <ul> <li> <code>version</code>               (<code>str</code>)           \u2013            <p>The version string of the pixi CUDA envirnonent.</p> </li> <li> <code>prefix</code>               (<code>str</code>, default:                   <code>'../'</code> )           \u2013            <p>The prefix to the PIXI installation. E.g. when in the <code>./notebooks</code> directory, the prefix is <code>../</code></p> </li> </ul> Source code in <code>darts/src/darts/utils/cuda.py</code> <pre><code>def set_pixi_cuda_env(version: Literal[\"cuda121\", \"cuda124\", \"cuda126\", \"cuda128\"], prefix: str = \"../\"):\n    \"\"\"Set the CUDA environment variables.\n\n    This is useful when working with a notebook wich does not load the Pixi environment.\n\n    !!! warnign\n\n        This does not work currently as expected!\n\n    Args:\n        version (str): The version string of the pixi CUDA envirnonent.\n        prefix (str): The prefix to the PIXI installation. E.g. when in the `./notebooks` directory, the prefix is `../`\n\n    \"\"\"\n    prefix = str(Path(prefix).resolve())\n    os.environ[\"PATH\"] = f\"{prefix}/.pixi/envs/{version}/bin:{os.environ.get('PATH', '')}\"\n    os.environ[\"CUDA_HOME\"] = f\"{prefix}/.pixi/envs/{version}\"\n    os.environ[\"CUDA_PATH\"] = f\"{prefix}/.pixi/envs/{version}\"\n    os.environ[\"LD_LIBRARY_PATH\"] = f\"{prefix}/.pixi/envs/{version}/lib:{prefix}/.pixi/envs/{version}/lib/stubs\"\n    os.environ[\"NUMBA_CUDA_DRIVER\"] = f\"{prefix}/.pixi/envs/{version}/lib/stubs/libcuda.so\"\n    os.environ[\"NUMBA_CUDA_INCLUDE_PATH\"] = f\"{prefix}/.pixi/envs/{version}/include\"\n</code></pre>"},{"location":"reference/darts/utils/earthengine/","title":"earthengine","text":""},{"location":"reference/darts/utils/earthengine/#darts.utils.earthengine","title":"darts.utils.earthengine","text":"<p>Earth Engine utilities.</p>"},{"location":"reference/darts/utils/earthengine/#darts.utils.earthengine.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/earthengine/#darts.utils.earthengine.init_ee","title":"init_ee","text":"<pre><code>init_ee(\n    project: str | None = None, use_highvolume: bool = True\n) -&gt; None\n</code></pre> <p>Initialize Earth Engine. Authenticate if necessary.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The project name.</p> </li> <li> <code>use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> </ul> Source code in <code>darts/src/darts/utils/earthengine.py</code> <pre><code>def init_ee(project: str | None = None, use_highvolume: bool = True) -&gt; None:\n    \"\"\"Initialize Earth Engine. Authenticate if necessary.\n\n    Args:\n        project (str): The project name.\n        use_highvolume (bool): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n\n    \"\"\"\n    logger.debug(f\"Initializing Earth Engine with project {project} {'with high volume' if use_highvolume else ''}\")\n    opt_url = \"https://earthengine-highvolume.googleapis.com\" if use_highvolume else None\n    try:\n        ee.Initialize(project=project, opt_url=opt_url)\n        # geemap.ee_initialize(project=project, opt_url=\"https://earthengine-highvolume.googleapis.com\")\n    except Exception:\n        logger.debug(\"Initializing Earth Engine failed, trying to authenticate before\")\n        ee.Authenticate()\n        ee.Initialize(project=project, opt_url=opt_url)\n        # geemap.ee_initialize(project=project, opt_url=\"https://earthengine-highvolume.googleapis.com\")\n    logger.debug(\"Earth Engine initialized\")\n</code></pre>"},{"location":"reference/darts/utils/logging/","title":"logging","text":""},{"location":"reference/darts/utils/logging/#darts.utils.logging","title":"darts.utils.logging","text":"<p>Utility functions for logging.</p>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManager","title":"LoggingManager  <code>module-attribute</code>","text":"<pre><code>LoggingManager = (\n    darts.utils.logging.LoggingManagerSingleton()\n)\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton","title":"LoggingManagerSingleton","text":"<pre><code>LoggingManagerSingleton()\n</code></pre> <p>A singleton class to manage logging handlers for the application.</p> <p>Initialize the LoggingManager.</p> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the LoggingManager.\"\"\"\n    self._console_handler = None\n    self._file_handler = None\n    self._managed_loggers = []\n    self._verbosity: VerbosityLevel = VerbosityLevel.NORMAL\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.logger","title":"logger  <code>property</code>","text":"<pre><code>logger\n</code></pre> <p>Get the logger for the application.</p>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create a new instance of the LoggingManager if it does not exist yet.</p> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def __new__(cls):\n    \"\"\"Create a new instance of the LoggingManager if it does not exist yet.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n\n    return cls._instance\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.add_logging_handlers","title":"add_logging_handlers","text":"<pre><code>add_logging_handlers(\n    command: str,\n    log_dir: pathlib.Path,\n    verbosity: darts.utils.logging.VerbosityLevel,\n    log_plain: bool = False,\n)\n</code></pre> <p>Add logging handlers (rich-console and file) to the application.</p> <p>Parameters:</p> <ul> <li> <code>command</code>               (<code>str</code>)           \u2013            <p>The command that is run.</p> </li> <li> <code>log_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to save the logs to.</p> </li> <li> <code>verbosity</code>               (<code>darts.utils.logging.VerbosityLevel</code>)           \u2013            <p>The verbosity level.</p> </li> <li> <code>log_plain</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>uses the RichHandler as output by default, enable this to use a common print handler</p> </li> </ul> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def add_logging_handlers(\n    self,\n    command: str,\n    log_dir: Path,\n    verbosity: VerbosityLevel,\n    log_plain: bool = False,\n):\n    \"\"\"Add logging handlers (rich-console and file) to the application.\n\n    Args:\n        command (str): The command that is run.\n        log_dir (Path): The directory to save the logs to.\n        verbosity (VerbosityLevel): The verbosity level.\n        log_plain (bool, optional): uses the RichHandler as output by default,\n            enable this to use a common print handler\n\n    \"\"\"\n    self._verbosity = verbosity\n\n    if self._console_handler is not None or self._file_handler is not None:\n        logger.warning(\"Logging handlers already added.\")\n        return\n\n    log_dir.mkdir(parents=True, exist_ok=True)\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n    # Configure the rich console handler\n    if verbosity &lt;= VerbosityLevel.VERY_VERBOSE:\n        supress_module_names = [\n            \"torch\",\n            \"torch.utils.data\",\n            \"xarray\",\n            \"distributed\",\n            \"pandas\",\n            # \"lightning\",\n            \"stopuhr\",\n            \"contextlib\",\n        ]\n        traceback_suppress = [cyclopts]\n        for module_name in supress_module_names:\n            try:\n                module = importlib.import_module(module_name)\n                traceback_suppress.append(module)\n            except ImportError:\n                logger.warning(f\"Module {module_name} not found, skipping traceback suppression for it.\")\n                continue\n    else:\n        traceback_suppress = []\n\n    if not log_plain:\n        console_fmt = (\n            \"%(message)s\"\n            if not verbosity &gt;= VerbosityLevel.DEBUG\n            else \"%(processName)s(%(process)d)-%(threadName)s(%(thread)d)@%(name)s - %(message)s\"\n        )\n        console_handler = RichHandler(\n            console=rich.get_console(),\n            rich_tracebacks=True,\n            tracebacks_suppress=traceback_suppress,\n            tracebacks_show_locals=verbosity &gt;= VerbosityLevel.VERY_VERBOSE,\n        )\n    else:\n        console_fmt = \"** %(levelname)s %(asctime)s **\\n   [%(pathname)s:%(lineno)d]\\n\"\n        console_fmt += (\n            \"%(message)s\"\n            if not verbosity &gt;= VerbosityLevel.DEBUG\n            else \"   [%(name)s@%(processName)s(%(process)d)-%(threadName)s(%(thread)d)]\\n%(message)s\\n\"\n        )\n        console_handler = logging.StreamHandler(sys.stdout)\n\n    console_formatter = logging.Formatter(console_fmt, datefmt=\"[%Y-%m-%d %H:%M:%S]\")\n    console_handler.setFormatter(console_formatter)\n    self._console_handler = console_handler\n\n    # Configure the file handler (no fancy)\n    file_handler = logging.FileHandler(log_dir / f\"darts_{command}_{current_time}.log\")\n    file_fmt = \"%(processName)s(%(process)d)-%(threadName)s(%(thread)d)@%(name)s:%(levelname)s - %(message)s (in %(filename)s:%(lineno)d)\"  # noqa: E501\n    file_handler.setFormatter(\n        logging.Formatter(\n            file_fmt,\n            datefmt=\"[%Y-%m-%d %H:%M:%S]\",\n        )\n    )\n    self._file_handler = file_handler\n\n    darts_logger = logging.getLogger(\"darts\")\n    darts_logger.addHandler(console_handler)\n    darts_logger.addHandler(file_handler)\n    darts_logger.setLevel(logging.DEBUG if verbosity &gt;= VerbosityLevel.VERBOSE else logging.INFO)\n\n    if verbosity &gt;= VerbosityLevel.VERY_VERBOSE:\n        very_verbose_modules = [\n            \"smart_geocubes\",\n            \"dask\",\n            \"lightning\",\n            \"pytorch_lightning\",\n            \"torch\",\n            \"torch.utils.data\",\n            \"xarray\",\n            \"distributed\",\n            \"pandas\",\n        ]\n        module_level = logging.DEBUG if verbosity &gt;= VerbosityLevel.DEBUG else logging.INFO\n        self.apply_logging_handlers(*very_verbose_modules, level=module_level)\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.apply_logging_handlers","title":"apply_logging_handlers","text":"<pre><code>apply_logging_handlers(\n    *names: str, level: int = logging.INFO\n)\n</code></pre> <p>Apply the logging handlers to a (third-party) logger.</p> <p>Parameters:</p> <ul> <li> <code>names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>The names of the loggers to apply the handlers to.</p> </li> <li> <code>level</code>               (<code>int</code>, default:                   <code>logging.INFO</code> )           \u2013            <p>The log level to set for the loggers. Defaults to logging.INFO.</p> </li> </ul> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def apply_logging_handlers(self, *names: str, level: int = logging.INFO):\n    \"\"\"Apply the logging handlers to a (third-party) logger.\n\n    Args:\n        names (str): The names of the loggers to apply the handlers to.\n        level (int, optional): The log level to set for the loggers.\n            Defaults to logging.INFO.\n\n    \"\"\"\n    for name in names:\n        third_party_logger = logging.getLogger(name)\n        if name in self._managed_loggers:\n            # Set level for existing managed logger (will overwrite pot. verbosity settings)\n            third_party_logger.setLevel(level)\n            continue\n        # Check if logger has a StreamHandler already and remove it if so\n        for handler in third_party_logger.handlers:\n            if isinstance(handler, logging.StreamHandler):\n                third_party_logger.removeHandler(handler)\n        third_party_logger.addHandler(self._console_handler)\n        third_party_logger.addHandler(self._file_handler)\n        # Set level for all handlers\n        third_party_logger.setLevel(level)\n\n        self._managed_loggers.append(name)\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging()\n</code></pre> <p>Set up logging for the application.</p> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def setup_logging(self):\n    \"\"\"Set up logging for the application.\"\"\"\n    # Set up logging for our own modules\n    logging.getLogger(\"darts\").setLevel(logging.INFO)\n    logging.captureWarnings(True)\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.VerbosityLevel","title":"VerbosityLevel","text":"<p>               Bases: <code>enum.IntEnum</code></p> <p>Enum for verbosity levels.</p>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.VerbosityLevel.DEBUG","title":"DEBUG  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEBUG = 3\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.VerbosityLevel.NORMAL","title":"NORMAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>NORMAL = 0\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.VerbosityLevel.VERBOSE","title":"VERBOSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VERBOSE = 1\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.VerbosityLevel.VERY_VERBOSE","title":"VERY_VERBOSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>VERY_VERBOSE = 2\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.VerbosityLevel.from_cli","title":"from_cli  <code>classmethod</code>","text":"<pre><code>from_cli(\n    verbose: bool, very_verbose: bool, debug: bool\n) -&gt; darts.utils.logging.VerbosityLevel\n</code></pre> <p>Get the verbosity level from CLI flags.</p> <p>Parameters:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>)           \u2013            <p>Whether the verbose flag is set.</p> </li> <li> <code>very_verbose</code>               (<code>bool</code>)           \u2013            <p>Whether the very verbose flag is set.</p> </li> <li> <code>debug</code>               (<code>bool</code>)           \u2013            <p>Whether the debug flag is set.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>VerbosityLevel</code> (              <code>darts.utils.logging.VerbosityLevel</code> )          \u2013            <p>The corresponding verbosity level.</p> </li> </ul> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>@classmethod\ndef from_cli(cls, verbose: bool, very_verbose: bool, debug: bool) -&gt; \"VerbosityLevel\":\n    \"\"\"Get the verbosity level from CLI flags.\n\n    Args:\n        verbose (bool): Whether the verbose flag is set.\n        very_verbose (bool): Whether the very verbose flag is set.\n        debug (bool): Whether the debug flag is set.\n\n    Returns:\n        VerbosityLevel: The corresponding verbosity level.\n\n    \"\"\"\n    if debug:\n        return cls.DEBUG\n    if very_verbose:\n        return cls.VERY_VERBOSE\n    if verbose:\n        return cls.VERBOSE\n    return cls.NORMAL\n</code></pre>"},{"location":"reference/darts_acquisition/","title":"darts_acquisition","text":""},{"location":"reference/darts_acquisition/#darts_acquisition","title":"darts_acquisition","text":"<p>Acquisition of data from various sources for the DARTS dataset.</p>"},{"location":"reference/darts_acquisition/#darts_acquisition.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.download_admin_files","title":"download_admin_files","text":"<pre><code>download_admin_files(admin_dir: pathlib.Path)\n</code></pre> <p>Download the admin files for the regions.</p> <p>Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.</p> <p>Parameters:</p> <ul> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the admin files.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>@stopwatch.f(\"Downloading admin files\", printer=logger.debug)\ndef download_admin_files(admin_dir: Path):\n    \"\"\"Download the admin files for the regions.\n\n    Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.\n\n    Args:\n        admin_dir (Path): The path to the admin files.\n\n    \"\"\"\n    # Download the admin files\n    admin_1_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.zip\"\n    admin_2_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.zip\"\n\n    admin_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.debug(f\"Downloading {admin_1_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_1_url, admin_dir)\n\n    logger.debug(f\"Downloading {admin_2_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_2_url, admin_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.download_arcticdem","title":"download_arcticdem","text":"<pre><code>download_arcticdem(\n    aoi: geopandas.GeoDataFrame,\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n) -&gt; None\n</code></pre> <p>Download ArcticDEM data for the specified area of interest.</p> <p>This function downloads ArcticDEM elevation tiles from AWS S3 for the given area of interest and stores them in a local icechunk data store for efficient access.</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>Area of interest for which to download ArcticDEM data. Can be in any CRS; will be reprojected to EPSG:3413 (ArcticDEM's native CRS).</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>Path to the icechunk data directory (must have .icechunk suffix). Must contain the resolution in the name (e.g., \"arcticdem_2m.icechunk\").</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in meters. Must match the resolution indicated in the data_dir name.</p> </li> </ul> Note <p>This function automatically configures AWS access with unsigned requests to the public ArcticDEM S3 bucket. No AWS credentials are required.</p> Example <p>Download ArcticDEM for a study area:</p> <pre><code>import geopandas as gpd\nfrom shapely.geometry import box\nfrom darts_acquisition import download_arcticdem\n\n# Define area of interest\naoi = gpd.GeoDataFrame(\n    geometry=[box(-50, 70, -49, 71)],\n    crs=\"EPSG:4326\"\n)\n\n# Download 2m resolution ArcticDEM\ndownload_arcticdem(\n    aoi=aoi,\n    data_dir=\"/data/arcticdem_2m.icechunk\",\n    resolution=2\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopwatch.f(\"Downloading ArcticDEM\", printer=logger.debug, print_kwargs=[\"data_dir\", \"resolution\"])\ndef download_arcticdem(\n    aoi: gpd.GeoDataFrame,\n    data_dir: Path | str,\n    resolution: RESOLUTIONS,\n) -&gt; None:\n    \"\"\"Download ArcticDEM data for the specified area of interest.\n\n    This function downloads ArcticDEM elevation tiles from AWS S3 for the given area\n    of interest and stores them in a local icechunk data store for efficient access.\n\n    Args:\n        aoi (gpd.GeoDataFrame): Area of interest for which to download ArcticDEM data.\n            Can be in any CRS; will be reprojected to EPSG:3413 (ArcticDEM's native CRS).\n        data_dir (Path | str): Path to the icechunk data directory (must have .icechunk suffix).\n            Must contain the resolution in the name (e.g., \"arcticdem_2m.icechunk\").\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in meters.\n            Must match the resolution indicated in the data_dir name.\n\n    Note:\n        This function automatically configures AWS access with unsigned requests to the\n        public ArcticDEM S3 bucket. No AWS credentials are required.\n\n    Example:\n        Download ArcticDEM for a study area:\n\n        ```python\n        import geopandas as gpd\n        from shapely.geometry import box\n        from darts_acquisition import download_arcticdem\n\n        # Define area of interest\n        aoi = gpd.GeoDataFrame(\n            geometry=[box(-50, 70, -49, 71)],\n            crs=\"EPSG:4326\"\n        )\n\n        # Download 2m resolution ArcticDEM\n        download_arcticdem(\n            aoi=aoi,\n            data_dir=\"/data/arcticdem_2m.icechunk\",\n            resolution=2\n        )\n        ```\n\n    \"\"\"\n    odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    accessor = _validate_and_get_accessor(data_dir, resolution)\n    accessor.download(aoi)\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.download_cdse_s2_sr_scene","title":"download_cdse_s2_sr_scene","text":"<pre><code>download_cdse_s2_sr_scene(\n    s2item: str | pystac.Item,\n    store: pathlib.Path,\n    bands_mapping: dict | typing.Literal[\"all\"] = {\n        \"B02_10m\": \"blue\",\n        \"B03_10m\": \"green\",\n        \"B04_10m\": \"red\",\n        \"B08_10m\": \"nir\",\n    },\n    aws_profile_name: str = \"default\",\n)\n</code></pre> <p>Download a Sentinel-2 scene from CDSE via STAC API and store it in the local data store.</p> <p>This function downloads Sentinel-2 Level-2A surface reflectance data from the Copernicus Data Space Ecosystem (CDSE) and stores it locally in a compressed zarr store for efficient repeated access.</p> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | pystac.Item</code>)           \u2013            <p>Sentinel-2 scene identifier (e.g., \"S2A_MSIL2A_20230615T...\") or a PySTAC Item object from a STAC search.</p> </li> <li> <code>store</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the local zarr store directory where the scene will be saved.</p> </li> <li> <code>bands_mapping</code>               (<code>dict | typing.Literal['all']</code>, default:                   <code>{'B02_10m': 'blue', 'B03_10m': 'green', 'B04_10m': 'red', 'B08_10m': 'nir'}</code> )           \u2013            <p>Mapping of Sentinel-2 band names to custom band names. Keys should be CDSE band names (e.g., \"B02_10m\", \"B03_10m\"), values are the desired output names. Use \"all\" to load all optical bands and SCL. Defaults to {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"}.</p> </li> <li> <code>aws_profile_name</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>AWS profile name for authentication with the Copernicus S3 bucket. Defaults to \"default\".</p> </li> </ul> Note <ul> <li>Requires Copernicus Data Space authentication. Use <code>darts_utils.copernicus.init_copernicus()</code>   to set up credentials before calling this function.</li> <li>All bands are resampled to 10m resolution during download.</li> <li>Data is stored with zstd compression for efficient storage.</li> <li>The SCL (Scene Classification Layer) band is automatically included if not specified.</li> </ul> Example <p>Download Sentinel-2 scenes for a project:</p> <pre><code>from pathlib import Path\nfrom darts_acquisition import download_cdse_s2_sr_scene\nfrom darts_utils.copernicus import init_copernicus\n\n# Setup authentication\ninit_copernicus(profile_name=\"default\")\n\n# Download scene with all bands\ndownload_cdse_s2_sr_scene(\n    s2item=\"S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678\",\n    store=Path(\"/data/s2_store\"),\n    bands_mapping=\"all\",\n    aws_profile_name=\"default\"\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch.f(\"Downloading Sentinel-2 scene from CDSE if missing\", printer=logger.debug, print_kwargs=[\"s2item\"])\ndef download_cdse_s2_sr_scene(\n    s2item: str | Item,\n    store: Path,\n    bands_mapping: dict | Literal[\"all\"] = {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"},\n    aws_profile_name: str = \"default\",\n):\n    \"\"\"Download a Sentinel-2 scene from CDSE via STAC API and store it in the local data store.\n\n    This function downloads Sentinel-2 Level-2A surface reflectance data from the Copernicus\n    Data Space Ecosystem (CDSE) and stores it locally in a compressed zarr store for efficient\n    repeated access.\n\n    Args:\n        s2item (str | Item): Sentinel-2 scene identifier (e.g., \"S2A_MSIL2A_20230615T...\") or\n            a PySTAC Item object from a STAC search.\n        store (Path): Path to the local zarr store directory where the scene will be saved.\n        bands_mapping (dict | Literal[\"all\"], optional): Mapping of Sentinel-2 band names to\n            custom band names. Keys should be CDSE band names (e.g., \"B02_10m\", \"B03_10m\"),\n            values are the desired output names. Use \"all\" to load all optical bands and SCL.\n            Defaults to {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"}.\n        aws_profile_name (str, optional): AWS profile name for authentication with the\n            Copernicus S3 bucket. Defaults to \"default\".\n\n    Note:\n        - Requires Copernicus Data Space authentication. Use `darts_utils.copernicus.init_copernicus()`\n          to set up credentials before calling this function.\n        - All bands are resampled to 10m resolution during download.\n        - Data is stored with zstd compression for efficient storage.\n        - The SCL (Scene Classification Layer) band is automatically included if not specified.\n\n    Example:\n        Download Sentinel-2 scenes for a project:\n\n        ```python\n        from pathlib import Path\n        from darts_acquisition import download_cdse_s2_sr_scene\n        from darts_utils.copernicus import init_copernicus\n\n        # Setup authentication\n        init_copernicus(profile_name=\"default\")\n\n        # Download scene with all bands\n        download_cdse_s2_sr_scene(\n            s2item=\"S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678\",\n            store=Path(\"/data/s2_store\"),\n            bands_mapping=\"all\",\n            aws_profile_name=\"default\"\n        )\n        ```\n\n    \"\"\"\n    bands_mapping = _get_band_mapping(bands_mapping)\n    store_manager = CDSEStoreManager(\n        store=store,\n        bands_mapping=bands_mapping,\n        aws_profile_name=aws_profile_name,\n    )\n\n    store_manager.download_and_store(s2item)\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.download_gee_s2_sr_scene","title":"download_gee_s2_sr_scene","text":"<pre><code>download_gee_s2_sr_scene(\n    s2item: str | ee.Image,\n    store: pathlib.Path,\n    bands_mapping: dict | typing.Literal[\"all\"] = {\n        \"B2\": \"blue\",\n        \"B3\": \"green\",\n        \"B4\": \"red\",\n        \"B8\": \"nir\",\n    },\n)\n</code></pre> <p>Download a Sentinel-2 scene from Google Earth Engine and store it in the local data store.</p> <p>This function downloads Sentinel-2 Level-2A surface reflectance data from Google Earth Engine (GEE) and stores it locally in a compressed zarr store for efficient repeated access.</p> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | ee.Image</code>)           \u2013            <p>Sentinel-2 scene identifier (e.g., \"20230615T123456_20230615T123659_T33UUP\") or an ee.Image object from the COPERNICUS/S2_SR collection.</p> </li> <li> <code>store</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the local zarr store directory where the scene will be saved.</p> </li> <li> <code>bands_mapping</code>               (<code>dict | typing.Literal['all']</code>, default:                   <code>{'B2': 'blue', 'B3': 'green', 'B4': 'red', 'B8': 'nir'}</code> )           \u2013            <p>Mapping of Sentinel-2 band names to custom band names. Keys should be GEE band names (e.g., \"B2\", \"B3\"), values are the desired output names. Use \"all\" to load all optical bands and SCL. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> </ul> Note <ul> <li>Requires Google Earth Engine authentication. Use <code>ee.Initialize()</code> before calling.</li> <li>All bands are downloaded at 10m resolution.</li> <li>Data is stored with zstd compression for efficient storage.</li> <li>The SCL (Scene Classification Layer) band is automatically included if not specified.</li> </ul> Example <p>Download Sentinel-2 scenes from GEE:</p> <pre><code>import ee\nfrom pathlib import Path\nfrom darts_acquisition import download_gee_s2_sr_scene\n\n# Initialize Earth Engine\nee.Initialize()\n\n# Download scene with all bands\ndownload_gee_s2_sr_scene(\n    s2item=\"20230615T123456_20230615T123659_T33UUP\",\n    store=Path(\"/data/s2_store\"),\n    bands_mapping=\"all\"\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>@stopwatch.f(\"Downloading Sentinel-2 scene from GEE if missing\", printer=logger.debug, print_kwargs=[\"s2item\"])\ndef download_gee_s2_sr_scene(\n    s2item: str | ee.Image,\n    store: Path,\n    bands_mapping: dict | Literal[\"all\"] = {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"},\n):\n    \"\"\"Download a Sentinel-2 scene from Google Earth Engine and store it in the local data store.\n\n    This function downloads Sentinel-2 Level-2A surface reflectance data from Google Earth\n    Engine (GEE) and stores it locally in a compressed zarr store for efficient repeated access.\n\n    Args:\n        s2item (str | ee.Image): Sentinel-2 scene identifier (e.g., \"20230615T123456_20230615T123659_T33UUP\")\n            or an ee.Image object from the COPERNICUS/S2_SR collection.\n        store (Path): Path to the local zarr store directory where the scene will be saved.\n        bands_mapping (dict | Literal[\"all\"], optional): Mapping of Sentinel-2 band names to\n            custom band names. Keys should be GEE band names (e.g., \"B2\", \"B3\"), values are\n            the desired output names. Use \"all\" to load all optical bands and SCL.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n\n    Note:\n        - Requires Google Earth Engine authentication. Use `ee.Initialize()` before calling.\n        - All bands are downloaded at 10m resolution.\n        - Data is stored with zstd compression for efficient storage.\n        - The SCL (Scene Classification Layer) band is automatically included if not specified.\n\n    Example:\n        Download Sentinel-2 scenes from GEE:\n\n        ```python\n        import ee\n        from pathlib import Path\n        from darts_acquisition import download_gee_s2_sr_scene\n\n        # Initialize Earth Engine\n        ee.Initialize()\n\n        # Download scene with all bands\n        download_gee_s2_sr_scene(\n            s2item=\"20230615T123456_20230615T123659_T33UUP\",\n            store=Path(\"/data/s2_store\"),\n            bands_mapping=\"all\"\n        )\n        ```\n\n    \"\"\"\n    bands_mapping = _get_band_mapping(bands_mapping)\n    store_manager = GEEStoreManager(\n        store=store,\n        bands_mapping=bands_mapping,\n    )\n\n    store_manager.download_and_store(s2item)\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.download_sentinel_2_grid","title":"download_sentinel_2_grid","text":"<pre><code>download_sentinel_2_grid(grid_dir: pathlib.Path)\n</code></pre> <p>Download the Sentinel 2 grid files.</p> <p>Files will be stored under [grid_dir]/adm1.shp and [grid_dir]/...</p> <p>Parameters:</p> <ul> <li> <code>grid_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the grid.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/grid.py</code> <pre><code>@stopwatch.f(\"Downloading Sentinel 2 grid\", printer=logger.debug)\ndef download_sentinel_2_grid(grid_dir: Path):\n    \"\"\"Download the Sentinel 2 grid files.\n\n    Files will be stored under [grid_dir]/adm1.shp and [grid_dir]/...\n\n    Args:\n        grid_dir (Path): The path to the grid.\n\n    \"\"\"\n    grid_dir.mkdir(exist_ok=True, parents=True)\n    grid_url = \"https://github.com/justinelliotmeyers/Sentinel-2-Shapefile-Index/archive/refs/heads/master.zip\"\n    logger.debug(f\"Downloading {grid_url} to {grid_dir.resolve()}\")\n    _download_zip(grid_url, grid_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.download_tcvis","title":"download_tcvis","text":"<pre><code>download_tcvis(\n    aoi: geopandas.GeoDataFrame,\n    data_dir: pathlib.Path | str,\n) -&gt; None\n</code></pre> <p>Download TCVIS (Tasseled Cap trends) data for the specified area of interest.</p> <p>This function downloads Tasseled Cap trend data from Google Earth Engine for the given area of interest and stores it in a local icechunk data store for efficient access.</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>Area of interest for which to download TCVIS data. Can be in any CRS; will be reprojected to the TCVIS dataset's native CRS.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>Path to the icechunk data directory (must have .icechunk suffix).</p> </li> </ul> Note <p>Requires Google Earth Engine authentication to be set up before calling this function. Use <code>ee.Initialize()</code> or <code>ee.Authenticate()</code> as needed.</p> Example <p>Download TCVIS for a study area:</p> <pre><code>import geopandas as gpd\nfrom shapely.geometry import box\nfrom darts_acquisition import download_tcvis\n\n# Define area of interest\naoi = gpd.GeoDataFrame(\n    geometry=[box(-50, 70, -49, 71)],\n    crs=\"EPSG:4326\"\n)\n\n# Download TCVIS\ndownload_tcvis(\n    aoi=aoi,\n    data_dir=\"/data/tcvis.icechunk\"\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopwatch.f(\"Downloading TCVIS\", printer=logger.debug, print_kwargs=[\"data_dir\"])\ndef download_tcvis(\n    aoi: gpd.GeoDataFrame,\n    data_dir: Path | str,\n) -&gt; None:\n    \"\"\"Download TCVIS (Tasseled Cap trends) data for the specified area of interest.\n\n    This function downloads Tasseled Cap trend data from Google Earth Engine for the given\n    area of interest and stores it in a local icechunk data store for efficient access.\n\n    Args:\n        aoi (gpd.GeoDataFrame): Area of interest for which to download TCVIS data.\n            Can be in any CRS; will be reprojected to the TCVIS dataset's native CRS.\n        data_dir (Path | str): Path to the icechunk data directory (must have .icechunk suffix).\n\n    Note:\n        Requires Google Earth Engine authentication to be set up before calling this function.\n        Use `ee.Initialize()` or `ee.Authenticate()` as needed.\n\n    Example:\n        Download TCVIS for a study area:\n\n        ```python\n        import geopandas as gpd\n        from shapely.geometry import box\n        from darts_acquisition import download_tcvis\n\n        # Define area of interest\n        aoi = gpd.GeoDataFrame(\n            geometry=[box(-50, 70, -49, 71)],\n            crs=\"EPSG:4326\"\n        )\n\n        # Download TCVIS\n        download_tcvis(\n            aoi=aoi,\n            data_dir=\"/data/tcvis.icechunk\"\n        )\n        ```\n\n    \"\"\"\n    assert \".icechunk\" == data_dir.suffix, f\"Data directory {data_dir} must have an .icechunk suffix!\"\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n    accessor.assert_created()\n    accessor.download(aoi)\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.get_aoi_from_cdse_scene_ids","title":"get_aoi_from_cdse_scene_ids","text":"<pre><code>get_aoi_from_cdse_scene_ids(\n    scene_ids: list[str],\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Get the area of interest (AOI) as a GeoDataFrame from a list of Sentinel-2 scene IDs.</p> <p>Parameters:</p> <ul> <li> <code>scene_ids</code>               (<code>list[str]</code>)           \u2013            <p>List of Sentinel-2 scene IDs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>geopandas.GeoDataFrame</code>           \u2013            <p>gpd.GeoDataFrame: The AOI as a GeoDataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no Sentinel-2 items are found for the given scene IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Getting AOI from CDSE scene IDs\", printer=logger.debug)\ndef get_aoi_from_cdse_scene_ids(\n    scene_ids: list[str],\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get the area of interest (AOI) as a GeoDataFrame from a list of Sentinel-2 scene IDs.\n\n    Args:\n        scene_ids (list[str]): List of Sentinel-2 scene IDs.\n\n    Returns:\n        gpd.GeoDataFrame: The AOI as a GeoDataFrame.\n\n    Raises:\n        ValueError: If no Sentinel-2 items are found for the given scene IDs.\n\n    \"\"\"\n    catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n    search = catalog.search(\n        collections=[\"sentinel-2-l2a\"],\n        ids=scene_ids,\n    )\n    items = list(search.items())\n    if not items:\n        raise ValueError(\"No Sentinel-2 items found for the given scene IDs.\")\n    gdf = gpd.GeoDataFrame.from_features(\n        [item.to_dict() for item in items],\n        crs=\"EPSG:4326\",\n    )\n    return gdf\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.get_aoi_from_gee_scene_ids","title":"get_aoi_from_gee_scene_ids","text":"<pre><code>get_aoi_from_gee_scene_ids(\n    scene_ids: list[str],\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Get the area of interest (AOI) as a GeoDataFrame from a list of Sentinel-2 scene IDs.</p> <p>Parameters:</p> <ul> <li> <code>scene_ids</code>               (<code>list[str]</code>)           \u2013            <p>List of Sentinel-2 scene IDs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>geopandas.GeoDataFrame</code>           \u2013            <p>gpd.GeoDataFrame: The AOI as a GeoDataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no Sentinel-2 items are found for the given scene IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>def get_aoi_from_gee_scene_ids(\n    scene_ids: list[str],\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get the area of interest (AOI) as a GeoDataFrame from a list of Sentinel-2 scene IDs.\n\n    Args:\n        scene_ids (list[str]): List of Sentinel-2 scene IDs.\n\n    Returns:\n        gpd.GeoDataFrame: The AOI as a GeoDataFrame.\n\n    Raises:\n        ValueError: If no Sentinel-2 items are found for the given scene IDs.\n\n    \"\"\"\n    geoms = []\n    for s2id in scene_ids:\n        s2item = ee.Image(f\"COPERNICUS/S2_SR/{s2id}\")\n        geom = s2item.geometry().getInfo()\n        geoms.append(geom)\n\n    if not geoms:\n        raise ValueError(\"No Sentinel-2 items found for the given scene IDs.\")\n\n    features = [{\"type\": \"Feature\", \"geometry\": geom, \"properties\": {}} for geom in geoms]\n    feature_collection = {\"type\": \"FeatureCollection\", \"features\": features}\n    gdf = gpd.GeoDataFrame.from_features(feature_collection, crs=\"EPSG:4326\")\n    return gdf\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.get_cdse_s2_sr_scene_ids_from_geodataframe","title":"get_cdse_s2_sr_scene_ids_from_geodataframe","text":"<pre><code>get_cdse_s2_sr_scene_ids_from_geodataframe(\n    aoi: geopandas.GeoDataFrame | pathlib.Path | str,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n    simplify_geometry: float\n    | typing.Literal[False] = False,\n) -&gt; dict[str, pystac.Item]\n</code></pre> <p>Search for Sentinel-2 scenes via STAC based on an area of interest (aoi).</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame | pathlib.Path | str</code>)           \u2013            <p>AOI as a GeoDataFrame or path to a shapefile. If a path is provided, it will be read using geopandas.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> <li> <code>months</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of months (1-12) to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> <li> <code>years</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of years to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> <li> <code>simplify_geometry</code>               (<code>float | typing.Literal[False]</code>, default:                   <code>False</code> )           \u2013            <p>If a float is provided, the geometry will be simplified using the <code>simplify</code> method of geopandas. If False, no simplification will be done. This may become useful for large / weird AOIs which are too large for the STAC API. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, pystac.Item]</code>           \u2013            <p>dict[str, Item]: A dictionary of found Sentinel-2 items.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Searching for Sentinel-2 scenes in CDSE from AOI\", printer=logger.debug)\ndef get_cdse_s2_sr_scene_ids_from_geodataframe(\n    aoi: gpd.GeoDataFrame | Path | str,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n    simplify_geometry: float | Literal[False] = False,\n) -&gt; dict[str, Item]:\n    \"\"\"Search for Sentinel-2 scenes via STAC based on an area of interest (aoi).\n\n    Args:\n        aoi (gpd.GeoDataFrame | Path | str): AOI as a GeoDataFrame or path to a shapefile.\n            If a path is provided, it will be read using geopandas.\n        start_date (str): Starting date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n        months (list[int] | None, optional): List of months (1-12) to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n        years (list[int] | None, optional): List of years to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n        simplify_geometry (float | Literal[False], optional): If a float is provided, the geometry will be simplified\n            using the `simplify` method of geopandas. If False, no simplification will be done.\n            This may become useful for large / weird AOIs which are too large for the STAC API.\n            Defaults to False.\n\n    Returns:\n        dict[str, Item]: A dictionary of found Sentinel-2 items.\n\n    \"\"\"\n    if isinstance(aoi, Path | str):\n        aoi = gpd.read_file(aoi)\n    s2items: dict[str, Item] = {}\n    if simplify_geometry:\n        aoi = aoi.copy()\n        aoi[\"geometry\"] = aoi.geometry.simplify(simplify_geometry)\n    for i, row in aoi.iterrows():\n        s2items.update(\n            search_cdse_s2_sr(\n                intersects=row.geometry,\n                start_date=start_date,\n                end_date=end_date,\n                max_cloud_cover=max_cloud_cover,\n                max_snow_cover=max_snow_cover,\n                months=months,\n                years=years,\n            )\n        )\n    return s2items\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.get_cdse_s2_sr_scene_ids_from_tile_ids","title":"get_cdse_s2_sr_scene_ids_from_tile_ids","text":"<pre><code>get_cdse_s2_sr_scene_ids_from_tile_ids(\n    tile_ids: list[str],\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n) -&gt; dict[str, pystac.Item]\n</code></pre> <p>Search for Sentinel-2 scenes via STAC based on a list of tile IDs.</p> <p>Parameters:</p> <ul> <li> <code>tile_ids</code>               (<code>list[str]</code>)           \u2013            <p>List of MGRS tile IDs to search for.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> <li> <code>months</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of months (1-12) to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> <li> <code>years</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of years to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, pystac.Item]</code>           \u2013            <p>dict[str, Item]: A dictionary of found Sentinel-2 items.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Searching for Sentinel-2 scenes in CDSE from Tile-IDs\", printer=logger.debug)\ndef get_cdse_s2_sr_scene_ids_from_tile_ids(\n    tile_ids: list[str],\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n) -&gt; dict[str, Item]:\n    \"\"\"Search for Sentinel-2 scenes via STAC based on a list of tile IDs.\n\n    Args:\n        tile_ids (list[str]): List of MGRS tile IDs to search for.\n        start_date (str): Starting date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n        months (list[int] | None, optional): List of months (1-12) to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n        years (list[int] | None, optional): List of years to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n\n    Returns:\n        dict[str, Item]: A dictionary of found Sentinel-2 items.\n\n    \"\"\"\n    return search_cdse_s2_sr(\n        tiles=tile_ids,\n        start_date=start_date,\n        end_date=end_date,\n        max_cloud_cover=max_cloud_cover,\n        max_snow_cover=max_snow_cover,\n        months=months,\n        years=years,\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.get_gee_s2_sr_scene_ids_from_geodataframe","title":"get_gee_s2_sr_scene_ids_from_geodataframe","text":"<pre><code>get_gee_s2_sr_scene_ids_from_geodataframe(\n    aoi: geopandas.GeoDataFrame | pathlib.Path | str,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; set[str]\n</code></pre> <p>Search for Sentinel-2 scenes via Earth Engine based on an aoi shapefile.</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame | pathlib.Path | str</code>)           \u2013            <p>AOI as a GeoDataFrame or path to a shapefile. If a path is provided, it will be read using geopandas.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by ee. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by ee. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>set[str]: Unique Sentinel-2 tile IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>@stopwatch(\"Searching for Sentinel-2 scenes in Earth Engine from AOI\", printer=logger.debug)\ndef get_gee_s2_sr_scene_ids_from_geodataframe(\n    aoi: gpd.GeoDataFrame | Path | str,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; set[str]:\n    \"\"\"Search for Sentinel-2 scenes via Earth Engine based on an aoi shapefile.\n\n    Args:\n        aoi (gpd.GeoDataFrame | Path | str): AOI as a GeoDataFrame or path to a shapefile.\n            If a path is provided, it will be read using geopandas.\n        start_date (str): Starting date in a format readable by ee.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by ee.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n\n    Returns:\n        set[str]: Unique Sentinel-2 tile IDs.\n\n    \"\"\"\n    # Disable max xxx cover if set to 100\n    if max_cloud_cover is not None and max_cloud_cover == 100:\n        max_cloud_cover = None\n    if max_snow_cover is not None and max_snow_cover == 100:\n        max_snow_cover = None\n\n    if isinstance(aoi, Path | str):\n        aoi = gpd.read_file(aoi)\n    aoi = aoi.to_crs(\"EPSG:4326\")\n    s2ids = set()\n    for i, row in aoi.iterrows():\n        geom = ee.Geometry.Polygon(list(row.geometry.exterior.coords))\n        if start_date is not None and end_date is not None:\n            ic = ee.ImageCollection(\"COPERNICUS/S2_SR\").filterBounds(geom).filterDate(start_date, end_date)\n            if max_cloud_cover:\n                ic = ic.filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", max_cloud_cover)\n            if max_snow_cover:\n                ic = ic.filterMetadata(\"SNOW_ICE_PERCENTAGE\", \"less_than\", max_snow_cover)\n            s2ids.update(ic.aggregate_array(\"system:index\").getInfo())\n        else:\n            logger.warning(\"No valid date filtering provided. This may result in a too large number of scenes for GEE.\")\n            ic = ee.ImageCollection(\"COPERNICUS/S2_SR\").filterBounds(geom)\n            if max_cloud_cover:\n                ic = ic.filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", max_cloud_cover)\n            if max_snow_cover:\n                ic = ic.filterMetadata(\"SNOW_ICE_PERCENTAGE\", \"less_than\", max_snow_cover)\n            s2ids.update(ic.aggregate_array(\"system:index\").getInfo())\n\n    logger.debug(f\"Found {len(s2ids)} Sentinel-2 tiles via ee.\")\n    return s2ids\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.get_gee_s2_sr_scene_ids_from_tile_ids","title":"get_gee_s2_sr_scene_ids_from_tile_ids","text":"<pre><code>get_gee_s2_sr_scene_ids_from_tile_ids(\n    tiles: list[str],\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; set[str]\n</code></pre> <p>Search for Sentinel-2 scenes via Earth Engine based on a list of tile IDs.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[str]</code>)           \u2013            <p>List of Sentinel-2 tile IDs.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by ee. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by ee. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>set[str]: Unique Sentinel-2 tile IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>def get_gee_s2_sr_scene_ids_from_tile_ids(\n    tiles: list[str],\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; set[str]:\n    \"\"\"Search for Sentinel-2 scenes via Earth Engine based on a list of tile IDs.\n\n    Args:\n        tiles (list[str]): List of Sentinel-2 tile IDs.\n        start_date (str): Starting date in a format readable by ee.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by ee.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n\n    Returns:\n        set[str]: Unique Sentinel-2 tile IDs.\n\n    \"\"\"\n    # Disable max xxx cover if set to 100\n    if max_cloud_cover is not None and max_cloud_cover == 100:\n        max_cloud_cover = None\n    if max_snow_cover is not None and max_snow_cover == 100:\n        max_snow_cover = None\n\n    s2ids = set()\n    for tile in tiles:\n        if start_date is not None and end_date is not None:\n            ic = (\n                ee.ImageCollection(\"COPERNICUS/S2_SR\")\n                .filterDate(start_date, end_date)\n                .filterMetadata(\"MGRS_TILE\", \"equals\", tile)\n            )\n            if max_cloud_cover:\n                ic = ic.filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", max_cloud_cover)\n            if max_snow_cover:\n                ic = ic.filterMetadata(\"SNOW_ICE_PERCENTAGE\", \"less_than\", max_snow_cover)\n            s2ids.update(ic.aggregate_array(\"system:index\").getInfo())\n        else:\n            logger.warning(\"No valid date filtering provided. This may result in a too large number of scenes for GEE.\")\n            ic = ee.ImageCollection(\"COPERNICUS/S2_SR\").filterMetadata(\"MGRS_TILE\", \"equals\", tile)\n            if max_cloud_cover:\n                ic = ic.filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", max_cloud_cover)\n            if max_snow_cover:\n                ic = ic.filterMetadata(\"SNOW_ICE_PERCENTAGE\", \"less_than\", max_snow_cover)\n            s2ids.update(ic.aggregate_array(\"system:index\").getInfo())\n\n    logger.debug(f\"Found {len(s2ids)} Sentinel-2 tiles via ee.\")\n    return s2ids\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.get_planet_geometry","title":"get_planet_geometry","text":"<pre><code>get_planet_geometry(\n    fpath: str | pathlib.Path,\n) -&gt; odc.geo.Geometry\n</code></pre> <p>Get the geometry of a Planet scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The file path to the Planet scene from which to derive the geometry.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>odc.geo.Geometry</code>           \u2013            <p>odc.geo.Geometry: The geometry of the Planet scene.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def get_planet_geometry(fpath: str | Path) -&gt; odc.geo.Geometry:\n    \"\"\"Get the geometry of a Planet scene.\n\n    Args:\n        fpath (str | Path): The file path to the Planet scene from which to derive the geometry.\n\n    Returns:\n        odc.geo.Geometry: The geometry of the Planet scene.\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    planet_raster = rasterio.open(ps_image)\n    return odc.geo.BoundingBox(*planet_raster.bounds, crs=planet_raster.crs)\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_arcticdem","title":"load_arcticdem","text":"<pre><code>load_arcticdem(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n    buffer: int = 0,\n    offline: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>This function loads ArcticDEM elevation data from a local icechunk store. If <code>offline=False</code>, missing data will be automatically downloaded from the AWS-hosted STAC server and stored locally for future use. The loaded data is returned in the ArcticDEM's native CRS (EPSG:3413).</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which the tile should be loaded. Must be in a meter-based CRS.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>Path to the icechunk data directory (must have .icechunk suffix). This directory stores downloaded ArcticDEM data for faster consecutive access.</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in meters. Must match the resolution indicated in the data_dir name (e.g., \"arcticdem_2m.icechunk\").</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Buffer around the geobox in pixels. The buffer is applied in the ArcticDEM's native CRS (EPSG:3413) after reprojecting the input geobox. Useful for edge effect removal in terrain analysis. Defaults to 0.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only loads data already present in the local store without attempting any downloads. If False, missing data is downloaded from AWS. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The ArcticDEM dataset with the following data variables: - dem (float32): Elevation values in meters, clipped to [-100, 3000] range - arcticdem_data_mask (uint8): Data validity mask (1=valid, 0=invalid)</p> <p>The dataset is in the ArcticDEM's native CRS (EPSG:3413) with the buffer applied. It is NOT automatically reprojected to match the input geobox's CRS and resolution.</p> </li> </ul> Note <p>The <code>offline</code> parameter controls data fetching behavior:</p> <ul> <li>When <code>offline=False</code>: Uses <code>smart_geocubes</code> accessor's <code>load()</code> method which automatically   downloads missing tiles from AWS and persists them to the icechunk store.</li> <li>When <code>offline=True</code>: Uses the accessor's <code>open_xarray()</code> method to open the existing store   and crops it to the requested region. Raises an error if data is missing.</li> </ul> Warning <ul> <li>The input geobox must be in a meter-based CRS.</li> <li>The data_dir must have an <code>.icechunk</code> suffix and contain the resolution in the name.</li> <li>The returned dataset is in EPSG:3413, not the input geobox's CRS.</li> </ul> Example <p>Load ArcticDEM with a buffer for terrain analysis:</p> <pre><code>from math import ceil, sqrt\nfrom darts_acquisition import load_arcticdem\n\n# Assume \"optical\" is a loaded Sentinel-2 dataset\narcticdem = load_arcticdem(\n    geobox=optical.odc.geobox,\n    data_dir=\"/data/arcticdem_2m.icechunk\",\n    resolution=2,\n    buffer=ceil(128 / 2 * sqrt(2)),  # Buffer for TPI with 128m radius\n    offline=False\n)\n\n# Reproject to match optical data's CRS and resolution\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopwatch.f(\"Loading ArcticDEM\", printer=logger.debug, print_kwargs=[\"data_dir\", \"resolution\", \"buffer\", \"offline\"])\ndef load_arcticdem(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    resolution: RESOLUTIONS,\n    buffer: int = 0,\n    offline: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    This function loads ArcticDEM elevation data from a local icechunk store. If `offline=False`,\n    missing data will be automatically downloaded from the AWS-hosted STAC server and stored\n    locally for future use. The loaded data is returned in the ArcticDEM's native CRS (EPSG:3413).\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded. Must be in a meter-based CRS.\n        data_dir (Path | str): Path to the icechunk data directory (must have .icechunk suffix).\n            This directory stores downloaded ArcticDEM data for faster consecutive access.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in meters.\n            Must match the resolution indicated in the data_dir name (e.g., \"arcticdem_2m.icechunk\").\n        buffer (int, optional): Buffer around the geobox in pixels. The buffer is applied in the\n            ArcticDEM's native CRS (EPSG:3413) after reprojecting the input geobox. Useful for\n            edge effect removal in terrain analysis. Defaults to 0.\n        offline (bool, optional): If True, only loads data already present in the local store\n            without attempting any downloads. If False, missing data is downloaded from AWS.\n            Defaults to False.\n\n    Returns:\n        xr.Dataset: The ArcticDEM dataset with the following data variables:\n            - dem (float32): Elevation values in meters, clipped to [-100, 3000] range\n            - arcticdem_data_mask (uint8): Data validity mask (1=valid, 0=invalid)\n\n            The dataset is in the ArcticDEM's native CRS (EPSG:3413) with the buffer applied.\n            It is NOT automatically reprojected to match the input geobox's CRS and resolution.\n\n    Note:\n        The `offline` parameter controls data fetching behavior:\n\n        - When `offline=False`: Uses `smart_geocubes` accessor's `load()` method which automatically\n          downloads missing tiles from AWS and persists them to the icechunk store.\n        - When `offline=True`: Uses the accessor's `open_xarray()` method to open the existing store\n          and crops it to the requested region. Raises an error if data is missing.\n\n    Warning:\n        - The input geobox must be in a meter-based CRS.\n        - The data_dir must have an `.icechunk` suffix and contain the resolution in the name.\n        - The returned dataset is in EPSG:3413, not the input geobox's CRS.\n\n    Example:\n        Load ArcticDEM with a buffer for terrain analysis:\n\n        ```python\n        from math import ceil, sqrt\n        from darts_acquisition import load_arcticdem\n\n        # Assume \"optical\" is a loaded Sentinel-2 dataset\n        arcticdem = load_arcticdem(\n            geobox=optical.odc.geobox,\n            data_dir=\"/data/arcticdem_2m.icechunk\",\n            resolution=2,\n            buffer=ceil(128 / 2 * sqrt(2)),  # Buffer for TPI with 128m radius\n            offline=False\n        )\n\n        # Reproject to match optical data's CRS and resolution\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"\n    if not offline:\n        odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n\n    accessor = _validate_and_get_accessor(data_dir, resolution)\n\n    if not offline:\n        arcticdem = accessor.load(geobox, buffer=buffer, persist=True)\n    else:\n        xrcube = accessor.open_xarray()\n        reference_geobox = geobox.to_crs(accessor.extent.crs, resolution=accessor.extent.resolution.x).pad(buffer)\n        arcticdem = xrcube.odc.crop(reference_geobox.extent, apply_mask=False)\n        arcticdem = arcticdem.load()\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem[\"arcticdem_data_mask\"] = arcticdem.datamask.astype(\"uint8\")\n\n    # Clip values to -100, 3000 range (see docs about bands)\n    arcticdem[\"dem\"] = arcticdem[\"dem\"].clip(-100, 3000)\n\n    # Change dtype of arcticdem to float32 to save memory (original is float64)\n    arcticdem[\"dem\"] = arcticdem[\"dem\"].astype(\"float32\")\n\n    return arcticdem\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_cdse_s2_sr_scene","title":"load_cdse_s2_sr_scene","text":"<pre><code>load_cdse_s2_sr_scene(\n    s2item: str | pystac.Item,\n    bands_mapping: dict | typing.Literal[\"all\"] = {\n        \"B02_10m\": \"blue\",\n        \"B03_10m\": \"green\",\n        \"B04_10m\": \"red\",\n        \"B08_10m\": \"nir\",\n    },\n    store: pathlib.Path | None = None,\n    aws_profile_name: str = \"default\",\n    offline: bool = False,\n    output_dir_for_debug_geotiff: pathlib.Path\n    | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_utils.cuda.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel-2 scene from CDSE, downloading from STAC API if necessary.</p> <p>This function loads Sentinel-2 Level-2A surface reflectance data from the Copernicus Data Space Ecosystem (CDSE). If a local store is provided, the data is cached for efficient repeated access. The function handles quality masking, reflectance scaling, and optional GPU acceleration.</p> <p>The download logic is basically as follows:</p> <pre><code>IF flag:raw-data-store THEN\n    IF exist_local THEN\n        open -&gt; memory\n    ELIF online THEN\n        download -&gt; memory\n        save\n    ELIF offline THEN\n        RAISE ERROR\n    ENDIF\nELIF online THEN\n    download -&gt; memory\nELIF offline THEN\n    RAISE ERROR\nENDIF\n</code></pre> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | pystac.Item</code>)           \u2013            <p>Sentinel-2 scene identifier or PySTAC Item object.</p> </li> <li> <code>bands_mapping</code>               (<code>dict | typing.Literal['all']</code>, default:                   <code>{'B02_10m': 'blue', 'B03_10m': 'green', 'B04_10m': 'red', 'B08_10m': 'nir'}</code> )           \u2013            <p>Mapping of Sentinel-2 band names to custom band names. Keys should be CDSE band names (e.g., \"B02_10m\"), values are output names. Use \"all\" to load all optical bands and SCL. Defaults to {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"}.</p> </li> <li> <code>store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to local zarr store for caching. If None, data is loaded directly without caching. Defaults to None.</p> </li> <li> <code>aws_profile_name</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>AWS profile name for Copernicus S3 authentication. Defaults to \"default\".</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only loads from local store without downloading. Requires <code>store</code> to be provided. If False, missing data is downloaded. Defaults to False.</p> </li> <li> <code>output_dir_for_debug_geotiff</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>If provided, writes raw data as GeoTIFF files for debugging. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_utils.cuda.DEFAULT_DEVICE</code> )           \u2013            <p>Device for processing (GPU or CPU). Defaults to DEFAULT_DEVICE.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Sentinel-2 dataset with the following data variables based on bands_mapping: - Optical bands (float32): Surface reflectance values [~-0.1 to ~1.0]   Default bands: blue, green, red, nir   Additional bands available: coastal, rededge071, rededge075, rededge078,   nir08, nir09, swir16, swir22   Each has attributes:   - long_name: \"Sentinel-2 {Band}\"   - units: \"Reflectance\"   - data_source: \"Sentinel-2 L2A via Copernicus STAC API (sentinel-2-l2a)\" - s2_scl (uint8): Scene Classification Layer   Attributes: long_name, description of class values (0=NO_DATA, 1=SATURATED, etc.) - quality_data_mask (uint8): Derived quality mask   - 0 = Invalid (no data, saturated, or defective)   - 1 = Low quality (shadows, clouds, cirrus, snow/ice, water)   - 2 = High quality (clear vegetation or non-vegetated land) - valid_data_mask (uint8): Binary validity mask (1=valid, 0=invalid)</p> <p>Dataset attributes: - azimuth (float): Solar azimuth angle from view:azimuth - elevation (float): Solar elevation angle from view:sun_elevation - s2_tile_id (str): Scene identifier - tile_id (str): Scene identifier (same as s2_tile_id) - Plus additional STAC metadata fields</p> </li> </ul> Note <p>The <code>offline</code> parameter controls data fetching: - When <code>offline=False</code>: Automatically downloads missing data from CDSE and stores it   in the local zarr store (if store is provided). - When <code>offline=True</code>: Only reads from the local store. Raises an error if data is   missing or if store is None.</p> <p>Reflectance processing: - Raw DN values are scaled: (DN / 10000.0) - 0.1 - Pixels where SCL==0 or DN==0 are masked as NaN - This matches the data format from GEE and Planet loaders</p> <p>Quality mask derivation from SCL: - Invalid (0): NO_DATA, SATURATED_OR_DEFECTIVE - Low quality (1): CAST_SHADOWS, CLOUD_SHADOWS, CLOUD_*, THIN_CIRRUS, SNOW/ICE, WATER - High quality (2): VEGETATION, NOT_VEGETATED</p> Example <p>Load scene with local caching:</p> <pre><code>from pathlib import Path\nfrom darts_acquisition import load_cdse_s2_sr_scene\nfrom darts_utils.copernicus import init_copernicus\n\n# Setup authentication\ninit_copernicus(profile_name=\"default\")\n\n# Load with caching\ns2_ds = load_cdse_s2_sr_scene(\n    s2item=\"S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678\",\n    bands_mapping=\"all\",\n    store=Path(\"/data/s2_store\"),\n    offline=False  # Download if not cached\n)\n\n# Compute NDVI\nndvi = (s2_ds.nir - s2_ds.red) / (s2_ds.nir + s2_ds.red)\n\n# Filter to high quality pixels\ns2_filtered = s2_ds.where(s2_ds.quality_data_mask == 2)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel-2 scene from CDSE\", printer=logger.debug, print_kwargs=[\"s2item\"])\ndef load_cdse_s2_sr_scene(\n    s2item: str | Item,\n    bands_mapping: dict | Literal[\"all\"] = {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"},\n    store: Path | None = None,\n    aws_profile_name: str = \"default\",\n    offline: bool = False,\n    output_dir_for_debug_geotiff: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel-2 scene from CDSE, downloading from STAC API if necessary.\n\n    This function loads Sentinel-2 Level-2A surface reflectance data from the Copernicus\n    Data Space Ecosystem (CDSE). If a local store is provided, the data is cached for\n    efficient repeated access. The function handles quality masking, reflectance scaling,\n    and optional GPU acceleration.\n\n    The download logic is basically as follows:\n\n    ```\n    IF flag:raw-data-store THEN\n        IF exist_local THEN\n            open -&gt; memory\n        ELIF online THEN\n            download -&gt; memory\n            save\n        ELIF offline THEN\n            RAISE ERROR\n        ENDIF\n    ELIF online THEN\n        download -&gt; memory\n    ELIF offline THEN\n        RAISE ERROR\n    ENDIF\n    ```\n\n    Args:\n        s2item (str | Item): Sentinel-2 scene identifier or PySTAC Item object.\n        bands_mapping (dict | Literal[\"all\"], optional): Mapping of Sentinel-2 band names to\n            custom band names. Keys should be CDSE band names (e.g., \"B02_10m\"), values are\n            output names. Use \"all\" to load all optical bands and SCL.\n            Defaults to {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"}.\n        store (Path | None, optional): Path to local zarr store for caching. If None, data is\n            loaded directly without caching. Defaults to None.\n        aws_profile_name (str, optional): AWS profile name for Copernicus S3 authentication.\n            Defaults to \"default\".\n        offline (bool, optional): If True, only loads from local store without downloading.\n            Requires `store` to be provided. If False, missing data is downloaded.\n            Defaults to False.\n        output_dir_for_debug_geotiff (Path | None, optional): If provided, writes raw data as\n            GeoTIFF files for debugging. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): Device for processing (GPU or CPU).\n            Defaults to DEFAULT_DEVICE.\n\n    Returns:\n        xr.Dataset: Sentinel-2 dataset with the following data variables based on bands_mapping:\n            - Optical bands (float32): Surface reflectance values [~-0.1 to ~1.0]\n              Default bands: blue, green, red, nir\n              Additional bands available: coastal, rededge071, rededge075, rededge078,\n              nir08, nir09, swir16, swir22\n              Each has attributes:\n              - long_name: \"Sentinel-2 {Band}\"\n              - units: \"Reflectance\"\n              - data_source: \"Sentinel-2 L2A via Copernicus STAC API (sentinel-2-l2a)\"\n            - s2_scl (uint8): Scene Classification Layer\n              Attributes: long_name, description of class values (0=NO_DATA, 1=SATURATED, etc.)\n            - quality_data_mask (uint8): Derived quality mask\n              - 0 = Invalid (no data, saturated, or defective)\n              - 1 = Low quality (shadows, clouds, cirrus, snow/ice, water)\n              - 2 = High quality (clear vegetation or non-vegetated land)\n            - valid_data_mask (uint8): Binary validity mask (1=valid, 0=invalid)\n\n            Dataset attributes:\n            - azimuth (float): Solar azimuth angle from view:azimuth\n            - elevation (float): Solar elevation angle from view:sun_elevation\n            - s2_tile_id (str): Scene identifier\n            - tile_id (str): Scene identifier (same as s2_tile_id)\n            - Plus additional STAC metadata fields\n\n    Note:\n        The `offline` parameter controls data fetching:\n        - When `offline=False`: Automatically downloads missing data from CDSE and stores it\n          in the local zarr store (if store is provided).\n        - When `offline=True`: Only reads from the local store. Raises an error if data is\n          missing or if store is None.\n\n        Reflectance processing:\n        - Raw DN values are scaled: (DN / 10000.0) - 0.1\n        - Pixels where SCL==0 or DN==0 are masked as NaN\n        - This matches the data format from GEE and Planet loaders\n\n        Quality mask derivation from SCL:\n        - Invalid (0): NO_DATA, SATURATED_OR_DEFECTIVE\n        - Low quality (1): CAST_SHADOWS, CLOUD_SHADOWS, CLOUD_*, THIN_CIRRUS, SNOW/ICE, WATER\n        - High quality (2): VEGETATION, NOT_VEGETATED\n\n    Example:\n        Load scene with local caching:\n\n        ```python\n        from pathlib import Path\n        from darts_acquisition import load_cdse_s2_sr_scene\n        from darts_utils.copernicus import init_copernicus\n\n        # Setup authentication\n        init_copernicus(profile_name=\"default\")\n\n        # Load with caching\n        s2_ds = load_cdse_s2_sr_scene(\n            s2item=\"S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678\",\n            bands_mapping=\"all\",\n            store=Path(\"/data/s2_store\"),\n            offline=False  # Download if not cached\n        )\n\n        # Compute NDVI\n        ndvi = (s2_ds.nir - s2_ds.red) / (s2_ds.nir + s2_ds.red)\n\n        # Filter to high quality pixels\n        s2_filtered = s2_ds.where(s2_ds.quality_data_mask == 2)\n        ```\n\n    \"\"\"\n    s2id = s2item.id if isinstance(s2item, Item) else s2item\n\n    bands_mapping = _get_band_mapping(bands_mapping)\n    store_manager = CDSEStoreManager(\n        store=store,\n        bands_mapping=bands_mapping,\n        aws_profile_name=aws_profile_name,\n    )\n\n    with stopwatch(\"Load Sentinel-2 scene from store\", printer=logger.debug):\n        if not offline:\n            ds_s2 = store_manager.load(s2item)\n        else:\n            assert store is not None, \"Store must be provided in offline mode!\"\n            ds_s2 = store_manager.open(s2item)\n\n    if output_dir_for_debug_geotiff is not None:\n        save_debug_geotiff(\n            dataset=ds_s2,\n            output_path=output_dir_for_debug_geotiff,\n            optical_bands=[band for band in bands_mapping.keys() if band.startswith(\"B\")],\n            mask_bands=[\"SCL_20m\"] if \"SCL_20m\" in bands_mapping.keys() else None,\n        )\n\n    # ? The following part takes ~2.5s on CPU and ~0.1 on GPU,\n    # however moving to GPU and back takes ~2.2s\n    ds_s2 = move_to_device(ds_s2, device)\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n    optical_bands = [band for name, band in bands_mapping.items() if name.startswith(\"B\")]\n    for band in optical_bands:\n        # Set values where SCL_20m == 0 to NaN in all other bands\n        # This way the data is similar to data from gee or planet data\n        # We need to filter out 0 values, since they are not valid reflectance values\n        # But also not reflected in the SCL for some reason\n        ds_s2[band] = ds_s2[band].where(ds_s2.s2_scl != 0).where(ds_s2[band].astype(\"float32\") != 0) / 10000.0 - 0.1\n        ds_s2[band].attrs[\"long_name\"] = f\"Sentinel-2 {band.capitalize()}\"\n        ds_s2[band].attrs[\"units\"] = \"Reflectance\"\n    ds_s2[\"s2_scl\"].attrs = {\n        \"long_name\": \"Sentinel-2 Scene Classification Layer\",\n        \"description\": (\n            \"0: NO_DATA - 1: SATURATED_OR_DEFECTIVE - 2: CAST_SHADOWS - 3: CLOUD_SHADOWS - 4: VEGETATION\"\n            \" - 5: NOT_VEGETATED - 6: WATER - 7: UNCLASSIFIED - 8: CLOUD_MEDIUM_PROBABILITY\"\n            \" - 9: CLOUD_HIGH_PROBABILITY - 10: THIN_CIRRUS - 11: SNOW or ICE\"\n        ),\n    }\n    for band in ds_s2.data_vars:\n        ds_s2[band].attrs[\"data_source\"] = \"Sentinel-2 L2A via Copernicus STAC API (sentinel-2-l2a)\"\n\n    # ? This takes approx. 1.5s on CPU\n    # For some reason, this takes ~1.2s on the GPU\n    ds_s2 = convert_masks(ds_s2)\n\n    ds_s2 = move_to_host(ds_s2)\n\n    # Convert sun elevation and azimuth to match our naming\n    ds_s2.attrs[\"azimuth\"] = ds_s2.attrs.get(\"view:sun_azimuth\", float(\"nan\"))\n    ds_s2.attrs[\"elevation\"] = ds_s2.attrs.get(\"view:sun_elevation\", float(\"nan\"))\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_gee_s2_sr_scene","title":"load_gee_s2_sr_scene","text":"<pre><code>load_gee_s2_sr_scene(\n    s2item: str | ee.Image,\n    bands_mapping: dict | typing.Literal[\"all\"] = {\n        \"B2\": \"blue\",\n        \"B3\": \"green\",\n        \"B4\": \"red\",\n        \"B8\": \"nir\",\n    },\n    store: pathlib.Path | None = None,\n    offline: bool = False,\n    output_dir_for_debug_geotiff: pathlib.Path\n    | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_utils.cuda.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel-2 scene from Google Earth Engine, downloading if necessary.</p> <p>This function loads Sentinel-2 Level-2A surface reflectance data from Google Earth Engine. If a local store is provided, the data is cached for efficient repeated access. The function handles quality masking, reflectance scaling with time-dependent offsets, and optional GPU acceleration. It also handles NaN values in the data by masking them as invalid.</p> <p>The download logic is basically as follows:</p> <pre><code>IF flag:raw-data-store THEN\n    IF exist_local THEN\n        open -&gt; memory\n    ELIF online THEN\n        download -&gt; memory\n        save\n    ELIF offline THEN\n        RAISE ERROR\n    ENDIF\nELIF online THEN\n    download -&gt; memory\nELIF offline THEN\n    RAISE ERROR\nENDIF\n</code></pre> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | ee.Image</code>)           \u2013            <p>Sentinel-2 scene identifier or ee.Image object from COPERNICUS/S2_SR.</p> </li> <li> <code>bands_mapping</code>               (<code>dict | typing.Literal['all']</code>, default:                   <code>{'B2': 'blue', 'B3': 'green', 'B4': 'red', 'B8': 'nir'}</code> )           \u2013            <p>Mapping of Sentinel-2 band names to custom band names. Keys should be GEE band names (e.g., \"B2\", \"B3\"), values are output names. Use \"all\" to load all optical bands and SCL. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to local zarr store for caching. If None, data is loaded directly without caching. Defaults to None.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only loads from local store without downloading. Requires <code>store</code> to be provided. If False, missing data is downloaded. Defaults to False.</p> </li> <li> <code>output_dir_for_debug_geotiff</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>If provided, writes raw data as GeoTIFF files for debugging. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_utils.cuda.DEFAULT_DEVICE</code> )           \u2013            <p>Device for processing (GPU or CPU). Defaults to DEFAULT_DEVICE.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Sentinel-2 dataset with the following data variables based on bands_mapping: - Optical bands (float32): Surface reflectance values [~-0.1 to ~1.0 for newer scenes,   ~0.0 to ~1.0 for scenes before 2022-01-25]   Default bands: blue, green, red, nir   Additional bands available: coastal, rededge071, rededge075, rededge078,   nir08, nir09, swir16, swir22   Each has attributes:   - long_name: \"Sentinel 2 {Band}\"   - units: \"Reflectance\"   - data_source: \"Sentinel-2 L2A via Google Earth Engine (COPERNICUS/S2_SR)\" - s2_scl (uint8): Scene Classification Layer   Attributes: long_name, description of class values (0=NO_DATA, 1=SATURATED, etc.) - quality_data_mask (uint8): Derived quality mask   - 0 = Invalid (no data, saturated, defective, or NaN values)   - 1 = Low quality (shadows, clouds, cirrus, snow/ice, water)   - 2 = High quality (clear vegetation or non-vegetated land) - valid_data_mask (uint8): Binary validity mask (1=valid, 0=invalid)</p> <p>Dataset attributes: - azimuth (float): Solar azimuth angle from MEAN_SOLAR_AZIMUTH_ANGLE - elevation (float): Solar elevation angle from MEAN_SOLAR_ZENITH_ANGLE - s2_tile_id (str): Full PRODUCT_ID from GEE - tile_id (str): Scene identifier - time (str): Acquisition timestamp</p> </li> </ul> Note <p>The <code>offline</code> parameter controls data fetching: - When <code>offline=False</code>: Automatically downloads missing data from GEE and stores it   in the local zarr store (if store is provided). - When <code>offline=True</code>: Only reads from the local store. Raises an error if data is   missing or if store is None.</p> <p>Reflectance processing: - For scenes &gt;= 2022-01-25: (DN / 10000.0) - 0.1 (processing baseline 04.00+) - For scenes &lt; 2022-01-25: DN / 10000.0 (older processing baseline) - NaN values are filled with 0 and marked as invalid in quality_data_mask - Pixels where SCL is NaN are also masked as invalid</p> <p>This function handles spatially random NaN values that can occur in GEE data by marking them as invalid and filling with 0 to prevent propagation in calculations.</p> <p>Quality mask derivation from SCL: - Invalid (0): NO_DATA, SATURATED_OR_DEFECTIVE, or NaN values - Low quality (1): CAST_SHADOWS, CLOUD_SHADOWS, CLOUD_*, THIN_CIRRUS, SNOW/ICE, WATER - High quality (2): VEGETATION, NOT_VEGETATED</p> Example <p>Load scene with local caching:</p> <pre><code>import ee\nfrom pathlib import Path\nfrom darts_acquisition import load_gee_s2_sr_scene\n\n# Initialize Earth Engine\nee.Initialize()\n\n# Load with caching\ns2_ds = load_gee_s2_sr_scene(\n    s2item=\"20230615T123456_20230615T123659_T33UUP\",\n    bands_mapping=\"all\",\n    store=Path(\"/data/s2_store\"),\n    offline=False  # Download if not cached\n)\n\n# Compute NDVI\nndvi = (s2_ds.nir - s2_ds.red) / (s2_ds.nir + s2_ds.red)\n\n# Filter to high quality pixels\ns2_filtered = s2_ds.where(s2_ds.quality_data_mask == 2)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel-2 scene from GEE\", printer=logger.debug, print_kwargs=[\"s2item\"])\ndef load_gee_s2_sr_scene(\n    s2item: str | ee.Image,\n    bands_mapping: dict | Literal[\"all\"] = {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"},\n    store: Path | None = None,\n    offline: bool = False,\n    output_dir_for_debug_geotiff: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel-2 scene from Google Earth Engine, downloading if necessary.\n\n    This function loads Sentinel-2 Level-2A surface reflectance data from Google Earth Engine.\n    If a local store is provided, the data is cached for efficient repeated access. The function\n    handles quality masking, reflectance scaling with time-dependent offsets, and optional GPU\n    acceleration. It also handles NaN values in the data by masking them as invalid.\n\n    The download logic is basically as follows:\n\n    ```\n    IF flag:raw-data-store THEN\n        IF exist_local THEN\n            open -&gt; memory\n        ELIF online THEN\n            download -&gt; memory\n            save\n        ELIF offline THEN\n            RAISE ERROR\n        ENDIF\n    ELIF online THEN\n        download -&gt; memory\n    ELIF offline THEN\n        RAISE ERROR\n    ENDIF\n    ```\n\n    Args:\n        s2item (str | ee.Image): Sentinel-2 scene identifier or ee.Image object from COPERNICUS/S2_SR.\n        bands_mapping (dict | Literal[\"all\"], optional): Mapping of Sentinel-2 band names to\n            custom band names. Keys should be GEE band names (e.g., \"B2\", \"B3\"), values are\n            output names. Use \"all\" to load all optical bands and SCL.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        store (Path | None, optional): Path to local zarr store for caching. If None, data is\n            loaded directly without caching. Defaults to None.\n        offline (bool, optional): If True, only loads from local store without downloading.\n            Requires `store` to be provided. If False, missing data is downloaded.\n            Defaults to False.\n        output_dir_for_debug_geotiff (Path | None, optional): If provided, writes raw data as\n            GeoTIFF files for debugging. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): Device for processing (GPU or CPU).\n            Defaults to DEFAULT_DEVICE.\n\n    Returns:\n        xr.Dataset: Sentinel-2 dataset with the following data variables based on bands_mapping:\n            - Optical bands (float32): Surface reflectance values [~-0.1 to ~1.0 for newer scenes,\n              ~0.0 to ~1.0 for scenes before 2022-01-25]\n              Default bands: blue, green, red, nir\n              Additional bands available: coastal, rededge071, rededge075, rededge078,\n              nir08, nir09, swir16, swir22\n              Each has attributes:\n              - long_name: \"Sentinel 2 {Band}\"\n              - units: \"Reflectance\"\n              - data_source: \"Sentinel-2 L2A via Google Earth Engine (COPERNICUS/S2_SR)\"\n            - s2_scl (uint8): Scene Classification Layer\n              Attributes: long_name, description of class values (0=NO_DATA, 1=SATURATED, etc.)\n            - quality_data_mask (uint8): Derived quality mask\n              - 0 = Invalid (no data, saturated, defective, or NaN values)\n              - 1 = Low quality (shadows, clouds, cirrus, snow/ice, water)\n              - 2 = High quality (clear vegetation or non-vegetated land)\n            - valid_data_mask (uint8): Binary validity mask (1=valid, 0=invalid)\n\n            Dataset attributes:\n            - azimuth (float): Solar azimuth angle from MEAN_SOLAR_AZIMUTH_ANGLE\n            - elevation (float): Solar elevation angle from MEAN_SOLAR_ZENITH_ANGLE\n            - s2_tile_id (str): Full PRODUCT_ID from GEE\n            - tile_id (str): Scene identifier\n            - time (str): Acquisition timestamp\n\n    Note:\n        The `offline` parameter controls data fetching:\n        - When `offline=False`: Automatically downloads missing data from GEE and stores it\n          in the local zarr store (if store is provided).\n        - When `offline=True`: Only reads from the local store. Raises an error if data is\n          missing or if store is None.\n\n        Reflectance processing:\n        - For scenes &gt;= 2022-01-25: (DN / 10000.0) - 0.1 (processing baseline 04.00+)\n        - For scenes &lt; 2022-01-25: DN / 10000.0 (older processing baseline)\n        - NaN values are filled with 0 and marked as invalid in quality_data_mask\n        - Pixels where SCL is NaN are also masked as invalid\n\n        This function handles spatially random NaN values that can occur in GEE data by\n        marking them as invalid and filling with 0 to prevent propagation in calculations.\n\n        Quality mask derivation from SCL:\n        - Invalid (0): NO_DATA, SATURATED_OR_DEFECTIVE, or NaN values\n        - Low quality (1): CAST_SHADOWS, CLOUD_SHADOWS, CLOUD_*, THIN_CIRRUS, SNOW/ICE, WATER\n        - High quality (2): VEGETATION, NOT_VEGETATED\n\n    Example:\n        Load scene with local caching:\n\n        ```python\n        import ee\n        from pathlib import Path\n        from darts_acquisition import load_gee_s2_sr_scene\n\n        # Initialize Earth Engine\n        ee.Initialize()\n\n        # Load with caching\n        s2_ds = load_gee_s2_sr_scene(\n            s2item=\"20230615T123456_20230615T123659_T33UUP\",\n            bands_mapping=\"all\",\n            store=Path(\"/data/s2_store\"),\n            offline=False  # Download if not cached\n        )\n\n        # Compute NDVI\n        ndvi = (s2_ds.nir - s2_ds.red) / (s2_ds.nir + s2_ds.red)\n\n        # Filter to high quality pixels\n        s2_filtered = s2_ds.where(s2_ds.quality_data_mask == 2)\n        ```\n\n    \"\"\"\n    if isinstance(s2item, str):\n        s2id = s2item\n        s2item = ee.Image(f\"COPERNICUS/S2_SR/{s2id}\")\n    else:\n        s2id = s2item.id().getInfo().split(\"/\")[-1]\n    logger.debug(f\"Loading Sentinel-2 tile {s2id=} from GEE\")\n\n    bands_mapping = _get_band_mapping(bands_mapping)\n    store_manager = GEEStoreManager(\n        store=store,\n        bands_mapping=bands_mapping,\n    )\n\n    if not offline:\n        ds_s2 = store_manager.load(s2item)\n    else:\n        assert store is not None, \"Store must be provided in offline mode!\"\n        ds_s2 = store_manager.open(s2item)\n\n    if output_dir_for_debug_geotiff is not None:\n        save_debug_geotiff(\n            dataset=ds_s2,\n            output_path=output_dir_for_debug_geotiff,\n            optical_bands=[band for band in bands_mapping.keys() if band.startswith(\"B\")],\n            mask_bands=[\"SCL\"],\n        )\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n\n    optical_bands = [band for name, band in bands_mapping.items() if name.startswith(\"B\")]\n\n    # Fix new preprocessing offset -&gt; See docs about bands\n    dt = datetime.strptime(ds_s2.attrs[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f000\")\n    offset = 0.1 if dt &gt;= datetime(2022, 1, 25) else 0.0\n\n    ds_s2 = move_to_device(ds_s2, device)\n    for band in optical_bands:\n        # Apply scale and offset\n        ds_s2[band] = ds_s2[band].astype(\"float32\") / 10000.0 - offset\n        ds_s2[band].attrs[\"long_name\"] = f\"Sentinel 2 {band.capitalize()}\"\n        ds_s2[band].attrs[\"units\"] = \"Reflectance\"\n    ds_s2[\"s2_scl\"].attrs = {\n        \"long_name\": \"Sentinel-2 Scene Classification Layer\",\n        \"description\": (\n            \"0: NO_DATA - 1: SATURATED_OR_DEFECTIVE - 2: CAST_SHADOWS - 3: CLOUD_SHADOWS - 4: VEGETATION\"\n            \" - 5: NOT_VEGETATED - 6: WATER - 7: UNCLASSIFIED - 8: CLOUD_MEDIUM_PROBABILITY - 9: CLOUD_HIGH_PROBABILITY\"\n            \" - 10: THIN_CIRRUS - 11: SNOW or ICE\"\n        ),\n    }\n    for band in ds_s2.data_vars:\n        ds_s2[band].attrs[\"data_source\"] = \"Sentinel-2 L2A via Google Earth Engine (COPERNICUS/S2_SR)\"\n\n    ds_s2 = convert_masks(ds_s2)\n    qdm_attrs = ds_s2[\"quality_data_mask\"].attrs.copy()\n\n    # For some reason, there are some spatially random nan values in the data, not only at the borders\n    # To workaround this, set all nan values to 0 and add this information to the quality_data_mask\n    # This workaround is quite computational expensive, but it works for now\n    # TODO: Find other solutions for this problem!\n    with stopwatch(f\"Fixing nan values in {s2id=}\", printer=logger.debug):\n        for band in optical_bands:\n            ds_s2[\"quality_data_mask\"] = xr.where(ds_s2[band].isnull(), 0, ds_s2[\"quality_data_mask\"])\n            ds_s2[band] = ds_s2[band].fillna(0)\n            # Turn real nan values (s2_scl is nan) into invalid data\n            ds_s2[band] = ds_s2[band].where(~ds_s2[\"s2_scl\"].isnull())\n    ds_s2 = move_to_host(ds_s2)\n\n    ds_s2[\"quality_data_mask\"].attrs = qdm_attrs\n    ds_s2.attrs[\"s2_tile_id\"] = s2item.getInfo()[\"properties\"][\"PRODUCT_ID\"]\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_planet_masks","title":"load_planet_masks","text":"<pre><code>load_planet_masks(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load quality and validity masks from a PlanetScope scene's UDM-2 data.</p> <p>This function extracts data quality information from the PlanetScope Usable Data Mask (UDM-2) to create simplified quality masks for filtering and analysis.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>Path to the directory containing the PlanetScope scene data. Must contain _udm2.tif (or _udm2_clip.tif) file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Dataset containing quality mask information with the following data variables: - quality_data_mask (uint8): Combined quality indicator     * 0 = Invalid (no data)     * 1 = Low quality (clouds, shadows, haze, snow, or other artifacts)     * 2 = High quality (clear, usable data)   Attributes: data_source=\"planet\", long_name=\"Quality data mask\",   description=\"0 = Invalid, 1 = Low Quality, 2 = High Quality\" - planet_udm (uint8): Raw UDM-2 bands (8 bands)   Attributes: data_source=\"planet\", long_name=\"Planet UDM\",   description=\"Usable Data Mask\"</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If the UDM-2 TIFF file is not found in the directory.</p> </li> </ul> Note <p>Quality mask derivation logic: - Invalid: UDM band 8 (no data) is set - Low quality: Any of UDM bands 2-6 (clouds, shadows, haze, snow, or artifacts) is set - High quality: Neither invalid nor low quality</p> <p>UDM-2 band definitions: 1. Clear - 2. Snow - 3. Shadow - 4. Light Haze - 5. Heavy Haze 6. Cloud - 7. Confidence - 8. No Data</p> Example <p>Load and apply quality masks:</p> <pre><code>from darts_acquisition import load_planet_scene, load_planet_masks\n\n# Load scene and masks\nscene = load_planet_scene(\"/data/planet/20230615_123045_1234\")\nmasks = load_planet_masks(\"/data/planet/20230615_123045_1234\")\n\n# Filter to high quality pixels only\nscene_filtered = scene.where(masks.quality_data_mask == 2)\n\n# Count quality distribution\nimport numpy as np\nunique, counts = np.unique(\n    masks.quality_data_mask.values,\n    return_counts=True\n)\nprint(dict(zip(unique, counts)))\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopwatch.f(\"Loading Planet masks\", printer=logger.debug)\ndef load_planet_masks(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load quality and validity masks from a PlanetScope scene's UDM-2 data.\n\n    This function extracts data quality information from the PlanetScope Usable Data Mask\n    (UDM-2) to create simplified quality masks for filtering and analysis.\n\n    Args:\n        fpath (str | Path): Path to the directory containing the PlanetScope scene data.\n            Must contain *_udm2.tif (or *_udm2_clip.tif) file.\n\n    Returns:\n        xr.Dataset: Dataset containing quality mask information with the following data variables:\n            - quality_data_mask (uint8): Combined quality indicator\n                * 0 = Invalid (no data)\n                * 1 = Low quality (clouds, shadows, haze, snow, or other artifacts)\n                * 2 = High quality (clear, usable data)\n              Attributes: data_source=\"planet\", long_name=\"Quality data mask\",\n              description=\"0 = Invalid, 1 = Low Quality, 2 = High Quality\"\n            - planet_udm (uint8): Raw UDM-2 bands (8 bands)\n              Attributes: data_source=\"planet\", long_name=\"Planet UDM\",\n              description=\"Usable Data Mask\"\n\n    Raises:\n        FileNotFoundError: If the UDM-2 TIFF file is not found in the directory.\n\n    Note:\n        Quality mask derivation logic:\n        - Invalid: UDM band 8 (no data) is set\n        - Low quality: Any of UDM bands 2-6 (clouds, shadows, haze, snow, or artifacts) is set\n        - High quality: Neither invalid nor low quality\n\n        UDM-2 band definitions:\n        1. Clear - 2. Snow - 3. Shadow - 4. Light Haze - 5. Heavy Haze\n        6. Cloud - 7. Confidence - 8. No Data\n\n    Example:\n        Load and apply quality masks:\n\n        ```python\n        from darts_acquisition import load_planet_scene, load_planet_masks\n\n        # Load scene and masks\n        scene = load_planet_scene(\"/data/planet/20230615_123045_1234\")\n        masks = load_planet_masks(\"/data/planet/20230615_123045_1234\")\n\n        # Filter to high quality pixels only\n        scene_filtered = scene.where(masks.quality_data_mask == 2)\n\n        # Count quality distribution\n        import numpy as np\n        unique, counts = np.unique(\n            masks.quality_data_mask.values,\n            return_counts=True\n        )\n        print(dict(zip(unique, counts)))\n        ```\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # Get imagepath\n    udm_path = next(fpath.glob(\"*_udm2.tif\"), None)\n    if not udm_path:\n        udm_path = next(fpath.glob(\"*_udm2_clip.tif\"), None)\n    if not udm_path:\n        raise FileNotFoundError(f\"No matching UDM-2 TIFF files found in {fpath.resolve()} (.glob('*_udm2.tif'))\")\n\n    # See udm classes here: https://developers.planet.com/docs/data/udm-2/\n    da_udm = xr.open_dataarray(udm_path).astype(\"uint8\")\n    invalids = da_udm.sel(band=8).fillna(0) != 0\n    low_quality = da_udm.sel(band=[2, 3, 4, 5, 6]).max(axis=0) == 1\n    high_quality = ~low_quality &amp; ~invalids\n    qa_ds = (\n        xr.where(high_quality, 2, 0)\n        .where(~low_quality, 1)\n        .where(~invalids, 0)\n        .astype(\"uint8\")\n        .to_dataset(name=\"quality_data_mask\")\n        .drop_vars(\"band\")\n    )\n    qa_ds[\"planet_udm\"] = da_udm\n\n    qa_ds[\"quality_data_mask\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Quality data mask\",\n        \"description\": \"0 = Invalid, 1 = Low Quality, 2 = High Quality\",\n    }\n    qa_ds[\"planet_udm\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Planet UDM\",\n        \"description\": \"Usable Data Mask\",\n    }\n\n    return qa_ds\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_planet_scene","title":"load_planet_scene","text":"<pre><code>load_planet_scene(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a PlanetScope satellite scene from GeoTIFF files.</p> <p>This function loads PlanetScope surface reflectance data (PSScene or PSOrthoTile) from a directory containing TIFF files and metadata. The scene type is automatically detected from the directory name format.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>Path to the directory containing the PlanetScope scene data. The directory must follow PlanetScope naming conventions: - Scene: YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX - Orthotile: NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX Must contain _SR.tif (or _SR_clip.tif) and *_metadata.json files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded PlanetScope dataset with the following data variables: - blue (float32): Blue band surface reflectance [0-1] - green (float32): Green band surface reflectance [0-1] - red (float32): Red band surface reflectance [0-1] - nir (float32): Near-infrared band surface reflectance [0-1]</p> <p>Each variable has attributes: - long_name: \"PLANET {Band}\" - units: \"Reflectance\" - data_source: \"planet\" - planet_type: \"scene\" or \"orthotile\"</p> <p>Dataset-level attributes: - azimuth (float): Solar azimuth angle in degrees - elevation (float): Solar elevation angle in degrees - tile_id (str): Unique identifier for the scene - planet_scene_id (str): Scene identifier (for scenes) or scene portion (for orthotiles) - planet_orthotile_id (str): Orthotile identifier (only for orthotiles)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If required TIFF or metadata files are not found in the directory.</p> </li> </ul> Note <ul> <li>Input DN values are divided by 10000 to convert to reflectance [0-1].</li> <li>The scene type (PSScene vs PSOrthoTile) is automatically detected from the directory name.</li> <li>Solar geometry is extracted from the metadata JSON file.</li> </ul> Example <p>Load a PlanetScope scene:</p> <pre><code>from darts_acquisition import load_planet_scene\n\n# Load scene data\nplanet_ds = load_planet_scene(\"/data/planet/20230615_123045_1234\")\n\n# Access bands\nndvi = (planet_ds.nir - planet_ds.red) / (planet_ds.nir + planet_ds.red)\n\n# Check solar geometry\nprint(f\"Solar azimuth: {planet_ds.azimuth}\")\nprint(f\"Solar elevation: {planet_ds.elevation}\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopwatch.f(\"Loading Planet scene\", printer=logger.debug)\ndef load_planet_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a PlanetScope satellite scene from GeoTIFF files.\n\n    This function loads PlanetScope surface reflectance data (PSScene or PSOrthoTile) from\n    a directory containing TIFF files and metadata. The scene type is automatically detected\n    from the directory name format.\n\n    Args:\n        fpath (str | Path): Path to the directory containing the PlanetScope scene data.\n            The directory must follow PlanetScope naming conventions:\n            - Scene: YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX\n            - Orthotile: NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n            Must contain *_SR.tif (or *_SR_clip.tif) and *_metadata.json files.\n\n    Returns:\n        xr.Dataset: The loaded PlanetScope dataset with the following data variables:\n            - blue (float32): Blue band surface reflectance [0-1]\n            - green (float32): Green band surface reflectance [0-1]\n            - red (float32): Red band surface reflectance [0-1]\n            - nir (float32): Near-infrared band surface reflectance [0-1]\n\n            Each variable has attributes:\n            - long_name: \"PLANET {Band}\"\n            - units: \"Reflectance\"\n            - data_source: \"planet\"\n            - planet_type: \"scene\" or \"orthotile\"\n\n            Dataset-level attributes:\n            - azimuth (float): Solar azimuth angle in degrees\n            - elevation (float): Solar elevation angle in degrees\n            - tile_id (str): Unique identifier for the scene\n            - planet_scene_id (str): Scene identifier (for scenes) or scene portion (for orthotiles)\n            - planet_orthotile_id (str): Orthotile identifier (only for orthotiles)\n\n    Raises:\n        FileNotFoundError: If required TIFF or metadata files are not found in the directory.\n\n    Note:\n        - Input DN values are divided by 10000 to convert to reflectance [0-1].\n        - The scene type (PSScene vs PSOrthoTile) is automatically detected from the directory name.\n        - Solar geometry is extracted from the metadata JSON file.\n\n    Example:\n        Load a PlanetScope scene:\n\n        ```python\n        from darts_acquisition import load_planet_scene\n\n        # Load scene data\n        planet_ds = load_planet_scene(\"/data/planet/20230615_123045_1234\")\n\n        # Access bands\n        ndvi = (planet_ds.nir - planet_ds.red) / (planet_ds.nir + planet_ds.red)\n\n        # Check solar geometry\n        print(f\"Solar azimuth: {planet_ds.azimuth}\")\n        print(f\"Solar elevation: {planet_ds.elevation}\")\n        ```\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    # Check if the directory contains a PSOrthoTile or PSScene\n    planet_type = parse_planet_type(fpath)\n    logger.debug(f\"Loading Planet PS {planet_type.capitalize()} from {fpath.resolve()}\")\n\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    ps_meta = next(fpath.glob(\"*_metadata.json\"), None)\n    if not ps_meta:\n        raise FileNotFoundError(\n            f\"No matching metadata JSON files found in {fpath.resolve()} (.glob('*_metadata.json'))\"\n        )\n    metadata = json.load(ps_meta.open())\n\n    # Define band names and corresponding indices\n    planet_da = xr.open_dataarray(ps_image)\n\n    # Divide by 10000 to get reflectance between 0 and 1\n    planet_da = planet_da.astype(\"float32\") / 10000.0\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_planet = planet_da.assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n    for var in bands:\n        ds_planet[var].attrs[\"long_name\"] = f\"PLANET {var.capitalize()}\"\n        ds_planet[var].attrs[\"units\"] = \"Reflectance\"\n\n    for var in ds_planet.data_vars:\n        ds_planet[var].attrs[\"data_source\"] = \"planet\"\n        ds_planet[var].attrs[\"planet_type\"] = planet_type\n\n    # Add sun and elevation from metadata\n    ds_planet.attrs[\"azimuth\"] = metadata.get(\"sun_azimuth\", float(\"nan\"))\n    ds_planet.attrs[\"elevation\"] = metadata.get(\"sun_elevation\", float(\"nan\"))\n\n    if planet_type == \"scene\":\n        ds_planet.attrs[\"tile_id\"] = fpath.stem\n        ds_planet.attrs[\"planet_scene_id\"] = fpath.stem\n    elif planet_type == \"orthotile\":\n        ds_planet.attrs[\"tile_id\"] = f\"{fpath.parent.stem}-{fpath.stem}\"\n        ds_planet.attrs[\"planet_orthotile_id\"] = fpath.parent.stem\n        ds_planet.attrs[\"planet_scene_id\"] = fpath.stem\n\n    return ds_planet\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_tcvis","title":"load_tcvis","text":"<pre><code>load_tcvis(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    buffer: int = 0,\n    offline: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load TCVIS (Tasseled Cap trends) for the given geobox, fetch new data from GEE if necessary.</p> <p>This function loads Tasseled Cap trend data from a local icechunk store. If <code>offline=False</code>, missing data will be automatically downloaded from Google Earth Engine and stored locally. The data contains temporal trends in brightness, greenness, and wetness derived from Landsat imagery.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which to load the data. Can be in any CRS.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>Path to the icechunk data directory (must have .icechunk suffix). This directory stores downloaded TCVIS data for faster consecutive access.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Buffer around the geobox in pixels. The buffer is applied in the TCVIS dataset's native CRS after reprojecting the input geobox. Defaults to 0.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only loads data already present in the local store without attempting any downloads. If False, missing data is downloaded from GEE. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The TCVIS dataset with the following data variables: - tc_brightness (float): Temporal trend in Tasseled Cap brightness component - tc_greenness (float): Temporal trend in Tasseled Cap greenness component - tc_wetness (float): Temporal trend in Tasseled Cap wetness component</p> <p>The dataset is in the TCVIS native CRS with the buffer applied. It is NOT automatically reprojected to match the input geobox's CRS.</p> </li> </ul> Note <p>The <code>offline</code> parameter controls data fetching behavior:</p> <ul> <li>When <code>offline=False</code>: Uses <code>smart_geocubes</code> accessor's <code>load()</code> method which automatically   downloads missing tiles from GEE and persists them to the icechunk store.</li> <li>When <code>offline=True</code>: Uses the accessor's <code>open_xarray()</code> method to open the existing store   and crops it to the requested region. Raises an error if data is missing.</li> </ul> <p>Variable naming: The original TCB_slope, TCG_slope, and TCW_slope variables are renamed to follow DARTS conventions (tc_brightness, tc_greenness, tc_wetness).</p> Example <p>Load TCVIS data aligned with optical imagery:</p> <pre><code>from darts_acquisition import load_tcvis\n\n# Assume \"optical\" is a loaded Sentinel-2 dataset\ntcvis = load_tcvis(\n    geobox=optical.odc.geobox,\n    data_dir=\"/data/tcvis.icechunk\",\n    buffer=0,\n    offline=False\n)\n\n# Reproject to match optical data's CRS and resolution\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopwatch.f(\"Loading TCVIS\", printer=logger.debug, print_kwargs=[\"data_dir\", \"buffer\", \"offline\"])\ndef load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    offline: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Load TCVIS (Tasseled Cap trends) for the given geobox, fetch new data from GEE if necessary.\n\n    This function loads Tasseled Cap trend data from a local icechunk store. If `offline=False`,\n    missing data will be automatically downloaded from Google Earth Engine and stored locally.\n    The data contains temporal trends in brightness, greenness, and wetness derived from\n    Landsat imagery.\n\n    Args:\n        geobox (GeoBox): The geobox for which to load the data. Can be in any CRS.\n        data_dir (Path | str): Path to the icechunk data directory (must have .icechunk suffix).\n            This directory stores downloaded TCVIS data for faster consecutive access.\n        buffer (int, optional): Buffer around the geobox in pixels. The buffer is applied in the\n            TCVIS dataset's native CRS after reprojecting the input geobox. Defaults to 0.\n        offline (bool, optional): If True, only loads data already present in the local store\n            without attempting any downloads. If False, missing data is downloaded from GEE.\n            Defaults to False.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset with the following data variables:\n            - tc_brightness (float): Temporal trend in Tasseled Cap brightness component\n            - tc_greenness (float): Temporal trend in Tasseled Cap greenness component\n            - tc_wetness (float): Temporal trend in Tasseled Cap wetness component\n\n            The dataset is in the TCVIS native CRS with the buffer applied.\n            It is NOT automatically reprojected to match the input geobox's CRS.\n\n    Note:\n        The `offline` parameter controls data fetching behavior:\n\n        - When `offline=False`: Uses `smart_geocubes` accessor's `load()` method which automatically\n          downloads missing tiles from GEE and persists them to the icechunk store.\n        - When `offline=True`: Uses the accessor's `open_xarray()` method to open the existing store\n          and crops it to the requested region. Raises an error if data is missing.\n\n        Variable naming: The original TCB_slope, TCG_slope, and TCW_slope variables are renamed\n        to follow DARTS conventions (tc_brightness, tc_greenness, tc_wetness).\n\n    Example:\n        Load TCVIS data aligned with optical imagery:\n\n        ```python\n        from darts_acquisition import load_tcvis\n\n        # Assume \"optical\" is a loaded Sentinel-2 dataset\n        tcvis = load_tcvis(\n            geobox=optical.odc.geobox,\n            data_dir=\"/data/tcvis.icechunk\",\n            buffer=0,\n            offline=False\n        )\n\n        # Reproject to match optical data's CRS and resolution\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"\n    assert \".icechunk\" == data_dir.suffix, f\"Data directory {data_dir} must have an .icechunk suffix!\"\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n\n    # We want to assume that the datacube is already created to be save in a multi-process environment\n    accessor.assert_created()\n\n    if not offline:\n        tcvis = accessor.load(geobox, buffer=buffer, persist=True)\n    else:\n        xrcube = accessor.open_xarray()\n        reference_geobox = geobox.to_crs(accessor.extent.crs, resolution=accessor.extent.resolution.x).pad(buffer)\n        tcvis = xrcube.odc.crop(reference_geobox.extent, apply_mask=False)\n        tcvis = tcvis.load()\n\n    # Rename to follow our conventions\n    tcvis = tcvis.rename_vars(\n        {\n            \"TCB_slope\": \"tc_brightness\",\n            \"TCG_slope\": \"tc_greenness\",\n            \"TCW_slope\": \"tc_wetness\",\n        }\n    )\n\n    return tcvis\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.match_cdse_s2_sr_scene_ids_from_geodataframe","title":"match_cdse_s2_sr_scene_ids_from_geodataframe","text":"<pre><code>match_cdse_s2_sr_scene_ids_from_geodataframe(\n    aoi: geopandas.GeoDataFrame,\n    day_range: int = 60,\n    max_cloud_cover: int = 20,\n    min_intersects: float = 0.7,\n    simplify_geometry: float\n    | typing.Literal[False] = False,\n    save_scores: pathlib.Path | None = None,\n) -&gt; dict[int, pystac.Item | None]\n</code></pre> <p>Match items from a GeoDataFrame with Sentinel-2 items from the STAC API based on a date range.</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The area of interest as a GeoDataFrame.</p> </li> <li> <code>day_range</code>               (<code>int</code>, default:                   <code>60</code> )           \u2013            <p>The number of days before and after the date to search for. Defaults to 60.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>The maximum cloud cover percentage. Defaults to 20.</p> </li> <li> <code>min_intersects</code>               (<code>float</code>, default:                   <code>0.7</code> )           \u2013            <p>The minimum intersection area ratio to consider a match. Defaults to 0.7.</p> </li> <li> <code>simplify_geometry</code>               (<code>float | typing.Literal[False]</code>, default:                   <code>False</code> )           \u2013            <p>If a float is provided, the geometry will be simplified using the <code>simplify</code> method of geopandas. If False, no simplification will be done. This may become useful for large / weird AOIs which are too large for the STAC API. Defaults to False.</p> </li> <li> <code>save_scores</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>If provided, the scores will be saved to this path as a Parquet file.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the 'date' column is not present or not of type datetime.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[int, pystac.Item | None]</code>           \u2013            <p>dict[int, Item | None]: A dictionary mapping each row to its best matching Sentinel-2 item. The keys are the indices of the rows in the GeoDataFrame, and the values are the matching Sentinel-2 items. If no matching item is found, the value will be None.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Matching Sentinel-2 scenes in CDSE from AOI\", printer=logger.debug)\ndef match_cdse_s2_sr_scene_ids_from_geodataframe(\n    aoi: gpd.GeoDataFrame,\n    day_range: int = 60,\n    max_cloud_cover: int = 20,\n    min_intersects: float = 0.7,\n    simplify_geometry: float | Literal[False] = False,\n    save_scores: Path | None = None,\n) -&gt; dict[int, Item | None]:\n    \"\"\"Match items from a GeoDataFrame with Sentinel-2 items from the STAC API based on a date range.\n\n    Args:\n        aoi (gpd.GeoDataFrame): The area of interest as a GeoDataFrame.\n        day_range (int): The number of days before and after the date to search for.\n            Defaults to 60.\n        max_cloud_cover (int, optional): The maximum cloud cover percentage. Defaults to 20.\n        min_intersects (float, optional): The minimum intersection area ratio to consider a match. Defaults to 0.7.\n        simplify_geometry (float | Literal[False], optional): If a float is provided, the geometry will be simplified\n            using the `simplify` method of geopandas. If False, no simplification will be done.\n            This may become useful for large / weird AOIs which are too large for the STAC API.\n            Defaults to False.\n        save_scores (Path | None, optional): If provided, the scores will be saved to this path as a Parquet file.\n\n    Raises:\n        ValueError: If the 'date' column is not present or not of type datetime.\n\n    Returns:\n        dict[int, Item | None]: A dictionary mapping each row to its best matching Sentinel-2 item.\n            The keys are the indices of the rows in the GeoDataFrame, and the values are the matching Sentinel-2 items.\n            If no matching item is found, the value will be None.\n\n    \"\"\"\n    # Check weather the \"date\" column is present and of type datetime\n    if \"date\" not in aoi.columns or not pd.api.types.is_datetime64_any_dtype(aoi[\"date\"]):\n        raise ValueError(\"The 'date' column must be present and of type datetime in the GeoDataFrame.\")\n\n    if simplify_geometry:\n        aoi = aoi.copy()\n        aoi[\"geometry\"] = aoi.geometry.simplify(simplify_geometry)\n\n    matches = {}\n    scores = []\n    for i, row in aoi.iterrows():\n        intersects = row.geometry.__geo_interface__\n        start_date = (row[\"date\"] - pd.Timedelta(days=day_range)).strftime(\"%Y-%m-%d\")\n        end_date = (row[\"date\"] + pd.Timedelta(days=day_range)).strftime(\"%Y-%m-%d\")\n        intersecting_items = search_cdse_s2_sr(\n            intersects=intersects,\n            start_date=start_date,\n            end_date=end_date,\n            max_cloud_cover=max_cloud_cover,\n        )\n        if not intersecting_items:\n            logger.info(f\"No Sentinel-2 items found for footprint #{i} in the date range {start_date} to {end_date}.\")\n            matches[i] = None\n            continue\n        intersecting_items_gdf = gpd.GeoDataFrame.from_features(\n            [item.to_dict() for item in intersecting_items.values()],\n            crs=\"EPSG:4326\",\n        )\n        intersecting_items_gdf[\"footprint_index\"] = i\n        intersecting_items_gdf[\"s2id\"] = list(intersecting_items.keys())\n        # Some item geometries might be invalid (probably because of the arctic circle)\n        # We will drop those items, since they cannot be used for intersection calculations\n        intersecting_items_gdf = intersecting_items_gdf[intersecting_items_gdf.geometry.is_valid]\n        if intersecting_items_gdf.empty:\n            logger.info(\n                f\"No valid Sentinel-2 items found for footprint #{i} in the date range {start_date} to {end_date}.\"\n            )\n            matches[i] = None\n            continue\n        # Get to UTM zone for better area calculations\n        utm_zone = intersecting_items_gdf.estimate_utm_crs()\n        # Calculate intersection area ratio\n        intersecting_items_gdf[\"intersection_area\"] = (\n            intersecting_items_gdf.intersection(row.geometry).to_crs(utm_zone).area\n        )\n        # We need a geodataframe containing only our wanted row, since to_crs is not available for a single row\n        intersecting_items_gdf[\"aoi_area\"] = aoi.loc[[i]].to_crs(utm_zone).iloc[0].geometry.area\n        intersecting_items_gdf[\"intersection_ratio\"] = (\n            intersecting_items_gdf[\"intersection_area\"] / intersecting_items_gdf[\"aoi_area\"]\n        )\n        # Filter items based on the minimum intersection ratio\n        max_intersection = intersecting_items_gdf[\"intersection_ratio\"].max()\n        intersecting_items_gdf = intersecting_items_gdf[intersecting_items_gdf[\"intersection_ratio\"] &gt;= min_intersects]\n        if intersecting_items_gdf.empty:\n            logger.info(\n                f\"No Sentinel-2 items found for {i} with sufficient intersection ratio \"\n                f\"({min_intersects}, maximum was {max_intersection:.4f})\"\n                f\" in the date range {start_date} to {end_date}.\"\n            )\n            matches[i] = None\n            continue\n        intersecting_items_gdf[\"datetime\"] = pd.to_datetime(intersecting_items_gdf[\"datetime\"])\n        intersecting_items_gdf[\"time_diff\"] = abs(intersecting_items_gdf[\"datetime\"] - row[\"date\"])\n        intersecting_items_gdf[\"score_cloud\"] = ((100.0 - intersecting_items_gdf[\"eo:cloud_cover\"]) / 5) ** 2\n        intersecting_items_gdf[\"score_fill\"] = ((100.0 - intersecting_items_gdf[\"intersection_ratio\"] * 100) / 5) ** 2\n        intersecting_items_gdf[\"score_time_diff\"] = (\n            intersecting_items_gdf[\"time_diff\"].dt.total_seconds() / (2 * 24 * 3600)\n        ) ** 2\n\n        intersecting_items_gdf[\"score\"] = (\n            intersecting_items_gdf[\"score_cloud\"]\n            + intersecting_items_gdf[\"score_fill\"]\n            + intersecting_items_gdf[\"score_time_diff\"]\n        )\n\n        # Debug the scoring\n        score_msg = f\"Scores for {i}:\\n\"\n        for j, match in intersecting_items_gdf.iterrows():\n            score_msg += (\n                f\"\\n- Match with {j}: \"\n                f\"Cloud Cover={match['eo:cloud_cover']}, \"\n                f\"Intersection Ratio={match['intersection_ratio']:.2f}, \"\n                f\"Time Diff={match['time_diff']}, \"\n                f\"Score Cloud={match['score_cloud']:.2f}, \"\n                f\"Score Fill={match['score_fill']:.2f}, \"\n                f\"Score Time Diff={match['score_time_diff']:.2f}, \"\n                f\"-&gt; Score={match['score']:.2f}\"\n            )\n        logger.debug(score_msg)\n\n        # Get the s2id with the lowest score\n        best_item = intersecting_items_gdf.loc[intersecting_items_gdf[\"score\"].idxmin()]\n        matches[i] = intersecting_items[best_item[\"s2id\"]]\n        scores.append(intersecting_items_gdf)\n\n    if save_scores:\n        scores_df = gpd.GeoDataFrame(pd.concat(scores))\n        scores_df.to_parquet(save_scores)\n\n    return matches\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.parse_planet_type","title":"parse_planet_type","text":"<pre><code>parse_planet_type(\n    fpath: pathlib.Path,\n) -&gt; typing.Literal[\"orthotile\", \"scene\"]\n</code></pre> <p>Parse the type of Planet data from the directory path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory path to the Planet data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['orthotile', 'scene']</code>           \u2013            <p>Literal[\"orthotile\", \"scene\"]: The type of Planet data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the Planet data type cannot be parsed from the file path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def parse_planet_type(fpath: Path) -&gt; Literal[\"orthotile\", \"scene\"]:\n    \"\"\"Parse the type of Planet data from the directory path.\n\n    Args:\n        fpath (Path): The directory path to the Planet data.\n\n    Returns:\n        Literal[\"orthotile\", \"scene\"]: The type of Planet data.\n\n    Raises:\n        ValueError: If the Planet data type cannot be parsed from the file path.\n\n    \"\"\"\n    # Cases for Scenes:\n    # - YYYYMMDD_HHMMSS_NN_XXXX\n    # - YYYYMMDD_HHMMSS_XXXX\n\n    # Cases for Orthotiles:\n    # NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n    # NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n\n    assert fpath.is_dir(), \"fpath must be the parent directory!\"\n\n    ps_name_parts = fpath.stem.split(\"_\")\n\n    if len(ps_name_parts) == 3:\n        # Must be scene or invalid\n        date, time, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and len(ident) == 4:\n            return \"scene\"\n\n    if len(ps_name_parts) == 4:\n        # Assume scene\n        date, time, n, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and n.isdigit() and len(ident) == 4:\n            return \"scene\"\n        # Is not scene, assume orthotile\n        chunkid, tileid, date, ident = ps_name_parts\n        if chunkid.isdigit() and tileid.isdigit() and _is_valid_date(date, \"%Y-%m-%d\") and len(ident) == 4:\n            return \"orthotile\"\n\n    raise ValueError(\n        f\"Could not parse Planet data type from {fpath}.\"\n        f\"Expected a format of YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX for scene, \"\n        \"or NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX or NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX for orthotile.\"\n        f\"Got {fpath.stem} instead.\"\n        \"Please ensure that the parent directory of the file is used, instead of the file itself.\"\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.search_cdse_s2_sr","title":"search_cdse_s2_sr","text":"<pre><code>search_cdse_s2_sr(\n    intersects=None,\n    tiles: list[str] | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n) -&gt; dict[str, pystac.Item]\n</code></pre> <p>Search for Sentinel-2 scenes via STAC based on an area of interest (intersects) and date range.</p> Note <p><code>start_date</code> and <code>end_date</code> will be concatted with a <code>/</code> to form a date range. Read more about the date format here: https://pystac-client.readthedocs.io/en/stable/api.html#pystac_client.Client.search</p> <p>Parameters:</p> <ul> <li> <code>intersects</code>               (<code>any</code>, default:                   <code>None</code> )           \u2013            <p>The geometry object to search for Sentinel-2 tiles. Can be anything implementing the <code>__geo_interface__</code> protocol, such as a GeoDataFrame or a shapely geometry. If None, and tiles is also None, the search will be performed globally. If set and tiles is also set, will be ignored.</p> </li> <li> <code>tiles</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of MGRS tile IDs to filter the search. If set, ignores intersects parameter. Defaults to None.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> <li> <code>months</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of months (1-12) to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> <li> <code>years</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of years to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, pystac.Item]</code>           \u2013            <p>dict[str, Item]: A dictionary of found Sentinel-2 items as values and the s2id as keys.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Searching for Sentinel-2 scenes in CDSE\", printer=logger.debug)\ndef search_cdse_s2_sr(\n    intersects=None,\n    tiles: list[str] | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n) -&gt; dict[str, Item]:\n    \"\"\"Search for Sentinel-2 scenes via STAC based on an area of interest (intersects) and date range.\n\n    Note:\n        `start_date` and `end_date` will be concatted with a `/` to form a date range.\n        Read more about the date format here: https://pystac-client.readthedocs.io/en/stable/api.html#pystac_client.Client.search\n\n    Args:\n        intersects (any): The geometry object to search for Sentinel-2 tiles.\n            Can be anything implementing the `__geo_interface__` protocol, such as a GeoDataFrame or a shapely geometry.\n            If None, and tiles is also None, the search will be performed globally.\n            If set and tiles is also set, will be ignored.\n        tiles (list[str] | None, optional): List of MGRS tile IDs to filter the search.\n            If set, ignores intersects parameter.\n            Defaults to None.\n        start_date (str): Starting date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n        months (list[int] | None, optional): List of months (1-12) to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n        years (list[int] | None, optional): List of years to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n\n    Returns:\n        dict[str, Item]: A dictionary of found Sentinel-2 items as values and the s2id as keys.\n\n    \"\"\"\n    catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n\n    if tiles is not None and intersects is not None:\n        logger.warning(\"Both tile and intersects provided. Ignoring intersects parameter.\")\n        intersects = None\n\n    cql2_filter = _build_cql2_filter(tiles, max_cloud_cover, max_snow_cover)\n\n    if start_date is not None and end_date is not None:\n        if months is not None or years is not None:\n            logger.warning(\"Both date range and months/years filtering provided. Ignoring months/years filter.\")\n        logger.debug(\n            f\"Searching CDSE for scenes between {start_date} and {end_date} ({cql2_filter}, {type(intersects)}).\"\n        )\n        search = catalog.search(\n            collections=[\"sentinel-2-l2a\"],\n            intersects=intersects,\n            datetime=f\"{start_date}/{end_date}\",\n            filter=cql2_filter,\n        )\n        found_items = list(search.items())\n    elif months is not None or years is not None:\n        if months is None:\n            months = list(range(1, 13))\n        if years is None:\n            years = list(range(2017, 2026))\n        found_items = set()\n        for year in years:\n            for month in months:\n                search = catalog.search(\n                    collections=[\"sentinel-2-l2a\"],\n                    intersects=intersects,\n                    datetime=f\"{year}-{month:02d}\",\n                    filter=cql2_filter,\n                )\n                found_items.update(list(search.items()))\n    else:\n        logger.warning(\"No valid date filtering provided. This may result in a too large number of scenes for CDSE.\")\n        search = catalog.search(\n            collections=[\"sentinel-2-l2a\"],\n            intersects=intersects,\n            filter=cql2_filter,\n        )\n        found_items = list(search.items())\n\n    if len(found_items) == 0:\n        logger.debug(\n            \"No Sentinel-2 items found for the given parameters:\"\n            f\" {intersects=}, {start_date=}, {end_date=}, {max_cloud_cover=}\"\n        )\n        return {}\n    logger.debug(f\"Found {len(found_items)} Sentinel-2 items in CDSE.\")\n    return {item.id: item for item in found_items}\n</code></pre>"},{"location":"reference/darts_acquisition/admin/","title":"admin","text":""},{"location":"reference/darts_acquisition/admin/#darts_acquisition.admin","title":"darts_acquisition.admin","text":"<p>Download of admin level files for the regions.</p>"},{"location":"reference/darts_acquisition/admin/#darts_acquisition.admin.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/admin/#darts_acquisition.admin._download_zip","title":"_download_zip","text":"<pre><code>_download_zip(url: str, admin_dir: pathlib.Path)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>@stopwatch.f(\"Downloading and extracting zip file\", printer=logger.debug)\ndef _download_zip(url: str, admin_dir: Path):\n    response = requests.get(url)\n\n    # Get the downloaded data as a byte string\n    data = response.content\n    logger.debug(f\"Downloaded {len(data)} bytes\")\n\n    # Create a bytesIO object\n    with io.BytesIO(data) as buffer:\n        # Create a zipfile.ZipFile object and extract the files to a directory\n        admin_dir.mkdir(parents=True, exist_ok=True)\n        with zipfile.ZipFile(buffer, \"r\") as zip_ref:\n            # Extract the files to the specified directory\n            zip_ref.extractall(admin_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/admin/#darts_acquisition.admin.download_admin_files","title":"download_admin_files","text":"<pre><code>download_admin_files(admin_dir: pathlib.Path)\n</code></pre> <p>Download the admin files for the regions.</p> <p>Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.</p> <p>Parameters:</p> <ul> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the admin files.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>@stopwatch.f(\"Downloading admin files\", printer=logger.debug)\ndef download_admin_files(admin_dir: Path):\n    \"\"\"Download the admin files for the regions.\n\n    Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.\n\n    Args:\n        admin_dir (Path): The path to the admin files.\n\n    \"\"\"\n    # Download the admin files\n    admin_1_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.zip\"\n    admin_2_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.zip\"\n\n    admin_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.debug(f\"Downloading {admin_1_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_1_url, admin_dir)\n\n    logger.debug(f\"Downloading {admin_2_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_2_url, admin_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/arcticdem/","title":"arcticdem","text":""},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem","title":"darts_acquisition.arcticdem","text":"<p>Downloading and loading related functions for the Zarr-Datacube approach.</p>"},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem.RESOLUTIONS","title":"RESOLUTIONS  <code>module-attribute</code>","text":"<pre><code>RESOLUTIONS = typing.Literal[2, 10, 32]\n</code></pre>"},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem._validate_and_get_accessor","title":"_validate_and_get_accessor","text":"<pre><code>_validate_and_get_accessor(\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n) -&gt; (\n    smart_geocubes.ArcticDEM2m\n    | smart_geocubes.ArcticDEM10m\n    | smart_geocubes.ArcticDEM32m\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>def _validate_and_get_accessor(\n    data_dir: Path | str,\n    resolution: RESOLUTIONS,\n) -&gt; smart_geocubes.ArcticDEM2m | smart_geocubes.ArcticDEM10m | smart_geocubes.ArcticDEM32m:\n    data_dir = Path(data_dir)\n    assert \".icechunk\" == data_dir.suffix, f\"Data directory {data_dir} must have an .icechunk suffix!\"\n\n    match resolution:\n        case 2:\n            assert \"2m\" in data_dir.stem and \"32m\" not in data_dir.stem, (\n                f\"Data directory {data_dir} must have a '2m' in the name!\"\n            )\n            accessor = smart_geocubes.ArcticDEM2m(data_dir)\n        case 10:\n            assert \"10m\" in data_dir.stem, f\"Data directory {data_dir} must have a '10m' in the name!\"\n            accessor = smart_geocubes.ArcticDEM10m(data_dir)\n        case 32:\n            assert \"32m\" in data_dir.stem, f\"Data directory {data_dir} must have a '32m' in the name!\"\n            accessor = smart_geocubes.ArcticDEM32m(data_dir)\n        case _:\n            raise ValueError(f\"Resolution {resolution} not supported, only 2m, 10m and 32m are supported\")\n    accessor.assert_created()\n    return accessor\n</code></pre>"},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem.download_arcticdem","title":"download_arcticdem","text":"<pre><code>download_arcticdem(\n    aoi: geopandas.GeoDataFrame,\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n) -&gt; None\n</code></pre> <p>Download ArcticDEM data for the specified area of interest.</p> <p>This function downloads ArcticDEM elevation tiles from AWS S3 for the given area of interest and stores them in a local icechunk data store for efficient access.</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>Area of interest for which to download ArcticDEM data. Can be in any CRS; will be reprojected to EPSG:3413 (ArcticDEM's native CRS).</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>Path to the icechunk data directory (must have .icechunk suffix). Must contain the resolution in the name (e.g., \"arcticdem_2m.icechunk\").</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in meters. Must match the resolution indicated in the data_dir name.</p> </li> </ul> Note <p>This function automatically configures AWS access with unsigned requests to the public ArcticDEM S3 bucket. No AWS credentials are required.</p> Example <p>Download ArcticDEM for a study area:</p> <pre><code>import geopandas as gpd\nfrom shapely.geometry import box\nfrom darts_acquisition import download_arcticdem\n\n# Define area of interest\naoi = gpd.GeoDataFrame(\n    geometry=[box(-50, 70, -49, 71)],\n    crs=\"EPSG:4326\"\n)\n\n# Download 2m resolution ArcticDEM\ndownload_arcticdem(\n    aoi=aoi,\n    data_dir=\"/data/arcticdem_2m.icechunk\",\n    resolution=2\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopwatch.f(\"Downloading ArcticDEM\", printer=logger.debug, print_kwargs=[\"data_dir\", \"resolution\"])\ndef download_arcticdem(\n    aoi: gpd.GeoDataFrame,\n    data_dir: Path | str,\n    resolution: RESOLUTIONS,\n) -&gt; None:\n    \"\"\"Download ArcticDEM data for the specified area of interest.\n\n    This function downloads ArcticDEM elevation tiles from AWS S3 for the given area\n    of interest and stores them in a local icechunk data store for efficient access.\n\n    Args:\n        aoi (gpd.GeoDataFrame): Area of interest for which to download ArcticDEM data.\n            Can be in any CRS; will be reprojected to EPSG:3413 (ArcticDEM's native CRS).\n        data_dir (Path | str): Path to the icechunk data directory (must have .icechunk suffix).\n            Must contain the resolution in the name (e.g., \"arcticdem_2m.icechunk\").\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in meters.\n            Must match the resolution indicated in the data_dir name.\n\n    Note:\n        This function automatically configures AWS access with unsigned requests to the\n        public ArcticDEM S3 bucket. No AWS credentials are required.\n\n    Example:\n        Download ArcticDEM for a study area:\n\n        ```python\n        import geopandas as gpd\n        from shapely.geometry import box\n        from darts_acquisition import download_arcticdem\n\n        # Define area of interest\n        aoi = gpd.GeoDataFrame(\n            geometry=[box(-50, 70, -49, 71)],\n            crs=\"EPSG:4326\"\n        )\n\n        # Download 2m resolution ArcticDEM\n        download_arcticdem(\n            aoi=aoi,\n            data_dir=\"/data/arcticdem_2m.icechunk\",\n            resolution=2\n        )\n        ```\n\n    \"\"\"\n    odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    accessor = _validate_and_get_accessor(data_dir, resolution)\n    accessor.download(aoi)\n</code></pre>"},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem.load_arcticdem","title":"load_arcticdem","text":"<pre><code>load_arcticdem(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n    buffer: int = 0,\n    offline: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>This function loads ArcticDEM elevation data from a local icechunk store. If <code>offline=False</code>, missing data will be automatically downloaded from the AWS-hosted STAC server and stored locally for future use. The loaded data is returned in the ArcticDEM's native CRS (EPSG:3413).</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which the tile should be loaded. Must be in a meter-based CRS.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>Path to the icechunk data directory (must have .icechunk suffix). This directory stores downloaded ArcticDEM data for faster consecutive access.</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in meters. Must match the resolution indicated in the data_dir name (e.g., \"arcticdem_2m.icechunk\").</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Buffer around the geobox in pixels. The buffer is applied in the ArcticDEM's native CRS (EPSG:3413) after reprojecting the input geobox. Useful for edge effect removal in terrain analysis. Defaults to 0.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only loads data already present in the local store without attempting any downloads. If False, missing data is downloaded from AWS. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The ArcticDEM dataset with the following data variables: - dem (float32): Elevation values in meters, clipped to [-100, 3000] range - arcticdem_data_mask (uint8): Data validity mask (1=valid, 0=invalid)</p> <p>The dataset is in the ArcticDEM's native CRS (EPSG:3413) with the buffer applied. It is NOT automatically reprojected to match the input geobox's CRS and resolution.</p> </li> </ul> Note <p>The <code>offline</code> parameter controls data fetching behavior:</p> <ul> <li>When <code>offline=False</code>: Uses <code>smart_geocubes</code> accessor's <code>load()</code> method which automatically   downloads missing tiles from AWS and persists them to the icechunk store.</li> <li>When <code>offline=True</code>: Uses the accessor's <code>open_xarray()</code> method to open the existing store   and crops it to the requested region. Raises an error if data is missing.</li> </ul> Warning <ul> <li>The input geobox must be in a meter-based CRS.</li> <li>The data_dir must have an <code>.icechunk</code> suffix and contain the resolution in the name.</li> <li>The returned dataset is in EPSG:3413, not the input geobox's CRS.</li> </ul> Example <p>Load ArcticDEM with a buffer for terrain analysis:</p> <pre><code>from math import ceil, sqrt\nfrom darts_acquisition import load_arcticdem\n\n# Assume \"optical\" is a loaded Sentinel-2 dataset\narcticdem = load_arcticdem(\n    geobox=optical.odc.geobox,\n    data_dir=\"/data/arcticdem_2m.icechunk\",\n    resolution=2,\n    buffer=ceil(128 / 2 * sqrt(2)),  # Buffer for TPI with 128m radius\n    offline=False\n)\n\n# Reproject to match optical data's CRS and resolution\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopwatch.f(\"Loading ArcticDEM\", printer=logger.debug, print_kwargs=[\"data_dir\", \"resolution\", \"buffer\", \"offline\"])\ndef load_arcticdem(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    resolution: RESOLUTIONS,\n    buffer: int = 0,\n    offline: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    This function loads ArcticDEM elevation data from a local icechunk store. If `offline=False`,\n    missing data will be automatically downloaded from the AWS-hosted STAC server and stored\n    locally for future use. The loaded data is returned in the ArcticDEM's native CRS (EPSG:3413).\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded. Must be in a meter-based CRS.\n        data_dir (Path | str): Path to the icechunk data directory (must have .icechunk suffix).\n            This directory stores downloaded ArcticDEM data for faster consecutive access.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in meters.\n            Must match the resolution indicated in the data_dir name (e.g., \"arcticdem_2m.icechunk\").\n        buffer (int, optional): Buffer around the geobox in pixels. The buffer is applied in the\n            ArcticDEM's native CRS (EPSG:3413) after reprojecting the input geobox. Useful for\n            edge effect removal in terrain analysis. Defaults to 0.\n        offline (bool, optional): If True, only loads data already present in the local store\n            without attempting any downloads. If False, missing data is downloaded from AWS.\n            Defaults to False.\n\n    Returns:\n        xr.Dataset: The ArcticDEM dataset with the following data variables:\n            - dem (float32): Elevation values in meters, clipped to [-100, 3000] range\n            - arcticdem_data_mask (uint8): Data validity mask (1=valid, 0=invalid)\n\n            The dataset is in the ArcticDEM's native CRS (EPSG:3413) with the buffer applied.\n            It is NOT automatically reprojected to match the input geobox's CRS and resolution.\n\n    Note:\n        The `offline` parameter controls data fetching behavior:\n\n        - When `offline=False`: Uses `smart_geocubes` accessor's `load()` method which automatically\n          downloads missing tiles from AWS and persists them to the icechunk store.\n        - When `offline=True`: Uses the accessor's `open_xarray()` method to open the existing store\n          and crops it to the requested region. Raises an error if data is missing.\n\n    Warning:\n        - The input geobox must be in a meter-based CRS.\n        - The data_dir must have an `.icechunk` suffix and contain the resolution in the name.\n        - The returned dataset is in EPSG:3413, not the input geobox's CRS.\n\n    Example:\n        Load ArcticDEM with a buffer for terrain analysis:\n\n        ```python\n        from math import ceil, sqrt\n        from darts_acquisition import load_arcticdem\n\n        # Assume \"optical\" is a loaded Sentinel-2 dataset\n        arcticdem = load_arcticdem(\n            geobox=optical.odc.geobox,\n            data_dir=\"/data/arcticdem_2m.icechunk\",\n            resolution=2,\n            buffer=ceil(128 / 2 * sqrt(2)),  # Buffer for TPI with 128m radius\n            offline=False\n        )\n\n        # Reproject to match optical data's CRS and resolution\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"\n    if not offline:\n        odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n\n    accessor = _validate_and_get_accessor(data_dir, resolution)\n\n    if not offline:\n        arcticdem = accessor.load(geobox, buffer=buffer, persist=True)\n    else:\n        xrcube = accessor.open_xarray()\n        reference_geobox = geobox.to_crs(accessor.extent.crs, resolution=accessor.extent.resolution.x).pad(buffer)\n        arcticdem = xrcube.odc.crop(reference_geobox.extent, apply_mask=False)\n        arcticdem = arcticdem.load()\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem[\"arcticdem_data_mask\"] = arcticdem.datamask.astype(\"uint8\")\n\n    # Clip values to -100, 3000 range (see docs about bands)\n    arcticdem[\"dem\"] = arcticdem[\"dem\"].clip(-100, 3000)\n\n    # Change dtype of arcticdem to float32 to save memory (original is float64)\n    arcticdem[\"dem\"] = arcticdem[\"dem\"].astype(\"float32\")\n\n    return arcticdem\n</code></pre>"},{"location":"reference/darts_acquisition/planet/","title":"planet","text":""},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet","title":"darts_acquisition.planet","text":"<p>PLANET related data loading. Should be used temporary and maybe moved to the acquisition package.</p>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet._is_valid_date","title":"_is_valid_date","text":"<pre><code>_is_valid_date(date_str: str, format: str) -&gt; bool\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def _is_valid_date(date_str: str, format: str) -&gt; bool:\n    try:\n        datetime.strptime(date_str, format)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet.get_planet_geometry","title":"get_planet_geometry","text":"<pre><code>get_planet_geometry(\n    fpath: str | pathlib.Path,\n) -&gt; odc.geo.Geometry\n</code></pre> <p>Get the geometry of a Planet scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The file path to the Planet scene from which to derive the geometry.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>odc.geo.Geometry</code>           \u2013            <p>odc.geo.Geometry: The geometry of the Planet scene.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def get_planet_geometry(fpath: str | Path) -&gt; odc.geo.Geometry:\n    \"\"\"Get the geometry of a Planet scene.\n\n    Args:\n        fpath (str | Path): The file path to the Planet scene from which to derive the geometry.\n\n    Returns:\n        odc.geo.Geometry: The geometry of the Planet scene.\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    planet_raster = rasterio.open(ps_image)\n    return odc.geo.BoundingBox(*planet_raster.bounds, crs=planet_raster.crs)\n</code></pre>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet.load_planet_masks","title":"load_planet_masks","text":"<pre><code>load_planet_masks(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load quality and validity masks from a PlanetScope scene's UDM-2 data.</p> <p>This function extracts data quality information from the PlanetScope Usable Data Mask (UDM-2) to create simplified quality masks for filtering and analysis.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>Path to the directory containing the PlanetScope scene data. Must contain _udm2.tif (or _udm2_clip.tif) file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Dataset containing quality mask information with the following data variables: - quality_data_mask (uint8): Combined quality indicator     * 0 = Invalid (no data)     * 1 = Low quality (clouds, shadows, haze, snow, or other artifacts)     * 2 = High quality (clear, usable data)   Attributes: data_source=\"planet\", long_name=\"Quality data mask\",   description=\"0 = Invalid, 1 = Low Quality, 2 = High Quality\" - planet_udm (uint8): Raw UDM-2 bands (8 bands)   Attributes: data_source=\"planet\", long_name=\"Planet UDM\",   description=\"Usable Data Mask\"</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If the UDM-2 TIFF file is not found in the directory.</p> </li> </ul> Note <p>Quality mask derivation logic: - Invalid: UDM band 8 (no data) is set - Low quality: Any of UDM bands 2-6 (clouds, shadows, haze, snow, or artifacts) is set - High quality: Neither invalid nor low quality</p> <p>UDM-2 band definitions: 1. Clear - 2. Snow - 3. Shadow - 4. Light Haze - 5. Heavy Haze 6. Cloud - 7. Confidence - 8. No Data</p> Example <p>Load and apply quality masks:</p> <pre><code>from darts_acquisition import load_planet_scene, load_planet_masks\n\n# Load scene and masks\nscene = load_planet_scene(\"/data/planet/20230615_123045_1234\")\nmasks = load_planet_masks(\"/data/planet/20230615_123045_1234\")\n\n# Filter to high quality pixels only\nscene_filtered = scene.where(masks.quality_data_mask == 2)\n\n# Count quality distribution\nimport numpy as np\nunique, counts = np.unique(\n    masks.quality_data_mask.values,\n    return_counts=True\n)\nprint(dict(zip(unique, counts)))\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopwatch.f(\"Loading Planet masks\", printer=logger.debug)\ndef load_planet_masks(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load quality and validity masks from a PlanetScope scene's UDM-2 data.\n\n    This function extracts data quality information from the PlanetScope Usable Data Mask\n    (UDM-2) to create simplified quality masks for filtering and analysis.\n\n    Args:\n        fpath (str | Path): Path to the directory containing the PlanetScope scene data.\n            Must contain *_udm2.tif (or *_udm2_clip.tif) file.\n\n    Returns:\n        xr.Dataset: Dataset containing quality mask information with the following data variables:\n            - quality_data_mask (uint8): Combined quality indicator\n                * 0 = Invalid (no data)\n                * 1 = Low quality (clouds, shadows, haze, snow, or other artifacts)\n                * 2 = High quality (clear, usable data)\n              Attributes: data_source=\"planet\", long_name=\"Quality data mask\",\n              description=\"0 = Invalid, 1 = Low Quality, 2 = High Quality\"\n            - planet_udm (uint8): Raw UDM-2 bands (8 bands)\n              Attributes: data_source=\"planet\", long_name=\"Planet UDM\",\n              description=\"Usable Data Mask\"\n\n    Raises:\n        FileNotFoundError: If the UDM-2 TIFF file is not found in the directory.\n\n    Note:\n        Quality mask derivation logic:\n        - Invalid: UDM band 8 (no data) is set\n        - Low quality: Any of UDM bands 2-6 (clouds, shadows, haze, snow, or artifacts) is set\n        - High quality: Neither invalid nor low quality\n\n        UDM-2 band definitions:\n        1. Clear - 2. Snow - 3. Shadow - 4. Light Haze - 5. Heavy Haze\n        6. Cloud - 7. Confidence - 8. No Data\n\n    Example:\n        Load and apply quality masks:\n\n        ```python\n        from darts_acquisition import load_planet_scene, load_planet_masks\n\n        # Load scene and masks\n        scene = load_planet_scene(\"/data/planet/20230615_123045_1234\")\n        masks = load_planet_masks(\"/data/planet/20230615_123045_1234\")\n\n        # Filter to high quality pixels only\n        scene_filtered = scene.where(masks.quality_data_mask == 2)\n\n        # Count quality distribution\n        import numpy as np\n        unique, counts = np.unique(\n            masks.quality_data_mask.values,\n            return_counts=True\n        )\n        print(dict(zip(unique, counts)))\n        ```\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # Get imagepath\n    udm_path = next(fpath.glob(\"*_udm2.tif\"), None)\n    if not udm_path:\n        udm_path = next(fpath.glob(\"*_udm2_clip.tif\"), None)\n    if not udm_path:\n        raise FileNotFoundError(f\"No matching UDM-2 TIFF files found in {fpath.resolve()} (.glob('*_udm2.tif'))\")\n\n    # See udm classes here: https://developers.planet.com/docs/data/udm-2/\n    da_udm = xr.open_dataarray(udm_path).astype(\"uint8\")\n    invalids = da_udm.sel(band=8).fillna(0) != 0\n    low_quality = da_udm.sel(band=[2, 3, 4, 5, 6]).max(axis=0) == 1\n    high_quality = ~low_quality &amp; ~invalids\n    qa_ds = (\n        xr.where(high_quality, 2, 0)\n        .where(~low_quality, 1)\n        .where(~invalids, 0)\n        .astype(\"uint8\")\n        .to_dataset(name=\"quality_data_mask\")\n        .drop_vars(\"band\")\n    )\n    qa_ds[\"planet_udm\"] = da_udm\n\n    qa_ds[\"quality_data_mask\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Quality data mask\",\n        \"description\": \"0 = Invalid, 1 = Low Quality, 2 = High Quality\",\n    }\n    qa_ds[\"planet_udm\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Planet UDM\",\n        \"description\": \"Usable Data Mask\",\n    }\n\n    return qa_ds\n</code></pre>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet.load_planet_scene","title":"load_planet_scene","text":"<pre><code>load_planet_scene(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a PlanetScope satellite scene from GeoTIFF files.</p> <p>This function loads PlanetScope surface reflectance data (PSScene or PSOrthoTile) from a directory containing TIFF files and metadata. The scene type is automatically detected from the directory name format.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>Path to the directory containing the PlanetScope scene data. The directory must follow PlanetScope naming conventions: - Scene: YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX - Orthotile: NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX Must contain _SR.tif (or _SR_clip.tif) and *_metadata.json files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded PlanetScope dataset with the following data variables: - blue (float32): Blue band surface reflectance [0-1] - green (float32): Green band surface reflectance [0-1] - red (float32): Red band surface reflectance [0-1] - nir (float32): Near-infrared band surface reflectance [0-1]</p> <p>Each variable has attributes: - long_name: \"PLANET {Band}\" - units: \"Reflectance\" - data_source: \"planet\" - planet_type: \"scene\" or \"orthotile\"</p> <p>Dataset-level attributes: - azimuth (float): Solar azimuth angle in degrees - elevation (float): Solar elevation angle in degrees - tile_id (str): Unique identifier for the scene - planet_scene_id (str): Scene identifier (for scenes) or scene portion (for orthotiles) - planet_orthotile_id (str): Orthotile identifier (only for orthotiles)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If required TIFF or metadata files are not found in the directory.</p> </li> </ul> Note <ul> <li>Input DN values are divided by 10000 to convert to reflectance [0-1].</li> <li>The scene type (PSScene vs PSOrthoTile) is automatically detected from the directory name.</li> <li>Solar geometry is extracted from the metadata JSON file.</li> </ul> Example <p>Load a PlanetScope scene:</p> <pre><code>from darts_acquisition import load_planet_scene\n\n# Load scene data\nplanet_ds = load_planet_scene(\"/data/planet/20230615_123045_1234\")\n\n# Access bands\nndvi = (planet_ds.nir - planet_ds.red) / (planet_ds.nir + planet_ds.red)\n\n# Check solar geometry\nprint(f\"Solar azimuth: {planet_ds.azimuth}\")\nprint(f\"Solar elevation: {planet_ds.elevation}\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopwatch.f(\"Loading Planet scene\", printer=logger.debug)\ndef load_planet_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a PlanetScope satellite scene from GeoTIFF files.\n\n    This function loads PlanetScope surface reflectance data (PSScene or PSOrthoTile) from\n    a directory containing TIFF files and metadata. The scene type is automatically detected\n    from the directory name format.\n\n    Args:\n        fpath (str | Path): Path to the directory containing the PlanetScope scene data.\n            The directory must follow PlanetScope naming conventions:\n            - Scene: YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX\n            - Orthotile: NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n            Must contain *_SR.tif (or *_SR_clip.tif) and *_metadata.json files.\n\n    Returns:\n        xr.Dataset: The loaded PlanetScope dataset with the following data variables:\n            - blue (float32): Blue band surface reflectance [0-1]\n            - green (float32): Green band surface reflectance [0-1]\n            - red (float32): Red band surface reflectance [0-1]\n            - nir (float32): Near-infrared band surface reflectance [0-1]\n\n            Each variable has attributes:\n            - long_name: \"PLANET {Band}\"\n            - units: \"Reflectance\"\n            - data_source: \"planet\"\n            - planet_type: \"scene\" or \"orthotile\"\n\n            Dataset-level attributes:\n            - azimuth (float): Solar azimuth angle in degrees\n            - elevation (float): Solar elevation angle in degrees\n            - tile_id (str): Unique identifier for the scene\n            - planet_scene_id (str): Scene identifier (for scenes) or scene portion (for orthotiles)\n            - planet_orthotile_id (str): Orthotile identifier (only for orthotiles)\n\n    Raises:\n        FileNotFoundError: If required TIFF or metadata files are not found in the directory.\n\n    Note:\n        - Input DN values are divided by 10000 to convert to reflectance [0-1].\n        - The scene type (PSScene vs PSOrthoTile) is automatically detected from the directory name.\n        - Solar geometry is extracted from the metadata JSON file.\n\n    Example:\n        Load a PlanetScope scene:\n\n        ```python\n        from darts_acquisition import load_planet_scene\n\n        # Load scene data\n        planet_ds = load_planet_scene(\"/data/planet/20230615_123045_1234\")\n\n        # Access bands\n        ndvi = (planet_ds.nir - planet_ds.red) / (planet_ds.nir + planet_ds.red)\n\n        # Check solar geometry\n        print(f\"Solar azimuth: {planet_ds.azimuth}\")\n        print(f\"Solar elevation: {planet_ds.elevation}\")\n        ```\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    # Check if the directory contains a PSOrthoTile or PSScene\n    planet_type = parse_planet_type(fpath)\n    logger.debug(f\"Loading Planet PS {planet_type.capitalize()} from {fpath.resolve()}\")\n\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    ps_meta = next(fpath.glob(\"*_metadata.json\"), None)\n    if not ps_meta:\n        raise FileNotFoundError(\n            f\"No matching metadata JSON files found in {fpath.resolve()} (.glob('*_metadata.json'))\"\n        )\n    metadata = json.load(ps_meta.open())\n\n    # Define band names and corresponding indices\n    planet_da = xr.open_dataarray(ps_image)\n\n    # Divide by 10000 to get reflectance between 0 and 1\n    planet_da = planet_da.astype(\"float32\") / 10000.0\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_planet = planet_da.assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n    for var in bands:\n        ds_planet[var].attrs[\"long_name\"] = f\"PLANET {var.capitalize()}\"\n        ds_planet[var].attrs[\"units\"] = \"Reflectance\"\n\n    for var in ds_planet.data_vars:\n        ds_planet[var].attrs[\"data_source\"] = \"planet\"\n        ds_planet[var].attrs[\"planet_type\"] = planet_type\n\n    # Add sun and elevation from metadata\n    ds_planet.attrs[\"azimuth\"] = metadata.get(\"sun_azimuth\", float(\"nan\"))\n    ds_planet.attrs[\"elevation\"] = metadata.get(\"sun_elevation\", float(\"nan\"))\n\n    if planet_type == \"scene\":\n        ds_planet.attrs[\"tile_id\"] = fpath.stem\n        ds_planet.attrs[\"planet_scene_id\"] = fpath.stem\n    elif planet_type == \"orthotile\":\n        ds_planet.attrs[\"tile_id\"] = f\"{fpath.parent.stem}-{fpath.stem}\"\n        ds_planet.attrs[\"planet_orthotile_id\"] = fpath.parent.stem\n        ds_planet.attrs[\"planet_scene_id\"] = fpath.stem\n\n    return ds_planet\n</code></pre>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet.parse_planet_type","title":"parse_planet_type","text":"<pre><code>parse_planet_type(\n    fpath: pathlib.Path,\n) -&gt; typing.Literal[\"orthotile\", \"scene\"]\n</code></pre> <p>Parse the type of Planet data from the directory path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory path to the Planet data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['orthotile', 'scene']</code>           \u2013            <p>Literal[\"orthotile\", \"scene\"]: The type of Planet data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the Planet data type cannot be parsed from the file path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def parse_planet_type(fpath: Path) -&gt; Literal[\"orthotile\", \"scene\"]:\n    \"\"\"Parse the type of Planet data from the directory path.\n\n    Args:\n        fpath (Path): The directory path to the Planet data.\n\n    Returns:\n        Literal[\"orthotile\", \"scene\"]: The type of Planet data.\n\n    Raises:\n        ValueError: If the Planet data type cannot be parsed from the file path.\n\n    \"\"\"\n    # Cases for Scenes:\n    # - YYYYMMDD_HHMMSS_NN_XXXX\n    # - YYYYMMDD_HHMMSS_XXXX\n\n    # Cases for Orthotiles:\n    # NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n    # NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n\n    assert fpath.is_dir(), \"fpath must be the parent directory!\"\n\n    ps_name_parts = fpath.stem.split(\"_\")\n\n    if len(ps_name_parts) == 3:\n        # Must be scene or invalid\n        date, time, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and len(ident) == 4:\n            return \"scene\"\n\n    if len(ps_name_parts) == 4:\n        # Assume scene\n        date, time, n, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and n.isdigit() and len(ident) == 4:\n            return \"scene\"\n        # Is not scene, assume orthotile\n        chunkid, tileid, date, ident = ps_name_parts\n        if chunkid.isdigit() and tileid.isdigit() and _is_valid_date(date, \"%Y-%m-%d\") and len(ident) == 4:\n            return \"orthotile\"\n\n    raise ValueError(\n        f\"Could not parse Planet data type from {fpath}.\"\n        f\"Expected a format of YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX for scene, \"\n        \"or NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX or NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX for orthotile.\"\n        f\"Got {fpath.stem} instead.\"\n        \"Please ensure that the parent directory of the file is used, instead of the file itself.\"\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/s2/","title":"s2","text":""},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2","title":"darts_acquisition.s2","text":"<p>Sentinel 2 acquisition functions.</p>"},{"location":"reference/darts_acquisition/s2/cdse_scene/","title":"cdse_scene","text":""},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene","title":"darts_acquisition.s2.cdse_scene","text":"<p>Sentinel-2 related data loading. Should be used temporary and maybe moved to the acquisition package.</p>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager","title":"CDSEStoreManager","text":"<pre><code>CDSEStoreManager(\n    store: pathlib.Path | str | None,\n    bands_mapping: dict[str, str],\n    aws_profile_name: str,\n)\n</code></pre> <p>               Bases: <code>darts_acquisition.s2.raw_data_store.StoreManager[pystac.Item]</code></p> <p>Raw Data Store manager for CDSE.</p> <p>Initialize the store manager.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>str | pathlib.Path | None</code>)           \u2013            <p>Directory path for storing raw sentinel 2 data</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>)           \u2013            <p>A mapping from bands to obtain.</p> </li> <li> <code>aws_profile_name</code>               (<code>str</code>)           \u2013            <p>AWS profile name for authentication</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>def __init__(self, store: Path | str | None, bands_mapping: dict[str, str], aws_profile_name: str):\n    \"\"\"Initialize the store manager.\n\n    Args:\n        store (str | Path | None): Directory path for storing raw sentinel 2 data\n        bands_mapping (dict[str, str]): A mapping from bands to obtain.\n        aws_profile_name (str): AWS profile name for authentication\n\n    \"\"\"\n    bands = list(bands_mapping.keys())\n    super().__init__(bands, store)\n    self.aws_profile_name = aws_profile_name\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.aws_profile_name","title":"aws_profile_name  <code>instance-attribute</code>","text":"<pre><code>aws_profile_name = (\n    darts_acquisition.s2.cdse_scene.CDSEStoreManager(\n        aws_profile_name\n    )\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands = darts_acquisition.s2.raw_data_store.StoreManager(\n    bands\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.store","title":"store  <code>instance-attribute</code>","text":"<pre><code>store = (\n    pathlib.Path(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        )\n    )\n    if isinstance(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        ),\n        str,\n    )\n    else darts_acquisition.s2.raw_data_store.StoreManager(\n        store\n    )\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.complete","title":"complete","text":"<pre><code>complete(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene in the store contains all requested bands.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if all requested bands are present, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def complete(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene in the store contains all requested bands.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if all requested bands are present, False otherwise\n\n    \"\"\"\n    return len(self.missing_bands(identifier)) == 0\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.download_and_store","title":"download_and_store","text":"<pre><code>download_and_store(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n)\n</code></pre> <p>Download a scene from the source and store it in the local store.</p> <p>Store must be provided! Will do nothing if all required bands are already present.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def download_and_store(self, item: str | SceneItem):\n    \"\"\"Download a scene from the source and store it in the local store.\n\n    Store must be provided!\n    Will do nothing if all required bands are already present.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to download and store scenes!\"\n    identifier = self.identifier(item)\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        return\n    dataset = self.download_scene_from_source(item, missing_bands)\n    self.save_to_store(dataset, identifier)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.download_scene_from_source","title":"download_scene_from_source","text":"<pre><code>download_scene_from_source(\n    s2item: str | pystac.Item, bands: list[str]\n) -&gt; xarray.Dataset\n</code></pre> <p>Download a Sentinel-2 scene from CDSE via STAC API.</p> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | pystac.Item</code>)           \u2013            <p>The Sentinel-2 image ID or the corresponing STAC Item.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>List of bands to download.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The downloaded scene as xarray Dataset.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>def download_scene_from_source(self, s2item: str | Item, bands: list[str]) -&gt; xr.Dataset:\n    \"\"\"Download a Sentinel-2 scene from CDSE via STAC API.\n\n    Args:\n        s2item (str | Item): The Sentinel-2 image ID or the corresponing STAC Item.\n        bands (list[str]): List of bands to download.\n\n    Returns:\n        xr.Dataset: The downloaded scene as xarray Dataset.\n\n    \"\"\"\n    s2id = s2item.id if isinstance(s2item, Item) else s2item\n\n    if isinstance(s2item, str):\n        catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n        search = catalog.search(\n            collections=[\"sentinel-2-l2a\"],\n            ids=[s2id],\n        )\n        s2item = next(search.items())\n\n    with stopwatch(\"Downloading data from CDSE\", printer=logger.debug):\n        # We can't use xpystac here, because they enforce chunking of 1024x1024, which results in long loading times\n        # and a potential AWS limit error.\n        init_copernicus(profile_name=self.aws_profile_name)\n        ds_s2 = stac_load(\n            [s2item],\n            bands=bands,\n            crs=\"utm\",\n            resolution=10,\n            resampling=\"nearest\",  # is used as default, but lets be sure\n        )\n\n    ds_s2.attrs = _flatten_dict(s2item.properties)\n    # Convert boolean values to int, since they are not supported in netcdf\n    # Also convert array, dicts and np types to str\n    for key, value in ds_s2.attrs.items():\n        if isinstance(value, bool):\n            ds_s2.attrs[key] = int(value)\n        elif isinstance(value, (list, dict, np.ndarray)):\n            ds_s2.attrs[key] = str(value)\n    ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n    ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\")\n\n    # Because of the resampling to 10m, the SCL is a float -&gt; fix it\n    if \"SCL_20m\" in ds_s2.data_vars:\n        ds_s2[\"SCL_20m\"] = ds_s2[\"SCL_20m\"].astype(\"uint8\")\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.encodings","title":"encodings","text":"<pre><code>encodings(bands: list[str]) -&gt; dict[str, dict[str, str]]\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>def encodings(self, bands: list[str]) -&gt; dict[str, dict[str, str]]:  # noqa: D102\n    encodings = {\n        band: {\n            \"dtype\": \"uint16\",\n            \"compressors\": BloscCodec(cname=\"zstd\", clevel=3, shuffle=\"bitshuffle\"),\n            \"chunks\": (4096, 4096),\n        }\n        for band in bands\n    }\n    for band in set(bands) - {\"SCL_20m\"}:\n        encodings[band][\"_FillValue\"] = 0\n    encodings[\"SCL_20m\"][\"dtype\"] = \"uint8\"\n    return encodings\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.exists","title":"exists","text":"<pre><code>exists(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene already exists in the local raw data store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the scene exists in the store, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def exists(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene already exists in the local raw data store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if the scene exists in the store, False otherwise\n\n    \"\"\"\n    if not self.store:\n        return False\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return scene_path.exists()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.identifier","title":"identifier","text":"<pre><code>identifier(s2item: str | pystac.Item) -&gt; str\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>def identifier(self, s2item: str | Item) -&gt; str:  # noqa: D102\n    s2id = s2item.id if isinstance(s2item, Item) else s2item\n    return f\"cdse-s2-sr-scene-{s2id}\"\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.load","title":"load","text":"<pre><code>load(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n    force: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a scene.</p> <p>If <code>force==True</code> will download the scene from source even if present in store. Else, will try to open the scene from store first and only download missing bands. Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will download the scene even if present. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def load(self, item: str | SceneItem, force: bool = False) -&gt; xr.Dataset:\n    \"\"\"Load a scene.\n\n    If `force==True` will download the scene from source even if present in store.\n    Else, will try to open the scene from store first and only download missing bands.\n    Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n        force (bool, optional): If True, will download the scene even if present. Defaults to False.\n\n    Returns:\n        xr.Dataset: The loaded scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    if force:\n        logger.debug(f\"Force downloading scene {identifier} from source.\")\n        dataset = self.download_scene_from_source(item, self.bands)\n        if self.store:\n            self.save_to_store(dataset, identifier)\n        return dataset\n\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        logger.debug(f\"Scene {identifier} is complete, opening from store.\")\n        return self.open(item)\n    logger.debug(f\"Scene {identifier} is missing bands {missing_bands}, downloading from source.\")\n    dataset = self.download_scene_from_source(item, missing_bands)\n    if self.store:\n        self.save_to_store(dataset, identifier)\n    return dataset\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.missing_bands","title":"missing_bands","text":"<pre><code>missing_bands(identifier: str) -&gt; list[str]\n</code></pre> <p>Get the list of missing bands for a scene in the store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: List of missing bands</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def missing_bands(self, identifier: str) -&gt; list[str]:\n    \"\"\"Get the list of missing bands for a scene in the store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        list[str]: List of missing bands\n\n    \"\"\"\n    if not self.store:\n        return self.bands\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    if not scene_path.exists():\n        return self.bands\n\n    dataset = xr.open_zarr(scene_path, consolidated=False)\n    required_bands = set(self.bands)\n    present_bands = set(dataset.data_vars)\n    missing = required_bands - present_bands\n    return list(missing)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.open","title":"open","text":"<pre><code>open(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n) -&gt; xarray.Dataset\n</code></pre> <p>Open a scene from local store.</p> <p>Store must be provided and the scene must be present in store!</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The opened scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def open(self, item: str | SceneItem) -&gt; xr.Dataset:\n    \"\"\"Open a scene from local store.\n\n    Store must be provided and the scene must be present in store!\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open\n\n    Returns:\n        xr.Dataset: The opened scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    assert self.complete(identifier), f\"Scene {identifier} is incomplete in store!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return xr.open_zarr(scene_path, consolidated=False).set_coords(\"spatial_ref\").load()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.CDSEStoreManager.save_to_store","title":"save_to_store","text":"<pre><code>save_to_store(\n    dataset: xarray.Dataset, identifier: str\n) -&gt; None\n</code></pre> <p>Save a scene dataset to the local raw data store.</p> <p>Will append new bands to existing store if scene already exists. Will overwrite existing bands in an existing store if scene already exists.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to save</p> </li> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def save_to_store(self, dataset: xr.Dataset, identifier: str) -&gt; None:\n    \"\"\"Save a scene dataset to the local raw data store.\n\n    Will append new bands to existing store if scene already exists.\n    Will overwrite existing bands in an existing store if scene already exists.\n\n    Args:\n        dataset (xr.Dataset): Dataset to save\n        identifier (str): Unique identifier for the scene\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to save scenes!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    encoding = self.encodings(list(dataset.data_vars))\n    if not scene_path.exists():\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"w\")\n    else:\n        # Assert that the coordinates match\n        existing_dataset = xr.open_zarr(scene_path, consolidated=False)\n        xr.testing.assert_allclose(existing_dataset.coords, dataset.coords)\n        # Overwrite dataset coords to avoid conflicts by floating point precision issues\n        dataset[\"x\"] = existing_dataset.x\n        dataset[\"y\"] = existing_dataset.y\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"a\")\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager","title":"StoreManager","text":"<pre><code>StoreManager(\n    bands: list[str],\n    store: str | pathlib.Path | None = None,\n)\n</code></pre> <p>               Bases: <code>abc.ABC</code>, <code>typing.Generic[darts_acquisition.s2.raw_data_store.SceneItem]</code></p> <p>Manager for storing raw sentinel 2 data.</p> <p>This class is an abstract base class and should be extended to implement the respective downloading methods.</p> <p>Usage:</p> <pre><code>1. \"Normal\" usage:\n\n```python\n    store_manager = StoreManager(store_path)\n    ds_s2 = store_manager.load(identifier, bands)\n```\n\n2. Force download:\n\n```python\n    store_manager = StoreManager(store_path)\n    ds_s2 = store_manager.load(identifier, force=True)\n```\n\n3. Download only (and only if missing) and store the scene:\n\n```python\n    store_manager = StoreManager(store_path)\n    store_manager.download(identifier) # store_path must be not None\n```\n\n4. Offline mode:\n\n```python\n    store_manager = StoreManager(store_path)\n    store_manager.open(identifier) # store_path must be not None, bands must be complete\n```\n</code></pre> <p>Initialize the store manager.</p> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>List of bands to manage</p> </li> <li> <code>store</code>               (<code>str | pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory path for storing raw sentinel 2 data</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def __init__(self, bands: list[str], store: str | Path | None = None):\n    \"\"\"Initialize the store manager.\n\n    Args:\n        bands (list[str]): List of bands to manage\n        store (str | Path | None): Directory path for storing raw sentinel 2 data\n\n    \"\"\"\n    self.bands = bands\n    self.store = Path(store) if isinstance(store, str) else store\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands = darts_acquisition.s2.raw_data_store.StoreManager(\n    bands\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.store","title":"store  <code>instance-attribute</code>","text":"<pre><code>store = (\n    pathlib.Path(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        )\n    )\n    if isinstance(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        ),\n        str,\n    )\n    else darts_acquisition.s2.raw_data_store.StoreManager(\n        store\n    )\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.complete","title":"complete","text":"<pre><code>complete(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene in the store contains all requested bands.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if all requested bands are present, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def complete(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene in the store contains all requested bands.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if all requested bands are present, False otherwise\n\n    \"\"\"\n    return len(self.missing_bands(identifier)) == 0\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.download_and_store","title":"download_and_store","text":"<pre><code>download_and_store(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n)\n</code></pre> <p>Download a scene from the source and store it in the local store.</p> <p>Store must be provided! Will do nothing if all required bands are already present.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def download_and_store(self, item: str | SceneItem):\n    \"\"\"Download a scene from the source and store it in the local store.\n\n    Store must be provided!\n    Will do nothing if all required bands are already present.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to download and store scenes!\"\n    identifier = self.identifier(item)\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        return\n    dataset = self.download_scene_from_source(item, missing_bands)\n    self.save_to_store(dataset, identifier)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.download_scene_from_source","title":"download_scene_from_source  <code>abstractmethod</code>","text":"<pre><code>download_scene_from_source(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n    bands: list[str],\n) -&gt; xarray.Dataset\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>@abstractmethod\ndef download_scene_from_source(self, item: str | SceneItem, bands: list[str]) -&gt; xr.Dataset: ...  # noqa: D102\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.encodings","title":"encodings  <code>abstractmethod</code>","text":"<pre><code>encodings(bands: list[str]) -&gt; dict[str, dict[str, str]]\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>@abstractmethod\ndef encodings(self, bands: list[str]) -&gt; dict[str, dict[str, str]]: ...  # noqa: D102\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.exists","title":"exists","text":"<pre><code>exists(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene already exists in the local raw data store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the scene exists in the store, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def exists(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene already exists in the local raw data store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if the scene exists in the store, False otherwise\n\n    \"\"\"\n    if not self.store:\n        return False\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return scene_path.exists()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.identifier","title":"identifier  <code>abstractmethod</code>","text":"<pre><code>identifier(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n) -&gt; str\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>@abstractmethod\ndef identifier(self, item: str | SceneItem) -&gt; str: ...  # noqa: D102\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.load","title":"load","text":"<pre><code>load(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n    force: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a scene.</p> <p>If <code>force==True</code> will download the scene from source even if present in store. Else, will try to open the scene from store first and only download missing bands. Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will download the scene even if present. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def load(self, item: str | SceneItem, force: bool = False) -&gt; xr.Dataset:\n    \"\"\"Load a scene.\n\n    If `force==True` will download the scene from source even if present in store.\n    Else, will try to open the scene from store first and only download missing bands.\n    Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n        force (bool, optional): If True, will download the scene even if present. Defaults to False.\n\n    Returns:\n        xr.Dataset: The loaded scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    if force:\n        logger.debug(f\"Force downloading scene {identifier} from source.\")\n        dataset = self.download_scene_from_source(item, self.bands)\n        if self.store:\n            self.save_to_store(dataset, identifier)\n        return dataset\n\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        logger.debug(f\"Scene {identifier} is complete, opening from store.\")\n        return self.open(item)\n    logger.debug(f\"Scene {identifier} is missing bands {missing_bands}, downloading from source.\")\n    dataset = self.download_scene_from_source(item, missing_bands)\n    if self.store:\n        self.save_to_store(dataset, identifier)\n    return dataset\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.missing_bands","title":"missing_bands","text":"<pre><code>missing_bands(identifier: str) -&gt; list[str]\n</code></pre> <p>Get the list of missing bands for a scene in the store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: List of missing bands</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def missing_bands(self, identifier: str) -&gt; list[str]:\n    \"\"\"Get the list of missing bands for a scene in the store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        list[str]: List of missing bands\n\n    \"\"\"\n    if not self.store:\n        return self.bands\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    if not scene_path.exists():\n        return self.bands\n\n    dataset = xr.open_zarr(scene_path, consolidated=False)\n    required_bands = set(self.bands)\n    present_bands = set(dataset.data_vars)\n    missing = required_bands - present_bands\n    return list(missing)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.open","title":"open","text":"<pre><code>open(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n) -&gt; xarray.Dataset\n</code></pre> <p>Open a scene from local store.</p> <p>Store must be provided and the scene must be present in store!</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The opened scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def open(self, item: str | SceneItem) -&gt; xr.Dataset:\n    \"\"\"Open a scene from local store.\n\n    Store must be provided and the scene must be present in store!\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open\n\n    Returns:\n        xr.Dataset: The opened scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    assert self.complete(identifier), f\"Scene {identifier} is incomplete in store!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return xr.open_zarr(scene_path, consolidated=False).set_coords(\"spatial_ref\").load()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.StoreManager.save_to_store","title":"save_to_store","text":"<pre><code>save_to_store(\n    dataset: xarray.Dataset, identifier: str\n) -&gt; None\n</code></pre> <p>Save a scene dataset to the local raw data store.</p> <p>Will append new bands to existing store if scene already exists. Will overwrite existing bands in an existing store if scene already exists.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to save</p> </li> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def save_to_store(self, dataset: xr.Dataset, identifier: str) -&gt; None:\n    \"\"\"Save a scene dataset to the local raw data store.\n\n    Will append new bands to existing store if scene already exists.\n    Will overwrite existing bands in an existing store if scene already exists.\n\n    Args:\n        dataset (xr.Dataset): Dataset to save\n        identifier (str): Unique identifier for the scene\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to save scenes!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    encoding = self.encodings(list(dataset.data_vars))\n    if not scene_path.exists():\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"w\")\n    else:\n        # Assert that the coordinates match\n        existing_dataset = xr.open_zarr(scene_path, consolidated=False)\n        xr.testing.assert_allclose(existing_dataset.coords, dataset.coords)\n        # Overwrite dataset coords to avoid conflicts by floating point precision issues\n        dataset[\"x\"] = existing_dataset.x\n        dataset[\"y\"] = existing_dataset.y\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"a\")\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene._build_cql2_filter","title":"_build_cql2_filter","text":"<pre><code>_build_cql2_filter(\n    tiles: list[str] | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; dict\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>def _build_cql2_filter(\n    tiles: list[str] | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; dict:\n    # Disable max xxx cover if set to 100\n    if max_cloud_cover is not None and max_cloud_cover == 100:\n        max_cloud_cover = None\n    if max_snow_cover is not None and max_snow_cover == 100:\n        max_snow_cover = None\n\n    if tiles is None and max_cloud_cover is None and max_snow_cover is None:\n        return None\n\n    filter = {}\n    filter[\"op\"] = \"and\"\n    filter[\"args\"] = []\n\n    if tiles is not None:\n        tiles = [f\"MGRS-{tile.lstrip('T')}\" for tile in tiles]\n        filter[\"args\"].append({\"op\": \"in\", \"args\": [{\"property\": \"grid:code\"}, tiles]})\n    if max_cloud_cover is not None:\n        filter[\"args\"].append({\"op\": \"lte\", \"args\": [{\"property\": \"eo:cloud_cover\"}, max_cloud_cover]})\n    if max_snow_cover is not None:\n        filter[\"args\"].append({\"op\": \"lte\", \"args\": [{\"property\": \"eo:snow_cover\"}, max_snow_cover]})\n    return filter\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene._flatten_dict","title":"_flatten_dict","text":"<pre><code>_flatten_dict(\n    d: collections.abc.MutableMapping,\n    parent_key: str = \"\",\n    sep: str = \".\",\n) -&gt; collections.abc.MutableMapping\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>def _flatten_dict(d: MutableMapping, parent_key: str = \"\", sep: str = \".\") -&gt; MutableMapping:\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, MutableMapping):\n            items.extend(_flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene._get_band_mapping","title":"_get_band_mapping","text":"<pre><code>_get_band_mapping(\n    bands_mapping: dict[str, str] | typing.Literal[\"all\"],\n) -&gt; dict[str, str]\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>def _get_band_mapping(bands_mapping: dict[str, str] | Literal[\"all\"]) -&gt; dict[str, str]:\n    if bands_mapping == \"all\":\n        # Mapping according to spyndex band common names:\n        # for key, band in spyndex.bands.items():\n        #     if not hasattr(band, \"sentinel2a\"): continue\n        #     print(f\"{band.sentinel2a.band}: {band.common_name}\")\n        bands_mapping = {\n            \"B01_20m\": \"coastal\",\n            \"B02_10m\": \"blue\",\n            \"B03_10m\": \"green\",\n            \"B04_10m\": \"red\",\n            \"B05_20m\": \"rededge071\",\n            \"B06_20m\": \"rededge075\",\n            \"B07_20m\": \"rededge078\",\n            \"B08_10m\": \"nir\",\n            \"B8A_20m\": \"nir08\",\n            \"B09_60m\": \"nir09\",\n            \"B11_20m\": \"swir16\",\n            \"B12_20m\": \"swir22\",\n        }\n\n    if \"SCL_20m\" not in bands_mapping.keys():\n        bands_mapping[\"SCL_20m\"] = \"s2_scl\"\n    return bands_mapping\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.convert_masks","title":"convert_masks","text":"<pre><code>convert_masks(ds_s2: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Convert the Sentinel-2 scl mask into our own mask format inplace.</p> <p>https://sentiwiki.copernicus.eu/web/s2-processing#S2Processing-ClassificationMaskGeneration</p> <p>Invalid: S2 SCL \u2192 0,1 Low Quality S2: S2 SCL != 0,1 \u2192 3,8,9,11 High Quality: S2 SCL != 0,1,3,8,9,11 \u2192 Alles andere (2,4,5,6,7,10)</p> <p>Parameters:</p> <ul> <li> <code>ds_s2</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Sentinel-2 dataset containing the SCL band.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The modified dataset.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/quality_mask.py</code> <pre><code>@stopwatch(\"Converting Sentinel-2 masks\", printer=logger.debug)\ndef convert_masks(ds_s2: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Convert the Sentinel-2 scl mask into our own mask format inplace.\n\n    https://sentiwiki.copernicus.eu/web/s2-processing#S2Processing-ClassificationMaskGeneration\n\n    Invalid: S2 SCL \u2192 0,1\n    Low Quality S2: S2 SCL != 0,1 \u2192 3,8,9,11\n    High Quality: S2 SCL != 0,1,3,8,9,11 \u2192 Alles andere (2,4,5,6,7,10)\n\n    Args:\n        ds_s2 (xr.Dataset): The Sentinel-2 dataset containing the SCL band.\n\n    Returns:\n        xr.Dataset: The modified dataset.\n\n    \"\"\"\n    assert \"s2_scl\" in ds_s2.data_vars, \"The dataset does not contain the SCL band.\"\n\n    if has_cuda_and_cupy() and ds_s2.cupy.is_cupy:\n        invalids = ds_s2[\"s2_scl\"].fillna(0).isin(cp.array([0, 1]))\n        high_quality = ds_s2[\"s2_scl\"].isin(cp.array([2, 4, 5, 6, 7, 10]))\n    else:\n        invalids = ds_s2[\"s2_scl\"].fillna(0).isin([0, 1])\n        high_quality = ds_s2[\"s2_scl\"].isin([2, 4, 5, 6, 7, 10])\n    ds_s2[\"quality_data_mask\"] = (\n        (~invalids).astype(\"uint8\")  # 0 for invalid, 1 for valid\n        + (high_quality).astype(\"uint8\")  # +1 for high quality\n    )\n\n    ds_s2[\"quality_data_mask\"].attrs[\"data_source\"] = \"s2\"\n    ds_s2[\"quality_data_mask\"].attrs[\"long_name\"] = \"Quality Data Mask\"\n    ds_s2[\"quality_data_mask\"].attrs[\"description\"] = \"0 = Invalid, 1 = Low Quality, 2 = High Quality\"\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.download_cdse_s2_sr_scene","title":"download_cdse_s2_sr_scene","text":"<pre><code>download_cdse_s2_sr_scene(\n    s2item: str | pystac.Item,\n    store: pathlib.Path,\n    bands_mapping: dict | typing.Literal[\"all\"] = {\n        \"B02_10m\": \"blue\",\n        \"B03_10m\": \"green\",\n        \"B04_10m\": \"red\",\n        \"B08_10m\": \"nir\",\n    },\n    aws_profile_name: str = \"default\",\n)\n</code></pre> <p>Download a Sentinel-2 scene from CDSE via STAC API and store it in the local data store.</p> <p>This function downloads Sentinel-2 Level-2A surface reflectance data from the Copernicus Data Space Ecosystem (CDSE) and stores it locally in a compressed zarr store for efficient repeated access.</p> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | pystac.Item</code>)           \u2013            <p>Sentinel-2 scene identifier (e.g., \"S2A_MSIL2A_20230615T...\") or a PySTAC Item object from a STAC search.</p> </li> <li> <code>store</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the local zarr store directory where the scene will be saved.</p> </li> <li> <code>bands_mapping</code>               (<code>dict | typing.Literal['all']</code>, default:                   <code>{'B02_10m': 'blue', 'B03_10m': 'green', 'B04_10m': 'red', 'B08_10m': 'nir'}</code> )           \u2013            <p>Mapping of Sentinel-2 band names to custom band names. Keys should be CDSE band names (e.g., \"B02_10m\", \"B03_10m\"), values are the desired output names. Use \"all\" to load all optical bands and SCL. Defaults to {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"}.</p> </li> <li> <code>aws_profile_name</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>AWS profile name for authentication with the Copernicus S3 bucket. Defaults to \"default\".</p> </li> </ul> Note <ul> <li>Requires Copernicus Data Space authentication. Use <code>darts_utils.copernicus.init_copernicus()</code>   to set up credentials before calling this function.</li> <li>All bands are resampled to 10m resolution during download.</li> <li>Data is stored with zstd compression for efficient storage.</li> <li>The SCL (Scene Classification Layer) band is automatically included if not specified.</li> </ul> Example <p>Download Sentinel-2 scenes for a project:</p> <pre><code>from pathlib import Path\nfrom darts_acquisition import download_cdse_s2_sr_scene\nfrom darts_utils.copernicus import init_copernicus\n\n# Setup authentication\ninit_copernicus(profile_name=\"default\")\n\n# Download scene with all bands\ndownload_cdse_s2_sr_scene(\n    s2item=\"S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678\",\n    store=Path(\"/data/s2_store\"),\n    bands_mapping=\"all\",\n    aws_profile_name=\"default\"\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch.f(\"Downloading Sentinel-2 scene from CDSE if missing\", printer=logger.debug, print_kwargs=[\"s2item\"])\ndef download_cdse_s2_sr_scene(\n    s2item: str | Item,\n    store: Path,\n    bands_mapping: dict | Literal[\"all\"] = {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"},\n    aws_profile_name: str = \"default\",\n):\n    \"\"\"Download a Sentinel-2 scene from CDSE via STAC API and store it in the local data store.\n\n    This function downloads Sentinel-2 Level-2A surface reflectance data from the Copernicus\n    Data Space Ecosystem (CDSE) and stores it locally in a compressed zarr store for efficient\n    repeated access.\n\n    Args:\n        s2item (str | Item): Sentinel-2 scene identifier (e.g., \"S2A_MSIL2A_20230615T...\") or\n            a PySTAC Item object from a STAC search.\n        store (Path): Path to the local zarr store directory where the scene will be saved.\n        bands_mapping (dict | Literal[\"all\"], optional): Mapping of Sentinel-2 band names to\n            custom band names. Keys should be CDSE band names (e.g., \"B02_10m\", \"B03_10m\"),\n            values are the desired output names. Use \"all\" to load all optical bands and SCL.\n            Defaults to {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"}.\n        aws_profile_name (str, optional): AWS profile name for authentication with the\n            Copernicus S3 bucket. Defaults to \"default\".\n\n    Note:\n        - Requires Copernicus Data Space authentication. Use `darts_utils.copernicus.init_copernicus()`\n          to set up credentials before calling this function.\n        - All bands are resampled to 10m resolution during download.\n        - Data is stored with zstd compression for efficient storage.\n        - The SCL (Scene Classification Layer) band is automatically included if not specified.\n\n    Example:\n        Download Sentinel-2 scenes for a project:\n\n        ```python\n        from pathlib import Path\n        from darts_acquisition import download_cdse_s2_sr_scene\n        from darts_utils.copernicus import init_copernicus\n\n        # Setup authentication\n        init_copernicus(profile_name=\"default\")\n\n        # Download scene with all bands\n        download_cdse_s2_sr_scene(\n            s2item=\"S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678\",\n            store=Path(\"/data/s2_store\"),\n            bands_mapping=\"all\",\n            aws_profile_name=\"default\"\n        )\n        ```\n\n    \"\"\"\n    bands_mapping = _get_band_mapping(bands_mapping)\n    store_manager = CDSEStoreManager(\n        store=store,\n        bands_mapping=bands_mapping,\n        aws_profile_name=aws_profile_name,\n    )\n\n    store_manager.download_and_store(s2item)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.get_aoi_from_cdse_scene_ids","title":"get_aoi_from_cdse_scene_ids","text":"<pre><code>get_aoi_from_cdse_scene_ids(\n    scene_ids: list[str],\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Get the area of interest (AOI) as a GeoDataFrame from a list of Sentinel-2 scene IDs.</p> <p>Parameters:</p> <ul> <li> <code>scene_ids</code>               (<code>list[str]</code>)           \u2013            <p>List of Sentinel-2 scene IDs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>geopandas.GeoDataFrame</code>           \u2013            <p>gpd.GeoDataFrame: The AOI as a GeoDataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no Sentinel-2 items are found for the given scene IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Getting AOI from CDSE scene IDs\", printer=logger.debug)\ndef get_aoi_from_cdse_scene_ids(\n    scene_ids: list[str],\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get the area of interest (AOI) as a GeoDataFrame from a list of Sentinel-2 scene IDs.\n\n    Args:\n        scene_ids (list[str]): List of Sentinel-2 scene IDs.\n\n    Returns:\n        gpd.GeoDataFrame: The AOI as a GeoDataFrame.\n\n    Raises:\n        ValueError: If no Sentinel-2 items are found for the given scene IDs.\n\n    \"\"\"\n    catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n    search = catalog.search(\n        collections=[\"sentinel-2-l2a\"],\n        ids=scene_ids,\n    )\n    items = list(search.items())\n    if not items:\n        raise ValueError(\"No Sentinel-2 items found for the given scene IDs.\")\n    gdf = gpd.GeoDataFrame.from_features(\n        [item.to_dict() for item in items],\n        crs=\"EPSG:4326\",\n    )\n    return gdf\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.get_cdse_s2_sr_scene_ids_from_geodataframe","title":"get_cdse_s2_sr_scene_ids_from_geodataframe","text":"<pre><code>get_cdse_s2_sr_scene_ids_from_geodataframe(\n    aoi: geopandas.GeoDataFrame | pathlib.Path | str,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n    simplify_geometry: float\n    | typing.Literal[False] = False,\n) -&gt; dict[str, pystac.Item]\n</code></pre> <p>Search for Sentinel-2 scenes via STAC based on an area of interest (aoi).</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame | pathlib.Path | str</code>)           \u2013            <p>AOI as a GeoDataFrame or path to a shapefile. If a path is provided, it will be read using geopandas.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> <li> <code>months</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of months (1-12) to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> <li> <code>years</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of years to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> <li> <code>simplify_geometry</code>               (<code>float | typing.Literal[False]</code>, default:                   <code>False</code> )           \u2013            <p>If a float is provided, the geometry will be simplified using the <code>simplify</code> method of geopandas. If False, no simplification will be done. This may become useful for large / weird AOIs which are too large for the STAC API. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, pystac.Item]</code>           \u2013            <p>dict[str, Item]: A dictionary of found Sentinel-2 items.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Searching for Sentinel-2 scenes in CDSE from AOI\", printer=logger.debug)\ndef get_cdse_s2_sr_scene_ids_from_geodataframe(\n    aoi: gpd.GeoDataFrame | Path | str,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n    simplify_geometry: float | Literal[False] = False,\n) -&gt; dict[str, Item]:\n    \"\"\"Search for Sentinel-2 scenes via STAC based on an area of interest (aoi).\n\n    Args:\n        aoi (gpd.GeoDataFrame | Path | str): AOI as a GeoDataFrame or path to a shapefile.\n            If a path is provided, it will be read using geopandas.\n        start_date (str): Starting date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n        months (list[int] | None, optional): List of months (1-12) to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n        years (list[int] | None, optional): List of years to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n        simplify_geometry (float | Literal[False], optional): If a float is provided, the geometry will be simplified\n            using the `simplify` method of geopandas. If False, no simplification will be done.\n            This may become useful for large / weird AOIs which are too large for the STAC API.\n            Defaults to False.\n\n    Returns:\n        dict[str, Item]: A dictionary of found Sentinel-2 items.\n\n    \"\"\"\n    if isinstance(aoi, Path | str):\n        aoi = gpd.read_file(aoi)\n    s2items: dict[str, Item] = {}\n    if simplify_geometry:\n        aoi = aoi.copy()\n        aoi[\"geometry\"] = aoi.geometry.simplify(simplify_geometry)\n    for i, row in aoi.iterrows():\n        s2items.update(\n            search_cdse_s2_sr(\n                intersects=row.geometry,\n                start_date=start_date,\n                end_date=end_date,\n                max_cloud_cover=max_cloud_cover,\n                max_snow_cover=max_snow_cover,\n                months=months,\n                years=years,\n            )\n        )\n    return s2items\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.get_cdse_s2_sr_scene_ids_from_tile_ids","title":"get_cdse_s2_sr_scene_ids_from_tile_ids","text":"<pre><code>get_cdse_s2_sr_scene_ids_from_tile_ids(\n    tile_ids: list[str],\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n) -&gt; dict[str, pystac.Item]\n</code></pre> <p>Search for Sentinel-2 scenes via STAC based on a list of tile IDs.</p> <p>Parameters:</p> <ul> <li> <code>tile_ids</code>               (<code>list[str]</code>)           \u2013            <p>List of MGRS tile IDs to search for.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> <li> <code>months</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of months (1-12) to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> <li> <code>years</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of years to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, pystac.Item]</code>           \u2013            <p>dict[str, Item]: A dictionary of found Sentinel-2 items.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Searching for Sentinel-2 scenes in CDSE from Tile-IDs\", printer=logger.debug)\ndef get_cdse_s2_sr_scene_ids_from_tile_ids(\n    tile_ids: list[str],\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n) -&gt; dict[str, Item]:\n    \"\"\"Search for Sentinel-2 scenes via STAC based on a list of tile IDs.\n\n    Args:\n        tile_ids (list[str]): List of MGRS tile IDs to search for.\n        start_date (str): Starting date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n        months (list[int] | None, optional): List of months (1-12) to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n        years (list[int] | None, optional): List of years to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n\n    Returns:\n        dict[str, Item]: A dictionary of found Sentinel-2 items.\n\n    \"\"\"\n    return search_cdse_s2_sr(\n        tiles=tile_ids,\n        start_date=start_date,\n        end_date=end_date,\n        max_cloud_cover=max_cloud_cover,\n        max_snow_cover=max_snow_cover,\n        months=months,\n        years=years,\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.init_copernicus","title":"init_copernicus","text":"<pre><code>init_copernicus(profile_name: str = 'default')\n</code></pre> <p>Configure odc.stac and rio to authenticate with Copernicus cloud.</p> <p>This functions expects that credentials are present in the .aws/credentials file. Credentials can be obtained from https://eodata-s3keysmanager.dataspace.copernicus.eu/</p> <p>Example credentials file:</p> <pre><code>[default]\nAWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\n</code></pre> <p>Parameters:</p> <ul> <li> <code>profile_name</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>The boto3 profile name. This must match with the name in the credentials file!. Defaults to \"default\".</p> </li> </ul> References <ul> <li>S3 access: https://documentation.dataspace.copernicus.eu/APIs/S3.html</li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/copernicus.py</code> <pre><code>def init_copernicus(profile_name: str = \"default\"):\n    \"\"\"Configure odc.stac and rio to authenticate with Copernicus cloud.\n\n    This functions expects that credentials are present in the .aws/credentials file.\n    Credentials can be obtained from https://eodata-s3keysmanager.dataspace.copernicus.eu/\n\n    Example credentials file:\n\n    ```\n    [default]\n    AWS_ACCESS_KEY_ID=...\n    AWS_SECRET_ACCESS_KEY=...\n    ```\n\n    Args:\n        profile_name (str, optional): The boto3 profile name. This must match with the name in the credentials file!.\n            Defaults to \"default\".\n\n    References:\n        - S3 access: https://documentation.dataspace.copernicus.eu/APIs/S3.html\n\n    \"\"\"\n    import boto3\n    import odc.stac\n\n    session = boto3.Session(profile_name=profile_name)\n    credentials = session.get_credentials()\n\n    odc.stac.configure_rio(\n        cloud_defaults=True,\n        verbose=False,\n        aws={\n            \"profile_name\": profile_name,\n            \"aws_access_key_id\": credentials.access_key,\n            \"aws_secret_access_key\": credentials.secret_key,\n            \"region_name\": \"default\",\n            \"endpoint_url\": \"eodata.dataspace.copernicus.eu\",\n        },\n        AWS_VIRTUAL_HOSTING=False,\n    )\n    logger.debug(\"Copernicus STAC initialized\")\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.load_cdse_s2_sr_scene","title":"load_cdse_s2_sr_scene","text":"<pre><code>load_cdse_s2_sr_scene(\n    s2item: str | pystac.Item,\n    bands_mapping: dict | typing.Literal[\"all\"] = {\n        \"B02_10m\": \"blue\",\n        \"B03_10m\": \"green\",\n        \"B04_10m\": \"red\",\n        \"B08_10m\": \"nir\",\n    },\n    store: pathlib.Path | None = None,\n    aws_profile_name: str = \"default\",\n    offline: bool = False,\n    output_dir_for_debug_geotiff: pathlib.Path\n    | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_utils.cuda.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel-2 scene from CDSE, downloading from STAC API if necessary.</p> <p>This function loads Sentinel-2 Level-2A surface reflectance data from the Copernicus Data Space Ecosystem (CDSE). If a local store is provided, the data is cached for efficient repeated access. The function handles quality masking, reflectance scaling, and optional GPU acceleration.</p> <p>The download logic is basically as follows:</p> <pre><code>IF flag:raw-data-store THEN\n    IF exist_local THEN\n        open -&gt; memory\n    ELIF online THEN\n        download -&gt; memory\n        save\n    ELIF offline THEN\n        RAISE ERROR\n    ENDIF\nELIF online THEN\n    download -&gt; memory\nELIF offline THEN\n    RAISE ERROR\nENDIF\n</code></pre> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | pystac.Item</code>)           \u2013            <p>Sentinel-2 scene identifier or PySTAC Item object.</p> </li> <li> <code>bands_mapping</code>               (<code>dict | typing.Literal['all']</code>, default:                   <code>{'B02_10m': 'blue', 'B03_10m': 'green', 'B04_10m': 'red', 'B08_10m': 'nir'}</code> )           \u2013            <p>Mapping of Sentinel-2 band names to custom band names. Keys should be CDSE band names (e.g., \"B02_10m\"), values are output names. Use \"all\" to load all optical bands and SCL. Defaults to {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"}.</p> </li> <li> <code>store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to local zarr store for caching. If None, data is loaded directly without caching. Defaults to None.</p> </li> <li> <code>aws_profile_name</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>AWS profile name for Copernicus S3 authentication. Defaults to \"default\".</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only loads from local store without downloading. Requires <code>store</code> to be provided. If False, missing data is downloaded. Defaults to False.</p> </li> <li> <code>output_dir_for_debug_geotiff</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>If provided, writes raw data as GeoTIFF files for debugging. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_utils.cuda.DEFAULT_DEVICE</code> )           \u2013            <p>Device for processing (GPU or CPU). Defaults to DEFAULT_DEVICE.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Sentinel-2 dataset with the following data variables based on bands_mapping: - Optical bands (float32): Surface reflectance values [~-0.1 to ~1.0]   Default bands: blue, green, red, nir   Additional bands available: coastal, rededge071, rededge075, rededge078,   nir08, nir09, swir16, swir22   Each has attributes:   - long_name: \"Sentinel-2 {Band}\"   - units: \"Reflectance\"   - data_source: \"Sentinel-2 L2A via Copernicus STAC API (sentinel-2-l2a)\" - s2_scl (uint8): Scene Classification Layer   Attributes: long_name, description of class values (0=NO_DATA, 1=SATURATED, etc.) - quality_data_mask (uint8): Derived quality mask   - 0 = Invalid (no data, saturated, or defective)   - 1 = Low quality (shadows, clouds, cirrus, snow/ice, water)   - 2 = High quality (clear vegetation or non-vegetated land) - valid_data_mask (uint8): Binary validity mask (1=valid, 0=invalid)</p> <p>Dataset attributes: - azimuth (float): Solar azimuth angle from view:azimuth - elevation (float): Solar elevation angle from view:sun_elevation - s2_tile_id (str): Scene identifier - tile_id (str): Scene identifier (same as s2_tile_id) - Plus additional STAC metadata fields</p> </li> </ul> Note <p>The <code>offline</code> parameter controls data fetching: - When <code>offline=False</code>: Automatically downloads missing data from CDSE and stores it   in the local zarr store (if store is provided). - When <code>offline=True</code>: Only reads from the local store. Raises an error if data is   missing or if store is None.</p> <p>Reflectance processing: - Raw DN values are scaled: (DN / 10000.0) - 0.1 - Pixels where SCL==0 or DN==0 are masked as NaN - This matches the data format from GEE and Planet loaders</p> <p>Quality mask derivation from SCL: - Invalid (0): NO_DATA, SATURATED_OR_DEFECTIVE - Low quality (1): CAST_SHADOWS, CLOUD_SHADOWS, CLOUD_*, THIN_CIRRUS, SNOW/ICE, WATER - High quality (2): VEGETATION, NOT_VEGETATED</p> Example <p>Load scene with local caching:</p> <pre><code>from pathlib import Path\nfrom darts_acquisition import load_cdse_s2_sr_scene\nfrom darts_utils.copernicus import init_copernicus\n\n# Setup authentication\ninit_copernicus(profile_name=\"default\")\n\n# Load with caching\ns2_ds = load_cdse_s2_sr_scene(\n    s2item=\"S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678\",\n    bands_mapping=\"all\",\n    store=Path(\"/data/s2_store\"),\n    offline=False  # Download if not cached\n)\n\n# Compute NDVI\nndvi = (s2_ds.nir - s2_ds.red) / (s2_ds.nir + s2_ds.red)\n\n# Filter to high quality pixels\ns2_filtered = s2_ds.where(s2_ds.quality_data_mask == 2)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel-2 scene from CDSE\", printer=logger.debug, print_kwargs=[\"s2item\"])\ndef load_cdse_s2_sr_scene(\n    s2item: str | Item,\n    bands_mapping: dict | Literal[\"all\"] = {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"},\n    store: Path | None = None,\n    aws_profile_name: str = \"default\",\n    offline: bool = False,\n    output_dir_for_debug_geotiff: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel-2 scene from CDSE, downloading from STAC API if necessary.\n\n    This function loads Sentinel-2 Level-2A surface reflectance data from the Copernicus\n    Data Space Ecosystem (CDSE). If a local store is provided, the data is cached for\n    efficient repeated access. The function handles quality masking, reflectance scaling,\n    and optional GPU acceleration.\n\n    The download logic is basically as follows:\n\n    ```\n    IF flag:raw-data-store THEN\n        IF exist_local THEN\n            open -&gt; memory\n        ELIF online THEN\n            download -&gt; memory\n            save\n        ELIF offline THEN\n            RAISE ERROR\n        ENDIF\n    ELIF online THEN\n        download -&gt; memory\n    ELIF offline THEN\n        RAISE ERROR\n    ENDIF\n    ```\n\n    Args:\n        s2item (str | Item): Sentinel-2 scene identifier or PySTAC Item object.\n        bands_mapping (dict | Literal[\"all\"], optional): Mapping of Sentinel-2 band names to\n            custom band names. Keys should be CDSE band names (e.g., \"B02_10m\"), values are\n            output names. Use \"all\" to load all optical bands and SCL.\n            Defaults to {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"}.\n        store (Path | None, optional): Path to local zarr store for caching. If None, data is\n            loaded directly without caching. Defaults to None.\n        aws_profile_name (str, optional): AWS profile name for Copernicus S3 authentication.\n            Defaults to \"default\".\n        offline (bool, optional): If True, only loads from local store without downloading.\n            Requires `store` to be provided. If False, missing data is downloaded.\n            Defaults to False.\n        output_dir_for_debug_geotiff (Path | None, optional): If provided, writes raw data as\n            GeoTIFF files for debugging. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): Device for processing (GPU or CPU).\n            Defaults to DEFAULT_DEVICE.\n\n    Returns:\n        xr.Dataset: Sentinel-2 dataset with the following data variables based on bands_mapping:\n            - Optical bands (float32): Surface reflectance values [~-0.1 to ~1.0]\n              Default bands: blue, green, red, nir\n              Additional bands available: coastal, rededge071, rededge075, rededge078,\n              nir08, nir09, swir16, swir22\n              Each has attributes:\n              - long_name: \"Sentinel-2 {Band}\"\n              - units: \"Reflectance\"\n              - data_source: \"Sentinel-2 L2A via Copernicus STAC API (sentinel-2-l2a)\"\n            - s2_scl (uint8): Scene Classification Layer\n              Attributes: long_name, description of class values (0=NO_DATA, 1=SATURATED, etc.)\n            - quality_data_mask (uint8): Derived quality mask\n              - 0 = Invalid (no data, saturated, or defective)\n              - 1 = Low quality (shadows, clouds, cirrus, snow/ice, water)\n              - 2 = High quality (clear vegetation or non-vegetated land)\n            - valid_data_mask (uint8): Binary validity mask (1=valid, 0=invalid)\n\n            Dataset attributes:\n            - azimuth (float): Solar azimuth angle from view:azimuth\n            - elevation (float): Solar elevation angle from view:sun_elevation\n            - s2_tile_id (str): Scene identifier\n            - tile_id (str): Scene identifier (same as s2_tile_id)\n            - Plus additional STAC metadata fields\n\n    Note:\n        The `offline` parameter controls data fetching:\n        - When `offline=False`: Automatically downloads missing data from CDSE and stores it\n          in the local zarr store (if store is provided).\n        - When `offline=True`: Only reads from the local store. Raises an error if data is\n          missing or if store is None.\n\n        Reflectance processing:\n        - Raw DN values are scaled: (DN / 10000.0) - 0.1\n        - Pixels where SCL==0 or DN==0 are masked as NaN\n        - This matches the data format from GEE and Planet loaders\n\n        Quality mask derivation from SCL:\n        - Invalid (0): NO_DATA, SATURATED_OR_DEFECTIVE\n        - Low quality (1): CAST_SHADOWS, CLOUD_SHADOWS, CLOUD_*, THIN_CIRRUS, SNOW/ICE, WATER\n        - High quality (2): VEGETATION, NOT_VEGETATED\n\n    Example:\n        Load scene with local caching:\n\n        ```python\n        from pathlib import Path\n        from darts_acquisition import load_cdse_s2_sr_scene\n        from darts_utils.copernicus import init_copernicus\n\n        # Setup authentication\n        init_copernicus(profile_name=\"default\")\n\n        # Load with caching\n        s2_ds = load_cdse_s2_sr_scene(\n            s2item=\"S2A_MSIL2A_20230615T123456_N0509_R012_T33UUP_20230615T145678\",\n            bands_mapping=\"all\",\n            store=Path(\"/data/s2_store\"),\n            offline=False  # Download if not cached\n        )\n\n        # Compute NDVI\n        ndvi = (s2_ds.nir - s2_ds.red) / (s2_ds.nir + s2_ds.red)\n\n        # Filter to high quality pixels\n        s2_filtered = s2_ds.where(s2_ds.quality_data_mask == 2)\n        ```\n\n    \"\"\"\n    s2id = s2item.id if isinstance(s2item, Item) else s2item\n\n    bands_mapping = _get_band_mapping(bands_mapping)\n    store_manager = CDSEStoreManager(\n        store=store,\n        bands_mapping=bands_mapping,\n        aws_profile_name=aws_profile_name,\n    )\n\n    with stopwatch(\"Load Sentinel-2 scene from store\", printer=logger.debug):\n        if not offline:\n            ds_s2 = store_manager.load(s2item)\n        else:\n            assert store is not None, \"Store must be provided in offline mode!\"\n            ds_s2 = store_manager.open(s2item)\n\n    if output_dir_for_debug_geotiff is not None:\n        save_debug_geotiff(\n            dataset=ds_s2,\n            output_path=output_dir_for_debug_geotiff,\n            optical_bands=[band for band in bands_mapping.keys() if band.startswith(\"B\")],\n            mask_bands=[\"SCL_20m\"] if \"SCL_20m\" in bands_mapping.keys() else None,\n        )\n\n    # ? The following part takes ~2.5s on CPU and ~0.1 on GPU,\n    # however moving to GPU and back takes ~2.2s\n    ds_s2 = move_to_device(ds_s2, device)\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n    optical_bands = [band for name, band in bands_mapping.items() if name.startswith(\"B\")]\n    for band in optical_bands:\n        # Set values where SCL_20m == 0 to NaN in all other bands\n        # This way the data is similar to data from gee or planet data\n        # We need to filter out 0 values, since they are not valid reflectance values\n        # But also not reflected in the SCL for some reason\n        ds_s2[band] = ds_s2[band].where(ds_s2.s2_scl != 0).where(ds_s2[band].astype(\"float32\") != 0) / 10000.0 - 0.1\n        ds_s2[band].attrs[\"long_name\"] = f\"Sentinel-2 {band.capitalize()}\"\n        ds_s2[band].attrs[\"units\"] = \"Reflectance\"\n    ds_s2[\"s2_scl\"].attrs = {\n        \"long_name\": \"Sentinel-2 Scene Classification Layer\",\n        \"description\": (\n            \"0: NO_DATA - 1: SATURATED_OR_DEFECTIVE - 2: CAST_SHADOWS - 3: CLOUD_SHADOWS - 4: VEGETATION\"\n            \" - 5: NOT_VEGETATED - 6: WATER - 7: UNCLASSIFIED - 8: CLOUD_MEDIUM_PROBABILITY\"\n            \" - 9: CLOUD_HIGH_PROBABILITY - 10: THIN_CIRRUS - 11: SNOW or ICE\"\n        ),\n    }\n    for band in ds_s2.data_vars:\n        ds_s2[band].attrs[\"data_source\"] = \"Sentinel-2 L2A via Copernicus STAC API (sentinel-2-l2a)\"\n\n    # ? This takes approx. 1.5s on CPU\n    # For some reason, this takes ~1.2s on the GPU\n    ds_s2 = convert_masks(ds_s2)\n\n    ds_s2 = move_to_host(ds_s2)\n\n    # Convert sun elevation and azimuth to match our naming\n    ds_s2.attrs[\"azimuth\"] = ds_s2.attrs.get(\"view:sun_azimuth\", float(\"nan\"))\n    ds_s2.attrs[\"elevation\"] = ds_s2.attrs.get(\"view:sun_elevation\", float(\"nan\"))\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.match_cdse_s2_sr_scene_ids_from_geodataframe","title":"match_cdse_s2_sr_scene_ids_from_geodataframe","text":"<pre><code>match_cdse_s2_sr_scene_ids_from_geodataframe(\n    aoi: geopandas.GeoDataFrame,\n    day_range: int = 60,\n    max_cloud_cover: int = 20,\n    min_intersects: float = 0.7,\n    simplify_geometry: float\n    | typing.Literal[False] = False,\n    save_scores: pathlib.Path | None = None,\n) -&gt; dict[int, pystac.Item | None]\n</code></pre> <p>Match items from a GeoDataFrame with Sentinel-2 items from the STAC API based on a date range.</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The area of interest as a GeoDataFrame.</p> </li> <li> <code>day_range</code>               (<code>int</code>, default:                   <code>60</code> )           \u2013            <p>The number of days before and after the date to search for. Defaults to 60.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>The maximum cloud cover percentage. Defaults to 20.</p> </li> <li> <code>min_intersects</code>               (<code>float</code>, default:                   <code>0.7</code> )           \u2013            <p>The minimum intersection area ratio to consider a match. Defaults to 0.7.</p> </li> <li> <code>simplify_geometry</code>               (<code>float | typing.Literal[False]</code>, default:                   <code>False</code> )           \u2013            <p>If a float is provided, the geometry will be simplified using the <code>simplify</code> method of geopandas. If False, no simplification will be done. This may become useful for large / weird AOIs which are too large for the STAC API. Defaults to False.</p> </li> <li> <code>save_scores</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>If provided, the scores will be saved to this path as a Parquet file.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the 'date' column is not present or not of type datetime.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[int, pystac.Item | None]</code>           \u2013            <p>dict[int, Item | None]: A dictionary mapping each row to its best matching Sentinel-2 item. The keys are the indices of the rows in the GeoDataFrame, and the values are the matching Sentinel-2 items. If no matching item is found, the value will be None.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Matching Sentinel-2 scenes in CDSE from AOI\", printer=logger.debug)\ndef match_cdse_s2_sr_scene_ids_from_geodataframe(\n    aoi: gpd.GeoDataFrame,\n    day_range: int = 60,\n    max_cloud_cover: int = 20,\n    min_intersects: float = 0.7,\n    simplify_geometry: float | Literal[False] = False,\n    save_scores: Path | None = None,\n) -&gt; dict[int, Item | None]:\n    \"\"\"Match items from a GeoDataFrame with Sentinel-2 items from the STAC API based on a date range.\n\n    Args:\n        aoi (gpd.GeoDataFrame): The area of interest as a GeoDataFrame.\n        day_range (int): The number of days before and after the date to search for.\n            Defaults to 60.\n        max_cloud_cover (int, optional): The maximum cloud cover percentage. Defaults to 20.\n        min_intersects (float, optional): The minimum intersection area ratio to consider a match. Defaults to 0.7.\n        simplify_geometry (float | Literal[False], optional): If a float is provided, the geometry will be simplified\n            using the `simplify` method of geopandas. If False, no simplification will be done.\n            This may become useful for large / weird AOIs which are too large for the STAC API.\n            Defaults to False.\n        save_scores (Path | None, optional): If provided, the scores will be saved to this path as a Parquet file.\n\n    Raises:\n        ValueError: If the 'date' column is not present or not of type datetime.\n\n    Returns:\n        dict[int, Item | None]: A dictionary mapping each row to its best matching Sentinel-2 item.\n            The keys are the indices of the rows in the GeoDataFrame, and the values are the matching Sentinel-2 items.\n            If no matching item is found, the value will be None.\n\n    \"\"\"\n    # Check weather the \"date\" column is present and of type datetime\n    if \"date\" not in aoi.columns or not pd.api.types.is_datetime64_any_dtype(aoi[\"date\"]):\n        raise ValueError(\"The 'date' column must be present and of type datetime in the GeoDataFrame.\")\n\n    if simplify_geometry:\n        aoi = aoi.copy()\n        aoi[\"geometry\"] = aoi.geometry.simplify(simplify_geometry)\n\n    matches = {}\n    scores = []\n    for i, row in aoi.iterrows():\n        intersects = row.geometry.__geo_interface__\n        start_date = (row[\"date\"] - pd.Timedelta(days=day_range)).strftime(\"%Y-%m-%d\")\n        end_date = (row[\"date\"] + pd.Timedelta(days=day_range)).strftime(\"%Y-%m-%d\")\n        intersecting_items = search_cdse_s2_sr(\n            intersects=intersects,\n            start_date=start_date,\n            end_date=end_date,\n            max_cloud_cover=max_cloud_cover,\n        )\n        if not intersecting_items:\n            logger.info(f\"No Sentinel-2 items found for footprint #{i} in the date range {start_date} to {end_date}.\")\n            matches[i] = None\n            continue\n        intersecting_items_gdf = gpd.GeoDataFrame.from_features(\n            [item.to_dict() for item in intersecting_items.values()],\n            crs=\"EPSG:4326\",\n        )\n        intersecting_items_gdf[\"footprint_index\"] = i\n        intersecting_items_gdf[\"s2id\"] = list(intersecting_items.keys())\n        # Some item geometries might be invalid (probably because of the arctic circle)\n        # We will drop those items, since they cannot be used for intersection calculations\n        intersecting_items_gdf = intersecting_items_gdf[intersecting_items_gdf.geometry.is_valid]\n        if intersecting_items_gdf.empty:\n            logger.info(\n                f\"No valid Sentinel-2 items found for footprint #{i} in the date range {start_date} to {end_date}.\"\n            )\n            matches[i] = None\n            continue\n        # Get to UTM zone for better area calculations\n        utm_zone = intersecting_items_gdf.estimate_utm_crs()\n        # Calculate intersection area ratio\n        intersecting_items_gdf[\"intersection_area\"] = (\n            intersecting_items_gdf.intersection(row.geometry).to_crs(utm_zone).area\n        )\n        # We need a geodataframe containing only our wanted row, since to_crs is not available for a single row\n        intersecting_items_gdf[\"aoi_area\"] = aoi.loc[[i]].to_crs(utm_zone).iloc[0].geometry.area\n        intersecting_items_gdf[\"intersection_ratio\"] = (\n            intersecting_items_gdf[\"intersection_area\"] / intersecting_items_gdf[\"aoi_area\"]\n        )\n        # Filter items based on the minimum intersection ratio\n        max_intersection = intersecting_items_gdf[\"intersection_ratio\"].max()\n        intersecting_items_gdf = intersecting_items_gdf[intersecting_items_gdf[\"intersection_ratio\"] &gt;= min_intersects]\n        if intersecting_items_gdf.empty:\n            logger.info(\n                f\"No Sentinel-2 items found for {i} with sufficient intersection ratio \"\n                f\"({min_intersects}, maximum was {max_intersection:.4f})\"\n                f\" in the date range {start_date} to {end_date}.\"\n            )\n            matches[i] = None\n            continue\n        intersecting_items_gdf[\"datetime\"] = pd.to_datetime(intersecting_items_gdf[\"datetime\"])\n        intersecting_items_gdf[\"time_diff\"] = abs(intersecting_items_gdf[\"datetime\"] - row[\"date\"])\n        intersecting_items_gdf[\"score_cloud\"] = ((100.0 - intersecting_items_gdf[\"eo:cloud_cover\"]) / 5) ** 2\n        intersecting_items_gdf[\"score_fill\"] = ((100.0 - intersecting_items_gdf[\"intersection_ratio\"] * 100) / 5) ** 2\n        intersecting_items_gdf[\"score_time_diff\"] = (\n            intersecting_items_gdf[\"time_diff\"].dt.total_seconds() / (2 * 24 * 3600)\n        ) ** 2\n\n        intersecting_items_gdf[\"score\"] = (\n            intersecting_items_gdf[\"score_cloud\"]\n            + intersecting_items_gdf[\"score_fill\"]\n            + intersecting_items_gdf[\"score_time_diff\"]\n        )\n\n        # Debug the scoring\n        score_msg = f\"Scores for {i}:\\n\"\n        for j, match in intersecting_items_gdf.iterrows():\n            score_msg += (\n                f\"\\n- Match with {j}: \"\n                f\"Cloud Cover={match['eo:cloud_cover']}, \"\n                f\"Intersection Ratio={match['intersection_ratio']:.2f}, \"\n                f\"Time Diff={match['time_diff']}, \"\n                f\"Score Cloud={match['score_cloud']:.2f}, \"\n                f\"Score Fill={match['score_fill']:.2f}, \"\n                f\"Score Time Diff={match['score_time_diff']:.2f}, \"\n                f\"-&gt; Score={match['score']:.2f}\"\n            )\n        logger.debug(score_msg)\n\n        # Get the s2id with the lowest score\n        best_item = intersecting_items_gdf.loc[intersecting_items_gdf[\"score\"].idxmin()]\n        matches[i] = intersecting_items[best_item[\"s2id\"]]\n        scores.append(intersecting_items_gdf)\n\n    if save_scores:\n        scores_df = gpd.GeoDataFrame(pd.concat(scores))\n        scores_df.to_parquet(save_scores)\n\n    return matches\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.save_debug_geotiff","title":"save_debug_geotiff","text":"<pre><code>save_debug_geotiff(\n    dataset: xarray.Dataset,\n    output_path: pathlib.Path,\n    optical_bands: list[str],\n    mask_bands: list[str] | None = None,\n) -&gt; None\n</code></pre> <p>Save the raw dataset as a GeoTIFF file for debugging purposes.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to save</p> </li> <li> <code>output_path</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the output GeoTIFF file</p> </li> <li> <code>optical_bands</code>               (<code>list[str]</code>)           \u2013            <p>List of optical band names</p> </li> <li> <code>mask_bands</code>               (<code>list[str]</code>, default:                   <code>None</code> )           \u2013            <p>List of mask band names</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/debug_export.py</code> <pre><code>def save_debug_geotiff(\n    dataset: xr.Dataset,\n    output_path: Path,\n    optical_bands: list[str],\n    mask_bands: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Save the raw dataset as a GeoTIFF file for debugging purposes.\n\n    Args:\n        dataset (xr.Dataset): Dataset to save\n        output_path (Path): Path to the output GeoTIFF file\n        optical_bands (list[str]): List of optical band names\n        mask_bands (list[str]): List of mask band names\n\n    \"\"\"\n    output_path.mkdir(parents=True, exist_ok=True)\n    optical = dataset[optical_bands].to_dataarray(dim=\"band\").fillna(0).astype(\"uint16\")\n    optical.rio.to_raster(output_path / \"optical_raw.tiff\")\n\n    band_info = \"Optical Bands:\\n\"\n    band_info += \"\\n\".join([f\" - {i + 1}: {band}\" for i, band in enumerate(optical_bands)])\n\n    if mask_bands:\n        masks = dataset[mask_bands].to_dataarray(dim=\"band\").fillna(0).astype(\"uint8\")\n        masks.rio.to_raster(output_path / \"mask_raw.tiff\")\n        band_info += \"\\n\\nMask Bands:\\n\"\n        band_info += \"\\n\".join([f\" - {i + 1}: {band}\" for i, band in enumerate(mask_bands)])\n\n    (output_path / \"bands.txt\").write_text(band_info)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/cdse_scene/#darts_acquisition.s2.cdse_scene.search_cdse_s2_sr","title":"search_cdse_s2_sr","text":"<pre><code>search_cdse_s2_sr(\n    intersects=None,\n    tiles: list[str] | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n) -&gt; dict[str, pystac.Item]\n</code></pre> <p>Search for Sentinel-2 scenes via STAC based on an area of interest (intersects) and date range.</p> Note <p><code>start_date</code> and <code>end_date</code> will be concatted with a <code>/</code> to form a date range. Read more about the date format here: https://pystac-client.readthedocs.io/en/stable/api.html#pystac_client.Client.search</p> <p>Parameters:</p> <ul> <li> <code>intersects</code>               (<code>any</code>, default:                   <code>None</code> )           \u2013            <p>The geometry object to search for Sentinel-2 tiles. Can be anything implementing the <code>__geo_interface__</code> protocol, such as a GeoDataFrame or a shapely geometry. If None, and tiles is also None, the search will be performed globally. If set and tiles is also set, will be ignored.</p> </li> <li> <code>tiles</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of MGRS tile IDs to filter the search. If set, ignores intersects parameter. Defaults to None.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by pystac_client. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> <li> <code>months</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of months (1-12) to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> <li> <code>years</code>               (<code>list[int] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of years to filter the search. Only used if start_date and end_date are None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, pystac.Item]</code>           \u2013            <p>dict[str, Item]: A dictionary of found Sentinel-2 items as values and the s2id as keys.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/cdse_scene.py</code> <pre><code>@stopwatch(\"Searching for Sentinel-2 scenes in CDSE\", printer=logger.debug)\ndef search_cdse_s2_sr(\n    intersects=None,\n    tiles: list[str] | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n    months: list[int] | None = None,\n    years: list[int] | None = None,\n) -&gt; dict[str, Item]:\n    \"\"\"Search for Sentinel-2 scenes via STAC based on an area of interest (intersects) and date range.\n\n    Note:\n        `start_date` and `end_date` will be concatted with a `/` to form a date range.\n        Read more about the date format here: https://pystac-client.readthedocs.io/en/stable/api.html#pystac_client.Client.search\n\n    Args:\n        intersects (any): The geometry object to search for Sentinel-2 tiles.\n            Can be anything implementing the `__geo_interface__` protocol, such as a GeoDataFrame or a shapely geometry.\n            If None, and tiles is also None, the search will be performed globally.\n            If set and tiles is also set, will be ignored.\n        tiles (list[str] | None, optional): List of MGRS tile IDs to filter the search.\n            If set, ignores intersects parameter.\n            Defaults to None.\n        start_date (str): Starting date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by pystac_client.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n        months (list[int] | None, optional): List of months (1-12) to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n        years (list[int] | None, optional): List of years to filter the search.\n            Only used if start_date and end_date are None.\n            Defaults to None.\n\n    Returns:\n        dict[str, Item]: A dictionary of found Sentinel-2 items as values and the s2id as keys.\n\n    \"\"\"\n    catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n\n    if tiles is not None and intersects is not None:\n        logger.warning(\"Both tile and intersects provided. Ignoring intersects parameter.\")\n        intersects = None\n\n    cql2_filter = _build_cql2_filter(tiles, max_cloud_cover, max_snow_cover)\n\n    if start_date is not None and end_date is not None:\n        if months is not None or years is not None:\n            logger.warning(\"Both date range and months/years filtering provided. Ignoring months/years filter.\")\n        logger.debug(\n            f\"Searching CDSE for scenes between {start_date} and {end_date} ({cql2_filter}, {type(intersects)}).\"\n        )\n        search = catalog.search(\n            collections=[\"sentinel-2-l2a\"],\n            intersects=intersects,\n            datetime=f\"{start_date}/{end_date}\",\n            filter=cql2_filter,\n        )\n        found_items = list(search.items())\n    elif months is not None or years is not None:\n        if months is None:\n            months = list(range(1, 13))\n        if years is None:\n            years = list(range(2017, 2026))\n        found_items = set()\n        for year in years:\n            for month in months:\n                search = catalog.search(\n                    collections=[\"sentinel-2-l2a\"],\n                    intersects=intersects,\n                    datetime=f\"{year}-{month:02d}\",\n                    filter=cql2_filter,\n                )\n                found_items.update(list(search.items()))\n    else:\n        logger.warning(\"No valid date filtering provided. This may result in a too large number of scenes for CDSE.\")\n        search = catalog.search(\n            collections=[\"sentinel-2-l2a\"],\n            intersects=intersects,\n            filter=cql2_filter,\n        )\n        found_items = list(search.items())\n\n    if len(found_items) == 0:\n        logger.debug(\n            \"No Sentinel-2 items found for the given parameters:\"\n            f\" {intersects=}, {start_date=}, {end_date=}, {max_cloud_cover=}\"\n        )\n        return {}\n    logger.debug(f\"Found {len(found_items)} Sentinel-2 items in CDSE.\")\n    return {item.id: item for item in found_items}\n</code></pre>"},{"location":"reference/darts_acquisition/s2/debug_export/","title":"debug_export","text":""},{"location":"reference/darts_acquisition/s2/debug_export/#darts_acquisition.s2.debug_export","title":"darts_acquisition.s2.debug_export","text":"<p>Utility functions for exporting debug data.</p>"},{"location":"reference/darts_acquisition/s2/debug_export/#darts_acquisition.s2.debug_export.save_debug_geotiff","title":"save_debug_geotiff","text":"<pre><code>save_debug_geotiff(\n    dataset: xarray.Dataset,\n    output_path: pathlib.Path,\n    optical_bands: list[str],\n    mask_bands: list[str] | None = None,\n) -&gt; None\n</code></pre> <p>Save the raw dataset as a GeoTIFF file for debugging purposes.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to save</p> </li> <li> <code>output_path</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the output GeoTIFF file</p> </li> <li> <code>optical_bands</code>               (<code>list[str]</code>)           \u2013            <p>List of optical band names</p> </li> <li> <code>mask_bands</code>               (<code>list[str]</code>, default:                   <code>None</code> )           \u2013            <p>List of mask band names</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/debug_export.py</code> <pre><code>def save_debug_geotiff(\n    dataset: xr.Dataset,\n    output_path: Path,\n    optical_bands: list[str],\n    mask_bands: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Save the raw dataset as a GeoTIFF file for debugging purposes.\n\n    Args:\n        dataset (xr.Dataset): Dataset to save\n        output_path (Path): Path to the output GeoTIFF file\n        optical_bands (list[str]): List of optical band names\n        mask_bands (list[str]): List of mask band names\n\n    \"\"\"\n    output_path.mkdir(parents=True, exist_ok=True)\n    optical = dataset[optical_bands].to_dataarray(dim=\"band\").fillna(0).astype(\"uint16\")\n    optical.rio.to_raster(output_path / \"optical_raw.tiff\")\n\n    band_info = \"Optical Bands:\\n\"\n    band_info += \"\\n\".join([f\" - {i + 1}: {band}\" for i, band in enumerate(optical_bands)])\n\n    if mask_bands:\n        masks = dataset[mask_bands].to_dataarray(dim=\"band\").fillna(0).astype(\"uint8\")\n        masks.rio.to_raster(output_path / \"mask_raw.tiff\")\n        band_info += \"\\n\\nMask Bands:\\n\"\n        band_info += \"\\n\".join([f\" - {i + 1}: {band}\" for i, band in enumerate(mask_bands)])\n\n    (output_path / \"bands.txt\").write_text(band_info)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/","title":"gee_scene","text":""},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene","title":"darts_acquisition.s2.gee_scene","text":"<p>Sentinel-2 related data loading. Should be used temporary and maybe moved to the acquisition package.</p>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager","title":"GEEStoreManager","text":"<pre><code>GEEStoreManager(\n    store: pathlib.Path | str | None,\n    bands_mapping: dict[str, str],\n)\n</code></pre> <p>               Bases: <code>darts_acquisition.s2.raw_data_store.StoreManager[ee.Image]</code></p> <p>Raw Data Store manager for GEE.</p> <p>Initialize the store manager.</p> <p>Parameters:</p> <ul> <li> <code>store</code>               (<code>str | pathlib.Path | None</code>)           \u2013            <p>Directory path for storing raw sentinel 2 data</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>)           \u2013            <p>A mapping from bands to obtain.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>def __init__(self, store: Path | str | None, bands_mapping: dict[str, str]):\n    \"\"\"Initialize the store manager.\n\n    Args:\n        store (str | Path | None): Directory path for storing raw sentinel 2 data\n        bands_mapping (dict[str, str]): A mapping from bands to obtain.\n\n    \"\"\"\n    bands = list(bands_mapping.keys())\n    super().__init__(bands, store)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands = darts_acquisition.s2.raw_data_store.StoreManager(\n    bands\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.store","title":"store  <code>instance-attribute</code>","text":"<pre><code>store = (\n    pathlib.Path(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        )\n    )\n    if isinstance(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        ),\n        str,\n    )\n    else darts_acquisition.s2.raw_data_store.StoreManager(\n        store\n    )\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.complete","title":"complete","text":"<pre><code>complete(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene in the store contains all requested bands.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if all requested bands are present, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def complete(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene in the store contains all requested bands.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if all requested bands are present, False otherwise\n\n    \"\"\"\n    return len(self.missing_bands(identifier)) == 0\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.download_and_store","title":"download_and_store","text":"<pre><code>download_and_store(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n)\n</code></pre> <p>Download a scene from the source and store it in the local store.</p> <p>Store must be provided! Will do nothing if all required bands are already present.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def download_and_store(self, item: str | SceneItem):\n    \"\"\"Download a scene from the source and store it in the local store.\n\n    Store must be provided!\n    Will do nothing if all required bands are already present.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to download and store scenes!\"\n    identifier = self.identifier(item)\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        return\n    dataset = self.download_scene_from_source(item, missing_bands)\n    self.save_to_store(dataset, identifier)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.download_scene_from_source","title":"download_scene_from_source","text":"<pre><code>download_scene_from_source(\n    s2item: str | ee.Image, bands: list[str]\n) -&gt; xarray.Dataset\n</code></pre> <p>Download a Sentinel-2 scene from GEE.</p> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | ee.Image</code>)           \u2013            <p>The Sentinel-2 image ID or the corresponding ee.Image.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>List of bands to download.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The downloaded scene as xarray Dataset.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>def download_scene_from_source(self, s2item: str | ee.Image, bands: list[str]) -&gt; xr.Dataset:\n    \"\"\"Download a Sentinel-2 scene from GEE.\n\n    Args:\n        s2item (str | ee.Image): The Sentinel-2 image ID or the corresponding ee.Image.\n        bands (list[str]): List of bands to download.\n\n    Returns:\n        xr.Dataset: The downloaded scene as xarray Dataset.\n\n    \"\"\"\n    if isinstance(s2item, str):\n        s2id = s2item\n        s2item = ee.Image(f\"COPERNICUS/S2_SR/{s2id}\")\n    else:\n        s2id = s2item.id().getInfo().split(\"/\")[-1]\n\n    s2item = s2item.select(bands)\n\n    ds_s2 = xr.open_dataset(\n        s2item,\n        engine=\"ee\",\n        geometry=s2item.geometry(),\n        crs=s2item.select(0).projection().crs().getInfo(),\n        scale=10,\n    )\n    props = s2item.getInfo()[\"properties\"]\n    ds_s2.attrs[\"azimuth\"] = props.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\", float(\"nan\"))\n    ds_s2.attrs[\"elevation\"] = props.get(\"MEAN_SOLAR_ZENITH_ANGLE\", float(\"nan\"))\n\n    ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n    ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\").rename({\"X\": \"x\", \"Y\": \"y\"}).transpose(\"y\", \"x\")\n    ds_s2 = ds_s2.odc.assign_crs(ds_s2.attrs[\"crs\"])\n    with stopwatch(\"Downloading data from GEE\", printer=logger.debug):\n        ds_s2.load()\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.encodings","title":"encodings","text":"<pre><code>encodings(bands: list[str]) -&gt; dict[str, dict[str, str]]\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>def encodings(self, bands: list[str]) -&gt; dict[str, dict[str, str]]:  # noqa: D102\n    encodings = {\n        band: {\n            \"dtype\": \"uint16\",\n            \"compressors\": BloscCodec(cname=\"zstd\", clevel=3, shuffle=\"bitshuffle\"),\n            \"chunks\": (4096, 4096),\n        }\n        for band in bands\n    }\n    for band in set(bands) - {\"SCL\"}:\n        encodings[band][\"_FillValue\"] = 0\n    encodings[\"SCL\"][\"dtype\"] = \"uint8\"\n    return encodings\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.exists","title":"exists","text":"<pre><code>exists(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene already exists in the local raw data store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the scene exists in the store, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def exists(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene already exists in the local raw data store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if the scene exists in the store, False otherwise\n\n    \"\"\"\n    if not self.store:\n        return False\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return scene_path.exists()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.identifier","title":"identifier","text":"<pre><code>identifier(s2item: str | ee.Image) -&gt; str\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>def identifier(self, s2item: str | ee.Image) -&gt; str:  # noqa: D102\n    s2id = s2item.id().getInfo().split(\"/\")[-1] if isinstance(s2item, ee.Image) else s2item\n    return f\"gee-s2-sr-scene-{s2id}\"\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.load","title":"load","text":"<pre><code>load(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n    force: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a scene.</p> <p>If <code>force==True</code> will download the scene from source even if present in store. Else, will try to open the scene from store first and only download missing bands. Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will download the scene even if present. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def load(self, item: str | SceneItem, force: bool = False) -&gt; xr.Dataset:\n    \"\"\"Load a scene.\n\n    If `force==True` will download the scene from source even if present in store.\n    Else, will try to open the scene from store first and only download missing bands.\n    Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n        force (bool, optional): If True, will download the scene even if present. Defaults to False.\n\n    Returns:\n        xr.Dataset: The loaded scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    if force:\n        logger.debug(f\"Force downloading scene {identifier} from source.\")\n        dataset = self.download_scene_from_source(item, self.bands)\n        if self.store:\n            self.save_to_store(dataset, identifier)\n        return dataset\n\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        logger.debug(f\"Scene {identifier} is complete, opening from store.\")\n        return self.open(item)\n    logger.debug(f\"Scene {identifier} is missing bands {missing_bands}, downloading from source.\")\n    dataset = self.download_scene_from_source(item, missing_bands)\n    if self.store:\n        self.save_to_store(dataset, identifier)\n    return dataset\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.missing_bands","title":"missing_bands","text":"<pre><code>missing_bands(identifier: str) -&gt; list[str]\n</code></pre> <p>Get the list of missing bands for a scene in the store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: List of missing bands</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def missing_bands(self, identifier: str) -&gt; list[str]:\n    \"\"\"Get the list of missing bands for a scene in the store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        list[str]: List of missing bands\n\n    \"\"\"\n    if not self.store:\n        return self.bands\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    if not scene_path.exists():\n        return self.bands\n\n    dataset = xr.open_zarr(scene_path, consolidated=False)\n    required_bands = set(self.bands)\n    present_bands = set(dataset.data_vars)\n    missing = required_bands - present_bands\n    return list(missing)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.open","title":"open","text":"<pre><code>open(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n) -&gt; xarray.Dataset\n</code></pre> <p>Open a scene from local store.</p> <p>Store must be provided and the scene must be present in store!</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The opened scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def open(self, item: str | SceneItem) -&gt; xr.Dataset:\n    \"\"\"Open a scene from local store.\n\n    Store must be provided and the scene must be present in store!\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open\n\n    Returns:\n        xr.Dataset: The opened scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    assert self.complete(identifier), f\"Scene {identifier} is incomplete in store!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return xr.open_zarr(scene_path, consolidated=False).set_coords(\"spatial_ref\").load()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.GEEStoreManager.save_to_store","title":"save_to_store","text":"<pre><code>save_to_store(\n    dataset: xarray.Dataset, identifier: str\n) -&gt; None\n</code></pre> <p>Save a scene dataset to the local raw data store.</p> <p>Will append new bands to existing store if scene already exists. Will overwrite existing bands in an existing store if scene already exists.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to save</p> </li> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def save_to_store(self, dataset: xr.Dataset, identifier: str) -&gt; None:\n    \"\"\"Save a scene dataset to the local raw data store.\n\n    Will append new bands to existing store if scene already exists.\n    Will overwrite existing bands in an existing store if scene already exists.\n\n    Args:\n        dataset (xr.Dataset): Dataset to save\n        identifier (str): Unique identifier for the scene\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to save scenes!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    encoding = self.encodings(list(dataset.data_vars))\n    if not scene_path.exists():\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"w\")\n    else:\n        # Assert that the coordinates match\n        existing_dataset = xr.open_zarr(scene_path, consolidated=False)\n        xr.testing.assert_allclose(existing_dataset.coords, dataset.coords)\n        # Overwrite dataset coords to avoid conflicts by floating point precision issues\n        dataset[\"x\"] = existing_dataset.x\n        dataset[\"y\"] = existing_dataset.y\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"a\")\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager","title":"StoreManager","text":"<pre><code>StoreManager(\n    bands: list[str],\n    store: str | pathlib.Path | None = None,\n)\n</code></pre> <p>               Bases: <code>abc.ABC</code>, <code>typing.Generic[darts_acquisition.s2.raw_data_store.SceneItem]</code></p> <p>Manager for storing raw sentinel 2 data.</p> <p>This class is an abstract base class and should be extended to implement the respective downloading methods.</p> <p>Usage:</p> <pre><code>1. \"Normal\" usage:\n\n```python\n    store_manager = StoreManager(store_path)\n    ds_s2 = store_manager.load(identifier, bands)\n```\n\n2. Force download:\n\n```python\n    store_manager = StoreManager(store_path)\n    ds_s2 = store_manager.load(identifier, force=True)\n```\n\n3. Download only (and only if missing) and store the scene:\n\n```python\n    store_manager = StoreManager(store_path)\n    store_manager.download(identifier) # store_path must be not None\n```\n\n4. Offline mode:\n\n```python\n    store_manager = StoreManager(store_path)\n    store_manager.open(identifier) # store_path must be not None, bands must be complete\n```\n</code></pre> <p>Initialize the store manager.</p> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>List of bands to manage</p> </li> <li> <code>store</code>               (<code>str | pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory path for storing raw sentinel 2 data</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def __init__(self, bands: list[str], store: str | Path | None = None):\n    \"\"\"Initialize the store manager.\n\n    Args:\n        bands (list[str]): List of bands to manage\n        store (str | Path | None): Directory path for storing raw sentinel 2 data\n\n    \"\"\"\n    self.bands = bands\n    self.store = Path(store) if isinstance(store, str) else store\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands = darts_acquisition.s2.raw_data_store.StoreManager(\n    bands\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.store","title":"store  <code>instance-attribute</code>","text":"<pre><code>store = (\n    pathlib.Path(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        )\n    )\n    if isinstance(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        ),\n        str,\n    )\n    else darts_acquisition.s2.raw_data_store.StoreManager(\n        store\n    )\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.complete","title":"complete","text":"<pre><code>complete(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene in the store contains all requested bands.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if all requested bands are present, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def complete(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene in the store contains all requested bands.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if all requested bands are present, False otherwise\n\n    \"\"\"\n    return len(self.missing_bands(identifier)) == 0\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.download_and_store","title":"download_and_store","text":"<pre><code>download_and_store(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n)\n</code></pre> <p>Download a scene from the source and store it in the local store.</p> <p>Store must be provided! Will do nothing if all required bands are already present.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def download_and_store(self, item: str | SceneItem):\n    \"\"\"Download a scene from the source and store it in the local store.\n\n    Store must be provided!\n    Will do nothing if all required bands are already present.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to download and store scenes!\"\n    identifier = self.identifier(item)\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        return\n    dataset = self.download_scene_from_source(item, missing_bands)\n    self.save_to_store(dataset, identifier)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.download_scene_from_source","title":"download_scene_from_source  <code>abstractmethod</code>","text":"<pre><code>download_scene_from_source(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n    bands: list[str],\n) -&gt; xarray.Dataset\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>@abstractmethod\ndef download_scene_from_source(self, item: str | SceneItem, bands: list[str]) -&gt; xr.Dataset: ...  # noqa: D102\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.encodings","title":"encodings  <code>abstractmethod</code>","text":"<pre><code>encodings(bands: list[str]) -&gt; dict[str, dict[str, str]]\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>@abstractmethod\ndef encodings(self, bands: list[str]) -&gt; dict[str, dict[str, str]]: ...  # noqa: D102\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.exists","title":"exists","text":"<pre><code>exists(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene already exists in the local raw data store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the scene exists in the store, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def exists(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene already exists in the local raw data store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if the scene exists in the store, False otherwise\n\n    \"\"\"\n    if not self.store:\n        return False\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return scene_path.exists()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.identifier","title":"identifier  <code>abstractmethod</code>","text":"<pre><code>identifier(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n) -&gt; str\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>@abstractmethod\ndef identifier(self, item: str | SceneItem) -&gt; str: ...  # noqa: D102\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.load","title":"load","text":"<pre><code>load(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n    force: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a scene.</p> <p>If <code>force==True</code> will download the scene from source even if present in store. Else, will try to open the scene from store first and only download missing bands. Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will download the scene even if present. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def load(self, item: str | SceneItem, force: bool = False) -&gt; xr.Dataset:\n    \"\"\"Load a scene.\n\n    If `force==True` will download the scene from source even if present in store.\n    Else, will try to open the scene from store first and only download missing bands.\n    Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n        force (bool, optional): If True, will download the scene even if present. Defaults to False.\n\n    Returns:\n        xr.Dataset: The loaded scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    if force:\n        logger.debug(f\"Force downloading scene {identifier} from source.\")\n        dataset = self.download_scene_from_source(item, self.bands)\n        if self.store:\n            self.save_to_store(dataset, identifier)\n        return dataset\n\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        logger.debug(f\"Scene {identifier} is complete, opening from store.\")\n        return self.open(item)\n    logger.debug(f\"Scene {identifier} is missing bands {missing_bands}, downloading from source.\")\n    dataset = self.download_scene_from_source(item, missing_bands)\n    if self.store:\n        self.save_to_store(dataset, identifier)\n    return dataset\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.missing_bands","title":"missing_bands","text":"<pre><code>missing_bands(identifier: str) -&gt; list[str]\n</code></pre> <p>Get the list of missing bands for a scene in the store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: List of missing bands</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def missing_bands(self, identifier: str) -&gt; list[str]:\n    \"\"\"Get the list of missing bands for a scene in the store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        list[str]: List of missing bands\n\n    \"\"\"\n    if not self.store:\n        return self.bands\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    if not scene_path.exists():\n        return self.bands\n\n    dataset = xr.open_zarr(scene_path, consolidated=False)\n    required_bands = set(self.bands)\n    present_bands = set(dataset.data_vars)\n    missing = required_bands - present_bands\n    return list(missing)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.open","title":"open","text":"<pre><code>open(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n) -&gt; xarray.Dataset\n</code></pre> <p>Open a scene from local store.</p> <p>Store must be provided and the scene must be present in store!</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The opened scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def open(self, item: str | SceneItem) -&gt; xr.Dataset:\n    \"\"\"Open a scene from local store.\n\n    Store must be provided and the scene must be present in store!\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open\n\n    Returns:\n        xr.Dataset: The opened scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    assert self.complete(identifier), f\"Scene {identifier} is incomplete in store!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return xr.open_zarr(scene_path, consolidated=False).set_coords(\"spatial_ref\").load()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.StoreManager.save_to_store","title":"save_to_store","text":"<pre><code>save_to_store(\n    dataset: xarray.Dataset, identifier: str\n) -&gt; None\n</code></pre> <p>Save a scene dataset to the local raw data store.</p> <p>Will append new bands to existing store if scene already exists. Will overwrite existing bands in an existing store if scene already exists.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to save</p> </li> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def save_to_store(self, dataset: xr.Dataset, identifier: str) -&gt; None:\n    \"\"\"Save a scene dataset to the local raw data store.\n\n    Will append new bands to existing store if scene already exists.\n    Will overwrite existing bands in an existing store if scene already exists.\n\n    Args:\n        dataset (xr.Dataset): Dataset to save\n        identifier (str): Unique identifier for the scene\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to save scenes!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    encoding = self.encodings(list(dataset.data_vars))\n    if not scene_path.exists():\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"w\")\n    else:\n        # Assert that the coordinates match\n        existing_dataset = xr.open_zarr(scene_path, consolidated=False)\n        xr.testing.assert_allclose(existing_dataset.coords, dataset.coords)\n        # Overwrite dataset coords to avoid conflicts by floating point precision issues\n        dataset[\"x\"] = existing_dataset.x\n        dataset[\"y\"] = existing_dataset.y\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"a\")\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene._get_band_mapping","title":"_get_band_mapping","text":"<pre><code>_get_band_mapping(\n    bands_mapping: dict[str, str] | typing.Literal[\"all\"],\n) -&gt; dict[str, str]\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>def _get_band_mapping(bands_mapping: dict[str, str] | Literal[\"all\"]) -&gt; dict[str, str]:\n    if bands_mapping == \"all\":\n        # Mapping according to spyndex band common names:\n        # for key, band in spyndex.bands.items():\n        #     if not hasattr(band, \"sentinel2a\"): continue\n        #     print(f\"{band.sentinel2a.band}: {band.common_name}\")\n        bands_mapping = {\n            \"B1\": \"coastal\",\n            \"B2\": \"blue\",\n            \"B3\": \"green\",\n            \"B4\": \"red\",\n            \"B5\": \"rededge071\",\n            \"B6\": \"rededge075\",\n            \"B7\": \"rededge078\",\n            \"B8\": \"nir\",\n            \"B8A\": \"nir08\",\n            \"B9\": \"nir09\",\n            \"B11\": \"swir16\",\n            \"B12\": \"swir22\",\n        }\n\n    if \"SCL\" not in bands_mapping.keys():\n        bands_mapping[\"SCL\"] = \"s2_scl\"\n    return bands_mapping\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.convert_masks","title":"convert_masks","text":"<pre><code>convert_masks(ds_s2: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Convert the Sentinel-2 scl mask into our own mask format inplace.</p> <p>https://sentiwiki.copernicus.eu/web/s2-processing#S2Processing-ClassificationMaskGeneration</p> <p>Invalid: S2 SCL \u2192 0,1 Low Quality S2: S2 SCL != 0,1 \u2192 3,8,9,11 High Quality: S2 SCL != 0,1,3,8,9,11 \u2192 Alles andere (2,4,5,6,7,10)</p> <p>Parameters:</p> <ul> <li> <code>ds_s2</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Sentinel-2 dataset containing the SCL band.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The modified dataset.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/quality_mask.py</code> <pre><code>@stopwatch(\"Converting Sentinel-2 masks\", printer=logger.debug)\ndef convert_masks(ds_s2: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Convert the Sentinel-2 scl mask into our own mask format inplace.\n\n    https://sentiwiki.copernicus.eu/web/s2-processing#S2Processing-ClassificationMaskGeneration\n\n    Invalid: S2 SCL \u2192 0,1\n    Low Quality S2: S2 SCL != 0,1 \u2192 3,8,9,11\n    High Quality: S2 SCL != 0,1,3,8,9,11 \u2192 Alles andere (2,4,5,6,7,10)\n\n    Args:\n        ds_s2 (xr.Dataset): The Sentinel-2 dataset containing the SCL band.\n\n    Returns:\n        xr.Dataset: The modified dataset.\n\n    \"\"\"\n    assert \"s2_scl\" in ds_s2.data_vars, \"The dataset does not contain the SCL band.\"\n\n    if has_cuda_and_cupy() and ds_s2.cupy.is_cupy:\n        invalids = ds_s2[\"s2_scl\"].fillna(0).isin(cp.array([0, 1]))\n        high_quality = ds_s2[\"s2_scl\"].isin(cp.array([2, 4, 5, 6, 7, 10]))\n    else:\n        invalids = ds_s2[\"s2_scl\"].fillna(0).isin([0, 1])\n        high_quality = ds_s2[\"s2_scl\"].isin([2, 4, 5, 6, 7, 10])\n    ds_s2[\"quality_data_mask\"] = (\n        (~invalids).astype(\"uint8\")  # 0 for invalid, 1 for valid\n        + (high_quality).astype(\"uint8\")  # +1 for high quality\n    )\n\n    ds_s2[\"quality_data_mask\"].attrs[\"data_source\"] = \"s2\"\n    ds_s2[\"quality_data_mask\"].attrs[\"long_name\"] = \"Quality Data Mask\"\n    ds_s2[\"quality_data_mask\"].attrs[\"description\"] = \"0 = Invalid, 1 = Low Quality, 2 = High Quality\"\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.download_gee_s2_sr_scene","title":"download_gee_s2_sr_scene","text":"<pre><code>download_gee_s2_sr_scene(\n    s2item: str | ee.Image,\n    store: pathlib.Path,\n    bands_mapping: dict | typing.Literal[\"all\"] = {\n        \"B2\": \"blue\",\n        \"B3\": \"green\",\n        \"B4\": \"red\",\n        \"B8\": \"nir\",\n    },\n)\n</code></pre> <p>Download a Sentinel-2 scene from Google Earth Engine and store it in the local data store.</p> <p>This function downloads Sentinel-2 Level-2A surface reflectance data from Google Earth Engine (GEE) and stores it locally in a compressed zarr store for efficient repeated access.</p> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | ee.Image</code>)           \u2013            <p>Sentinel-2 scene identifier (e.g., \"20230615T123456_20230615T123659_T33UUP\") or an ee.Image object from the COPERNICUS/S2_SR collection.</p> </li> <li> <code>store</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the local zarr store directory where the scene will be saved.</p> </li> <li> <code>bands_mapping</code>               (<code>dict | typing.Literal['all']</code>, default:                   <code>{'B2': 'blue', 'B3': 'green', 'B4': 'red', 'B8': 'nir'}</code> )           \u2013            <p>Mapping of Sentinel-2 band names to custom band names. Keys should be GEE band names (e.g., \"B2\", \"B3\"), values are the desired output names. Use \"all\" to load all optical bands and SCL. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> </ul> Note <ul> <li>Requires Google Earth Engine authentication. Use <code>ee.Initialize()</code> before calling.</li> <li>All bands are downloaded at 10m resolution.</li> <li>Data is stored with zstd compression for efficient storage.</li> <li>The SCL (Scene Classification Layer) band is automatically included if not specified.</li> </ul> Example <p>Download Sentinel-2 scenes from GEE:</p> <pre><code>import ee\nfrom pathlib import Path\nfrom darts_acquisition import download_gee_s2_sr_scene\n\n# Initialize Earth Engine\nee.Initialize()\n\n# Download scene with all bands\ndownload_gee_s2_sr_scene(\n    s2item=\"20230615T123456_20230615T123659_T33UUP\",\n    store=Path(\"/data/s2_store\"),\n    bands_mapping=\"all\"\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>@stopwatch.f(\"Downloading Sentinel-2 scene from GEE if missing\", printer=logger.debug, print_kwargs=[\"s2item\"])\ndef download_gee_s2_sr_scene(\n    s2item: str | ee.Image,\n    store: Path,\n    bands_mapping: dict | Literal[\"all\"] = {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"},\n):\n    \"\"\"Download a Sentinel-2 scene from Google Earth Engine and store it in the local data store.\n\n    This function downloads Sentinel-2 Level-2A surface reflectance data from Google Earth\n    Engine (GEE) and stores it locally in a compressed zarr store for efficient repeated access.\n\n    Args:\n        s2item (str | ee.Image): Sentinel-2 scene identifier (e.g., \"20230615T123456_20230615T123659_T33UUP\")\n            or an ee.Image object from the COPERNICUS/S2_SR collection.\n        store (Path): Path to the local zarr store directory where the scene will be saved.\n        bands_mapping (dict | Literal[\"all\"], optional): Mapping of Sentinel-2 band names to\n            custom band names. Keys should be GEE band names (e.g., \"B2\", \"B3\"), values are\n            the desired output names. Use \"all\" to load all optical bands and SCL.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n\n    Note:\n        - Requires Google Earth Engine authentication. Use `ee.Initialize()` before calling.\n        - All bands are downloaded at 10m resolution.\n        - Data is stored with zstd compression for efficient storage.\n        - The SCL (Scene Classification Layer) band is automatically included if not specified.\n\n    Example:\n        Download Sentinel-2 scenes from GEE:\n\n        ```python\n        import ee\n        from pathlib import Path\n        from darts_acquisition import download_gee_s2_sr_scene\n\n        # Initialize Earth Engine\n        ee.Initialize()\n\n        # Download scene with all bands\n        download_gee_s2_sr_scene(\n            s2item=\"20230615T123456_20230615T123659_T33UUP\",\n            store=Path(\"/data/s2_store\"),\n            bands_mapping=\"all\"\n        )\n        ```\n\n    \"\"\"\n    bands_mapping = _get_band_mapping(bands_mapping)\n    store_manager = GEEStoreManager(\n        store=store,\n        bands_mapping=bands_mapping,\n    )\n\n    store_manager.download_and_store(s2item)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.get_aoi_from_gee_scene_ids","title":"get_aoi_from_gee_scene_ids","text":"<pre><code>get_aoi_from_gee_scene_ids(\n    scene_ids: list[str],\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Get the area of interest (AOI) as a GeoDataFrame from a list of Sentinel-2 scene IDs.</p> <p>Parameters:</p> <ul> <li> <code>scene_ids</code>               (<code>list[str]</code>)           \u2013            <p>List of Sentinel-2 scene IDs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>geopandas.GeoDataFrame</code>           \u2013            <p>gpd.GeoDataFrame: The AOI as a GeoDataFrame.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no Sentinel-2 items are found for the given scene IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>def get_aoi_from_gee_scene_ids(\n    scene_ids: list[str],\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get the area of interest (AOI) as a GeoDataFrame from a list of Sentinel-2 scene IDs.\n\n    Args:\n        scene_ids (list[str]): List of Sentinel-2 scene IDs.\n\n    Returns:\n        gpd.GeoDataFrame: The AOI as a GeoDataFrame.\n\n    Raises:\n        ValueError: If no Sentinel-2 items are found for the given scene IDs.\n\n    \"\"\"\n    geoms = []\n    for s2id in scene_ids:\n        s2item = ee.Image(f\"COPERNICUS/S2_SR/{s2id}\")\n        geom = s2item.geometry().getInfo()\n        geoms.append(geom)\n\n    if not geoms:\n        raise ValueError(\"No Sentinel-2 items found for the given scene IDs.\")\n\n    features = [{\"type\": \"Feature\", \"geometry\": geom, \"properties\": {}} for geom in geoms]\n    feature_collection = {\"type\": \"FeatureCollection\", \"features\": features}\n    gdf = gpd.GeoDataFrame.from_features(feature_collection, crs=\"EPSG:4326\")\n    return gdf\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.get_gee_s2_sr_scene_ids_from_geodataframe","title":"get_gee_s2_sr_scene_ids_from_geodataframe","text":"<pre><code>get_gee_s2_sr_scene_ids_from_geodataframe(\n    aoi: geopandas.GeoDataFrame | pathlib.Path | str,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; set[str]\n</code></pre> <p>Search for Sentinel-2 scenes via Earth Engine based on an aoi shapefile.</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame | pathlib.Path | str</code>)           \u2013            <p>AOI as a GeoDataFrame or path to a shapefile. If a path is provided, it will be read using geopandas.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by ee. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by ee. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>set[str]: Unique Sentinel-2 tile IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>@stopwatch(\"Searching for Sentinel-2 scenes in Earth Engine from AOI\", printer=logger.debug)\ndef get_gee_s2_sr_scene_ids_from_geodataframe(\n    aoi: gpd.GeoDataFrame | Path | str,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; set[str]:\n    \"\"\"Search for Sentinel-2 scenes via Earth Engine based on an aoi shapefile.\n\n    Args:\n        aoi (gpd.GeoDataFrame | Path | str): AOI as a GeoDataFrame or path to a shapefile.\n            If a path is provided, it will be read using geopandas.\n        start_date (str): Starting date in a format readable by ee.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by ee.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n\n    Returns:\n        set[str]: Unique Sentinel-2 tile IDs.\n\n    \"\"\"\n    # Disable max xxx cover if set to 100\n    if max_cloud_cover is not None and max_cloud_cover == 100:\n        max_cloud_cover = None\n    if max_snow_cover is not None and max_snow_cover == 100:\n        max_snow_cover = None\n\n    if isinstance(aoi, Path | str):\n        aoi = gpd.read_file(aoi)\n    aoi = aoi.to_crs(\"EPSG:4326\")\n    s2ids = set()\n    for i, row in aoi.iterrows():\n        geom = ee.Geometry.Polygon(list(row.geometry.exterior.coords))\n        if start_date is not None and end_date is not None:\n            ic = ee.ImageCollection(\"COPERNICUS/S2_SR\").filterBounds(geom).filterDate(start_date, end_date)\n            if max_cloud_cover:\n                ic = ic.filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", max_cloud_cover)\n            if max_snow_cover:\n                ic = ic.filterMetadata(\"SNOW_ICE_PERCENTAGE\", \"less_than\", max_snow_cover)\n            s2ids.update(ic.aggregate_array(\"system:index\").getInfo())\n        else:\n            logger.warning(\"No valid date filtering provided. This may result in a too large number of scenes for GEE.\")\n            ic = ee.ImageCollection(\"COPERNICUS/S2_SR\").filterBounds(geom)\n            if max_cloud_cover:\n                ic = ic.filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", max_cloud_cover)\n            if max_snow_cover:\n                ic = ic.filterMetadata(\"SNOW_ICE_PERCENTAGE\", \"less_than\", max_snow_cover)\n            s2ids.update(ic.aggregate_array(\"system:index\").getInfo())\n\n    logger.debug(f\"Found {len(s2ids)} Sentinel-2 tiles via ee.\")\n    return s2ids\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.get_gee_s2_sr_scene_ids_from_tile_ids","title":"get_gee_s2_sr_scene_ids_from_tile_ids","text":"<pre><code>get_gee_s2_sr_scene_ids_from_tile_ids(\n    tiles: list[str],\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; set[str]\n</code></pre> <p>Search for Sentinel-2 scenes via Earth Engine based on a list of tile IDs.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[str]</code>)           \u2013            <p>List of Sentinel-2 tile IDs.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Starting date in a format readable by ee. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Ending date in a format readable by ee. If None, months and years parameters will be used for filtering if set. Defaults to None.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 10.</p> </li> <li> <code>max_snow_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum percentage of snow cover. Defaults to 10.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>set[str]: Unique Sentinel-2 tile IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>def get_gee_s2_sr_scene_ids_from_tile_ids(\n    tiles: list[str],\n    start_date: str | None = None,\n    end_date: str | None = None,\n    max_cloud_cover: int | None = 10,\n    max_snow_cover: int | None = 10,\n) -&gt; set[str]:\n    \"\"\"Search for Sentinel-2 scenes via Earth Engine based on a list of tile IDs.\n\n    Args:\n        tiles (list[str]): List of Sentinel-2 tile IDs.\n        start_date (str): Starting date in a format readable by ee.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        end_date (str): Ending date in a format readable by ee.\n            If None, months and years parameters will be used for filtering if set.\n            Defaults to None.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 10.\n        max_snow_cover (int, optional): Maximum percentage of snow cover. Defaults to 10.\n\n    Returns:\n        set[str]: Unique Sentinel-2 tile IDs.\n\n    \"\"\"\n    # Disable max xxx cover if set to 100\n    if max_cloud_cover is not None and max_cloud_cover == 100:\n        max_cloud_cover = None\n    if max_snow_cover is not None and max_snow_cover == 100:\n        max_snow_cover = None\n\n    s2ids = set()\n    for tile in tiles:\n        if start_date is not None and end_date is not None:\n            ic = (\n                ee.ImageCollection(\"COPERNICUS/S2_SR\")\n                .filterDate(start_date, end_date)\n                .filterMetadata(\"MGRS_TILE\", \"equals\", tile)\n            )\n            if max_cloud_cover:\n                ic = ic.filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", max_cloud_cover)\n            if max_snow_cover:\n                ic = ic.filterMetadata(\"SNOW_ICE_PERCENTAGE\", \"less_than\", max_snow_cover)\n            s2ids.update(ic.aggregate_array(\"system:index\").getInfo())\n        else:\n            logger.warning(\"No valid date filtering provided. This may result in a too large number of scenes for GEE.\")\n            ic = ee.ImageCollection(\"COPERNICUS/S2_SR\").filterMetadata(\"MGRS_TILE\", \"equals\", tile)\n            if max_cloud_cover:\n                ic = ic.filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", max_cloud_cover)\n            if max_snow_cover:\n                ic = ic.filterMetadata(\"SNOW_ICE_PERCENTAGE\", \"less_than\", max_snow_cover)\n            s2ids.update(ic.aggregate_array(\"system:index\").getInfo())\n\n    logger.debug(f\"Found {len(s2ids)} Sentinel-2 tiles via ee.\")\n    return s2ids\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.load_gee_s2_sr_scene","title":"load_gee_s2_sr_scene","text":"<pre><code>load_gee_s2_sr_scene(\n    s2item: str | ee.Image,\n    bands_mapping: dict | typing.Literal[\"all\"] = {\n        \"B2\": \"blue\",\n        \"B3\": \"green\",\n        \"B4\": \"red\",\n        \"B8\": \"nir\",\n    },\n    store: pathlib.Path | None = None,\n    offline: bool = False,\n    output_dir_for_debug_geotiff: pathlib.Path\n    | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_utils.cuda.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel-2 scene from Google Earth Engine, downloading if necessary.</p> <p>This function loads Sentinel-2 Level-2A surface reflectance data from Google Earth Engine. If a local store is provided, the data is cached for efficient repeated access. The function handles quality masking, reflectance scaling with time-dependent offsets, and optional GPU acceleration. It also handles NaN values in the data by masking them as invalid.</p> <p>The download logic is basically as follows:</p> <pre><code>IF flag:raw-data-store THEN\n    IF exist_local THEN\n        open -&gt; memory\n    ELIF online THEN\n        download -&gt; memory\n        save\n    ELIF offline THEN\n        RAISE ERROR\n    ENDIF\nELIF online THEN\n    download -&gt; memory\nELIF offline THEN\n    RAISE ERROR\nENDIF\n</code></pre> <p>Parameters:</p> <ul> <li> <code>s2item</code>               (<code>str | ee.Image</code>)           \u2013            <p>Sentinel-2 scene identifier or ee.Image object from COPERNICUS/S2_SR.</p> </li> <li> <code>bands_mapping</code>               (<code>dict | typing.Literal['all']</code>, default:                   <code>{'B2': 'blue', 'B3': 'green', 'B4': 'red', 'B8': 'nir'}</code> )           \u2013            <p>Mapping of Sentinel-2 band names to custom band names. Keys should be GEE band names (e.g., \"B2\", \"B3\"), values are output names. Use \"all\" to load all optical bands and SCL. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>store</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to local zarr store for caching. If None, data is loaded directly without caching. Defaults to None.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only loads from local store without downloading. Requires <code>store</code> to be provided. If False, missing data is downloaded. Defaults to False.</p> </li> <li> <code>output_dir_for_debug_geotiff</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>If provided, writes raw data as GeoTIFF files for debugging. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_utils.cuda.DEFAULT_DEVICE</code> )           \u2013            <p>Device for processing (GPU or CPU). Defaults to DEFAULT_DEVICE.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Sentinel-2 dataset with the following data variables based on bands_mapping: - Optical bands (float32): Surface reflectance values [~-0.1 to ~1.0 for newer scenes,   ~0.0 to ~1.0 for scenes before 2022-01-25]   Default bands: blue, green, red, nir   Additional bands available: coastal, rededge071, rededge075, rededge078,   nir08, nir09, swir16, swir22   Each has attributes:   - long_name: \"Sentinel 2 {Band}\"   - units: \"Reflectance\"   - data_source: \"Sentinel-2 L2A via Google Earth Engine (COPERNICUS/S2_SR)\" - s2_scl (uint8): Scene Classification Layer   Attributes: long_name, description of class values (0=NO_DATA, 1=SATURATED, etc.) - quality_data_mask (uint8): Derived quality mask   - 0 = Invalid (no data, saturated, defective, or NaN values)   - 1 = Low quality (shadows, clouds, cirrus, snow/ice, water)   - 2 = High quality (clear vegetation or non-vegetated land) - valid_data_mask (uint8): Binary validity mask (1=valid, 0=invalid)</p> <p>Dataset attributes: - azimuth (float): Solar azimuth angle from MEAN_SOLAR_AZIMUTH_ANGLE - elevation (float): Solar elevation angle from MEAN_SOLAR_ZENITH_ANGLE - s2_tile_id (str): Full PRODUCT_ID from GEE - tile_id (str): Scene identifier - time (str): Acquisition timestamp</p> </li> </ul> Note <p>The <code>offline</code> parameter controls data fetching: - When <code>offline=False</code>: Automatically downloads missing data from GEE and stores it   in the local zarr store (if store is provided). - When <code>offline=True</code>: Only reads from the local store. Raises an error if data is   missing or if store is None.</p> <p>Reflectance processing: - For scenes &gt;= 2022-01-25: (DN / 10000.0) - 0.1 (processing baseline 04.00+) - For scenes &lt; 2022-01-25: DN / 10000.0 (older processing baseline) - NaN values are filled with 0 and marked as invalid in quality_data_mask - Pixels where SCL is NaN are also masked as invalid</p> <p>This function handles spatially random NaN values that can occur in GEE data by marking them as invalid and filling with 0 to prevent propagation in calculations.</p> <p>Quality mask derivation from SCL: - Invalid (0): NO_DATA, SATURATED_OR_DEFECTIVE, or NaN values - Low quality (1): CAST_SHADOWS, CLOUD_SHADOWS, CLOUD_*, THIN_CIRRUS, SNOW/ICE, WATER - High quality (2): VEGETATION, NOT_VEGETATED</p> Example <p>Load scene with local caching:</p> <pre><code>import ee\nfrom pathlib import Path\nfrom darts_acquisition import load_gee_s2_sr_scene\n\n# Initialize Earth Engine\nee.Initialize()\n\n# Load with caching\ns2_ds = load_gee_s2_sr_scene(\n    s2item=\"20230615T123456_20230615T123659_T33UUP\",\n    bands_mapping=\"all\",\n    store=Path(\"/data/s2_store\"),\n    offline=False  # Download if not cached\n)\n\n# Compute NDVI\nndvi = (s2_ds.nir - s2_ds.red) / (s2_ds.nir + s2_ds.red)\n\n# Filter to high quality pixels\ns2_filtered = s2_ds.where(s2_ds.quality_data_mask == 2)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/gee_scene.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel-2 scene from GEE\", printer=logger.debug, print_kwargs=[\"s2item\"])\ndef load_gee_s2_sr_scene(\n    s2item: str | ee.Image,\n    bands_mapping: dict | Literal[\"all\"] = {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"},\n    store: Path | None = None,\n    offline: bool = False,\n    output_dir_for_debug_geotiff: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel-2 scene from Google Earth Engine, downloading if necessary.\n\n    This function loads Sentinel-2 Level-2A surface reflectance data from Google Earth Engine.\n    If a local store is provided, the data is cached for efficient repeated access. The function\n    handles quality masking, reflectance scaling with time-dependent offsets, and optional GPU\n    acceleration. It also handles NaN values in the data by masking them as invalid.\n\n    The download logic is basically as follows:\n\n    ```\n    IF flag:raw-data-store THEN\n        IF exist_local THEN\n            open -&gt; memory\n        ELIF online THEN\n            download -&gt; memory\n            save\n        ELIF offline THEN\n            RAISE ERROR\n        ENDIF\n    ELIF online THEN\n        download -&gt; memory\n    ELIF offline THEN\n        RAISE ERROR\n    ENDIF\n    ```\n\n    Args:\n        s2item (str | ee.Image): Sentinel-2 scene identifier or ee.Image object from COPERNICUS/S2_SR.\n        bands_mapping (dict | Literal[\"all\"], optional): Mapping of Sentinel-2 band names to\n            custom band names. Keys should be GEE band names (e.g., \"B2\", \"B3\"), values are\n            output names. Use \"all\" to load all optical bands and SCL.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        store (Path | None, optional): Path to local zarr store for caching. If None, data is\n            loaded directly without caching. Defaults to None.\n        offline (bool, optional): If True, only loads from local store without downloading.\n            Requires `store` to be provided. If False, missing data is downloaded.\n            Defaults to False.\n        output_dir_for_debug_geotiff (Path | None, optional): If provided, writes raw data as\n            GeoTIFF files for debugging. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): Device for processing (GPU or CPU).\n            Defaults to DEFAULT_DEVICE.\n\n    Returns:\n        xr.Dataset: Sentinel-2 dataset with the following data variables based on bands_mapping:\n            - Optical bands (float32): Surface reflectance values [~-0.1 to ~1.0 for newer scenes,\n              ~0.0 to ~1.0 for scenes before 2022-01-25]\n              Default bands: blue, green, red, nir\n              Additional bands available: coastal, rededge071, rededge075, rededge078,\n              nir08, nir09, swir16, swir22\n              Each has attributes:\n              - long_name: \"Sentinel 2 {Band}\"\n              - units: \"Reflectance\"\n              - data_source: \"Sentinel-2 L2A via Google Earth Engine (COPERNICUS/S2_SR)\"\n            - s2_scl (uint8): Scene Classification Layer\n              Attributes: long_name, description of class values (0=NO_DATA, 1=SATURATED, etc.)\n            - quality_data_mask (uint8): Derived quality mask\n              - 0 = Invalid (no data, saturated, defective, or NaN values)\n              - 1 = Low quality (shadows, clouds, cirrus, snow/ice, water)\n              - 2 = High quality (clear vegetation or non-vegetated land)\n            - valid_data_mask (uint8): Binary validity mask (1=valid, 0=invalid)\n\n            Dataset attributes:\n            - azimuth (float): Solar azimuth angle from MEAN_SOLAR_AZIMUTH_ANGLE\n            - elevation (float): Solar elevation angle from MEAN_SOLAR_ZENITH_ANGLE\n            - s2_tile_id (str): Full PRODUCT_ID from GEE\n            - tile_id (str): Scene identifier\n            - time (str): Acquisition timestamp\n\n    Note:\n        The `offline` parameter controls data fetching:\n        - When `offline=False`: Automatically downloads missing data from GEE and stores it\n          in the local zarr store (if store is provided).\n        - When `offline=True`: Only reads from the local store. Raises an error if data is\n          missing or if store is None.\n\n        Reflectance processing:\n        - For scenes &gt;= 2022-01-25: (DN / 10000.0) - 0.1 (processing baseline 04.00+)\n        - For scenes &lt; 2022-01-25: DN / 10000.0 (older processing baseline)\n        - NaN values are filled with 0 and marked as invalid in quality_data_mask\n        - Pixels where SCL is NaN are also masked as invalid\n\n        This function handles spatially random NaN values that can occur in GEE data by\n        marking them as invalid and filling with 0 to prevent propagation in calculations.\n\n        Quality mask derivation from SCL:\n        - Invalid (0): NO_DATA, SATURATED_OR_DEFECTIVE, or NaN values\n        - Low quality (1): CAST_SHADOWS, CLOUD_SHADOWS, CLOUD_*, THIN_CIRRUS, SNOW/ICE, WATER\n        - High quality (2): VEGETATION, NOT_VEGETATED\n\n    Example:\n        Load scene with local caching:\n\n        ```python\n        import ee\n        from pathlib import Path\n        from darts_acquisition import load_gee_s2_sr_scene\n\n        # Initialize Earth Engine\n        ee.Initialize()\n\n        # Load with caching\n        s2_ds = load_gee_s2_sr_scene(\n            s2item=\"20230615T123456_20230615T123659_T33UUP\",\n            bands_mapping=\"all\",\n            store=Path(\"/data/s2_store\"),\n            offline=False  # Download if not cached\n        )\n\n        # Compute NDVI\n        ndvi = (s2_ds.nir - s2_ds.red) / (s2_ds.nir + s2_ds.red)\n\n        # Filter to high quality pixels\n        s2_filtered = s2_ds.where(s2_ds.quality_data_mask == 2)\n        ```\n\n    \"\"\"\n    if isinstance(s2item, str):\n        s2id = s2item\n        s2item = ee.Image(f\"COPERNICUS/S2_SR/{s2id}\")\n    else:\n        s2id = s2item.id().getInfo().split(\"/\")[-1]\n    logger.debug(f\"Loading Sentinel-2 tile {s2id=} from GEE\")\n\n    bands_mapping = _get_band_mapping(bands_mapping)\n    store_manager = GEEStoreManager(\n        store=store,\n        bands_mapping=bands_mapping,\n    )\n\n    if not offline:\n        ds_s2 = store_manager.load(s2item)\n    else:\n        assert store is not None, \"Store must be provided in offline mode!\"\n        ds_s2 = store_manager.open(s2item)\n\n    if output_dir_for_debug_geotiff is not None:\n        save_debug_geotiff(\n            dataset=ds_s2,\n            output_path=output_dir_for_debug_geotiff,\n            optical_bands=[band for band in bands_mapping.keys() if band.startswith(\"B\")],\n            mask_bands=[\"SCL\"],\n        )\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n\n    optical_bands = [band for name, band in bands_mapping.items() if name.startswith(\"B\")]\n\n    # Fix new preprocessing offset -&gt; See docs about bands\n    dt = datetime.strptime(ds_s2.attrs[\"time\"], \"%Y-%m-%dT%H:%M:%S.%f000\")\n    offset = 0.1 if dt &gt;= datetime(2022, 1, 25) else 0.0\n\n    ds_s2 = move_to_device(ds_s2, device)\n    for band in optical_bands:\n        # Apply scale and offset\n        ds_s2[band] = ds_s2[band].astype(\"float32\") / 10000.0 - offset\n        ds_s2[band].attrs[\"long_name\"] = f\"Sentinel 2 {band.capitalize()}\"\n        ds_s2[band].attrs[\"units\"] = \"Reflectance\"\n    ds_s2[\"s2_scl\"].attrs = {\n        \"long_name\": \"Sentinel-2 Scene Classification Layer\",\n        \"description\": (\n            \"0: NO_DATA - 1: SATURATED_OR_DEFECTIVE - 2: CAST_SHADOWS - 3: CLOUD_SHADOWS - 4: VEGETATION\"\n            \" - 5: NOT_VEGETATED - 6: WATER - 7: UNCLASSIFIED - 8: CLOUD_MEDIUM_PROBABILITY - 9: CLOUD_HIGH_PROBABILITY\"\n            \" - 10: THIN_CIRRUS - 11: SNOW or ICE\"\n        ),\n    }\n    for band in ds_s2.data_vars:\n        ds_s2[band].attrs[\"data_source\"] = \"Sentinel-2 L2A via Google Earth Engine (COPERNICUS/S2_SR)\"\n\n    ds_s2 = convert_masks(ds_s2)\n    qdm_attrs = ds_s2[\"quality_data_mask\"].attrs.copy()\n\n    # For some reason, there are some spatially random nan values in the data, not only at the borders\n    # To workaround this, set all nan values to 0 and add this information to the quality_data_mask\n    # This workaround is quite computational expensive, but it works for now\n    # TODO: Find other solutions for this problem!\n    with stopwatch(f\"Fixing nan values in {s2id=}\", printer=logger.debug):\n        for band in optical_bands:\n            ds_s2[\"quality_data_mask\"] = xr.where(ds_s2[band].isnull(), 0, ds_s2[\"quality_data_mask\"])\n            ds_s2[band] = ds_s2[band].fillna(0)\n            # Turn real nan values (s2_scl is nan) into invalid data\n            ds_s2[band] = ds_s2[band].where(~ds_s2[\"s2_scl\"].isnull())\n    ds_s2 = move_to_host(ds_s2)\n\n    ds_s2[\"quality_data_mask\"].attrs = qdm_attrs\n    ds_s2.attrs[\"s2_tile_id\"] = s2item.getInfo()[\"properties\"][\"PRODUCT_ID\"]\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/gee_scene/#darts_acquisition.s2.gee_scene.save_debug_geotiff","title":"save_debug_geotiff","text":"<pre><code>save_debug_geotiff(\n    dataset: xarray.Dataset,\n    output_path: pathlib.Path,\n    optical_bands: list[str],\n    mask_bands: list[str] | None = None,\n) -&gt; None\n</code></pre> <p>Save the raw dataset as a GeoTIFF file for debugging purposes.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to save</p> </li> <li> <code>output_path</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the output GeoTIFF file</p> </li> <li> <code>optical_bands</code>               (<code>list[str]</code>)           \u2013            <p>List of optical band names</p> </li> <li> <code>mask_bands</code>               (<code>list[str]</code>, default:                   <code>None</code> )           \u2013            <p>List of mask band names</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/debug_export.py</code> <pre><code>def save_debug_geotiff(\n    dataset: xr.Dataset,\n    output_path: Path,\n    optical_bands: list[str],\n    mask_bands: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Save the raw dataset as a GeoTIFF file for debugging purposes.\n\n    Args:\n        dataset (xr.Dataset): Dataset to save\n        output_path (Path): Path to the output GeoTIFF file\n        optical_bands (list[str]): List of optical band names\n        mask_bands (list[str]): List of mask band names\n\n    \"\"\"\n    output_path.mkdir(parents=True, exist_ok=True)\n    optical = dataset[optical_bands].to_dataarray(dim=\"band\").fillna(0).astype(\"uint16\")\n    optical.rio.to_raster(output_path / \"optical_raw.tiff\")\n\n    band_info = \"Optical Bands:\\n\"\n    band_info += \"\\n\".join([f\" - {i + 1}: {band}\" for i, band in enumerate(optical_bands)])\n\n    if mask_bands:\n        masks = dataset[mask_bands].to_dataarray(dim=\"band\").fillna(0).astype(\"uint8\")\n        masks.rio.to_raster(output_path / \"mask_raw.tiff\")\n        band_info += \"\\n\\nMask Bands:\\n\"\n        band_info += \"\\n\".join([f\" - {i + 1}: {band}\" for i, band in enumerate(mask_bands)])\n\n    (output_path / \"bands.txt\").write_text(band_info)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/grid/","title":"grid","text":""},{"location":"reference/darts_acquisition/s2/grid/#darts_acquisition.s2.grid","title":"darts_acquisition.s2.grid","text":"<p>Download of the s2 mgrs based grid.</p>"},{"location":"reference/darts_acquisition/s2/grid/#darts_acquisition.s2.grid.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/grid/#darts_acquisition.s2.grid._download_zip","title":"_download_zip","text":"<pre><code>_download_zip(url: str, grid_dir: pathlib.Path)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/grid.py</code> <pre><code>@stopwatch.f(\"Downloading and extracting zip file\", printer=logger.debug)\ndef _download_zip(url: str, grid_dir: Path):\n    response = requests.get(url)\n\n    # Get the downloaded data as a byte string\n    data = response.content\n    logger.debug(f\"Downloaded {len(data)} bytes\")\n\n    # Create a bytesIO object\n    with io.BytesIO(data) as buffer:\n        # Create a zipfile.ZipFile object and extract the files to a directory\n        grid_dir.mkdir(parents=True, exist_ok=True)\n        with zipfile.ZipFile(buffer, \"r\") as zip_ref:\n            # Extract the files to the specified directory\n            zip_ref.extractall(grid_dir)\n\n    # Move the extracted files one level up\n    extracted_folder = grid_dir / \"Sentinel-2-Shapefile-Index-master\"\n    for item in extracted_folder.iterdir():\n        item.rename(grid_dir / item.name)\n    extracted_folder.rmdir()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/grid/#darts_acquisition.s2.grid.download_sentinel_2_grid","title":"download_sentinel_2_grid","text":"<pre><code>download_sentinel_2_grid(grid_dir: pathlib.Path)\n</code></pre> <p>Download the Sentinel 2 grid files.</p> <p>Files will be stored under [grid_dir]/adm1.shp and [grid_dir]/...</p> <p>Parameters:</p> <ul> <li> <code>grid_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the grid.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/grid.py</code> <pre><code>@stopwatch.f(\"Downloading Sentinel 2 grid\", printer=logger.debug)\ndef download_sentinel_2_grid(grid_dir: Path):\n    \"\"\"Download the Sentinel 2 grid files.\n\n    Files will be stored under [grid_dir]/adm1.shp and [grid_dir]/...\n\n    Args:\n        grid_dir (Path): The path to the grid.\n\n    \"\"\"\n    grid_dir.mkdir(exist_ok=True, parents=True)\n    grid_url = \"https://github.com/justinelliotmeyers/Sentinel-2-Shapefile-Index/archive/refs/heads/master.zip\"\n    logger.debug(f\"Downloading {grid_url} to {grid_dir.resolve()}\")\n    _download_zip(grid_url, grid_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/quality_mask/","title":"quality_mask","text":""},{"location":"reference/darts_acquisition/s2/quality_mask/#darts_acquisition.s2.quality_mask","title":"darts_acquisition.s2.quality_mask","text":"<p>Sentinel-2 related data loading. Should be used temporary and maybe moved to the acquisition package.</p>"},{"location":"reference/darts_acquisition/s2/quality_mask/#darts_acquisition.s2.quality_mask.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/quality_mask/#darts_acquisition.s2.quality_mask.convert_masks","title":"convert_masks","text":"<pre><code>convert_masks(ds_s2: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Convert the Sentinel-2 scl mask into our own mask format inplace.</p> <p>https://sentiwiki.copernicus.eu/web/s2-processing#S2Processing-ClassificationMaskGeneration</p> <p>Invalid: S2 SCL \u2192 0,1 Low Quality S2: S2 SCL != 0,1 \u2192 3,8,9,11 High Quality: S2 SCL != 0,1,3,8,9,11 \u2192 Alles andere (2,4,5,6,7,10)</p> <p>Parameters:</p> <ul> <li> <code>ds_s2</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Sentinel-2 dataset containing the SCL band.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The modified dataset.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/quality_mask.py</code> <pre><code>@stopwatch(\"Converting Sentinel-2 masks\", printer=logger.debug)\ndef convert_masks(ds_s2: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Convert the Sentinel-2 scl mask into our own mask format inplace.\n\n    https://sentiwiki.copernicus.eu/web/s2-processing#S2Processing-ClassificationMaskGeneration\n\n    Invalid: S2 SCL \u2192 0,1\n    Low Quality S2: S2 SCL != 0,1 \u2192 3,8,9,11\n    High Quality: S2 SCL != 0,1,3,8,9,11 \u2192 Alles andere (2,4,5,6,7,10)\n\n    Args:\n        ds_s2 (xr.Dataset): The Sentinel-2 dataset containing the SCL band.\n\n    Returns:\n        xr.Dataset: The modified dataset.\n\n    \"\"\"\n    assert \"s2_scl\" in ds_s2.data_vars, \"The dataset does not contain the SCL band.\"\n\n    if has_cuda_and_cupy() and ds_s2.cupy.is_cupy:\n        invalids = ds_s2[\"s2_scl\"].fillna(0).isin(cp.array([0, 1]))\n        high_quality = ds_s2[\"s2_scl\"].isin(cp.array([2, 4, 5, 6, 7, 10]))\n    else:\n        invalids = ds_s2[\"s2_scl\"].fillna(0).isin([0, 1])\n        high_quality = ds_s2[\"s2_scl\"].isin([2, 4, 5, 6, 7, 10])\n    ds_s2[\"quality_data_mask\"] = (\n        (~invalids).astype(\"uint8\")  # 0 for invalid, 1 for valid\n        + (high_quality).astype(\"uint8\")  # +1 for high quality\n    )\n\n    ds_s2[\"quality_data_mask\"].attrs[\"data_source\"] = \"s2\"\n    ds_s2[\"quality_data_mask\"].attrs[\"long_name\"] = \"Quality Data Mask\"\n    ds_s2[\"quality_data_mask\"].attrs[\"description\"] = \"0 = Invalid, 1 = Low Quality, 2 = High Quality\"\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/","title":"raw_data_store","text":""},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store","title":"darts_acquisition.s2.raw_data_store","text":"<p>Raw Data Store for Sentinel 2 data.</p>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.SceneItem","title":"SceneItem  <code>module-attribute</code>","text":"<pre><code>SceneItem = typing.TypeVar('SceneItem')\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager","title":"StoreManager","text":"<pre><code>StoreManager(\n    bands: list[str],\n    store: str | pathlib.Path | None = None,\n)\n</code></pre> <p>               Bases: <code>abc.ABC</code>, <code>typing.Generic[darts_acquisition.s2.raw_data_store.SceneItem]</code></p> <p>Manager for storing raw sentinel 2 data.</p> <p>This class is an abstract base class and should be extended to implement the respective downloading methods.</p> <p>Usage:</p> <pre><code>1. \"Normal\" usage:\n\n```python\n    store_manager = StoreManager(store_path)\n    ds_s2 = store_manager.load(identifier, bands)\n```\n\n2. Force download:\n\n```python\n    store_manager = StoreManager(store_path)\n    ds_s2 = store_manager.load(identifier, force=True)\n```\n\n3. Download only (and only if missing) and store the scene:\n\n```python\n    store_manager = StoreManager(store_path)\n    store_manager.download(identifier) # store_path must be not None\n```\n\n4. Offline mode:\n\n```python\n    store_manager = StoreManager(store_path)\n    store_manager.open(identifier) # store_path must be not None, bands must be complete\n```\n</code></pre> <p>Initialize the store manager.</p> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>List of bands to manage</p> </li> <li> <code>store</code>               (<code>str | pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory path for storing raw sentinel 2 data</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def __init__(self, bands: list[str], store: str | Path | None = None):\n    \"\"\"Initialize the store manager.\n\n    Args:\n        bands (list[str]): List of bands to manage\n        store (str | Path | None): Directory path for storing raw sentinel 2 data\n\n    \"\"\"\n    self.bands = bands\n    self.store = Path(store) if isinstance(store, str) else store\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands = darts_acquisition.s2.raw_data_store.StoreManager(\n    bands\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.store","title":"store  <code>instance-attribute</code>","text":"<pre><code>store = (\n    pathlib.Path(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        )\n    )\n    if isinstance(\n        darts_acquisition.s2.raw_data_store.StoreManager(\n            store\n        ),\n        str,\n    )\n    else darts_acquisition.s2.raw_data_store.StoreManager(\n        store\n    )\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.complete","title":"complete","text":"<pre><code>complete(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene in the store contains all requested bands.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if all requested bands are present, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def complete(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene in the store contains all requested bands.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if all requested bands are present, False otherwise\n\n    \"\"\"\n    return len(self.missing_bands(identifier)) == 0\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.download_and_store","title":"download_and_store","text":"<pre><code>download_and_store(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n)\n</code></pre> <p>Download a scene from the source and store it in the local store.</p> <p>Store must be provided! Will do nothing if all required bands are already present.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def download_and_store(self, item: str | SceneItem):\n    \"\"\"Download a scene from the source and store it in the local store.\n\n    Store must be provided!\n    Will do nothing if all required bands are already present.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to download and store scenes!\"\n    identifier = self.identifier(item)\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        return\n    dataset = self.download_scene_from_source(item, missing_bands)\n    self.save_to_store(dataset, identifier)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.download_scene_from_source","title":"download_scene_from_source  <code>abstractmethod</code>","text":"<pre><code>download_scene_from_source(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n    bands: list[str],\n) -&gt; xarray.Dataset\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>@abstractmethod\ndef download_scene_from_source(self, item: str | SceneItem, bands: list[str]) -&gt; xr.Dataset: ...  # noqa: D102\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.encodings","title":"encodings  <code>abstractmethod</code>","text":"<pre><code>encodings(bands: list[str]) -&gt; dict[str, dict[str, str]]\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>@abstractmethod\ndef encodings(self, bands: list[str]) -&gt; dict[str, dict[str, str]]: ...  # noqa: D102\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.exists","title":"exists","text":"<pre><code>exists(identifier: str) -&gt; bool\n</code></pre> <p>Check if a scene already exists in the local raw data store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the scene exists in the store, False otherwise</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def exists(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a scene already exists in the local raw data store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        bool: True if the scene exists in the store, False otherwise\n\n    \"\"\"\n    if not self.store:\n        return False\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return scene_path.exists()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.identifier","title":"identifier  <code>abstractmethod</code>","text":"<pre><code>identifier(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n) -&gt; str\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>@abstractmethod\ndef identifier(self, item: str | SceneItem) -&gt; str: ...  # noqa: D102\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.load","title":"load","text":"<pre><code>load(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n    force: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a scene.</p> <p>If <code>force==True</code> will download the scene from source even if present in store. Else, will try to open the scene from store first and only download missing bands. Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open.</p> </li> <li> <code>force</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, will download the scene even if present. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def load(self, item: str | SceneItem, force: bool = False) -&gt; xr.Dataset:\n    \"\"\"Load a scene.\n\n    If `force==True` will download the scene from source even if present in store.\n    Else, will try to open the scene from store first and only download missing bands.\n    Will always store the downloaded scene in local store if store is provided, potentially overwriting existing.\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open.\n        force (bool, optional): If True, will download the scene even if present. Defaults to False.\n\n    Returns:\n        xr.Dataset: The loaded scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    if force:\n        logger.debug(f\"Force downloading scene {identifier} from source.\")\n        dataset = self.download_scene_from_source(item, self.bands)\n        if self.store:\n            self.save_to_store(dataset, identifier)\n        return dataset\n\n    missing_bands = self.missing_bands(identifier)\n    if not missing_bands:\n        logger.debug(f\"Scene {identifier} is complete, opening from store.\")\n        return self.open(item)\n    logger.debug(f\"Scene {identifier} is missing bands {missing_bands}, downloading from source.\")\n    dataset = self.download_scene_from_source(item, missing_bands)\n    if self.store:\n        self.save_to_store(dataset, identifier)\n    return dataset\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.missing_bands","title":"missing_bands","text":"<pre><code>missing_bands(identifier: str) -&gt; list[str]\n</code></pre> <p>Get the list of missing bands for a scene in the store.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: List of missing bands</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def missing_bands(self, identifier: str) -&gt; list[str]:\n    \"\"\"Get the list of missing bands for a scene in the store.\n\n    Args:\n        identifier (str): Unique identifier for the scene\n\n    Returns:\n        list[str]: List of missing bands\n\n    \"\"\"\n    if not self.store:\n        return self.bands\n\n    scene_path = self.store / f\"{identifier}.zarr\"\n    if not scene_path.exists():\n        return self.bands\n\n    dataset = xr.open_zarr(scene_path, consolidated=False)\n    required_bands = set(self.bands)\n    present_bands = set(dataset.data_vars)\n    missing = required_bands - present_bands\n    return list(missing)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.open","title":"open","text":"<pre><code>open(\n    item: str\n    | darts_acquisition.s2.raw_data_store.SceneItem,\n) -&gt; xarray.Dataset\n</code></pre> <p>Open a scene from local store.</p> <p>Store must be provided and the scene must be present in store!</p> <p>Parameters:</p> <ul> <li> <code>item</code>               (<code>str | darts_acquisition.s2.raw_data_store.SceneItem</code>)           \u2013            <p>Item or scene-id to open</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The opened scene as xarray Dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def open(self, item: str | SceneItem) -&gt; xr.Dataset:\n    \"\"\"Open a scene from local store.\n\n    Store must be provided and the scene must be present in store!\n\n    Args:\n        item (str | SceneItem): Item or scene-id to open\n\n    Returns:\n        xr.Dataset: The opened scene as xarray Dataset\n\n    \"\"\"\n    identifier = self.identifier(item)\n    assert self.complete(identifier), f\"Scene {identifier} is incomplete in store!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    return xr.open_zarr(scene_path, consolidated=False).set_coords(\"spatial_ref\").load()\n</code></pre>"},{"location":"reference/darts_acquisition/s2/raw_data_store/#darts_acquisition.s2.raw_data_store.StoreManager.save_to_store","title":"save_to_store","text":"<pre><code>save_to_store(\n    dataset: xarray.Dataset, identifier: str\n) -&gt; None\n</code></pre> <p>Save a scene dataset to the local raw data store.</p> <p>Will append new bands to existing store if scene already exists. Will overwrite existing bands in an existing store if scene already exists.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to save</p> </li> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the scene</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2/raw_data_store.py</code> <pre><code>def save_to_store(self, dataset: xr.Dataset, identifier: str) -&gt; None:\n    \"\"\"Save a scene dataset to the local raw data store.\n\n    Will append new bands to existing store if scene already exists.\n    Will overwrite existing bands in an existing store if scene already exists.\n\n    Args:\n        dataset (xr.Dataset): Dataset to save\n        identifier (str): Unique identifier for the scene\n\n    \"\"\"\n    assert self.store is not None, \"Store must be provided to save scenes!\"\n    scene_path = self.store / f\"{identifier}.zarr\"\n    encoding = self.encodings(list(dataset.data_vars))\n    if not scene_path.exists():\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"w\")\n    else:\n        # Assert that the coordinates match\n        existing_dataset = xr.open_zarr(scene_path, consolidated=False)\n        xr.testing.assert_allclose(existing_dataset.coords, dataset.coords)\n        # Overwrite dataset coords to avoid conflicts by floating point precision issues\n        dataset[\"x\"] = existing_dataset.x\n        dataset[\"y\"] = existing_dataset.y\n        dataset.to_zarr(scene_path, encoding=encoding, consolidated=False, mode=\"a\")\n</code></pre>"},{"location":"reference/darts_acquisition/tcvis/","title":"tcvis","text":""},{"location":"reference/darts_acquisition/tcvis/#darts_acquisition.tcvis","title":"darts_acquisition.tcvis","text":"<p>Landsat Trends related Data Loading. Should be used temporary and maybe moved to the acquisition package.</p>"},{"location":"reference/darts_acquisition/tcvis/#darts_acquisition.tcvis.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/tcvis/#darts_acquisition.tcvis.download_tcvis","title":"download_tcvis","text":"<pre><code>download_tcvis(\n    aoi: geopandas.GeoDataFrame,\n    data_dir: pathlib.Path | str,\n) -&gt; None\n</code></pre> <p>Download TCVIS (Tasseled Cap trends) data for the specified area of interest.</p> <p>This function downloads Tasseled Cap trend data from Google Earth Engine for the given area of interest and stores it in a local icechunk data store for efficient access.</p> <p>Parameters:</p> <ul> <li> <code>aoi</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>Area of interest for which to download TCVIS data. Can be in any CRS; will be reprojected to the TCVIS dataset's native CRS.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>Path to the icechunk data directory (must have .icechunk suffix).</p> </li> </ul> Note <p>Requires Google Earth Engine authentication to be set up before calling this function. Use <code>ee.Initialize()</code> or <code>ee.Authenticate()</code> as needed.</p> Example <p>Download TCVIS for a study area:</p> <pre><code>import geopandas as gpd\nfrom shapely.geometry import box\nfrom darts_acquisition import download_tcvis\n\n# Define area of interest\naoi = gpd.GeoDataFrame(\n    geometry=[box(-50, 70, -49, 71)],\n    crs=\"EPSG:4326\"\n)\n\n# Download TCVIS\ndownload_tcvis(\n    aoi=aoi,\n    data_dir=\"/data/tcvis.icechunk\"\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopwatch.f(\"Downloading TCVIS\", printer=logger.debug, print_kwargs=[\"data_dir\"])\ndef download_tcvis(\n    aoi: gpd.GeoDataFrame,\n    data_dir: Path | str,\n) -&gt; None:\n    \"\"\"Download TCVIS (Tasseled Cap trends) data for the specified area of interest.\n\n    This function downloads Tasseled Cap trend data from Google Earth Engine for the given\n    area of interest and stores it in a local icechunk data store for efficient access.\n\n    Args:\n        aoi (gpd.GeoDataFrame): Area of interest for which to download TCVIS data.\n            Can be in any CRS; will be reprojected to the TCVIS dataset's native CRS.\n        data_dir (Path | str): Path to the icechunk data directory (must have .icechunk suffix).\n\n    Note:\n        Requires Google Earth Engine authentication to be set up before calling this function.\n        Use `ee.Initialize()` or `ee.Authenticate()` as needed.\n\n    Example:\n        Download TCVIS for a study area:\n\n        ```python\n        import geopandas as gpd\n        from shapely.geometry import box\n        from darts_acquisition import download_tcvis\n\n        # Define area of interest\n        aoi = gpd.GeoDataFrame(\n            geometry=[box(-50, 70, -49, 71)],\n            crs=\"EPSG:4326\"\n        )\n\n        # Download TCVIS\n        download_tcvis(\n            aoi=aoi,\n            data_dir=\"/data/tcvis.icechunk\"\n        )\n        ```\n\n    \"\"\"\n    assert \".icechunk\" == data_dir.suffix, f\"Data directory {data_dir} must have an .icechunk suffix!\"\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n    accessor.assert_created()\n    accessor.download(aoi)\n</code></pre>"},{"location":"reference/darts_acquisition/tcvis/#darts_acquisition.tcvis.load_tcvis","title":"load_tcvis","text":"<pre><code>load_tcvis(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    buffer: int = 0,\n    offline: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load TCVIS (Tasseled Cap trends) for the given geobox, fetch new data from GEE if necessary.</p> <p>This function loads Tasseled Cap trend data from a local icechunk store. If <code>offline=False</code>, missing data will be automatically downloaded from Google Earth Engine and stored locally. The data contains temporal trends in brightness, greenness, and wetness derived from Landsat imagery.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which to load the data. Can be in any CRS.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>Path to the icechunk data directory (must have .icechunk suffix). This directory stores downloaded TCVIS data for faster consecutive access.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Buffer around the geobox in pixels. The buffer is applied in the TCVIS dataset's native CRS after reprojecting the input geobox. Defaults to 0.</p> </li> <li> <code>offline</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, only loads data already present in the local store without attempting any downloads. If False, missing data is downloaded from GEE. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The TCVIS dataset with the following data variables: - tc_brightness (float): Temporal trend in Tasseled Cap brightness component - tc_greenness (float): Temporal trend in Tasseled Cap greenness component - tc_wetness (float): Temporal trend in Tasseled Cap wetness component</p> <p>The dataset is in the TCVIS native CRS with the buffer applied. It is NOT automatically reprojected to match the input geobox's CRS.</p> </li> </ul> Note <p>The <code>offline</code> parameter controls data fetching behavior:</p> <ul> <li>When <code>offline=False</code>: Uses <code>smart_geocubes</code> accessor's <code>load()</code> method which automatically   downloads missing tiles from GEE and persists them to the icechunk store.</li> <li>When <code>offline=True</code>: Uses the accessor's <code>open_xarray()</code> method to open the existing store   and crops it to the requested region. Raises an error if data is missing.</li> </ul> <p>Variable naming: The original TCB_slope, TCG_slope, and TCW_slope variables are renamed to follow DARTS conventions (tc_brightness, tc_greenness, tc_wetness).</p> Example <p>Load TCVIS data aligned with optical imagery:</p> <pre><code>from darts_acquisition import load_tcvis\n\n# Assume \"optical\" is a loaded Sentinel-2 dataset\ntcvis = load_tcvis(\n    geobox=optical.odc.geobox,\n    data_dir=\"/data/tcvis.icechunk\",\n    buffer=0,\n    offline=False\n)\n\n# Reproject to match optical data's CRS and resolution\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopwatch.f(\"Loading TCVIS\", printer=logger.debug, print_kwargs=[\"data_dir\", \"buffer\", \"offline\"])\ndef load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    offline: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Load TCVIS (Tasseled Cap trends) for the given geobox, fetch new data from GEE if necessary.\n\n    This function loads Tasseled Cap trend data from a local icechunk store. If `offline=False`,\n    missing data will be automatically downloaded from Google Earth Engine and stored locally.\n    The data contains temporal trends in brightness, greenness, and wetness derived from\n    Landsat imagery.\n\n    Args:\n        geobox (GeoBox): The geobox for which to load the data. Can be in any CRS.\n        data_dir (Path | str): Path to the icechunk data directory (must have .icechunk suffix).\n            This directory stores downloaded TCVIS data for faster consecutive access.\n        buffer (int, optional): Buffer around the geobox in pixels. The buffer is applied in the\n            TCVIS dataset's native CRS after reprojecting the input geobox. Defaults to 0.\n        offline (bool, optional): If True, only loads data already present in the local store\n            without attempting any downloads. If False, missing data is downloaded from GEE.\n            Defaults to False.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset with the following data variables:\n            - tc_brightness (float): Temporal trend in Tasseled Cap brightness component\n            - tc_greenness (float): Temporal trend in Tasseled Cap greenness component\n            - tc_wetness (float): Temporal trend in Tasseled Cap wetness component\n\n            The dataset is in the TCVIS native CRS with the buffer applied.\n            It is NOT automatically reprojected to match the input geobox's CRS.\n\n    Note:\n        The `offline` parameter controls data fetching behavior:\n\n        - When `offline=False`: Uses `smart_geocubes` accessor's `load()` method which automatically\n          downloads missing tiles from GEE and persists them to the icechunk store.\n        - When `offline=True`: Uses the accessor's `open_xarray()` method to open the existing store\n          and crops it to the requested region. Raises an error if data is missing.\n\n        Variable naming: The original TCB_slope, TCG_slope, and TCW_slope variables are renamed\n        to follow DARTS conventions (tc_brightness, tc_greenness, tc_wetness).\n\n    Example:\n        Load TCVIS data aligned with optical imagery:\n\n        ```python\n        from darts_acquisition import load_tcvis\n\n        # Assume \"optical\" is a loaded Sentinel-2 dataset\n        tcvis = load_tcvis(\n            geobox=optical.odc.geobox,\n            data_dir=\"/data/tcvis.icechunk\",\n            buffer=0,\n            offline=False\n        )\n\n        # Reproject to match optical data's CRS and resolution\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"\n    assert \".icechunk\" == data_dir.suffix, f\"Data directory {data_dir} must have an .icechunk suffix!\"\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n\n    # We want to assume that the datacube is already created to be save in a multi-process environment\n    accessor.assert_created()\n\n    if not offline:\n        tcvis = accessor.load(geobox, buffer=buffer, persist=True)\n    else:\n        xrcube = accessor.open_xarray()\n        reference_geobox = geobox.to_crs(accessor.extent.crs, resolution=accessor.extent.resolution.x).pad(buffer)\n        tcvis = xrcube.odc.crop(reference_geobox.extent, apply_mask=False)\n        tcvis = tcvis.load()\n\n    # Rename to follow our conventions\n    tcvis = tcvis.rename_vars(\n        {\n            \"TCB_slope\": \"tc_brightness\",\n            \"TCG_slope\": \"tc_greenness\",\n            \"TCW_slope\": \"tc_wetness\",\n        }\n    )\n\n    return tcvis\n</code></pre>"},{"location":"reference/darts_acquisition/utils/","title":"utils","text":""},{"location":"reference/darts_acquisition/utils/#darts_acquisition.utils","title":"darts_acquisition.utils","text":"<p>Utility functions for darts-acquisition.</p>"},{"location":"reference/darts_acquisition/utils/arosics/","title":"arosics","text":""},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics","title":"darts_acquisition.utils.arosics","text":"<p>Re-implementation of the AROSICS algorithm.</p>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.MultiOffsetInfo","title":"MultiOffsetInfo  <code>dataclass</code>","text":"<pre><code>MultiOffsetInfo(\n    x_offset: float | None,\n    y_offset: float | None,\n    avg_reliability: float | None,\n    avg_ssim_improvement: float | None,\n    offset_reduce_method: typing.Literal[\n        \"mean\", \"weighted_mean\", \"best\"\n    ]\n    | None,\n    offset_infos: dict[\n        str, darts_acquisition.utils.arosics.OffsetInfo\n    ],\n)\n</code></pre> <p>Dataclass to hold offset information of multiple bands.</p>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.MultiOffsetInfo.avg_reliability","title":"avg_reliability  <code>instance-attribute</code>","text":"<pre><code>avg_reliability: float | None\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.MultiOffsetInfo.avg_ssim_improvement","title":"avg_ssim_improvement  <code>instance-attribute</code>","text":"<pre><code>avg_ssim_improvement: float | None\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.MultiOffsetInfo.offset_infos","title":"offset_infos  <code>instance-attribute</code>","text":"<pre><code>offset_infos: dict[\n    str, darts_acquisition.utils.arosics.OffsetInfo\n]\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.MultiOffsetInfo.offset_reduce_method","title":"offset_reduce_method  <code>instance-attribute</code>","text":"<pre><code>offset_reduce_method: (\n    typing.Literal[\"mean\", \"weighted_mean\", \"best\"] | None\n)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.MultiOffsetInfo.x_offset","title":"x_offset  <code>instance-attribute</code>","text":"<pre><code>x_offset: float | None\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.MultiOffsetInfo.y_offset","title":"y_offset  <code>instance-attribute</code>","text":"<pre><code>y_offset: float | None\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.MultiOffsetInfo.is_valid","title":"is_valid","text":"<pre><code>is_valid()\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def is_valid(self):  # noqa: D102\n    return self.x_offset is not None and self.y_offset is not None\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.MultiOffsetInfo.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def to_dataframe(self):  # noqa: D102\n    import pandas as pd\n\n    df = pd.DataFrame.from_records(\n        [\n            {\n                \"band\": band,\n                \"x_offset\": oi.x_offset,\n                \"y_offset\": oi.y_offset,\n                \"ssim_before\": oi.ssim_before,\n                \"ssim_after\": oi.ssim_after,\n                \"shift_reliability\": oi.shift_reliability,\n            }\n            for band, oi in self.offset_infos.items()\n        ]\n    )\n    return df\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo","title":"OffsetInfo  <code>dataclass</code>","text":"<pre><code>OffsetInfo(\n    x_offset: float | None,\n    y_offset: float | None,\n    ssim_before: float | None = None,\n    ssim_after: float | None = None,\n    shift_reliability: float = 0.0,\n)\n</code></pre> <p>Dataclass to hold offset information.</p>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo.shift_reliability","title":"shift_reliability  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>shift_reliability: float = 0.0\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo.ssim_after","title":"ssim_after  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ssim_after: float | None = None\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo.ssim_before","title":"ssim_before  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ssim_before: float | None = None\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo.ssim_improved","title":"ssim_improved  <code>property</code>","text":"<pre><code>ssim_improved: bool\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo.ssim_improvement","title":"ssim_improvement  <code>property</code>","text":"<pre><code>ssim_improvement: float\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo.x_offset","title":"x_offset  <code>instance-attribute</code>","text":"<pre><code>x_offset: float | None\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo.xy","title":"xy  <code>property</code>","text":"<pre><code>xy: tuple[float, float]\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo.y_offset","title":"y_offset  <code>instance-attribute</code>","text":"<pre><code>y_offset: float | None\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.OffsetInfo.is_valid","title":"is_valid","text":"<pre><code>is_valid(max_offset: float, min_reliability: float) -&gt; bool\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def is_valid(self, max_offset: float, min_reliability: float) -&gt; bool:  # noqa: D102\n    if self.x_offset is None or self.y_offset is None:\n        return False\n    return (\n        abs(self.x_offset) &lt;= max_offset\n        and abs(self.y_offset) &lt;= max_offset\n        and self.shift_reliability &gt;= min_reliability\n        and self.ssim_after &gt;= self.ssim_before\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics._calc_shift_reliability","title":"_calc_shift_reliability","text":"<pre><code>_calc_shift_reliability(\n    shifted_cross_power_spectrum: xarray.DataArray,\n    x_peak: int,\n    y_peak: int,\n) -&gt; float\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def _calc_shift_reliability(shifted_cross_power_spectrum: xr.DataArray, x_peak: int, y_peak: int) -&gt; float:\n    # calculate mean power at peak\n    x_peak_slice = slice(x_peak - 1, x_peak + 2)\n    y_peak_slice = slice(y_peak - 1, y_peak + 2)\n    power_at_peak = shifted_cross_power_spectrum.isel(x=x_peak_slice, y=y_peak_slice).mean().item()\n\n    # calculate mean power without peak + 3* standard deviation\n    shifted_cross_power_spectrum_unpeaked = shifted_cross_power_spectrum.copy()\n    shifted_cross_power_spectrum_unpeaked[x_peak_slice, y_peak_slice] = -9999\n    shifted_cross_power_spectrum_unpeaked = np.ma.masked_equal(shifted_cross_power_spectrum_unpeaked.values, -9999)\n    power_without_peak = np.mean(shifted_cross_power_spectrum_unpeaked) + 2 * np.std(\n        shifted_cross_power_spectrum_unpeaked\n    )\n\n    # calculate confidence\n    shift_reliability = 100 - ((power_without_peak / power_at_peak) * 100)\n    shift_reliability = min(max(shift_reliability, 0), 100)\n\n    return shift_reliability\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics._calc_ssim","title":"_calc_ssim","text":"<pre><code>_calc_ssim(\n    target: xarray.DataArray,\n    reference: xarray.DataArray,\n    subset: dict[typing.Literal[\"x\", \"y\"], slice]\n    | typing.Literal[False],\n) -&gt; float\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def _calc_ssim(\n    target: xr.DataArray, reference: xr.DataArray, subset: dict[Literal[\"x\", \"y\"], slice] | Literal[False]\n) -&gt; float:\n    target_window, reference_window = _get_window_subsets(target, reference, subset)\n\n    # Normalise both arrays\n    target_window = (target_window - target_window.min()) / (target_window.max() - target_window.min())\n    reference_window = (reference_window - reference_window.min()) / (reference_window.max() - reference_window.min())\n    # Mask NaN values\n    target_window = np.ma.masked_array(target_window.astype(np.float64).values, mask=target_window.isnull())\n    reference_window = np.ma.masked_array(reference_window.astype(np.float64).values, mask=reference_window.isnull())\n    return ssim(target_window, reference_window, data_range=1)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics._calculate_offset","title":"_calculate_offset","text":"<pre><code>_calculate_offset(\n    reference: xarray.DataArray,\n    target: xarray.DataArray,\n    subset: dict[typing.Literal[\"x\", \"y\"], slice]\n    | typing.Literal[False] = False,\n    max_iter: int = 5,\n) -&gt; darts_acquisition.utils.arosics.OffsetInfo\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def _calculate_offset(\n    reference: xr.DataArray,\n    target: xr.DataArray,\n    subset: dict[Literal[\"x\", \"y\"], slice] | Literal[False] = False,\n    max_iter: int = 5,\n) -&gt; OffsetInfo:\n    # We need to transpose the arrays to ensure that it is always (y, x) since we operate outside of xarray\n    # This also ensures that altering the axes here does not change the original arrays axes\n    target = target.transpose(\"y\", \"x\")\n    reference = reference.transpose(\"y\", \"x\")\n\n    # Calculate the ssim before the offset calculation\n    ssim_before = _calc_ssim(target, reference, subset=subset)\n\n    potential_x_offset = 0\n    potential_y_offset = 0\n    for i in range(max_iter):\n        target_window, reference_window = _get_window_subsets(target, reference, subset)\n        shifted_cross_power_spectrum = _calculate_scps(reference_window, target_window)\n        # Find the peak in the cross power spectrum\n        # The peak position in relation to the images center corresponds to the offset between the two images\n        # Since we use unravel_index, it is important that the scps is in (y, x) order -&gt; transposed in the beginning\n        y_peak, x_peak = np.unravel_index(shifted_cross_power_spectrum.argmax(), shifted_cross_power_spectrum.shape)\n        x_offset = x_peak - reference_window.sizes[\"x\"] // 2\n        y_offset = y_peak - reference_window.sizes[\"y\"] // 2\n        # If no more offset is left, it means that we found the \"real\" offset in the iteration before\n        # in this case we break and continue with subpixel offset calculation and validation\n        if x_offset == 0 and y_offset == 0:\n            break\n\n        # Apply the offset to the target array\n        target[\"x\"] = target.x + x_offset * target.odc.geobox.resolution.x\n        target[\"y\"] = target.y + y_offset * target.odc.geobox.resolution.y\n\n        # Add to the overall offset\n        potential_x_offset += x_offset\n        potential_y_offset += y_offset\n    else:\n        logger.debug(f\"Could not find a suitable offset after {max_iter} iterations.\")\n        return OffsetInfo(x_offset=None, y_offset=None)\n\n    # Calculate subpixel shift\n    sm_left = shifted_cross_power_spectrum.isel(x=x_peak - 1, y=y_peak).item()\n    sm_right = shifted_cross_power_spectrum.isel(x=x_peak + 1, y=y_peak).item()\n    sm_above = shifted_cross_power_spectrum.isel(x=x_peak, y=y_peak - 1).item()\n    sm_below = shifted_cross_power_spectrum.isel(x=x_peak, y=y_peak + 1).item()\n\n    v_00 = shifted_cross_power_spectrum.max().item()\n    v_10 = max(sm_left, sm_right)  # x\n    v_01 = max(sm_above, sm_below)  # y\n\n    subpixel_x_offset = np.sign(sm_right - sm_left) * v_10 / (v_00 + v_10)\n    subpixel_y_offset = np.sign(sm_below - sm_above) * v_01 / (v_00 + v_01)\n    subpixel_x_offset = np.round(subpixel_x_offset, 3)\n    subpixel_y_offset = np.round(subpixel_y_offset, 3)\n\n    # Apply the offset to the target array\n    target[\"x\"] = target.x + subpixel_x_offset * target.odc.geobox.resolution.x\n    target[\"y\"] = target.y + subpixel_y_offset * target.odc.geobox.resolution.y\n\n    potential_x_offset += subpixel_x_offset\n    potential_y_offset += subpixel_y_offset\n\n    # Calculate metrics\n    shift_reliability = _calc_shift_reliability(shifted_cross_power_spectrum, x_peak, y_peak)\n\n    # Calculate the ssim after the offsets are applied\n    ssim_after = _calc_ssim(target=target, reference=reference, subset=subset)\n\n    return OffsetInfo(\n        x_offset=potential_x_offset,\n        y_offset=potential_y_offset,\n        shift_reliability=shift_reliability,\n        ssim_before=ssim_before,\n        ssim_after=ssim_after,\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics._calculate_scps","title":"_calculate_scps","text":"<pre><code>_calculate_scps(\n    reference_window: xarray.DataArray,\n    target_window: xarray.DataArray,\n) -&gt; xarray.DataArray\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def _calculate_scps(reference_window: xr.DataArray, target_window: xr.DataArray) -&gt; xr.DataArray:\n    # Calculate the shifted cross power spectrum\n    # This is a trick to avoid convoluted block matching:\n    # Convolutions can be computed in the frequency domain with fourier transforms\n    # Hence, we turn our images into the frequency domain, compute the cross power spectrum,\n    # and then turn it back into the spatial domain.\n    # The peak of the result tells us the spatial shift between the two images.\n    ref_freq = fft2(reference_window.fillna(0).astype(\"complex64\"))\n    target_freq = fft2(target_window.fillna(0).astype(\"complex64\"))\n    eps = np.abs(ref_freq).max() * 1e-15\n    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n        cross_power_spectrum = (ref_freq * target_freq.conj()) / (abs(ref_freq) * abs(target_freq) + eps)\n    cross_power_spectrum = ifft2(cross_power_spectrum)\n    cross_power_spectrum = abs(cross_power_spectrum)\n    shifted_cross_power_spectrum = fftshift(cross_power_spectrum)\n    shifted_cross_power_spectrum = reference_window.copy(data=shifted_cross_power_spectrum)\n    return shifted_cross_power_spectrum\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics._find_suitable_subset_slices","title":"_find_suitable_subset_slices","text":"<pre><code>_find_suitable_subset_slices(\n    target: xarray.Dataset | xarray.DataArray,\n    reference: xarray.Dataset | xarray.DataArray,\n    window_size: int = 256,\n    target_mask: xarray.DataArray | None = None,\n    reference_mask: xarray.DataArray | None = None,\n    max_invalid_ratio: float = 0.01,\n) -&gt; (\n    dict[typing.Literal[\"x\", \"y\"], slice]\n    | typing.Literal[False]\n)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def _find_suitable_subset_slices(\n    target: xr.Dataset | xr.DataArray,\n    reference: xr.Dataset | xr.DataArray,\n    window_size: int = 256,\n    target_mask: xr.DataArray | None = None,\n    reference_mask: xr.DataArray | None = None,\n    max_invalid_ratio: float = 0.01,\n) -&gt; dict[Literal[\"x\", \"y\"], slice] | Literal[False]:\n    # Start naive with the center\n    center_x = target.sizes[\"x\"] // 2\n    center_y = target.sizes[\"y\"] // 2\n\n    # Initialize direction vectors (right, bottom, left, top)\n    dx = [1, 0, -1, 0]\n    dy = [0, 1, 0, -1]\n\n    # Track current position, direction, turns and steps\n    corner = (center_x - window_size // 2, center_y - window_size // 2)\n    direction = 0\n    steps_in_direction = 0\n    turns_taken = 0\n\n    # This will search for a valid subset in a spiraling pattern\n    for tries in range(1000):\n        xslice = slice(corner[0], corner[0] + window_size)\n        yslice = slice(corner[1], corner[1] + window_size)\n        reference_subset = reference.isel(x=xslice, y=yslice)\n        target_subset = target.isel(x=xslice, y=yslice)\n        target_subset_mask = target_mask.isel(x=xslice, y=yslice) if target_mask is not None else None\n        reference_subset_mask = reference_mask.isel(x=xslice, y=yslice) if reference_mask is not None else None\n        # Check if the subset is valid\n        is_valid = _validate_subset(\n            target_subset,\n            mask=target_subset_mask,\n            max_invalid_ratio=max_invalid_ratio,\n        ) and _validate_subset(\n            reference_subset,\n            mask=reference_subset_mask,\n            max_invalid_ratio=max_invalid_ratio,\n        )\n        if is_valid:\n            logger.debug(f\"Found valid subset after {tries=} at corner {corner} with {direction=} and {window_size=}.\")\n            return {\"x\": xslice, \"y\": yslice}\n        # If not valid, shift the corner in a spiraling pattern\n        corner = (corner[0] + dx[direction], corner[1] + dy[direction])\n\n        # Check if we are still in bounds\n        if (corner[0] &lt; 0 or corner[0] + window_size &gt; target.sizes[\"x\"]) or (\n            corner[1] &lt; 0 or corner[1] + window_size &gt; target.sizes[\"y\"]\n        ):\n            logger.debug(\n                \"Couldn't find a valid subset in the target and reference datasets.\"\n                \" Please check the window size and masks. Falling back to calculate offsets with the complete images.\"\n            )\n            break\n\n        # Update direction if needed\n        steps_in_direction += 1\n        if steps_in_direction == turns_taken // 2 + 1:\n            direction = (direction + 1) % 4\n            steps_in_direction = 0\n            turns_taken += 1\n    return False\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics._get_bands","title":"_get_bands","text":"<pre><code>_get_bands(\n    target: xarray.Dataset,\n    reference: xarray.Dataset,\n    bands: list[str]\n    | typing.Literal[\"multiband\"]\n    | str = \"multiband\",\n) -&gt; list[str]\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def _get_bands(\n    target: xr.Dataset,\n    reference: xr.Dataset,\n    bands: list[str] | Literal[\"multiband\"] | str = \"multiband\",\n) -&gt; list[str]:\n    # Get a list of bands to align and validate them\n    if isinstance(bands, str):\n        if bands != \"multiband\":\n            bands = [bands]\n        else:\n            # Use all bands that are in both datasets\n            bands = list(set(target.data_vars) &amp; set(reference.data_vars))\n            if not bands:\n                raise ValueError(\"No common bands found in target and reference datasets.\")\n    for band in bands:\n        assert band in target.data_vars, f\"Band '{band}' not found in target dataset.\"\n        assert band in reference.data_vars, f\"Band '{band}' not found in reference dataset.\"\n        assert target[band].dtype == reference[band].dtype, (\n            f\"Band '{band}' has different dtype in target and reference datasets: \"\n            f\"{target[band].dtype} vs {reference[band].dtype}.\"\n        )\n    return bands\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics._get_window_subsets","title":"_get_window_subsets","text":"<pre><code>_get_window_subsets(\n    target: xarray.DataArray,\n    reference: xarray.DataArray,\n    subset: dict[typing.Literal[\"x\", \"y\"], slice]\n    | typing.Literal[False],\n) -&gt; tuple[xarray.DataArray, xarray.DataArray]\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def _get_window_subsets(\n    target: xr.DataArray,\n    reference: xr.DataArray,\n    subset: dict[Literal[\"x\", \"y\"], slice] | Literal[False],\n) -&gt; tuple[xr.DataArray, xr.DataArray]:\n    if not subset:\n        # In case of no subset, we need to find the common spatial intersection of the two images again, since the\n        # Target could be changed since inbetween calls\n        common_extent = reference.odc.geobox.geographic_extent.intersection(target.odc.geobox.geographic_extent)\n        reference_window = reference.odc.crop(common_extent)\n    else:\n        reference_window = reference.isel(**subset)\n    target_window = target.sel(x=reference_window.x, y=reference_window.y, method=\"nearest\")\n    return target_window, reference_window\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics._validate_subset","title":"_validate_subset","text":"<pre><code>_validate_subset(\n    subset: xarray.DataArray | xarray.Dataset,\n    mask: xarray.DataArray | None = None,\n    max_invalid_ratio: float = 0.01,\n) -&gt; bool\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def _validate_subset(\n    subset: xr.DataArray | xr.Dataset,\n    mask: xr.DataArray | None = None,  # True: valid, False: invalid\n    max_invalid_ratio: float = 0.01,\n) -&gt; bool:\n    if isinstance(subset, xr.Dataset):\n        for band in subset.variables:\n            if not _validate_subset(subset[band], mask, max_invalid_ratio):\n                return False\n        return True\n    # Check for NaN values\n    if subset.isnull().any():\n        return False\n    # Check for mask\n    if mask is not None:\n        mask_invalid_ratio = 1 - mask.mean()\n        if mask_invalid_ratio &gt; max_invalid_ratio:\n            return False\n    return True\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.align","title":"align","text":"<pre><code>align(\n    target: xarray.Dataset | xarray.DataArray,\n    reference: xarray.Dataset | xarray.DataArray,\n    bands: list[str]\n    | typing.Literal[\"multiband\"]\n    | str = \"multiband\",\n    subset: dict[typing.Literal[\"x\", \"y\"], slice]\n    | typing.Literal[False]\n    | None = None,\n    window_size: int = 256,\n    target_mask: xarray.DataArray | None = None,\n    reference_mask: xarray.DataArray | None = None,\n    max_invalid_ratio: float = 0.01,\n    max_iter: int = 5,\n    min_reliability: float = 30.0,\n    max_offset: float = 10.0,\n    resample_to: typing.Literal[\"reference\", \"target\"]\n    | None = None,\n    return_offset: bool = False,\n    round_axes: int | typing.Literal[False] = 3,\n    inplace: bool = False,\n) -&gt; (\n    tuple[\n        xarray.Dataset | xarray.DataArray,\n        darts_acquisition.utils.arosics.OffsetInfo\n        | darts_acquisition.utils.arosics.MultiOffsetInfo,\n    ]\n    | xarray.Dataset\n    | xarray.DataArray\n)\n</code></pre> <p>Align a target to an reference using the AROSICS algorithm.</p> Note <p>Assumes that the target and reference datasets have the same dimensions.</p> <p>Parameters:</p> <ul> <li> <code>target</code>               (<code>xarray.Dataset | xarray.DataArray</code>)           \u2013            <p>The target image dataset or dataarray to be aligned.</p> </li> <li> <code>reference</code>               (<code>xarray.Dataset | xarray.DataArray</code>)           \u2013            <p>The reference image dataset or dataarray.</p> </li> <li> <code>bands</code>               (<code>list[str] | typing.Literal['multiband'] | str</code>, default:                   <code>'multiband'</code> )           \u2013            <p>The bands to use for alignment. Only used if the target and reference are datasets. If \"multiband\", all bands are used. This expects the target and reference datasets to have the same band names. If string, the respective band is used for alignment. If a list of strings, only the specified bands are used for alignment. Note: All bands are shifted by the same offset, even when using \"multiband\". With multiband, the offset is calculated from the average of all common and valid band offsets. This is slower but more robust than using a single band. If a band-specific offset is desired, please use the <code>get_dataarray_offsets</code> function for each band separately. Defaults to \"multiband\".</p> </li> <li> <code>subset</code>               (<code>dict[typing.Literal['x', 'y'], slice] | typing.Literal[False] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of slices to use for alignment. If provided, only the specified subset of the target and reference datasets is used for alignment. The dictionary must contain the keys \"x\" and \"y\" with the respective slices. If False, the whole dataset is used for alignment. If None, will try to find a suitable subset automatically.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The size of the window to use for alignment in case no subset is provided. Defaults to 256.</p> </li> <li> <code>target_mask</code>               (<code>xarray.DataArray | None</code>, default:                   <code>None</code> )           \u2013            <p>A mask for the target dataset. If provided, searches for a valid subset of the target dataset where the mask is True. If not provided, a mask is automatically inferred from the nodata value of the target dataset. Defaults to None.</p> </li> <li> <code>reference_mask</code>               (<code>xarray.DataArray | None</code>, default:                   <code>None</code> )           \u2013            <p>A mask for the reference dataset. If provided, searches for a valid subset of the reference dataset where the mask is True. If not provided, a mask is automatically inferred from the nodata value of the reference dataset. Defaults to None.</p> </li> <li> <code>max_invalid_ratio</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>The maximum ratio of invalid pixels in the target and reference subset masks. Is not used if the masks are not provided. Defaults to 0.01.</p> </li> <li> <code>max_iter</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>The maximum number of iterations to find the offset. Defaults to 5.</p> </li> <li> <code>min_reliability</code>               (<code>float</code>, default:                   <code>30.0</code> )           \u2013            <p>The minimum reliability (in %) of the offset to consider it valid. Defaults to 30.0.</p> </li> <li> <code>max_offset</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>The maximum offset in pixels to consider the offset valid. Defaults to 10.0.</p> </li> <li> <code>resample_to</code>               (<code>typing.Literal['reference', 'target'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The dataset to resample the other dataset to. If \"reference\", the target dataset is resampled to the reference dataset. If \"target\", the reference dataset is resampled to the target dataset. Defaults to None. If None, no resampling is done. This assumes that the pixel grids of the target and reference datasets are already aligned.</p> </li> <li> <code>round_axes</code>               (<code>int | False</code>, default:                   <code>3</code> )           \u2013            <p>The number of decimal places to round the x and y coordinates of the target dataset. This may be necessary if the applying of offsets results in very small floating point errors that lead to misalignment when using further processing steps. If False, no rounding is done. Defaults to 3.</p> </li> <li> <code>return_offset</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, returns the offsets instead of aligning the target dataset.</p> </li> <li> <code>inplace</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, modifies the target dataset in place.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[xarray.Dataset | xarray.DataArray, darts_acquisition.utils.arosics.OffsetInfo | darts_acquisition.utils.arosics.MultiOffsetInfo] | xarray.Dataset | xarray.DataArray</code>           \u2013            <p>tuple[xr.Dataset | xr.DataArray, OffsetInfo | MultiOffsetInfo] | xr.Dataset | xr.DataArray: The aligned target dataset or dataarray. If return_offset is True, also returns the offsets in x and y direction as a tuple.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def align(\n    target: xr.Dataset | xr.DataArray,\n    reference: xr.Dataset | xr.DataArray,\n    bands: list[str] | Literal[\"multiband\"] | str = \"multiband\",\n    subset: dict[Literal[\"x\", \"y\"], slice] | Literal[False] | None = None,\n    window_size: int = 256,\n    target_mask: xr.DataArray | None = None,\n    reference_mask: xr.DataArray | None = None,\n    max_invalid_ratio: float = 0.01,\n    max_iter: int = 5,\n    min_reliability: float = 30.0,\n    max_offset: float = 10.0,\n    resample_to: Literal[\"reference\", \"target\"] | None = None,\n    return_offset: bool = False,\n    round_axes: int | Literal[False] = 3,\n    inplace: bool = False,\n) -&gt; tuple[xr.Dataset | xr.DataArray, OffsetInfo | MultiOffsetInfo] | xr.Dataset | xr.DataArray:\n    \"\"\"Align a target to an reference using the AROSICS algorithm.\n\n    Note:\n        Assumes that the target and reference datasets have the same dimensions.\n\n    Args:\n        target (xr.Dataset | xr.DataArray): The target image dataset or dataarray to be aligned.\n        reference (xr.Dataset | xr.DataArray): The reference image dataset or dataarray.\n        bands (list[str] | Literal[\"multiband\"] | str): The bands to use for alignment.\n            Only used if the target and reference are datasets.\n            If \"multiband\", all bands are used.\n            This expects the target and reference datasets to have the same band names.\n            If string, the respective band is used for alignment.\n            If a list of strings, only the specified bands are used for alignment.\n            Note: All bands are shifted by the same offset, even when using \"multiband\".\n            With multiband, the offset is calculated from the average of all common and valid band offsets.\n            This is slower but more robust than using a single band.\n            If a band-specific offset is desired,\n            please use the `get_dataarray_offsets` function for each band separately.\n            Defaults to \"multiband\".\n        subset (dict[Literal[\"x\", \"y\"], slice] | Literal[False] | None): A dictionary of slices to use for alignment.\n            If provided, only the specified subset of the target and reference datasets is used for alignment.\n            The dictionary must contain the keys \"x\" and \"y\" with the respective slices.\n            If False, the whole dataset is used for alignment.\n            If None, will try to find a suitable subset automatically.\n        window_size (int): The size of the window to use for alignment in case no subset is provided. Defaults to 256.\n        target_mask (xr.DataArray | None): A mask for the target dataset.\n            If provided, searches for a valid subset of the target dataset where the mask is True.\n            If not provided, a mask is automatically inferred from the nodata value of the target dataset.\n            Defaults to None.\n        reference_mask (xr.DataArray | None): A mask for the reference dataset.\n            If provided, searches for a valid subset of the reference dataset where the mask is True.\n            If not provided, a mask is automatically inferred from the nodata value of the reference dataset.\n            Defaults to None.\n        max_invalid_ratio (float): The maximum ratio of invalid pixels in the target and reference subset masks.\n            Is not used if the masks are not provided.\n            Defaults to 0.01.\n        max_iter (int): The maximum number of iterations to find the offset. Defaults to 5.\n        min_reliability (float): The minimum reliability (in %) of the offset to consider it valid.\n            Defaults to 30.0.\n        max_offset (float): The maximum offset in pixels to consider the offset valid.\n            Defaults to 10.0.\n        resample_to (Literal[\"reference\", \"target\"] | None): The dataset to resample the other dataset to.\n            If \"reference\", the target dataset is resampled to the reference dataset.\n            If \"target\", the reference dataset is resampled to the target dataset.\n            Defaults to None.\n            If None, no resampling is done.\n            This assumes that the pixel grids of the target and reference datasets are already aligned.\n        round_axes (int | False): The number of decimal places to round the x and y coordinates of the target dataset.\n            This may be necessary if the applying of offsets results in very small floating point errors\n            that lead to misalignment when using further processing steps.\n            If False, no rounding is done.\n            Defaults to 3.\n        return_offset (bool): If True, returns the offsets instead of aligning the target dataset.\n        inplace (bool): If True, modifies the target dataset in place.\n\n    Returns:\n        tuple[xr.Dataset | xr.DataArray, OffsetInfo | MultiOffsetInfo] | xr.Dataset | xr.DataArray:\n            The aligned target dataset or dataarray.\n            If return_offset is True, also returns the offsets in x and y direction as a tuple.\n\n    \"\"\"\n    offset_info = get_offsets(\n        target,\n        reference,\n        bands=bands,\n        subset=subset,\n        window_size=window_size,\n        target_mask=target_mask,\n        reference_mask=reference_mask,\n        max_invalid_ratio=max_invalid_ratio,\n        max_iter=max_iter,\n        max_offset=max_offset,\n        min_reliability=min_reliability,\n        resample_to=resample_to,\n    )\n\n    invalid_single = isinstance(offset_info, OffsetInfo) and not offset_info.is_valid(\n        max_offset=max_offset, min_reliability=min_reliability\n    )\n    invalid_multi = isinstance(offset_info, MultiOffsetInfo) and not offset_info.is_valid()\n    if invalid_single or invalid_multi:\n        logger.warning(\"No valid offset found. Returning the original target dataset.\")\n        return target, offset_info\n\n    x_offset = offset_info.x_offset\n    y_offset = offset_info.y_offset\n\n    if (x_offset == 0 and y_offset == 0) or (x_offset is None or y_offset is None):\n        return target, offset_info\n\n    # Apply the offset to the target dataset\n    if not inplace:\n        target = target.copy(deep=True)\n    target[\"x\"] = target.x + x_offset * target.odc.geobox.resolution.x\n    target[\"y\"] = target.y + y_offset * target.odc.geobox.resolution.y\n\n    if round_axes is not False:\n        target[\"x\"] = target.x.round(round_axes)\n        target[\"y\"] = target.y.round(round_axes)\n\n    if return_offset:\n        return target, offset_info\n    else:\n        return target\n</code></pre>"},{"location":"reference/darts_acquisition/utils/arosics/#darts_acquisition.utils.arosics.get_offsets","title":"get_offsets","text":"<pre><code>get_offsets(\n    target: xarray.Dataset | xarray.DataArray,\n    reference: xarray.Dataset | xarray.DataArray,\n    bands: list[str]\n    | typing.Literal[\"multiband\"]\n    | str = \"multiband\",\n    subset: dict[typing.Literal[\"x\", \"y\"], slice]\n    | typing.Literal[False]\n    | None = None,\n    window_size: int = 256,\n    target_mask: xarray.DataArray | None = None,\n    reference_mask: xarray.DataArray | None = None,\n    max_invalid_ratio: float = 0.01,\n    max_iter: int = 5,\n    min_reliability: float = 30.0,\n    max_offset: float = 10.0,\n    resample_to: typing.Literal[\"reference\", \"target\"]\n    | None = None,\n) -&gt; (\n    darts_acquisition.utils.arosics.OffsetInfo\n    | darts_acquisition.utils.arosics.MultiOffsetInfo\n)\n</code></pre> <p>Get the offsets between a target and a reference using the AROSICS algorithm.</p> Note <p>Assumes that the target and reference datasets have the same dimensions.</p> <p>Parameters:</p> <ul> <li> <code>target</code>               (<code>xarray.Dataset | xarray.DataArray</code>)           \u2013            <p>The target image dataset or dataarray to be aligned.</p> </li> <li> <code>reference</code>               (<code>xarray.Dataset | xarray.DataArray</code>)           \u2013            <p>The reference image dataset or dataarray.</p> </li> <li> <code>bands</code>               (<code>list[str] | typing.Literal['multiband'] | str</code>, default:                   <code>'multiband'</code> )           \u2013            <p>The bands to use for alignment. Only used if the target and reference are datasets. If \"multiband\", all bands are used. This expects the target and reference datasets to have the same band names. If string, the respective band is used for alignment. If a list of strings, only the specified bands are used for alignment. Note: All bands are shifted by the same offset, even when using \"multiband\". With multiband, the offset is calculated from the average of all common and valid band offsets. This is slower but more robust than using a single band. If a band-specific offset is desired, please use the <code>get_dataarray_offsets</code> function for each band separately. Defaults to \"multiband\".</p> </li> <li> <code>subset</code>               (<code>dict[typing.Literal['x', 'y'], slice] | typing.Literal[False] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of slices to use for alignment. If provided, only the specified subset of the target and reference datasets is used for alignment. The dictionary must contain the keys \"x\" and \"y\" with the respective slices. If False, the whole dataset is used for alignment. If None, will try to find a suitable subset automatically.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The size of the window to use for alignment in case no subset is provided. Defaults to 256.</p> </li> <li> <code>target_mask</code>               (<code>xarray.DataArray | None</code>, default:                   <code>None</code> )           \u2013            <p>A mask for the target dataset. If provided, searches for a valid subset of the target dataset where the mask is True. If not provided, a mask is automatically inferred from the nodata value of the target dataset. Defaults to None.</p> </li> <li> <code>reference_mask</code>               (<code>xarray.DataArray | None</code>, default:                   <code>None</code> )           \u2013            <p>A mask for the reference dataset. If provided, searches for a valid subset of the reference dataset where the mask is True. If not provided, a mask is automatically inferred from the nodata value of the reference dataset. Defaults to None.</p> </li> <li> <code>max_invalid_ratio</code>               (<code>float</code>, default:                   <code>0.01</code> )           \u2013            <p>The maximum ratio of invalid pixels in the target and reference subset masks. Is not used if the masks are not provided. Defaults to 0.01.</p> </li> <li> <code>max_iter</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>The maximum number of iterations to find the offset. Defaults to 5.</p> </li> <li> <code>min_reliability</code>               (<code>float</code>, default:                   <code>30.0</code> )           \u2013            <p>The minimum reliability (in %) of the offset to consider it valid. Defaults to 30.0.</p> </li> <li> <code>max_offset</code>               (<code>float</code>, default:                   <code>10.0</code> )           \u2013            <p>The maximum offset in pixels to consider the offset valid. Defaults to 10.0.</p> </li> <li> <code>resample_to</code>               (<code>typing.Literal['reference', 'target'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The dataset to resample the other dataset to. If \"reference\", the target dataset is resampled to the reference dataset. If \"target\", the reference dataset is resampled to the target dataset. Defaults to None. If None, no resampling is done. This assumes that the pixel grids of the target and reference datasets are already aligned.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>darts_acquisition.utils.arosics.OffsetInfo | darts_acquisition.utils.arosics.MultiOffsetInfo</code>           \u2013            <p>OffsetInfo | MultiOffsetInfo: The offsets in x and y direction between the target and reference datasets.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the target and reference are not both datasets or dataarrays.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If no suitable subset is found for alignment. This can happen if the window_size is too large or if the masks are too restrictive.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/arosics.py</code> <pre><code>def get_offsets(\n    target: xr.Dataset | xr.DataArray,\n    reference: xr.Dataset | xr.DataArray,\n    bands: list[str] | Literal[\"multiband\"] | str = \"multiband\",\n    subset: dict[Literal[\"x\", \"y\"], slice] | Literal[False] | None = None,\n    window_size: int = 256,\n    target_mask: xr.DataArray | None = None,\n    reference_mask: xr.DataArray | None = None,\n    max_invalid_ratio: float = 0.01,\n    max_iter: int = 5,\n    min_reliability: float = 30.0,\n    max_offset: float = 10.0,\n    resample_to: Literal[\"reference\", \"target\"] | None = None,\n) -&gt; OffsetInfo | MultiOffsetInfo:\n    \"\"\"Get the offsets between a target and a reference using the AROSICS algorithm.\n\n    Note:\n        Assumes that the target and reference datasets have the same dimensions.\n\n    Args:\n        target (xr.Dataset | xr.DataArray): The target image dataset or dataarray to be aligned.\n        reference (xr.Dataset | xr.DataArray): The reference image dataset or dataarray.\n        bands (list[str] | Literal[\"multiband\"] | str): The bands to use for alignment.\n            Only used if the target and reference are datasets.\n            If \"multiband\", all bands are used.\n            This expects the target and reference datasets to have the same band names.\n            If string, the respective band is used for alignment.\n            If a list of strings, only the specified bands are used for alignment.\n            Note: All bands are shifted by the same offset, even when using \"multiband\".\n            With multiband, the offset is calculated from the average of all common and valid band offsets.\n            This is slower but more robust than using a single band.\n            If a band-specific offset is desired,\n            please use the `get_dataarray_offsets` function for each band separately.\n            Defaults to \"multiband\".\n        subset (dict[Literal[\"x\", \"y\"], slice] | Literal[False] | None): A dictionary of slices to use for alignment.\n            If provided, only the specified subset of the target and reference datasets is used for alignment.\n            The dictionary must contain the keys \"x\" and \"y\" with the respective slices.\n            If False, the whole dataset is used for alignment.\n            If None, will try to find a suitable subset automatically.\n        window_size (int): The size of the window to use for alignment in case no subset is provided. Defaults to 256.\n        target_mask (xr.DataArray | None): A mask for the target dataset.\n            If provided, searches for a valid subset of the target dataset where the mask is True.\n            If not provided, a mask is automatically inferred from the nodata value of the target dataset.\n            Defaults to None.\n        reference_mask (xr.DataArray | None): A mask for the reference dataset.\n            If provided, searches for a valid subset of the reference dataset where the mask is True.\n            If not provided, a mask is automatically inferred from the nodata value of the reference dataset.\n            Defaults to None.\n        max_invalid_ratio (float): The maximum ratio of invalid pixels in the target and reference subset masks.\n            Is not used if the masks are not provided.\n            Defaults to 0.01.\n        max_iter (int): The maximum number of iterations to find the offset. Defaults to 5.\n        min_reliability (float): The minimum reliability (in %) of the offset to consider it valid.\n            Defaults to 30.0.\n        max_offset (float): The maximum offset in pixels to consider the offset valid.\n            Defaults to 10.0.\n        resample_to (Literal[\"reference\", \"target\"] | None): The dataset to resample the other dataset to.\n            If \"reference\", the target dataset is resampled to the reference dataset.\n            If \"target\", the reference dataset is resampled to the target dataset.\n            Defaults to None.\n            If None, no resampling is done.\n            This assumes that the pixel grids of the target and reference datasets are already aligned.\n\n    Returns:\n        OffsetInfo | MultiOffsetInfo: The offsets in x and y direction between the target and reference datasets.\n\n    Raises:\n        ValueError: If the target and reference are not both datasets or dataarrays.\n        ValueError: If no suitable subset is found for alignment.\n            This can happen if the window_size is too large or if the masks are too restrictive.\n\n    \"\"\"\n    # Check if both are datasets or dataarrays\n    both_are_datasets = isinstance(target, xr.Dataset) and isinstance(reference, xr.Dataset)\n    both_are_dataarrays = isinstance(target, xr.DataArray) and isinstance(reference, xr.DataArray)\n    if not (both_are_datasets or both_are_dataarrays):\n        raise ValueError(\"Both target and reference must be either xr.Dataset or xr.DataArray.\")\n\n    # Check if the dimentsions are x and y\n    if \"x\" not in target.dims or \"y\" not in target.dims:\n        raise ValueError(\"Target dataset must have dimensions 'x' and 'y'.\")\n    if \"x\" not in reference.dims or \"y\" not in reference.dims:\n        raise ValueError(\"Reference dataset must have dimensions 'x' and 'y'.\")\n\n    if both_are_datasets:\n        # Only work on subset of the bands\n        bands = _get_bands(target, reference, bands)\n        # Apply bands to the datasets\n        target = target[bands]\n        reference = reference[bands]\n\n    # Check for matching crs\n    if target.odc.geobox.crs != reference.odc.geobox.crs:\n        logger.warning(\n            f\"Target and reference datasets have different CRS: {target.odc.geobox.crs} vs {reference.odc.geobox.crs}. \"\n            \"Reprojecting reference to target CRS.\"\n        )\n        reference = reference.odc.reproject(target.odc.geobox.crs, resampling=\"nearest\")\n        reference_mask = (\n            reference_mask.astype(\"int8\").odc.reproject(target.odc.geobox.crs, resampling=\"nearest\").astype(\"bool\")\n            if reference_mask is not None\n            else None\n        )\n\n    # Get spatial intersection of the two images\n    common_extent = reference.odc.geobox.geographic_extent.intersection(target.odc.geobox.geographic_extent)\n    reference = reference.odc.crop(common_extent)\n    target = target.odc.crop(common_extent)\n    reference_mask = (\n        reference_mask.odc.crop(common_extent).fillna(0.0).astype(\"bool\") if reference_mask is not None else None\n    )\n    target_mask = target_mask.odc.crop(common_extent).fillna(0.0).astype(\"bool\") if target_mask is not None else None\n\n    # Resample if requested\n    # We can savely use nearest resampling here, since the datatype will be converted to complex64 anyway\n    # And the matching algorithm works in the frequency domain, so using we use the fastest resampling method\n    if resample_to == \"reference\":\n        target = target.odc.reproject(reference.odc.geobox, resampling=\"nearest\")\n        target_mask = (\n            target_mask.astype(\"int8\").odc.reproject(reference.odc.geobox, resampling=\"nearest\").astype(\"bool\")\n            if target_mask is not None\n            else None\n        )\n    elif resample_to == \"target\":\n        reference = reference.odc.reproject(target.odc.geobox, resampling=\"nearest\")\n        reference_mask = (\n            reference_mask.astype(\"int8\").odc.reproject(target.odc.geobox, resampling=\"nearest\").astype(\"bool\")\n            if reference_mask is not None\n            else None\n        )\n\n    if subset is None:\n        subset = _find_suitable_subset_slices(\n            target,\n            reference,\n            window_size=window_size,\n            target_mask=target_mask,\n            reference_mask=reference_mask,\n            max_invalid_ratio=max_invalid_ratio,\n        )\n\n    # Calculate the offset between the two subsets\n    if both_are_dataarrays:\n        offset_info = _calculate_offset(reference, target, subset=subset, max_iter=max_iter)\n        return offset_info\n    else:\n        offsets = {\n            band: _calculate_offset(reference[band], target[band], subset=subset, max_iter=max_iter) for band in bands\n        }\n        return MultiOffsetInfo._from_offsets(offsets, max_offset=max_offset, min_reliability=min_reliability)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/copernicus/","title":"copernicus","text":""},{"location":"reference/darts_acquisition/utils/copernicus/#darts_acquisition.utils.copernicus","title":"darts_acquisition.utils.copernicus","text":"<p>Copernicus STAC utilities.</p>"},{"location":"reference/darts_acquisition/utils/copernicus/#darts_acquisition.utils.copernicus.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/copernicus/#darts_acquisition.utils.copernicus.init_copernicus","title":"init_copernicus","text":"<pre><code>init_copernicus(profile_name: str = 'default')\n</code></pre> <p>Configure odc.stac and rio to authenticate with Copernicus cloud.</p> <p>This functions expects that credentials are present in the .aws/credentials file. Credentials can be obtained from https://eodata-s3keysmanager.dataspace.copernicus.eu/</p> <p>Example credentials file:</p> <pre><code>[default]\nAWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\n</code></pre> <p>Parameters:</p> <ul> <li> <code>profile_name</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>The boto3 profile name. This must match with the name in the credentials file!. Defaults to \"default\".</p> </li> </ul> References <ul> <li>S3 access: https://documentation.dataspace.copernicus.eu/APIs/S3.html</li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/copernicus.py</code> <pre><code>def init_copernicus(profile_name: str = \"default\"):\n    \"\"\"Configure odc.stac and rio to authenticate with Copernicus cloud.\n\n    This functions expects that credentials are present in the .aws/credentials file.\n    Credentials can be obtained from https://eodata-s3keysmanager.dataspace.copernicus.eu/\n\n    Example credentials file:\n\n    ```\n    [default]\n    AWS_ACCESS_KEY_ID=...\n    AWS_SECRET_ACCESS_KEY=...\n    ```\n\n    Args:\n        profile_name (str, optional): The boto3 profile name. This must match with the name in the credentials file!.\n            Defaults to \"default\".\n\n    References:\n        - S3 access: https://documentation.dataspace.copernicus.eu/APIs/S3.html\n\n    \"\"\"\n    import boto3\n    import odc.stac\n\n    session = boto3.Session(profile_name=profile_name)\n    credentials = session.get_credentials()\n\n    odc.stac.configure_rio(\n        cloud_defaults=True,\n        verbose=False,\n        aws={\n            \"profile_name\": profile_name,\n            \"aws_access_key_id\": credentials.access_key,\n            \"aws_secret_access_key\": credentials.secret_key,\n            \"region_name\": \"default\",\n            \"endpoint_url\": \"eodata.dataspace.copernicus.eu\",\n        },\n        AWS_VIRTUAL_HOSTING=False,\n    )\n    logger.debug(\"Copernicus STAC initialized\")\n</code></pre>"},{"location":"reference/darts_acquisition/utils/copernicus/#darts_acquisition.utils.copernicus.init_copernicus_from_keys","title":"init_copernicus_from_keys","text":"<pre><code>init_copernicus_from_keys(access_key: str, secret_key: str)\n</code></pre> <p>Set up the environment for accessing the Copernicus Data Space Ecosystem S3 storage.</p> <p>This will configure the necessary environment variables for accessing the S3 storage and calls configure_rio to set up the rasterio environment.</p> <p>Keys can be obtained from the Copernicus S3 Credentials manager: https://eodata-s3keysmanager.dataspace.copernicus.eu/panel/s3-credentials</p> <p>Parameters:</p> <ul> <li> <code>access_key</code>               (<code>str</code>)           \u2013            <p>The AWS access key ID.</p> </li> <li> <code>secret_key</code>               (<code>str</code>)           \u2013            <p>The AWS secret access key.</p> </li> </ul> References <ul> <li>S3 access: https://documentation.dataspace.copernicus.eu/APIs/S3.html</li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/copernicus.py</code> <pre><code>def init_copernicus_from_keys(access_key: str, secret_key: str):\n    \"\"\"Set up the environment for accessing the Copernicus Data Space Ecosystem S3 storage.\n\n    This will configure the necessary environment variables for accessing the S3 storage\n    and calls configure_rio to set up the rasterio environment.\n\n    Keys can be obtained from the Copernicus S3 Credentials manager:\n    https://eodata-s3keysmanager.dataspace.copernicus.eu/panel/s3-credentials\n\n    Args:\n        access_key (str): The AWS access key ID.\n        secret_key (str): The AWS secret access key.\n\n    References:\n        - S3 access: https://documentation.dataspace.copernicus.eu/APIs/S3.html\n\n    \"\"\"\n    import boto3\n    import odc.stac\n\n    os.environ[\"GDAL_HTTP_TCP_KEEPALIVE\"] = \"YES\"\n    os.environ[\"AWS_S3_ENDPOINT\"] = \"eodata.dataspace.copernicus.eu\"\n    os.environ[\"AWS_ACCESS_KEY_ID\"] = access_key\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret_key\n    os.environ[\"AWS_HTTPS\"] = \"YES\"\n    os.environ[\"AWS_VIRTUAL_HOSTING\"] = \"FALSE\"\n    os.environ[\"GDAL_HTTP_UNSAFESSL\"] = \"YES\"\n\n    session = boto3.session.Session(\n        aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=\"default\"\n    )\n    odc.stac.configure_rio(cloud_defaults=False, aws={\"session\": session, \"aws_unsigned\": False})\n    logger.debug(\"Copernicus STAC initialized\")\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/","title":"grid","text":""},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid","title":"darts_acquisition.utils.grid","text":"<p>Major Tom grid implementation following the paper https://arxiv.org/html/2402.12095v2.</p>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.R","title":"R  <code>module-attribute</code>","text":"<pre><code>R = 6378.137\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell","title":"Cell  <code>dataclass</code>","text":"<pre><code>Cell(\n    d: float,\n    row_idx: int,\n    col_idx: int,\n    row_direction: typing.Literal[\"U\", \"D\"],\n    col_direction: typing.Literal[\"L\", \"R\"],\n    lat: float,\n    lon: float,\n    utm_zone: int,\n)\n</code></pre> <p>Cell in the Major Tom grid.</p>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.col_direction","title":"col_direction  <code>instance-attribute</code>","text":"<pre><code>col_direction: typing.Literal['L', 'R']\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.col_idx","title":"col_idx  <code>instance-attribute</code>","text":"<pre><code>col_idx: int\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.d","title":"d  <code>instance-attribute</code>","text":"<pre><code>d: float\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.idx","title":"idx  <code>cached</code> <code>property</code>","text":"<pre><code>idx: str\n</code></pre> <p>Major Tom grid index of the cell.</p>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.lat","title":"lat  <code>instance-attribute</code>","text":"<pre><code>lat: float\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.lon","title":"lon  <code>instance-attribute</code>","text":"<pre><code>lon: float\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.row_direction","title":"row_direction  <code>instance-attribute</code>","text":"<pre><code>row_direction: typing.Literal['U', 'D']\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.row_idx","title":"row_idx  <code>instance-attribute</code>","text":"<pre><code>row_idx: int\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.utm_zone","title":"utm_zone  <code>instance-attribute</code>","text":"<pre><code>utm_zone: int\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def __repr__(self) -&gt; str:  # noqa: D105\n    return f\"Cell({self.idx})\"\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds(size: int, resolution: float) -&gt; shapely.Polygon\n</code></pre> <p>Get the bounds of the cell as a shapely Polygon in the respective local utm_zone coordinates.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The size of the tile in px.</p> </li> <li> <code>resolution</code>               (<code>float</code>)           \u2013            <p>The resolution of the tile in m/px.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>shapely.Polygon</code>           \u2013            <p>shapely.Polygon: The bounds of the cell as a shapely Polygon.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def get_bounds(self, size: int, resolution: float) -&gt; shapely.Polygon:\n    \"\"\"Get the bounds of the cell as a shapely Polygon in the respective local utm_zone coordinates.\n\n    Args:\n        size (int): The size of the tile in px.\n        resolution(float): The resolution of the tile in m/px.\n\n    Returns:\n        shapely.Polygon: The bounds of the cell as a shapely Polygon.\n\n    \"\"\"\n    transformer = pyproj.Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{self.utm_zone}\", always_xy=True)\n    x_start, y_start = transformer.transform(self.x, self.y)\n    x_end, y_end = x_start + size * resolution, y_start + size * resolution\n    return shapely.geometry.box(x_start, y_start, x_end, y_end)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.Cell.to_shape","title":"to_shape","text":"<pre><code>to_shape() -&gt; shapely.Point\n</code></pre> <p>Convert the cell to a shapely Point object.</p> <p>Returns:</p> <ul> <li> <code>shapely.Point</code>           \u2013            <p>shapely.Point: The cell as a shapely Point object.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def to_shape(self) -&gt; shapely.Point:\n    \"\"\"Convert the cell to a shapely Point object.\n\n    Returns:\n        shapely.Point: The cell as a shapely Point object.\n\n    \"\"\"\n    return shapely.geometry.Point(self.lon, self.lat)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid","title":"MajorTomGrid","text":"<pre><code>MajorTomGrid(d: float)\n</code></pre> <p>Major Tom grid following the implementation from https://arxiv.org/html/2402.12095v2.</p> <p>Initialize the Major Tom grid with a given resolution.</p> <p>Parameters:</p> <ul> <li> <code>d</code>               (<code>float</code>)           \u2013            <p>Resolution of the grid in km.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def __init__(self, d: float):\n    \"\"\"Initialize the Major Tom grid with a given resolution.\n\n    Args:\n        d (float): Resolution of the grid in km.\n\n    \"\"\"\n    self.d = d\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.d","title":"d  <code>instance-attribute</code>","text":"<pre><code>d = darts_acquisition.utils.grid.MajorTomGrid(d)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.dlat","title":"dlat  <code>cached</code> <code>property</code>","text":"<pre><code>dlat: float\n</code></pre> <p>Latitude resolution in degrees.</p>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.n_r","title":"n_r  <code>cached</code> <code>property</code>","text":"<pre><code>n_r: int\n</code></pre> <p>Number of rows (latitudes) in the grid.</p>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: str) -&gt; darts_acquisition.utils.grid.Cell\n</code></pre> <p>Get the grid cell at the given index.</p> <p>Parameters:</p> <ul> <li> <code>idx</code>               (<code>str</code>)           \u2013            <p>Major Tom grid index in the format re'/\\d+(D|U)\\d+(L|R)/g'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Cell</code> (              <code>darts_acquisition.utils.grid.Cell</code> )          \u2013            <p>The grid cell at the given index.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def __getitem__(self, idx: str) -&gt; Cell:\n    r\"\"\"Get the grid cell at the given index.\n\n    Args:\n        idx (str): Major Tom grid index in the format re'/\\d+(D|U)\\d+(L|R)/g'.\n\n    Returns:\n        Cell: The grid cell at the given index.\n\n    \"\"\"\n    row_idx, row_direction, col_idx, col_direction = parse_cell_idx(idx)\n\n    # Get latitude information\n    assert row_direction in (\"U\", \"D\")\n    assert row_idx &lt;= (self.n_r // 2)\n    lat = row_idx * self.dlat * (-1 if row_direction == \"D\" else 1)\n\n    # Get longitude information\n    assert col_direction in (\"L\", \"R\")\n    assert col_idx &lt;= (self.n_c(lat) // 2)\n    lon = col_idx * self.dlon(lat) * (-1 if col_direction == \"L\" else 1)\n\n    utm_zone = get_utm_zone_from_latlng(lat, lon)\n    return Cell(self.d, row_idx, col_idx, row_direction, col_direction, lat, lon, utm_zone)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.__len__","title":"__len__  <code>cached</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>@functools.cache\ndef __len__(self) -&gt; int:  # noqa: D105\n    n = 0\n    for lat, _, _ in self.yield_latitudes():\n        n += self.n_c(lat)\n    return n\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.dlon","title":"dlon","text":"<pre><code>dlon(lat: float) -&gt; float\n</code></pre> <p>Longitude resolution in degrees at a given latitude.</p> <p>Parameters:</p> <ul> <li> <code>lat</code>               (<code>float</code>)           \u2013            <p>Latitude at which to calculate the longitude resolution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>Longitude resolution at the given latitude.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def dlon(self, lat: float) -&gt; float:\n    \"\"\"Longitude resolution in degrees at a given latitude.\n\n    Args:\n        lat (float): Latitude at which to calculate the longitude resolution.\n\n    Returns:\n        float: Longitude resolution at the given latitude.\n\n    \"\"\"\n    return 360 / self.n_c(lat)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.n_c","title":"n_c  <code>cached</code>","text":"<pre><code>n_c(lat: float) -&gt; int\n</code></pre> <p>Calculate the number of columns (longitudes) in the grid at a given latitude.</p> <p>Parameters:</p> <ul> <li> <code>lat</code>               (<code>float</code>)           \u2013            <p>Latitude at which to calculate the number of columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code> (              <code>int</code> )          \u2013            <p>Number of columns at the given latitude.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>@functools.lru_cache(maxsize=2)\ndef n_c(self, lat: float) -&gt; int:\n    \"\"\"Calculate the number of columns (longitudes) in the grid at a given latitude.\n\n    Args:\n        lat (float): Latitude at which to calculate the number of columns.\n\n    Returns:\n        int: Number of columns at the given latitude.\n\n    \"\"\"\n    c_r = 2 * np.pi * R * math.cos(np.pi * lat / 180)  # Circumference of the circle at the given latitude\n    return math.ceil(c_r / self.d)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.to_geodataframe","title":"to_geodataframe","text":"<pre><code>to_geodataframe(\n    bounds: tuple[float, float, float, float]\n    | shapely.Polygon\n    | None = None,\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Convert the Major Tom grid to a geopandas GeoDataFrame.</p> <p>Parameters:</p> <ul> <li> <code>bounds</code>               (<code>tuple[float, float, float, float] | shapely.Polygon | None</code>, default:                   <code>None</code> )           \u2013            <p>The bounds of the grid. If the bounds are a polygon, only cells within the polygon are yielded. If the bounds are a tuple, the format must be (min_lon, min_lat, max_lon, max_lat). If None, the entire grid is yielded. Coordinates must be in EPSG:4326. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>geopandas.GeoDataFrame</code>           \u2013            <p>geopandas.GeoDataFrame: The Major Tom grid as a GeoDataFrame.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def to_geodataframe(\n    self, bounds: tuple[float, float, float, float] | shapely.Polygon | None = None\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert the Major Tom grid to a geopandas GeoDataFrame.\n\n    Args:\n        bounds (tuple[float, float, float, float] | shapely.Polygon | None): The bounds of the grid.\n            If the bounds are a polygon, only cells within the polygon are yielded.\n            If the bounds are a tuple, the format must be (min_lon, min_lat, max_lon, max_lat).\n            If None, the entire grid is yielded.\n            Coordinates must be in EPSG:4326.\n            Defaults to None.\n\n    Returns:\n        geopandas.GeoDataFrame: The Major Tom grid as a GeoDataFrame.\n\n    \"\"\"\n    cells = [{\"geometry\": shapely.Point([cell.lon, cell.lat]), **asdict(cell)} for cell in self.yield_cells(bounds)]\n    gdf = gpd.GeoDataFrame(cells, crs=\"EPSG:4326\")\n    return gdf\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.yield_cells","title":"yield_cells","text":"<pre><code>yield_cells(\n    bounds: tuple[float, float, float, float]\n    | shapely.Polygon\n    | None = None,\n) -&gt; collections.abc.Generator[\n    darts_acquisition.utils.grid.Cell, None, None\n]\n</code></pre> <p>Generate all cells in the Major Tom grid.</p> <p>Starting at the North Pole, the generator yields cells from west to east and from north to south.</p> <p>Parameters:</p> <ul> <li> <code>bounds</code>               (<code>tuple[float, float, float, float] | shapely.Polygon | None</code>, default:                   <code>None</code> )           \u2013            <p>The bounds of the grid. If the bounds are a polygon, only cells within the polygon are yielded. If the bounds are a tuple, the format must be (min_lon, min_lat, max_lon, max_lat). If None, the entire grid is yielded. Coordinates must be in EPSG:4326. Please note that bounds around the antimetidian (180\u00b0/-180\u00b0) and the poles (90\u00b0/-90\u00b0) are not supported. Defaults to None.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Cell</code> (              <code>darts_acquisition.utils.grid.Cell</code> )          \u2013            <p>The next cell in the grid.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def yield_cells(\n    self,\n    bounds: tuple[float, float, float, float] | shapely.Polygon | None = None,\n) -&gt; Generator[Cell, None, None]:\n    \"\"\"Generate all cells in the Major Tom grid.\n\n    Starting at the North Pole, the generator yields cells from west to east and from north to south.\n\n    Args:\n        bounds (tuple[float, float, float, float] | shapely.Polygon | None): The bounds of the grid.\n            If the bounds are a polygon, only cells within the polygon are yielded.\n            If the bounds are a tuple, the format must be (min_lon, min_lat, max_lon, max_lat).\n            If None, the entire grid is yielded.\n            Coordinates must be in EPSG:4326.\n            Please note that bounds around the antimetidian (180\u00b0/-180\u00b0) and the poles (90\u00b0/-90\u00b0) are not supported.\n            Defaults to None.\n\n    Yields:\n        Cell: The next cell in the grid.\n\n    \"\"\"\n    is_polygon = isinstance(bounds, shapely.Polygon)\n    if is_polygon:\n        orig_min_lon, min_lat, max_lon, max_lat = bounds.bounds\n    else:\n        orig_min_lon, min_lat, max_lon, max_lat = bounds or (-180, -90, 180, 90)\n    # Include grid points just outside the bounds to ensure that their cell (to the north-west) is included\n    min_lat -= self.dlat\n\n    for lat, row_idx, row_direction in self.yield_latitudes():\n        # Include grid points just outside the bounds to ensure that their cell (to the north-west) is included\n        min_lon = orig_min_lon - self.dlon(lat)\n        for lon, col_idx, col_direction in self.yield_longitudes(lat):\n            # Do an easy-bound check first\n            if not (min_lat &lt;= lat &lt;= max_lat) or not (min_lon &lt;= lon &lt;= max_lon):\n                continue\n            # If the bounds are a polygon, do a more precise check\n            if (\n                is_polygon\n                and not bounds.contains(shapely.geometry.Point(lon, lat))\n                and not bounds.contains(shapely.geometry.Point(lon + self.dlon(lat), lat))\n                and not bounds.contains(shapely.geometry.Point(lon, lat + self.dlat))\n                and not bounds.contains(shapely.geometry.Point(lon + self.dlon(lat), lat + self.dlat))\n            ):\n                continue\n            utm_zone = get_utm_zone_from_latlng(lat, lon)\n            yield Cell(self.d, row_idx, col_idx, row_direction, col_direction, lat, lon, utm_zone)\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.yield_latitudes","title":"yield_latitudes","text":"<pre><code>yield_latitudes() -&gt; collections.abc.Generator[\n    tuple[float, int, str], None, None\n]\n</code></pre> <p>Generate the latitudes of the Major Tom grid from north to south.</p> <p>Yields:</p> <ul> <li> <code>tuple[float, int, str]</code>           \u2013            <p>tuple[float, int, str]: The next latitude in the grid. (latitude, row index, direction)</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def yield_latitudes(self) -&gt; Generator[tuple[float, int, str], None, None]:\n    \"\"\"Generate the latitudes of the Major Tom grid from north to south.\n\n    Yields:\n        tuple[float, int, str]: The next latitude in the grid. (latitude, row index, direction)\n\n    \"\"\"\n    # if n_r is even -90 is included, 0 is always included\n\n    # northern hemisphere\n    northern_point = 90 - self.dlat / 2 if self.n_r % 2 else 90 - self.dlat\n    n_northern = int(np.ceil(self.n_r / 2) - 1)\n    for i, lat in enumerate(np.linspace(northern_point, 0, n_northern, endpoint=False)):\n        yield lat, n_northern - i, \"U\"\n\n    # equator\n    yield 0, 0, \"U\"\n\n    # southern hemisphere\n    southern_point = -90 + self.dlat / 2 if self.n_r % 2 else -90\n    n_southern = self.n_r // 2\n    for i, lat in enumerate(np.linspace(-self.dlat, southern_point, n_southern, endpoint=True)):\n        yield lat, i + 1, \"D\"\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.MajorTomGrid.yield_longitudes","title":"yield_longitudes","text":"<pre><code>yield_longitudes(\n    lat: float,\n) -&gt; collections.abc.Generator[\n    tuple[float, int, str], None, None\n]\n</code></pre> <p>Generate the longitudes of the Major Tom grid at a given latitude.</p> <p>Parameters:</p> <ul> <li> <code>lat</code>               (<code>float</code>)           \u2013            <p>Latitude at which to generate the longitudes.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[float, int, str]</code>           \u2013            <p>tuple[float, int, str]: The next longitude at the given latitude. (longitude, column index, direction)</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def yield_longitudes(self, lat: float) -&gt; Generator[tuple[float, int, str], None, None]:\n    \"\"\"Generate the longitudes of the Major Tom grid at a given latitude.\n\n    Args:\n        lat (float): Latitude at which to generate the longitudes.\n\n    Yields:\n        tuple[float, int, str]: The next longitude at the given latitude. (longitude, column index, direction)\n\n    \"\"\"\n    n_c = self.n_c(lat)\n    dlon = self.dlon(lat)\n\n    # if n_c is even -180 is included, 0 is always included\n\n    # western hemisphere\n    western_point = -180 + dlon / 2 if n_c % 2 else -180\n    n_western = n_c // 2\n    for i, lon in enumerate(np.linspace(western_point, 0, n_western, endpoint=False)):\n        yield lon, n_western - i, \"L\"\n\n    # prime meridian\n    yield 0, 0, \"L\"\n\n    # eastern hemisphere\n    eastern_point = 180 - dlon / 2 if n_c % 2 else 180 - dlon\n    n_eastern = int(np.ceil(n_c / 2) - 1)\n    for i, lon in enumerate(np.linspace(dlon, eastern_point, n_eastern, endpoint=True)):\n        yield lon, i + 1, \"R\"\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.get_utm_zone_from_latlng","title":"get_utm_zone_from_latlng","text":"<pre><code>get_utm_zone_from_latlng(\n    latitude: float | int, longitude: float | int\n) -&gt; str\n</code></pre> <p>Get the UTM zone from a latlng list and return the corresponding EPSG code.</p> <p>Parameters:</p> <ul> <li> <code>latitude</code>               (<code>float | int</code>)           \u2013            <p>The latitude of the point.</p> </li> <li> <code>longitude</code>               (<code>float | int</code>)           \u2013            <p>The longitude of the point.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The EPSG code corresponding to the UTM zone of the point.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the point is out of bounds.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def get_utm_zone_from_latlng(latitude: float | int, longitude: float | int) -&gt; str:\n    \"\"\"Get the UTM zone from a latlng list and return the corresponding EPSG code.\n\n    Args:\n        latitude (float | int): The latitude of the point.\n        longitude (float | int): The longitude of the point.\n\n    Returns:\n        str: The EPSG code corresponding to the UTM zone of the point.\n\n    Raises:\n        ValueError: If the point is out of bounds.\n\n    \"\"\"\n    assert -180 &lt;= longitude &lt;= 180, f\"longitude: {longitude} is out of bounds\"\n    assert -90 &lt;= latitude &lt;= 90, f\"latitude: {latitude} is out of bounds\"\n    zone_number = (math.floor((longitude + 180) / 6)) % 60 + 1\n\n    # Special zones for Svalbard and Norway\n    if latitude &gt;= 56.0 and latitude &lt; 64.0 and longitude &gt;= 3.0 and longitude &lt; 12.0:\n        zone_number = 32\n    elif latitude &gt;= 72.0 and latitude &lt; 84.0:\n        if longitude &gt;= 0.0 and longitude &lt; 9.0:\n            zone_number = 31\n        elif longitude &gt;= 9.0 and longitude &lt; 21.0:\n            zone_number = 33\n        elif longitude &gt;= 21.0 and longitude &lt; 33.0:\n            zone_number = 35\n        elif longitude &gt;= 33.0 and longitude &lt; 42.0:\n            zone_number = 37\n\n    # Determine the hemisphere and construct the EPSG code\n    if latitude &lt; 0:\n        epsg_code = f\"327{zone_number:02d}\"\n    else:\n        epsg_code = f\"326{zone_number:02d}\"\n\n    if not re.match(r\"32[6-7](0[1-9]|[1-5][0-9]|60)\", epsg_code):\n        raise ValueError(\n            f\"The point {latitude=}, {longitude=} is out of bounds (resulted in {epsg_code=} and {zone_number=})\"\n        )\n\n    return epsg_code\n</code></pre>"},{"location":"reference/darts_acquisition/utils/grid/#darts_acquisition.utils.grid.parse_cell_idx","title":"parse_cell_idx","text":"<pre><code>parse_cell_idx(idx: str) -&gt; tuple[int, str, int, str]\n</code></pre> <p>Parse the Major Tom grid index into its components.</p> <p>Parameters:</p> <ul> <li> <code>idx</code>               (<code>str</code>)           \u2013            <p>Major Tom grid index in the format re'/\\d+(D|U)\\d+(L|R)/g'.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, str, int, str]</code>           \u2013            <p>tuple[int, str, int, str]: The row index, row direction, column index, and column direction.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the index is invalid.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/utils/grid.py</code> <pre><code>def parse_cell_idx(idx: str) -&gt; tuple[int, str, int, str]:\n    r\"\"\"Parse the Major Tom grid index into its components.\n\n    Args:\n        idx (str): Major Tom grid index in the format re'/\\d+(D|U)\\d+(L|R)/g'.\n\n    Returns:\n        tuple[int, str, int, str]: The row index, row direction, column index, and column direction.\n\n    Raises:\n        ValueError: If the index is invalid.\n\n    \"\"\"\n    row_idx = \"\"\n    row_direction = \"\"\n    col_idx = \"\"\n    col_direction = \"\"\n    is_row = True\n    for c in idx:\n        if c.isdigit():\n            if is_row:\n                row_idx += c\n            else:\n                col_idx += c\n        else:\n            if is_row:\n                row_direction = c\n                is_row = False\n            else:\n                col_direction = c\n    if not row_idx or not row_direction or not col_idx or not col_direction:\n        raise ValueError(f\"Invalid cell index: {idx}\")\n    return int(row_idx), row_direction, int(col_idx), col_direction\n</code></pre>"},{"location":"reference/darts_ensemble/","title":"darts_ensemble","text":""},{"location":"reference/darts_ensemble/#darts_ensemble","title":"darts_ensemble","text":"<p>Inference and model ensembling for the DARTS dataset.</p>"},{"location":"reference/darts_ensemble/#darts_ensemble.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1","title":"EnsembleV1","text":"<pre><code>EnsembleV1(\n    model_dict,\n    device: torch.device = darts_ensemble.ensemble_v1.DEFAULT_DEVICE,\n)\n</code></pre> <p>Model ensemble that averages predictions from multiple segmentation models.</p> <p>This class manages multiple trained segmentation models and combines their predictions by averaging, providing more robust and stable predictions than any single model. It's particularly useful for combining models trained with different data sources (e.g., with and without TCVIS data).</p> <p>Attributes:</p> <ul> <li> <code>models</code>               (<code>dict[str, darts_segmentation.segment.SMPSegmenter]</code>)           \u2013            <p>Dictionary mapping model names to loaded segmenters.</p> </li> </ul> Note <p>The ensemble automatically: - Manages multiple model instances with separate configurations - Handles band requirements across all models - Averages probability predictions (simple arithmetic mean) - Optionally preserves individual model outputs for analysis</p> Example <p>Create and use an ensemble:</p> <pre><code>from darts_ensemble import EnsembleV1\nimport torch\n\n# Initialize ensemble with multiple models\nensemble = EnsembleV1(\n    model_dict={\n        \"with_tcvis\": \"path/to/model_with_tcvis.ckpt\",\n        \"without_tcvis\": \"path/to/model_without_tcvis.ckpt\",\n    },\n    device=torch.device(\"cuda\")\n)\n\n# Check combined band requirements\nprint(ensemble.required_bands)\n# {'blue', 'green', 'red', 'nir', 'ndvi', 'tc_brightness', ...}\n\n# Run ensemble inference\nresult = ensemble.segment_tile(\n    tile=preprocessed_tile,\n    keep_inputs=True  # Keep individual model predictions\n)\n\n# Access predictions\nensemble_probs = result[\"probabilities\"]  # Averaged\nmodel1_probs = result[\"probabilities-with_tcvis\"]  # Individual\nmodel2_probs = result[\"probabilities-without_tcvis\"]  # Individual\n</code></pre> <p>Initialize the ensemble with multiple model checkpoints.</p> <p>Parameters:</p> <ul> <li> <code>model_dict</code>               (<code>dict[str, str | pathlib.Path]</code>)           \u2013            <p>Mapping of model identifiers to checkpoint paths. Keys are used to name individual model outputs (e.g., \"with_tcvis\", \"without_tcvis\"). Values are paths to model checkpoint files.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_ensemble.ensemble_v1.DEFAULT_DEVICE</code> )           \u2013            <p>Device to load all models on. Defaults to CUDA if available, else CPU.</p> </li> </ul> Note <p>All models are loaded on the same device. For multi-GPU ensembles, instantiate separate EnsembleV1 objects per device.</p> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __init__(\n    self,\n    model_dict,\n    device: torch.device = DEFAULT_DEVICE,\n):\n    \"\"\"Initialize the ensemble with multiple model checkpoints.\n\n    Args:\n        model_dict (dict[str, str | Path]): Mapping of model identifiers to checkpoint paths.\n            Keys are used to name individual model outputs (e.g., \"with_tcvis\", \"without_tcvis\").\n            Values are paths to model checkpoint files.\n        device (torch.device, optional): Device to load all models on.\n            Defaults to CUDA if available, else CPU.\n\n    Note:\n        All models are loaded on the same device. For multi-GPU ensembles, instantiate\n        separate EnsembleV1 objects per device.\n\n    \"\"\"\n    model_paths = {k: Path(v) for k, v in model_dict.items()}\n    logger.debug(\n        \"Loading models:\\n\" + \"\\n\".join([f\" - {k.upper()} model: {v.resolve()}\" for k, v in model_paths.items()])\n    )\n    self.models = {k: SMPSegmenter(v, device=device) for k, v in model_paths.items()}\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.model_names","title":"model_names  <code>property</code>","text":"<pre><code>model_names: list[str]\n</code></pre> <p>The names of the models in this ensemble.</p>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = {\n    k: (\n        darts_segmentation.segment.SMPSegmenter(\n            v,\n            device=darts_ensemble.ensemble_v1.EnsembleV1(\n                device\n            ),\n        )\n    )\n    for (k, v) in (model_paths.items())\n}\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.required_bands","title":"required_bands  <code>property</code>","text":"<pre><code>required_bands: set[str]\n</code></pre> <p>The combined bands required by all models in this ensemble.</p>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run the ensemble on the given tile.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile with the ensemble applied.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run the ensemble on the given tile.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        xr.Dataset: Output tile with the ensemble applied.\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    else:\n        raise ValueError(\"Input must be an xr.Dataset or a list of xr.Dataset.\")\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run ensemble inference on a single tile by averaging multiple model predictions.</p> <p>Each model in the ensemble processes the tile independently, then predictions are combined by simple arithmetic averaging to produce the final ensemble prediction.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Input tile containing preprocessed data. Must include all bands required by any model in the ensemble (union of all <code>required_bands</code>).</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Size of square patches for inference in pixels. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>Overlap between adjacent patches in pixels. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of patches to process simultaneously per model. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding applied to tile edges in pixels. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, preserves individual model predictions as separate variables (e.g., \"probabilities-with_tcvis\"). Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input tile augmented with: - probabilities (float32): Ensemble-averaged predictions in range [0, 1].   Attributes: long_name=\"Probabilities\" - probabilities-{model_name} (float32): Individual model predictions   (only if keep_inputs=True)</p> </li> </ul> Note <p>Averaging method: Simple arithmetic mean across all models. For N models: ensemble_prob = (prob_1 + prob_2 + ... + prob_N) / N</p> <p>This approach assumes equal confidence in all models. Consider weighted averaging if models have different validation performances.</p> Example <p>Run ensemble with analysis of individual models:</p> <pre><code>result = ensemble.segment_tile(\n    tile=preprocessed_tile,\n    patch_size=1024,\n    overlap=16,\n    keep_inputs=True  # Keep individual predictions\n)\n\n# Compare ensemble vs individual models\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(1, 3)\nresult[\"probabilities\"].plot(ax=axes[0], title=\"Ensemble\")\nresult[\"probabilities-with_tcvis\"].plot(ax=axes[1], title=\"Model 1\")\nresult[\"probabilities-without_tcvis\"].plot(ax=axes[2], title=\"Model 2\")\n</code></pre> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>@stopwatch.f(\n    \"Ensemble inference\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\", \"keep_inputs\"],\n)\ndef segment_tile(\n    self,\n    tile: xr.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run ensemble inference on a single tile by averaging multiple model predictions.\n\n    Each model in the ensemble processes the tile independently, then predictions are\n    combined by simple arithmetic averaging to produce the final ensemble prediction.\n\n    Args:\n        tile (xr.Dataset): Input tile containing preprocessed data. Must include all bands\n            required by any model in the ensemble (union of all `required_bands`).\n        patch_size (int, optional): Size of square patches for inference in pixels.\n            Defaults to 1024.\n        overlap (int, optional): Overlap between adjacent patches in pixels. Defaults to 16.\n        batch_size (int, optional): Number of patches to process simultaneously per model.\n            Defaults to 8.\n        reflection (int, optional): Reflection padding applied to tile edges in pixels.\n            Defaults to 0.\n        keep_inputs (bool, optional): If True, preserves individual model predictions as\n            separate variables (e.g., \"probabilities-with_tcvis\"). Defaults to False.\n\n    Returns:\n        xr.Dataset: Input tile augmented with:\n            - probabilities (float32): Ensemble-averaged predictions in range [0, 1].\n              Attributes: long_name=\"Probabilities\"\n            - probabilities-{model_name} (float32): Individual model predictions\n              (only if keep_inputs=True)\n\n    Note:\n        Averaging method: Simple arithmetic mean across all models. For N models:\n        ensemble_prob = (prob_1 + prob_2 + ... + prob_N) / N\n\n        This approach assumes equal confidence in all models. Consider weighted averaging\n        if models have different validation performances.\n\n    Example:\n        Run ensemble with analysis of individual models:\n\n        ```python\n        result = ensemble.segment_tile(\n            tile=preprocessed_tile,\n            patch_size=1024,\n            overlap=16,\n            keep_inputs=True  # Keep individual predictions\n        )\n\n        # Compare ensemble vs individual models\n        import matplotlib.pyplot as plt\n        fig, axes = plt.subplots(1, 3)\n        result[\"probabilities\"].plot(ax=axes[0], title=\"Ensemble\")\n        result[\"probabilities-with_tcvis\"].plot(ax=axes[1], title=\"Model 1\")\n        result[\"probabilities-without_tcvis\"].plot(ax=axes[2], title=\"Model 2\")\n        ```\n\n    \"\"\"\n    probabilities = {}\n    for model_name, model in self.models.items():\n        probabilities[model_name] = model.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )[\"probabilities\"].copy()\n\n    # calculate the mean\n    tile[\"probabilities\"] = xr.concat(probabilities.values(), dim=\"model_probs\").mean(dim=\"model_probs\")\n\n    if keep_inputs:\n        for k, v in probabilities.items():\n            tile[f\"probabilities-{k}\"] = v\n\n    return tile\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    return [\n        self.segment_tile(\n            tile,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n        for tile in tiles\n    ]\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/","title":"ensemble_v1","text":""},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1","title":"darts_ensemble.ensemble_v1","text":"<p>DARTS v1 ensemble based on two models, one trained with TCVIS data and the other without.</p>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.DEFAULT_DEVICE","title":"DEFAULT_DEVICE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DEVICE = torch.device(\n    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1","title":"EnsembleV1","text":"<pre><code>EnsembleV1(\n    model_dict,\n    device: torch.device = darts_ensemble.ensemble_v1.DEFAULT_DEVICE,\n)\n</code></pre> <p>Model ensemble that averages predictions from multiple segmentation models.</p> <p>This class manages multiple trained segmentation models and combines their predictions by averaging, providing more robust and stable predictions than any single model. It's particularly useful for combining models trained with different data sources (e.g., with and without TCVIS data).</p> <p>Attributes:</p> <ul> <li> <code>models</code>               (<code>dict[str, darts_segmentation.segment.SMPSegmenter]</code>)           \u2013            <p>Dictionary mapping model names to loaded segmenters.</p> </li> </ul> Note <p>The ensemble automatically: - Manages multiple model instances with separate configurations - Handles band requirements across all models - Averages probability predictions (simple arithmetic mean) - Optionally preserves individual model outputs for analysis</p> Example <p>Create and use an ensemble:</p> <pre><code>from darts_ensemble import EnsembleV1\nimport torch\n\n# Initialize ensemble with multiple models\nensemble = EnsembleV1(\n    model_dict={\n        \"with_tcvis\": \"path/to/model_with_tcvis.ckpt\",\n        \"without_tcvis\": \"path/to/model_without_tcvis.ckpt\",\n    },\n    device=torch.device(\"cuda\")\n)\n\n# Check combined band requirements\nprint(ensemble.required_bands)\n# {'blue', 'green', 'red', 'nir', 'ndvi', 'tc_brightness', ...}\n\n# Run ensemble inference\nresult = ensemble.segment_tile(\n    tile=preprocessed_tile,\n    keep_inputs=True  # Keep individual model predictions\n)\n\n# Access predictions\nensemble_probs = result[\"probabilities\"]  # Averaged\nmodel1_probs = result[\"probabilities-with_tcvis\"]  # Individual\nmodel2_probs = result[\"probabilities-without_tcvis\"]  # Individual\n</code></pre> <p>Initialize the ensemble with multiple model checkpoints.</p> <p>Parameters:</p> <ul> <li> <code>model_dict</code>               (<code>dict[str, str | pathlib.Path]</code>)           \u2013            <p>Mapping of model identifiers to checkpoint paths. Keys are used to name individual model outputs (e.g., \"with_tcvis\", \"without_tcvis\"). Values are paths to model checkpoint files.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_ensemble.ensemble_v1.DEFAULT_DEVICE</code> )           \u2013            <p>Device to load all models on. Defaults to CUDA if available, else CPU.</p> </li> </ul> Note <p>All models are loaded on the same device. For multi-GPU ensembles, instantiate separate EnsembleV1 objects per device.</p> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __init__(\n    self,\n    model_dict,\n    device: torch.device = DEFAULT_DEVICE,\n):\n    \"\"\"Initialize the ensemble with multiple model checkpoints.\n\n    Args:\n        model_dict (dict[str, str | Path]): Mapping of model identifiers to checkpoint paths.\n            Keys are used to name individual model outputs (e.g., \"with_tcvis\", \"without_tcvis\").\n            Values are paths to model checkpoint files.\n        device (torch.device, optional): Device to load all models on.\n            Defaults to CUDA if available, else CPU.\n\n    Note:\n        All models are loaded on the same device. For multi-GPU ensembles, instantiate\n        separate EnsembleV1 objects per device.\n\n    \"\"\"\n    model_paths = {k: Path(v) for k, v in model_dict.items()}\n    logger.debug(\n        \"Loading models:\\n\" + \"\\n\".join([f\" - {k.upper()} model: {v.resolve()}\" for k, v in model_paths.items()])\n    )\n    self.models = {k: SMPSegmenter(v, device=device) for k, v in model_paths.items()}\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.model_names","title":"model_names  <code>property</code>","text":"<pre><code>model_names: list[str]\n</code></pre> <p>The names of the models in this ensemble.</p>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = {\n    k: (\n        darts_segmentation.segment.SMPSegmenter(\n            v,\n            device=darts_ensemble.ensemble_v1.EnsembleV1(\n                device\n            ),\n        )\n    )\n    for (k, v) in (model_paths.items())\n}\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.required_bands","title":"required_bands  <code>property</code>","text":"<pre><code>required_bands: set[str]\n</code></pre> <p>The combined bands required by all models in this ensemble.</p>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run the ensemble on the given tile.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile with the ensemble applied.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run the ensemble on the given tile.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        xr.Dataset: Output tile with the ensemble applied.\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    else:\n        raise ValueError(\"Input must be an xr.Dataset or a list of xr.Dataset.\")\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run ensemble inference on a single tile by averaging multiple model predictions.</p> <p>Each model in the ensemble processes the tile independently, then predictions are combined by simple arithmetic averaging to produce the final ensemble prediction.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Input tile containing preprocessed data. Must include all bands required by any model in the ensemble (union of all <code>required_bands</code>).</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Size of square patches for inference in pixels. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>Overlap between adjacent patches in pixels. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of patches to process simultaneously per model. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding applied to tile edges in pixels. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, preserves individual model predictions as separate variables (e.g., \"probabilities-with_tcvis\"). Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input tile augmented with: - probabilities (float32): Ensemble-averaged predictions in range [0, 1].   Attributes: long_name=\"Probabilities\" - probabilities-{model_name} (float32): Individual model predictions   (only if keep_inputs=True)</p> </li> </ul> Note <p>Averaging method: Simple arithmetic mean across all models. For N models: ensemble_prob = (prob_1 + prob_2 + ... + prob_N) / N</p> <p>This approach assumes equal confidence in all models. Consider weighted averaging if models have different validation performances.</p> Example <p>Run ensemble with analysis of individual models:</p> <pre><code>result = ensemble.segment_tile(\n    tile=preprocessed_tile,\n    patch_size=1024,\n    overlap=16,\n    keep_inputs=True  # Keep individual predictions\n)\n\n# Compare ensemble vs individual models\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(1, 3)\nresult[\"probabilities\"].plot(ax=axes[0], title=\"Ensemble\")\nresult[\"probabilities-with_tcvis\"].plot(ax=axes[1], title=\"Model 1\")\nresult[\"probabilities-without_tcvis\"].plot(ax=axes[2], title=\"Model 2\")\n</code></pre> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>@stopwatch.f(\n    \"Ensemble inference\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\", \"keep_inputs\"],\n)\ndef segment_tile(\n    self,\n    tile: xr.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run ensemble inference on a single tile by averaging multiple model predictions.\n\n    Each model in the ensemble processes the tile independently, then predictions are\n    combined by simple arithmetic averaging to produce the final ensemble prediction.\n\n    Args:\n        tile (xr.Dataset): Input tile containing preprocessed data. Must include all bands\n            required by any model in the ensemble (union of all `required_bands`).\n        patch_size (int, optional): Size of square patches for inference in pixels.\n            Defaults to 1024.\n        overlap (int, optional): Overlap between adjacent patches in pixels. Defaults to 16.\n        batch_size (int, optional): Number of patches to process simultaneously per model.\n            Defaults to 8.\n        reflection (int, optional): Reflection padding applied to tile edges in pixels.\n            Defaults to 0.\n        keep_inputs (bool, optional): If True, preserves individual model predictions as\n            separate variables (e.g., \"probabilities-with_tcvis\"). Defaults to False.\n\n    Returns:\n        xr.Dataset: Input tile augmented with:\n            - probabilities (float32): Ensemble-averaged predictions in range [0, 1].\n              Attributes: long_name=\"Probabilities\"\n            - probabilities-{model_name} (float32): Individual model predictions\n              (only if keep_inputs=True)\n\n    Note:\n        Averaging method: Simple arithmetic mean across all models. For N models:\n        ensemble_prob = (prob_1 + prob_2 + ... + prob_N) / N\n\n        This approach assumes equal confidence in all models. Consider weighted averaging\n        if models have different validation performances.\n\n    Example:\n        Run ensemble with analysis of individual models:\n\n        ```python\n        result = ensemble.segment_tile(\n            tile=preprocessed_tile,\n            patch_size=1024,\n            overlap=16,\n            keep_inputs=True  # Keep individual predictions\n        )\n\n        # Compare ensemble vs individual models\n        import matplotlib.pyplot as plt\n        fig, axes = plt.subplots(1, 3)\n        result[\"probabilities\"].plot(ax=axes[0], title=\"Ensemble\")\n        result[\"probabilities-with_tcvis\"].plot(ax=axes[1], title=\"Model 1\")\n        result[\"probabilities-without_tcvis\"].plot(ax=axes[2], title=\"Model 2\")\n        ```\n\n    \"\"\"\n    probabilities = {}\n    for model_name, model in self.models.items():\n        probabilities[model_name] = model.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )[\"probabilities\"].copy()\n\n    # calculate the mean\n    tile[\"probabilities\"] = xr.concat(probabilities.values(), dim=\"model_probs\").mean(dim=\"model_probs\")\n\n    if keep_inputs:\n        for k, v in probabilities.items():\n            tile[f\"probabilities-{k}\"] = v\n\n    return tile\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    return [\n        self.segment_tile(\n            tile,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n        for tile in tiles\n    ]\n</code></pre>"},{"location":"reference/darts_export/","title":"darts_export","text":""},{"location":"reference/darts_export/#darts_export","title":"darts_export","text":"<p>Dataset export for the DARTS dataset.</p>"},{"location":"reference/darts_export/#darts_export.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_export/#darts_export.export_tile","title":"export_tile","text":"<pre><code>export_tile(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n    metadata: dict = {},\n    debug: bool = False,\n)\n</code></pre> <p>Export segmentation results to multiple file formats for analysis and distribution.</p> <p>This function exports a processed tile to an output directory, creating multiple file formats including GeoTIFFs, GeoPackages, Parquet files, and visualizations. It handles both ensemble-averaged results and individual model outputs.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Processed tile from prepare_export() containing segmentation results. Must include spatial reference information (CRS, coordinates).</p> </li> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory path. Created if it doesn't exist.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>List of data products to export. Options: - \"probabilities\": Probability maps as GeoTIFF (uint8, 0-100 scale, 255=nodata) - \"binarized\": Binary segmentation masks as GeoTIFF (uint8, 0/1) - \"polygonized\": Vectorized segmentations as GeoPackage and Parquet - \"extent\": Valid data extent as vector (GeoPackage and Parquet) - \"thumbnail\": RGB visualization as JPEG - \"optical\": Optical bands (red, green, blue, nir) as multi-band GeoTIFF - \"dem\": Terrain features (slope, relative_elevation) as multi-band GeoTIFF - \"tcvis\": TCVIS features (tc_brightness, tc_greenness, tc_wetness) as GeoTIFF - \"metadata\": Metadata JSON file - Any other variable name: Exported as single-band GeoTIFF Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>Names of individual ensemble models to export separately (e.g., [\"with_tcvis\", \"without_tcvis\"]). Creates suffixed files for each subset. Defaults to [].</p> </li> <li> <code>metadata</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Metadata dictionary to embed in raster tags and export as JSON. Automatically adds DARTS_exportdate timestamp. Defaults to {}.</p> </li> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, exports complete tile as NetCDF file (darts_inference_debug.nc) for debugging. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a specified band is not found in the tile's data variables.</p> </li> </ul> Note <p>Output files created (depending on <code>bands</code> parameter):</p> <p>Raster outputs (GeoTIFF with LZW compression): - probabilities.tif: Uint8 [0-100], 255=nodata - binarized.tif: Uint8 binary mask - optical.tif: Multi-band optical imagery - dem.tif: Multi-band terrain features - tcvis.tif: Multi-band TCVIS features - {custom_band}.tif: Single-band custom exports</p> <p>Vector outputs (GeoPackage + Parquet): - prediction_segments.gpkg/.parquet: Polygonized segmentation - prediction_extent.gpkg/.parquet: Valid data extent</p> <p>Visualization: - thumbnail.jpg: RGB composite with overlay</p> <p>Metadata: - darts_inference.json: Metadata dictionary</p> <p>Ensemble subsets: All raster and vector outputs get suffixed versions for each subset: - probabilities-{subset}.tif - binarized-{subset}.tif - prediction_segments-{subset}.gpkg/.parquet</p> Example <p>Standard export workflow:</p> <pre><code>from pathlib import Path\nfrom darts_export import export_tile\n\n# After prepare_export()\nexport_tile(\n    tile=processed_tile,\n    out_dir=Path(\"/output/scene_12345\"),\n    bands=[\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets=[\"with_tcvis\", \"without_tcvis\"],\n    metadata={\n        \"scene_id\": \"S2A_MSIL2A_20230615...\",\n        \"model_version\": \"v2.1\",\n        \"processing_date\": \"2023-06-20\"\n    },\n    debug=False\n)\n\n# Creates:\n# /output/scene_12345/\n#   \u251c\u2500\u2500 probabilities.tif\n#   \u251c\u2500\u2500 probabilities-with_tcvis.tif\n#   \u251c\u2500\u2500 probabilities-without_tcvis.tif\n#   \u251c\u2500\u2500 binarized.tif\n#   \u251c\u2500\u2500 binarized-with_tcvis.tif\n#   \u251c\u2500\u2500 binarized-without_tcvis.tif\n#   \u251c\u2500\u2500 prediction_segments.gpkg\n#   \u251c\u2500\u2500 prediction_segments.parquet\n#   \u251c\u2500\u2500 prediction_segments-with_tcvis.gpkg\n#   \u251c\u2500\u2500 prediction_segments-with_tcvis.parquet\n#   \u251c\u2500\u2500 prediction_segments-without_tcvis.gpkg\n#   \u251c\u2500\u2500 prediction_segments-without_tcvis.parquet\n#   \u251c\u2500\u2500 prediction_extent.gpkg\n#   \u251c\u2500\u2500 prediction_extent.parquet\n#   \u251c\u2500\u2500 thumbnail.jpg\n#   \u2514\u2500\u2500 darts_inference.json\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>@stopwatch.f(\"Exporting tile\", printer=logger.debug, print_kwargs=[\"bands\", \"ensemble_subsets\"])\ndef export_tile(  # noqa: C901\n    tile: xr.Dataset,\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n    metadata: dict = {},\n    debug: bool = False,\n):\n    \"\"\"Export segmentation results to multiple file formats for analysis and distribution.\n\n    This function exports a processed tile to an output directory, creating multiple file\n    formats including GeoTIFFs, GeoPackages, Parquet files, and visualizations. It handles\n    both ensemble-averaged results and individual model outputs.\n\n    Args:\n        tile (xr.Dataset): Processed tile from prepare_export() containing segmentation results.\n            Must include spatial reference information (CRS, coordinates).\n        out_dir (Path): Output directory path. Created if it doesn't exist.\n        bands (list[str], optional): List of data products to export. Options:\n            - \"probabilities\": Probability maps as GeoTIFF (uint8, 0-100 scale, 255=nodata)\n            - \"binarized\": Binary segmentation masks as GeoTIFF (uint8, 0/1)\n            - \"polygonized\": Vectorized segmentations as GeoPackage and Parquet\n            - \"extent\": Valid data extent as vector (GeoPackage and Parquet)\n            - \"thumbnail\": RGB visualization as JPEG\n            - \"optical\": Optical bands (red, green, blue, nir) as multi-band GeoTIFF\n            - \"dem\": Terrain features (slope, relative_elevation) as multi-band GeoTIFF\n            - \"tcvis\": TCVIS features (tc_brightness, tc_greenness, tc_wetness) as GeoTIFF\n            - \"metadata\": Metadata JSON file\n            - Any other variable name: Exported as single-band GeoTIFF\n            Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].\n        ensemble_subsets (list[str], optional): Names of individual ensemble models to export\n            separately (e.g., [\"with_tcvis\", \"without_tcvis\"]). Creates suffixed files for\n            each subset. Defaults to [].\n        metadata (dict, optional): Metadata dictionary to embed in raster tags and export as JSON.\n            Automatically adds DARTS_exportdate timestamp. Defaults to {}.\n        debug (bool, optional): If True, exports complete tile as NetCDF file\n            (darts_inference_debug.nc) for debugging. Defaults to False.\n\n    Raises:\n        ValueError: If a specified band is not found in the tile's data variables.\n\n    Note:\n        Output files created (depending on `bands` parameter):\n\n        Raster outputs (GeoTIFF with LZW compression):\n        - probabilities.tif: Uint8 [0-100], 255=nodata\n        - binarized.tif: Uint8 binary mask\n        - optical.tif: Multi-band optical imagery\n        - dem.tif: Multi-band terrain features\n        - tcvis.tif: Multi-band TCVIS features\n        - {custom_band}.tif: Single-band custom exports\n\n        Vector outputs (GeoPackage + Parquet):\n        - prediction_segments.gpkg/.parquet: Polygonized segmentation\n        - prediction_extent.gpkg/.parquet: Valid data extent\n\n        Visualization:\n        - thumbnail.jpg: RGB composite with overlay\n\n        Metadata:\n        - darts_inference.json: Metadata dictionary\n\n        Ensemble subsets:\n        All raster and vector outputs get suffixed versions for each subset:\n        - probabilities-{subset}.tif\n        - binarized-{subset}.tif\n        - prediction_segments-{subset}.gpkg/.parquet\n\n    Example:\n        Standard export workflow:\n\n        ```python\n        from pathlib import Path\n        from darts_export import export_tile\n\n        # After prepare_export()\n        export_tile(\n            tile=processed_tile,\n            out_dir=Path(\"/output/scene_12345\"),\n            bands=[\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n            ensemble_subsets=[\"with_tcvis\", \"without_tcvis\"],\n            metadata={\n                \"scene_id\": \"S2A_MSIL2A_20230615...\",\n                \"model_version\": \"v2.1\",\n                \"processing_date\": \"2023-06-20\"\n            },\n            debug=False\n        )\n\n        # Creates:\n        # /output/scene_12345/\n        #   \u251c\u2500\u2500 probabilities.tif\n        #   \u251c\u2500\u2500 probabilities-with_tcvis.tif\n        #   \u251c\u2500\u2500 probabilities-without_tcvis.tif\n        #   \u251c\u2500\u2500 binarized.tif\n        #   \u251c\u2500\u2500 binarized-with_tcvis.tif\n        #   \u251c\u2500\u2500 binarized-without_tcvis.tif\n        #   \u251c\u2500\u2500 prediction_segments.gpkg\n        #   \u251c\u2500\u2500 prediction_segments.parquet\n        #   \u251c\u2500\u2500 prediction_segments-with_tcvis.gpkg\n        #   \u251c\u2500\u2500 prediction_segments-with_tcvis.parquet\n        #   \u251c\u2500\u2500 prediction_segments-without_tcvis.gpkg\n        #   \u251c\u2500\u2500 prediction_segments-without_tcvis.parquet\n        #   \u251c\u2500\u2500 prediction_extent.gpkg\n        #   \u251c\u2500\u2500 prediction_extent.parquet\n        #   \u251c\u2500\u2500 thumbnail.jpg\n        #   \u2514\u2500\u2500 darts_inference.json\n        ```\n\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    if len(metadata) &gt; 0:\n        metadata[\"DARTS_exportdate\"] = str(datetime.now(UTC))\n\n    raster_tags = metadata\n\n    if debug:\n        manager.to_netcdf(tile, out_dir / \"darts_inference_debug.nc\", crop=False)\n\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                _export_polygonized(tile, out_dir, ensemble_subsets)\n            case \"binarized\":\n                _export_binarized(tile, out_dir, ensemble_subsets, tags=raster_tags)\n            case \"probabilities\":\n                _export_probabilities(tile, out_dir, ensemble_subsets, tags=raster_tags)\n            case \"extent\":\n                _export_vector(tile, \"extent\", out_dir, fname=\"prediction_extent\")\n            case \"thumbnail\":\n                _export_thumbnail(tile, out_dir)\n            case \"optical\":\n                _export_raster(tile, [\"red\", \"green\", \"blue\", \"nir\"], out_dir, fname=\"optical\", tags=raster_tags)\n            case \"dem\":\n                _export_raster(tile, [\"slope\", \"relative_elevation\"], out_dir, fname=\"dem\", tags=raster_tags)\n            case \"tcvis\":\n                _export_raster(\n                    tile, [\"tc_brightness\", \"tc_greenness\", \"tc_wetness\"], out_dir, fname=\"tcvis\", tags=raster_tags\n                )\n            case \"metadata\":\n                _export_metadata(out_dir, metadata)\n            case _:\n                if band not in tile.data_vars:\n                    raise ValueError(\n                        f\"Band {band} not found in tile for export. Available bands are: {list(tile.data_vars.keys())}\"\n                    )\n                # Export the band as a raster\n                _export_raster(tile, band, out_dir, tags=raster_tags)\n</code></pre>"},{"location":"reference/darts_export/#darts_export.missing_outputs","title":"missing_outputs","text":"<pre><code>missing_outputs(\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n) -&gt; typing.Literal[\"all\", \"some\", \"none\"]\n</code></pre> <p>Check for missing output files in the given directory.</p> <p>Parameters:</p> <ul> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to check for missing files.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['all', 'some', 'none']</code>           \u2013            <p>Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files: - \"none\": No files are missing. - \"some\": Some files are missing, which one will be logged to debug. - \"all\": All files are missing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the output path is not a directory.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/check.py</code> <pre><code>def missing_outputs(  # noqa: C901\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n) -&gt; Literal[\"all\", \"some\", \"none\"]:\n    \"\"\"Check for missing output files in the given directory.\n\n    Args:\n        out_dir (Path): The directory to check for missing files.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Returns:\n        Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files:\n            - \"none\": No files are missing.\n            - \"some\": Some files are missing, which one will be logged to debug.\n            - \"all\": All files are missing.\n\n    Raises:\n        ValueError: If the output path is not a directory.\n\n    \"\"\"\n    if not out_dir.exists():\n        return []\n    if not out_dir.is_dir():\n        raise ValueError(f\"Output path {out_dir} is not a directory.\")\n    expected_files = []\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                expected_files += [\"prediction_segments.gpkg\"] + [\n                    f\"prediction_segments-{es}.gpkg\" for es in ensemble_subsets\n                ]\n                expected_files += [\"prediction_segments.parquet\"] + [\n                    f\"prediction_segments-{es}.parquet\" for es in ensemble_subsets\n                ]\n            case \"binarized\":\n                expected_files += [\"binarized.tif\"] + [f\"binarized-{es}.tif\" for es in ensemble_subsets]\n            case \"probabilities\":\n                expected_files += [\"probabilities.tif\"] + [f\"probabilities-{es}.tif\" for es in ensemble_subsets]\n            case \"extent\":\n                expected_files += [\"extent.gpkg\", \"extent.parquet\"]\n            case \"thumbnail\":\n                expected_files += [\"thumbnail.jpg\"]\n            case _:\n                expected_files += [f\"{band}.tif\"]\n\n    missing_files = _missing_files(out_dir, expected_files)\n    if len(missing_files) == 0:\n        return \"none\"\n    elif len(missing_files) == len(expected_files):\n        return \"all\"\n    else:\n        logger.debug(\n            f\"Missing files in {out_dir}: {', '.join(missing_files)}. Expected files: {', '.join(expected_files)}.\"\n        )\n        return \"some\"\n</code></pre>"},{"location":"reference/darts_export/check/","title":"check","text":""},{"location":"reference/darts_export/check/#darts_export.check","title":"darts_export.check","text":"<p>Check if outputpath already contains files.</p>"},{"location":"reference/darts_export/check/#darts_export.check.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_export/check/#darts_export.check._missing_files","title":"_missing_files","text":"<pre><code>_missing_files(\n    output_dir: pathlib.Path, file_names: list[str]\n) -&gt; list[str]\n</code></pre> <p>Check if the given files exist in the output directory.</p> <p>Parameters:</p> <ul> <li> <code>output_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to check for files.</p> </li> <li> <code>file_names</code>               (<code>list[str]</code>)           \u2013            <p>The list of file names to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of missing file names.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/check.py</code> <pre><code>def _missing_files(output_dir: Path, file_names: list[str]) -&gt; list[str]:\n    \"\"\"Check if the given files exist in the output directory.\n\n    Args:\n        output_dir (Path): The directory to check for files.\n        file_names (list[str]): The list of file names to check.\n\n    Returns:\n        list[str]: A list of missing file names.\n\n    \"\"\"\n    missing_files = []\n    for file_name in file_names:\n        file_path = output_dir / file_name\n        if not file_path.exists():\n            missing_files.append(file_name)\n    return missing_files\n</code></pre>"},{"location":"reference/darts_export/check/#darts_export.check.missing_outputs","title":"missing_outputs","text":"<pre><code>missing_outputs(\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n) -&gt; typing.Literal[\"all\", \"some\", \"none\"]\n</code></pre> <p>Check for missing output files in the given directory.</p> <p>Parameters:</p> <ul> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to check for missing files.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['all', 'some', 'none']</code>           \u2013            <p>Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files: - \"none\": No files are missing. - \"some\": Some files are missing, which one will be logged to debug. - \"all\": All files are missing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the output path is not a directory.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/check.py</code> <pre><code>def missing_outputs(  # noqa: C901\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n) -&gt; Literal[\"all\", \"some\", \"none\"]:\n    \"\"\"Check for missing output files in the given directory.\n\n    Args:\n        out_dir (Path): The directory to check for missing files.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Returns:\n        Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files:\n            - \"none\": No files are missing.\n            - \"some\": Some files are missing, which one will be logged to debug.\n            - \"all\": All files are missing.\n\n    Raises:\n        ValueError: If the output path is not a directory.\n\n    \"\"\"\n    if not out_dir.exists():\n        return []\n    if not out_dir.is_dir():\n        raise ValueError(f\"Output path {out_dir} is not a directory.\")\n    expected_files = []\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                expected_files += [\"prediction_segments.gpkg\"] + [\n                    f\"prediction_segments-{es}.gpkg\" for es in ensemble_subsets\n                ]\n                expected_files += [\"prediction_segments.parquet\"] + [\n                    f\"prediction_segments-{es}.parquet\" for es in ensemble_subsets\n                ]\n            case \"binarized\":\n                expected_files += [\"binarized.tif\"] + [f\"binarized-{es}.tif\" for es in ensemble_subsets]\n            case \"probabilities\":\n                expected_files += [\"probabilities.tif\"] + [f\"probabilities-{es}.tif\" for es in ensemble_subsets]\n            case \"extent\":\n                expected_files += [\"extent.gpkg\", \"extent.parquet\"]\n            case \"thumbnail\":\n                expected_files += [\"thumbnail.jpg\"]\n            case _:\n                expected_files += [f\"{band}.tif\"]\n\n    missing_files = _missing_files(out_dir, expected_files)\n    if len(missing_files) == 0:\n        return \"none\"\n    elif len(missing_files) == len(expected_files):\n        return \"all\"\n    else:\n        logger.debug(\n            f\"Missing files in {out_dir}: {', '.join(missing_files)}. Expected files: {', '.join(expected_files)}.\"\n        )\n        return \"some\"\n</code></pre>"},{"location":"reference/darts_export/conversion/","title":"conversion","text":""},{"location":"reference/darts_export/conversion/#darts_export.conversion","title":"darts_export.conversion","text":"<p>Collection of conversion functions to translate data for export and processing.</p>"},{"location":"reference/darts_export/conversion/#darts_export.conversion.numpy_to_gdal","title":"numpy_to_gdal","text":"<pre><code>numpy_to_gdal(\n    nparray: numpy.ndarray,\n    rio_georef: xarray.DataArray | xarray.Dataset,\n) -&gt; osgeo.gdal.Dataset\n</code></pre> <p>Convert a numpy ndarray into a gdal Dataset.</p> <p>Georeference is to be passed in terms of an xarray object augmented by the rioxarray module, meaning the '.rio' accessor is available.</p> <p>Parameters:</p> <ul> <li> <code>nparray</code>               (<code>numpy.ndarray</code>)           \u2013            <p>The data to convert</p> </li> <li> <code>rio_georef</code>               (<code>xarray.DataArray | xarray.Dataset</code>)           \u2013            <p>an xarray with rio accessor as georeference</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>osgeo.gdal.Dataset</code>           \u2013            <p>gdal.Dataset: description</p> </li> </ul> Source code in <code>darts-export/src/darts_export/conversion.py</code> <pre><code>def numpy_to_gdal(nparray: np.ndarray, rio_georef: xarray.DataArray | xarray.Dataset) -&gt; gdal.Dataset:\n    \"\"\"Convert a numpy ndarray into a gdal Dataset.\n\n    Georeference is to be passed in terms\n    of an xarray object augmented by the rioxarray module, meaning the '.rio' accessor is\n    available.\n\n    Args:\n        nparray (np.ndarray): The data to convert\n        rio_georef (xarray.DataArray | xarray.Dataset): an xarray with rio accessor as georeference\n\n    Returns:\n        gdal.Dataset: _description_\n\n    \"\"\"\n    # convert the xarray to a gdal dataset\n    dta = gdal_array.OpenArray(nparray)\n\n    # copy over the geodata\n    # the transform object of rasterio has to be converted into a tuple\n    affine_transform = rio_georef.rio.transform()\n    geotransform = (\n        affine_transform.c,\n        affine_transform.a,\n        affine_transform.b,\n        affine_transform.f,\n        affine_transform.d,\n        affine_transform.e,\n    )\n    dta.SetGeoTransform(geotransform)\n    dta.SetProjection(rio_georef.rio.crs.to_wkt())\n    return dta\n</code></pre>"},{"location":"reference/darts_export/conversion/#darts_export.conversion.ogrlyr_to_geopandas","title":"ogrlyr_to_geopandas","text":"<pre><code>ogrlyr_to_geopandas(\n    ogr_layer: osgeo.ogr.Layer,\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Convert a GDAL/OGR layer object to a geopandas dataframe.</p> <p>Parameters:</p> <ul> <li> <code>ogr_layer</code>               (<code>osgeo.ogr.Layer</code>)           \u2013            <p>the ogr layer object to convert</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>geopandas.GeoDataFrame</code>           \u2013            <p>gpd.GeoDataFrame: the resulting GeoDataFrame</p> </li> </ul> Source code in <code>darts-export/src/darts_export/conversion.py</code> <pre><code>def ogrlyr_to_geopandas(ogr_layer: ogr.Layer) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert a GDAL/OGR layer object to a geopandas dataframe.\n\n    Args:\n        ogr_layer (ogr.Layer): the ogr layer object to convert\n\n    Returns:\n        gpd.GeoDataFrame: the resulting GeoDataFrame\n\n    \"\"\"\n    # Initialize an empty list to store geometries and attributes\n    features = []\n\n    # Iterate over OGR features in the layer\n    for feature in ogr_layer:\n        geom = feature.GetGeometryRef()\n        # geom_json = geom.ExportToJson()  # Convert to GeoJSON format\n        geom_wkt = geom.ExportToWkt()  # Convert to Well-Known Text (WKT)\n        attributes = feature.items()  # Get attribute data as a dictionary\n\n        # Append a tuple (geometry, attributes)\n        features.append((geom_wkt, attributes))\n\n    # Create a GeoDataFrame from the geometries and attributes\n    gdf = gpd.GeoDataFrame(\n        [attr for geom, attr in features], geometry=gpd.GeoSeries.from_wkt([geom for geom, attr in features])\n    )\n\n    return gdf\n</code></pre>"},{"location":"reference/darts_export/conversion/#darts_export.conversion.rioxarrayds_to_gdal","title":"rioxarrayds_to_gdal","text":"<pre><code>rioxarrayds_to_gdal(\n    rix: xarray.DataArray,\n) -&gt; osgeo.gdal.Dataset\n</code></pre> <p>Convert a rioxarray object to a gdal dataset.</p> <p>Parameters:</p> <ul> <li> <code>rix</code>               (<code>xarray.DataArray</code>)           \u2013            <p>data to convert</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>osgeo.gdal.Dataset</code>           \u2013            <p>gdal.Dataset: the converted data</p> </li> </ul> Source code in <code>darts-export/src/darts_export/conversion.py</code> <pre><code>def rioxarrayds_to_gdal(rix: xarray.DataArray) -&gt; gdal.Dataset:\n    \"\"\"Convert a rioxarray object to a gdal dataset.\n\n    Args:\n        rix (xarray.DataArray): data to convert\n\n    Returns:\n        gdal.Dataset: the converted data\n\n    \"\"\"\n    return numpy_to_gdal(rix.to_numpy(), rix)\n</code></pre>"},{"location":"reference/darts_export/export/","title":"export","text":""},{"location":"reference/darts_export/export/#darts_export.export","title":"darts_export.export","text":"<p>Darts export module for inference results.</p>"},{"location":"reference/darts_export/export/#darts_export.export.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_binarized","title":"_export_binarized","text":"<pre><code>_export_binarized(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    ensemble_subsets: list[str] = [],\n    tags={},\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_binarized(tile: xr.Dataset, out_dir: Path, ensemble_subsets: list[str] = [], tags={}):\n    _export_raster(tile, \"binarized_segmentation\", out_dir, fname=\"binarized\")\n    for ensemble_subset in ensemble_subsets:\n        _export_raster(\n            tile,\n            f\"binarized_segmentation-{ensemble_subset}\",\n            out_dir,\n            fname=f\"binarized-{ensemble_subset}\",\n            tags=tags,\n        )\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_metadata","title":"_export_metadata","text":"<pre><code>_export_metadata(out_dir: pathlib.Path, metadata: dict)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_metadata(out_dir: Path, metadata: dict):\n    with (out_dir / \"darts_inference.json\").open(\"w\") as fp:\n        json.dump(metadata, fp, indent=2)\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_polygonized","title":"_export_polygonized","text":"<pre><code>_export_polygonized(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    ensemble_subsets: list[str] = [],\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_polygonized(tile: xr.Dataset, out_dir: Path, ensemble_subsets: list[str] = []):\n    _export_vector(tile, \"binarized_segmentation\", out_dir, fname=\"prediction_segments\")\n    for ensemble_subset in ensemble_subsets:\n        _export_vector(\n            tile,\n            f\"binarized_segmentation-{ensemble_subset}\",\n            out_dir,\n            fname=f\"prediction_segments-{ensemble_subset}\",\n        )\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_probabilities","title":"_export_probabilities","text":"<pre><code>_export_probabilities(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    ensemble_subsets: list[str] = [],\n    tags={},\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_probabilities(tile: xr.Dataset, out_dir: Path, ensemble_subsets: list[str] = [], tags={}):\n    tile[\"probabilities\"] = (tile[\"probabilities\"] * 100).fillna(255).astype(\"uint8\").rio.write_nodata(255)\n    _export_raster(tile, \"probabilities\", out_dir, fname=\"probabilities\", tags=tags)\n    for ensemble_subset in ensemble_subsets:\n        tile[f\"probabilities-{ensemble_subset}\"] = (\n            (tile[f\"probabilities-{ensemble_subset}\"] * 100).fillna(255).astype(\"uint8\").rio.write_nodata(255)\n        )\n        _export_raster(\n            tile,\n            f\"probabilities-{ensemble_subset}\",\n            out_dir,\n            fname=f\"probabilities-{ensemble_subset}\",\n            tags=tags,\n        )\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_raster","title":"_export_raster","text":"<pre><code>_export_raster(\n    tile: xarray.Dataset,\n    name: str,\n    out_dir: pathlib.Path,\n    fname: str | None = None,\n    tags={},\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_raster(tile: xr.Dataset, name: str, out_dir: Path, fname: str | None = None, tags={}):\n    if fname is None:\n        fname = name\n    fpath = out_dir / f\"{fname}.tif\"\n    with stopwatch(f\"Exporting {name} to {fpath.resolve()}\", printer=logger.debug):\n        if tile[name].dtype == \"bool\":\n            tile[name].astype(\"uint8\").rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\", tags=tags)\n        else:\n            tile[name].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\", tags=tags)\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_thumbnail","title":"_export_thumbnail","text":"<pre><code>_export_thumbnail(\n    tile: xarray.Dataset, out_dir: pathlib.Path\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_thumbnail(tile: xr.Dataset, out_dir: Path):\n    fpath = out_dir / \"thumbnail.jpg\"\n    with stopwatch(f\"Exporting thumbnail to {fpath}\", printer=logger.debug):\n        fig = miniviz.thumbnail(tile)\n        fig.savefig(fpath)\n        fig.clear()\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_vector","title":"_export_vector","text":"<pre><code>_export_vector(\n    tile: xarray.Dataset,\n    name: str,\n    out_dir: pathlib.Path,\n    fname: str | None = None,\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_vector(tile: xr.Dataset, name: str, out_dir: Path, fname: str | None = None):\n    if fname is None:\n        fname = name\n    fpath_gpkg = out_dir / f\"{fname}.gpkg\"\n    fpath_parquet = out_dir / f\"{fname}.parquet\"\n    with stopwatch(f\"Exporting {name} to {fpath_gpkg.resolve()} and {fpath_parquet.resolve()}\", printer=logger.debug):\n        polygon_gdf = vectorization.vectorize(tile, name)\n        polygon_gdf.to_file(fpath_gpkg, layer=f\"{fname}\")\n        polygon_gdf.to_parquet(fpath_parquet)\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export.export_tile","title":"export_tile","text":"<pre><code>export_tile(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n    metadata: dict = {},\n    debug: bool = False,\n)\n</code></pre> <p>Export segmentation results to multiple file formats for analysis and distribution.</p> <p>This function exports a processed tile to an output directory, creating multiple file formats including GeoTIFFs, GeoPackages, Parquet files, and visualizations. It handles both ensemble-averaged results and individual model outputs.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Processed tile from prepare_export() containing segmentation results. Must include spatial reference information (CRS, coordinates).</p> </li> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory path. Created if it doesn't exist.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>List of data products to export. Options: - \"probabilities\": Probability maps as GeoTIFF (uint8, 0-100 scale, 255=nodata) - \"binarized\": Binary segmentation masks as GeoTIFF (uint8, 0/1) - \"polygonized\": Vectorized segmentations as GeoPackage and Parquet - \"extent\": Valid data extent as vector (GeoPackage and Parquet) - \"thumbnail\": RGB visualization as JPEG - \"optical\": Optical bands (red, green, blue, nir) as multi-band GeoTIFF - \"dem\": Terrain features (slope, relative_elevation) as multi-band GeoTIFF - \"tcvis\": TCVIS features (tc_brightness, tc_greenness, tc_wetness) as GeoTIFF - \"metadata\": Metadata JSON file - Any other variable name: Exported as single-band GeoTIFF Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>Names of individual ensemble models to export separately (e.g., [\"with_tcvis\", \"without_tcvis\"]). Creates suffixed files for each subset. Defaults to [].</p> </li> <li> <code>metadata</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Metadata dictionary to embed in raster tags and export as JSON. Automatically adds DARTS_exportdate timestamp. Defaults to {}.</p> </li> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, exports complete tile as NetCDF file (darts_inference_debug.nc) for debugging. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a specified band is not found in the tile's data variables.</p> </li> </ul> Note <p>Output files created (depending on <code>bands</code> parameter):</p> <p>Raster outputs (GeoTIFF with LZW compression): - probabilities.tif: Uint8 [0-100], 255=nodata - binarized.tif: Uint8 binary mask - optical.tif: Multi-band optical imagery - dem.tif: Multi-band terrain features - tcvis.tif: Multi-band TCVIS features - {custom_band}.tif: Single-band custom exports</p> <p>Vector outputs (GeoPackage + Parquet): - prediction_segments.gpkg/.parquet: Polygonized segmentation - prediction_extent.gpkg/.parquet: Valid data extent</p> <p>Visualization: - thumbnail.jpg: RGB composite with overlay</p> <p>Metadata: - darts_inference.json: Metadata dictionary</p> <p>Ensemble subsets: All raster and vector outputs get suffixed versions for each subset: - probabilities-{subset}.tif - binarized-{subset}.tif - prediction_segments-{subset}.gpkg/.parquet</p> Example <p>Standard export workflow:</p> <pre><code>from pathlib import Path\nfrom darts_export import export_tile\n\n# After prepare_export()\nexport_tile(\n    tile=processed_tile,\n    out_dir=Path(\"/output/scene_12345\"),\n    bands=[\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets=[\"with_tcvis\", \"without_tcvis\"],\n    metadata={\n        \"scene_id\": \"S2A_MSIL2A_20230615...\",\n        \"model_version\": \"v2.1\",\n        \"processing_date\": \"2023-06-20\"\n    },\n    debug=False\n)\n\n# Creates:\n# /output/scene_12345/\n#   \u251c\u2500\u2500 probabilities.tif\n#   \u251c\u2500\u2500 probabilities-with_tcvis.tif\n#   \u251c\u2500\u2500 probabilities-without_tcvis.tif\n#   \u251c\u2500\u2500 binarized.tif\n#   \u251c\u2500\u2500 binarized-with_tcvis.tif\n#   \u251c\u2500\u2500 binarized-without_tcvis.tif\n#   \u251c\u2500\u2500 prediction_segments.gpkg\n#   \u251c\u2500\u2500 prediction_segments.parquet\n#   \u251c\u2500\u2500 prediction_segments-with_tcvis.gpkg\n#   \u251c\u2500\u2500 prediction_segments-with_tcvis.parquet\n#   \u251c\u2500\u2500 prediction_segments-without_tcvis.gpkg\n#   \u251c\u2500\u2500 prediction_segments-without_tcvis.parquet\n#   \u251c\u2500\u2500 prediction_extent.gpkg\n#   \u251c\u2500\u2500 prediction_extent.parquet\n#   \u251c\u2500\u2500 thumbnail.jpg\n#   \u2514\u2500\u2500 darts_inference.json\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>@stopwatch.f(\"Exporting tile\", printer=logger.debug, print_kwargs=[\"bands\", \"ensemble_subsets\"])\ndef export_tile(  # noqa: C901\n    tile: xr.Dataset,\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n    metadata: dict = {},\n    debug: bool = False,\n):\n    \"\"\"Export segmentation results to multiple file formats for analysis and distribution.\n\n    This function exports a processed tile to an output directory, creating multiple file\n    formats including GeoTIFFs, GeoPackages, Parquet files, and visualizations. It handles\n    both ensemble-averaged results and individual model outputs.\n\n    Args:\n        tile (xr.Dataset): Processed tile from prepare_export() containing segmentation results.\n            Must include spatial reference information (CRS, coordinates).\n        out_dir (Path): Output directory path. Created if it doesn't exist.\n        bands (list[str], optional): List of data products to export. Options:\n            - \"probabilities\": Probability maps as GeoTIFF (uint8, 0-100 scale, 255=nodata)\n            - \"binarized\": Binary segmentation masks as GeoTIFF (uint8, 0/1)\n            - \"polygonized\": Vectorized segmentations as GeoPackage and Parquet\n            - \"extent\": Valid data extent as vector (GeoPackage and Parquet)\n            - \"thumbnail\": RGB visualization as JPEG\n            - \"optical\": Optical bands (red, green, blue, nir) as multi-band GeoTIFF\n            - \"dem\": Terrain features (slope, relative_elevation) as multi-band GeoTIFF\n            - \"tcvis\": TCVIS features (tc_brightness, tc_greenness, tc_wetness) as GeoTIFF\n            - \"metadata\": Metadata JSON file\n            - Any other variable name: Exported as single-band GeoTIFF\n            Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].\n        ensemble_subsets (list[str], optional): Names of individual ensemble models to export\n            separately (e.g., [\"with_tcvis\", \"without_tcvis\"]). Creates suffixed files for\n            each subset. Defaults to [].\n        metadata (dict, optional): Metadata dictionary to embed in raster tags and export as JSON.\n            Automatically adds DARTS_exportdate timestamp. Defaults to {}.\n        debug (bool, optional): If True, exports complete tile as NetCDF file\n            (darts_inference_debug.nc) for debugging. Defaults to False.\n\n    Raises:\n        ValueError: If a specified band is not found in the tile's data variables.\n\n    Note:\n        Output files created (depending on `bands` parameter):\n\n        Raster outputs (GeoTIFF with LZW compression):\n        - probabilities.tif: Uint8 [0-100], 255=nodata\n        - binarized.tif: Uint8 binary mask\n        - optical.tif: Multi-band optical imagery\n        - dem.tif: Multi-band terrain features\n        - tcvis.tif: Multi-band TCVIS features\n        - {custom_band}.tif: Single-band custom exports\n\n        Vector outputs (GeoPackage + Parquet):\n        - prediction_segments.gpkg/.parquet: Polygonized segmentation\n        - prediction_extent.gpkg/.parquet: Valid data extent\n\n        Visualization:\n        - thumbnail.jpg: RGB composite with overlay\n\n        Metadata:\n        - darts_inference.json: Metadata dictionary\n\n        Ensemble subsets:\n        All raster and vector outputs get suffixed versions for each subset:\n        - probabilities-{subset}.tif\n        - binarized-{subset}.tif\n        - prediction_segments-{subset}.gpkg/.parquet\n\n    Example:\n        Standard export workflow:\n\n        ```python\n        from pathlib import Path\n        from darts_export import export_tile\n\n        # After prepare_export()\n        export_tile(\n            tile=processed_tile,\n            out_dir=Path(\"/output/scene_12345\"),\n            bands=[\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n            ensemble_subsets=[\"with_tcvis\", \"without_tcvis\"],\n            metadata={\n                \"scene_id\": \"S2A_MSIL2A_20230615...\",\n                \"model_version\": \"v2.1\",\n                \"processing_date\": \"2023-06-20\"\n            },\n            debug=False\n        )\n\n        # Creates:\n        # /output/scene_12345/\n        #   \u251c\u2500\u2500 probabilities.tif\n        #   \u251c\u2500\u2500 probabilities-with_tcvis.tif\n        #   \u251c\u2500\u2500 probabilities-without_tcvis.tif\n        #   \u251c\u2500\u2500 binarized.tif\n        #   \u251c\u2500\u2500 binarized-with_tcvis.tif\n        #   \u251c\u2500\u2500 binarized-without_tcvis.tif\n        #   \u251c\u2500\u2500 prediction_segments.gpkg\n        #   \u251c\u2500\u2500 prediction_segments.parquet\n        #   \u251c\u2500\u2500 prediction_segments-with_tcvis.gpkg\n        #   \u251c\u2500\u2500 prediction_segments-with_tcvis.parquet\n        #   \u251c\u2500\u2500 prediction_segments-without_tcvis.gpkg\n        #   \u251c\u2500\u2500 prediction_segments-without_tcvis.parquet\n        #   \u251c\u2500\u2500 prediction_extent.gpkg\n        #   \u251c\u2500\u2500 prediction_extent.parquet\n        #   \u251c\u2500\u2500 thumbnail.jpg\n        #   \u2514\u2500\u2500 darts_inference.json\n        ```\n\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    if len(metadata) &gt; 0:\n        metadata[\"DARTS_exportdate\"] = str(datetime.now(UTC))\n\n    raster_tags = metadata\n\n    if debug:\n        manager.to_netcdf(tile, out_dir / \"darts_inference_debug.nc\", crop=False)\n\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                _export_polygonized(tile, out_dir, ensemble_subsets)\n            case \"binarized\":\n                _export_binarized(tile, out_dir, ensemble_subsets, tags=raster_tags)\n            case \"probabilities\":\n                _export_probabilities(tile, out_dir, ensemble_subsets, tags=raster_tags)\n            case \"extent\":\n                _export_vector(tile, \"extent\", out_dir, fname=\"prediction_extent\")\n            case \"thumbnail\":\n                _export_thumbnail(tile, out_dir)\n            case \"optical\":\n                _export_raster(tile, [\"red\", \"green\", \"blue\", \"nir\"], out_dir, fname=\"optical\", tags=raster_tags)\n            case \"dem\":\n                _export_raster(tile, [\"slope\", \"relative_elevation\"], out_dir, fname=\"dem\", tags=raster_tags)\n            case \"tcvis\":\n                _export_raster(\n                    tile, [\"tc_brightness\", \"tc_greenness\", \"tc_wetness\"], out_dir, fname=\"tcvis\", tags=raster_tags\n                )\n            case \"metadata\":\n                _export_metadata(out_dir, metadata)\n            case _:\n                if band not in tile.data_vars:\n                    raise ValueError(\n                        f\"Band {band} not found in tile for export. Available bands are: {list(tile.data_vars.keys())}\"\n                    )\n                # Export the band as a raster\n                _export_raster(tile, band, out_dir, tags=raster_tags)\n</code></pre>"},{"location":"reference/darts_export/miniviz/","title":"miniviz","text":""},{"location":"reference/darts_export/miniviz/#darts_export.miniviz","title":"darts_export.miniviz","text":"<p>Small visuals previews for the output.</p>"},{"location":"reference/darts_export/miniviz/#darts_export.miniviz.thumbnail","title":"thumbnail","text":"<pre><code>thumbnail(tile: xarray.Dataset) -&gt; matplotlib.pyplot.Figure\n</code></pre> <p>Create a thumbnail of the tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The tile to create a thumbnail from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>matplotlib.pyplot.Figure</code>           \u2013            <p>plt.Figure: The figure with the thumbnail.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/miniviz.py</code> <pre><code>def thumbnail(tile: xr.Dataset) -&gt; plt.Figure:\n    \"\"\"Create a thumbnail of the tile.\n\n    Args:\n        tile (xr.Dataset): The tile to create a thumbnail from.\n\n    Returns:\n        plt.Figure: The figure with the thumbnail.\n\n    \"\"\"\n    prev_res = 512  # Prefered resolution for the thumbnail, will not exactly match\n    orig_res = max(tile.sizes.values())\n    if orig_res &gt; prev_res:\n        factor = int(orig_res / prev_res)\n        tile = tile.odc.reproject(tile.odc.geobox.zoom_out(factor))\n\n    tile_id = tile.attrs.get(\"s2_id\", \"unknown\")\n\n    # Add some statistics\n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    ax.set_title(f\"Tile {tile_id} (lowres) [epsg:{tile.odc.crs.epsg}]\")\n    rgba = tile.odc.to_rgba(bands=[\"red\", \"green\", \"blue\"], vmin=0, vmax=0.2)\n    rgba.plot.imshow(ax=ax)\n\n    # Prediction boundaries\n    tile.probabilities.plot.contour(ax=ax, levels=[0.5])\n    # Validity mask\n    tile.extent.plot.contour(ax=ax, levels=[0.5], colors=\"r\", alpha=0.5)\n    return fig\n</code></pre>"},{"location":"reference/darts_export/vectorization/","title":"vectorization","text":""},{"location":"reference/darts_export/vectorization/#darts_export.vectorization","title":"darts_export.vectorization","text":"<p>Module for various tasks during export.</p>"},{"location":"reference/darts_export/vectorization/#darts_export.vectorization.gdal_polygonization","title":"gdal_polygonization","text":"<pre><code>gdal_polygonization(\n    labels: numpy.ndarray,\n    rio_georef: xarray.Dataset,\n    as_gdf=True,\n    gpkg_path: pathlib.Path | None = None,\n) -&gt; typing.Union[gdal.ogr.Layer, geopandas.GeoDataFrame]\n</code></pre> <p>Polygonize a numpy array using GDAL.</p> <p>Detects regions with the same value in the numpy array and converts those into polygons. Can return the initial gdal result as ogr.Dataset or converts into a geopandas dataframe if as_gdf is enabled. If <code>gpkg_path</code> is set, the polsgonization result will be written one go into a GeoPackage file, otherwise the OGR dataset will reside purely in memory.</p> <p>Parameters:</p> <ul> <li> <code>labels</code>               (<code>numpy.ndarray</code>)           \u2013            <p>The input dataset as a ndarray with values designating labels</p> </li> <li> <code>rio_georef</code>               (<code>xarray.Dataset</code>)           \u2013            <p>an xarray with the rioxarray accessor to determine the CRS of the dataset</p> </li> <li> <code>as_gdf</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>returns result as geopandas.GeoDataFrame. Defaults to True.</p> </li> <li> <code>gpkg_path</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path where a GPKG file is written backing the OGR dataset. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Union[gdal.ogr.Layer, geopandas.GeoDataFrame]</code>           \u2013            <p>ogr.Layer | gpd.GeoDataFrame: the polyginization result</p> </li> </ul> Source code in <code>darts-export/src/darts_export/vectorization.py</code> <pre><code>def gdal_polygonization(\n    labels: np.ndarray, rio_georef: xarray.Dataset, as_gdf=True, gpkg_path: Path | None = None\n) -&gt; Union[\"gdal.ogr.Layer\", gpd.GeoDataFrame]:  # type: ignore # noqa: F821\n    \"\"\"Polygonize a numpy array using GDAL.\n\n    Detects regions with the same value in the numpy array and converts those\n    into polygons. Can return the initial gdal result as ogr.Dataset or converts into a geopandas dataframe if\n    as_gdf is enabled. If `gpkg_path` is set, the polsgonization result will be written one go\n    into a GeoPackage file, otherwise the OGR dataset will reside purely in memory.\n\n    Args:\n        labels (np.ndarray): The input dataset as a ndarray with values designating labels\n        rio_georef (xarray.Dataset): an xarray with the rioxarray accessor to determine the CRS of the dataset\n        as_gdf (bool, optional): returns result as geopandas.GeoDataFrame. Defaults to True.\n        gpkg_path (Path | None, optional): Path where a GPKG file is written backing the OGR dataset. Defaults to None.\n\n    Returns:\n        ogr.Layer | gpd.GeoDataFrame: the polyginization result\n\n    \"\"\"\n    from osgeo import gdal, ogr\n\n    from darts_export import conversion\n\n    gdal.UseExceptions()\n\n    # convert to a GDAL dataset\n    dta = conversion.numpy_to_gdal(labels, rio_georef)\n\n    # prepare the vector output datasets to write to\n    if gpkg_path is not None:\n        gpkg_path = Path(gpkg_path)\n        ds = ogr.GetDriverByName(\"GPKG\").CreateDataSource(gpkg_path)\n        ogr_layer = ds.CreateLayer(gpkg_path.stem, geom_type=ogr.wkbPolygon, srs=dta.GetSpatialRef())\n    else:\n        # work only in memory\n        ds = ogr.GetDriverByName(\"Memory\").CreateDataSource(\"gdal_polygonization\")\n        ogr_layer = ds.CreateLayer(\"gdal_polygonization\", geom_type=ogr.wkbPolygon, srs=dta.GetSpatialRef())\n\n    # add the field where to store region ID\n    field = ogr.FieldDefn(\"Region_ID\", ogr.OFTInteger)\n    ogr_layer.CreateField(field)\n\n    # do the polygonization\n    gdal.Polygonize(\n        dta.GetRasterBand(1),\n        None,  # no masking, polygonize everything\n        ogr_layer,  # where to write the vector data to\n        0,  # write the polygonization threshold in the first attribute (\"DN\")\n    )\n    # the region with the ID zero is the region unlabelled by measure label\n    # remove features polygonized from that region, that is all features where DN is not 1\n    ogr_layer.SetAttributeFilter(\"Region_ID = 0\")\n    for feature in ogr_layer:\n        feature_id = feature.GetFID()\n        ogr_layer.DeleteFeature(feature_id)\n    ogr_layer.SetAttributeFilter(None)\n\n    if not as_gdf:\n        return ogr_layer\n\n    # convert the gdal vector object zo a geopandas gdf\n    gdf_polygons = conversion.ogrlyr_to_geopandas(ogr_layer)\n    gdf_polygons.set_crs(rio_georef.rio.crs, inplace=True)\n    return gdf_polygons\n</code></pre>"},{"location":"reference/darts_export/vectorization/#darts_export.vectorization.rasterio_polygonization","title":"rasterio_polygonization","text":"<pre><code>rasterio_polygonization(\n    labels: numpy.ndarray, rio_georef: xarray.Dataset\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Polygonize a numpy array with rasterio.</p> <p>Detects regions with the same value in the numpy array and converts those into polygons. The <code>rio_georef</code> agrument determines the final CRS of the returned geopandas GeoDataFrame.</p> <p>Parameters:</p> <ul> <li> <code>labels</code>               (<code>numpy.ndarray</code>)           \u2013            <p>the array of regionalizable labels</p> </li> <li> <code>rio_georef</code>               (<code>xarray.Dataset</code>)           \u2013            <p>the CRS as an xarray/rioxarray Dataset with rio accessor</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>geopandas.GeoDataFrame</code>           \u2013            <p>geopandas.GeoDataFrame: the resolut of the polygonization</p> </li> </ul> Source code in <code>darts-export/src/darts_export/vectorization.py</code> <pre><code>def rasterio_polygonization(labels: np.ndarray, rio_georef: xarray.Dataset) -&gt; gpd.GeoDataFrame:\n    \"\"\"Polygonize a numpy array with rasterio.\n\n    Detects regions with the same value in the numpy array and converts those\n    into polygons. The `rio_georef` agrument determines the final CRS of\n    the returned geopandas GeoDataFrame.\n\n    Args:\n        labels (np.ndarray): the array of regionalizable labels\n        rio_georef (xarray.Dataset): the CRS as an xarray/rioxarray Dataset with rio accessor\n\n    Returns:\n        geopandas.GeoDataFrame: the resolut of the polygonization\n\n    \"\"\"\n    # shapes() needs int32 data, while scikit labels puts out int64\n    # cast with astype()\n    gdf = (\n        gpd.GeoDataFrame(\n            [\n                (shapely.geometry.shape(geom), int(region_Id))\n                for geom, region_Id in shapes(labels.astype(np.int32), transform=rio_georef.rio.transform())\n            ],\n            columns=[\"geometry\", \"Region_ID\"],\n        )\n        .set_crs(rio_georef.rio.crs)\n        .query(\"Region_ID &gt; 0\")\n    )\n    return gdf\n</code></pre>"},{"location":"reference/darts_export/vectorization/#darts_export.vectorization.vectorize","title":"vectorize","text":"<pre><code>vectorize(\n    xdat: xarray.Dataset,\n    layername: str = \"binarized_segmentation\",\n    polygonization_func: str = \"rasterio\",\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Vectorize an inference result dataset.</p> <p>Detects connected regions in the with the same value <code>binarized_segmentation</code> layer, polygonizes this into a vector dataset. Additionally this function writes zonal statistics of the <code>probabilities</code> layer to the polygon attributes.</p> <p>Parameters:</p> <ul> <li> <code>xdat</code>               (<code>xarray.Dataset</code>)           \u2013            <p>the input dataset augmented with the rioxarray <code>rio</code> accessor</p> </li> <li> <code>layername</code>               (<code>str</code>, default:                   <code>'binarized_segmentation'</code> )           \u2013            <p>the name of the layer in <code>xdat</code> to polygonize</p> </li> <li> <code>polygonization_func</code>               (<code>str</code>, default:                   <code>'rasterio'</code> )           \u2013            <p>the method to utilize for polygonization, either 'gdal' or 'rasterio', the default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_type_</code> (              <code>geopandas.GeoDataFrame</code> )          \u2013            <p>description</p> </li> </ul> Source code in <code>darts-export/src/darts_export/vectorization.py</code> <pre><code>def vectorize(\n    xdat: xarray.Dataset,\n    layername: str = \"binarized_segmentation\",\n    polygonization_func: str = \"rasterio\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Vectorize an inference result dataset.\n\n    Detects connected regions in the with the same value `binarized_segmentation` layer, polygonizes\n    this into a vector dataset.\n    Additionally this function writes zonal statistics of the `probabilities` layer to the polygon attributes.\n\n    Args:\n        xdat (xarray.Dataset): the input dataset augmented with the rioxarray `rio` accessor\n        layername (str, optional): the name of the layer in `xdat` to polygonize\n        polygonization_func (str, optional): the method to utilize for polygonization, either 'gdal' or 'rasterio',\n            the default.\n\n    Returns:\n        _type_: _description_\n\n    \"\"\"\n    layer = xdat[layername]\n\n    # Turn layer into int8 if bool\n    if layer.dtype == \"bool\":\n        layer = layer.astype(\"uint8\")\n\n    bin_labelled = measure.label(layer)\n\n    if polygonization_func.lower() == \"gdal\":\n        gdf_polygons = gdal_polygonization(bin_labelled, layer)\n    else:\n        gdf_polygons = rasterio_polygonization(bin_labelled, layer)\n\n    # execute the zonal stats:\n    # arguments must be in the specified order, matching regionprops\n    def median_intensity(region, intensities):\n        # note the ddof arg to get the sample var if you so desire!\n        return np.median(intensities[region])\n\n    region_stats = measure.regionprops(\n        bin_labelled, intensity_image=xdat.probabilities.values, extra_properties=[median_intensity]\n    )\n\n    # collect stats data:\n    stats_dict = {}\n    for region in region_stats:\n        stats_dict[region.label] = {\n            \"min\": int(region.min_intensity),\n            \"max\": int(region.max_intensity),\n            \"mean\": region.mean_intensity,\n            \"median\": region.median_intensity,\n            \"std\": region.intensity_std,\n            \"npixel\": region.num_pixels,\n        }\n\n    # add the zonal stats to the GeoPandas DataFrame\n    stats_df = gpd.pd.DataFrame.from_dict(stats_dict, orient=\"index\")\n    return gdf_polygons.merge(stats_df, left_on=\"Region_ID\", right_index=True)\n</code></pre>"},{"location":"reference/darts_preprocessing/","title":"darts_preprocessing","text":""},{"location":"reference/darts_preprocessing/#darts_preprocessing","title":"darts_preprocessing","text":"<p>Data preprocessing and feature engineering for the DARTS dataset.</p>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_aspect","title":"calculate_aspect","text":"<pre><code>calculate_aspect(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate aspect (compass direction) of the terrain surface from an ArcticDEM Dataset.</p> <p>Aspect indicates the downslope direction of the maximum rate of change in elevation.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>aspect (float32): Aspect in degrees clockwise from north [0-360], or -1 for flat areas.</p> </li> <li> <p>long_name: \"Aspect\"</p> </li> <li>units: \"degrees\"</li> <li>description: Compass direction of slope</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Aspect values:</p> <ul> <li>0\u00b0 or 360\u00b0: North-facing</li> <li>90\u00b0: East-facing</li> <li>180\u00b0: South-facing</li> <li>270\u00b0: West-facing</li> <li>-1: Flat (no dominant direction)</li> </ul> Example <pre><code>from darts_preprocessing import calculate_aspect\n\narcticdem_with_aspect = calculate_aspect(arcticdem_ds)\n\n# Identify south-facing slopes (135-225 degrees)\nsouth_facing = (arcticdem_with_aspect.aspect &gt; 135) &amp; (arcticdem_with_aspect.aspect &lt; 225)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating aspect\", printer=logger.debug)\ndef calculate_aspect(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate aspect (compass direction) of the terrain surface from an ArcticDEM Dataset.\n\n    Aspect indicates the downslope direction of the maximum rate of change in elevation.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - aspect (float32): Aspect in degrees clockwise from north [0-360], or -1 for flat areas.\n\n            - long_name: \"Aspect\"\n            - units: \"degrees\"\n            - description: Compass direction of slope\n            - source: \"ArcticDEM\"\n\n    Note:\n        Aspect values:\n\n        - 0\u00b0 or 360\u00b0: North-facing\n        - 90\u00b0: East-facing\n        - 180\u00b0: South-facing\n        - 270\u00b0: West-facing\n        - -1: Flat (no dominant direction)\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_aspect\n\n        arcticdem_with_aspect = calculate_aspect(arcticdem_ds)\n\n        # Identify south-facing slopes (135-225 degrees)\n        south_facing = (arcticdem_with_aspect.aspect &gt; 135) &amp; (arcticdem_with_aspect.aspect &lt; 225)\n        ```\n\n    \"\"\"\n    aspect_deg = aspect(arcticdem_ds.dem)\n\n    # Aspect is always calculated in the projection - thus \"north\" is rather an \"up\"\n    # To get the true north, we need to correct the aspect based on the coordinates\n    x = arcticdem_ds.x.expand_dims({\"y\": arcticdem_ds.y})\n    y = arcticdem_ds.y.expand_dims({\"x\": arcticdem_ds.x})\n    if arcticdem_ds.cupy.is_cupy:\n        x = cp.asarray(x)\n        y = cp.asarray(y)\n        correction_offset = cp.arctan2(x, y) * (180 / np.pi) + 90\n    else:\n        correction_offset = np.arctan2(x, y) * (180 / np.pi) + 90\n    aspect_deg = (aspect_deg + correction_offset) % 360\n\n    aspect_deg.attrs = {\n        \"long_name\": \"Aspect\",\n        \"units\": \"degrees\",\n        \"description\": \"The compass direction that the slope faces, in degrees clockwise from north.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"aspect\"] = aspect_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_ctvi","title":"calculate_ctvi","text":"<pre><code>calculate_ctvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate CTVI (Corrected Transformed Vegetation Index) from spectral bands.</p> <p>CTVI is a corrected version of TVI that maintains the sign of the original NDVI values while applying the transformation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - ndvi (float32): NDVI values (will be calculated if not present) - nir, red (float32): Required if NDVI not present</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: CTVI values with attributes: - long_name: \"CTVI\"</p> </li> </ul> Note <p>Formula: CTVI = (NDVI + 0.5) / |NDVI + 0.5| * sqrt(|NDVI + 0.5|)</p> <p>If NDVI is already in the dataset, it will be reused to avoid recalculation.</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_ctvi\n\nctvi = calculate_ctvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating CTVI\", printer=logger.debug)\ndef calculate_ctvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate CTVI (Corrected Transformed Vegetation Index) from spectral bands.\n\n    CTVI is a corrected version of TVI that maintains the sign of the original NDVI values\n    while applying the transformation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - ndvi (float32): NDVI values (will be calculated if not present)\n            - nir, red (float32): Required if NDVI not present\n\n    Returns:\n        xr.DataArray: CTVI values with attributes:\n            - long_name: \"CTVI\"\n\n    Note:\n        Formula: CTVI = (NDVI + 0.5) / |NDVI + 0.5| * sqrt(|NDVI + 0.5|)\n\n        If NDVI is already in the dataset, it will be reused to avoid recalculation.\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_ctvi\n\n        ctvi = calculate_ctvi(optical)\n        ```\n\n    \"\"\"\n    ndvi = optical[\"ndvi\"] if \"ndvi\" in optical else calculate_ndvi(optical)\n    ctvi = (ndvi + 0.5) / np.abs(ndvi + 0.5) * np.sqrt(np.abs(ndvi + 0.5))\n    ctvi = ctvi.assign_attrs({\"long_name\": \"CTVI\"}).rename(\"ctvi\")\n    return ctvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_curvature","title":"calculate_curvature","text":"<pre><code>calculate_curvature(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate curvature of the terrain surface from an ArcticDEM Dataset.</p> <p>Curvature measures the rate of change of slope, indicating terrain convexity or concavity.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>curvature (float32): Curvature values.</p> </li> <li> <p>long_name: \"Curvature\"</p> </li> <li>description: Rate of change of slope</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Curvature interpretation:</p> <ul> <li>Positive values: Convex terrain (hills, ridges)</li> <li>Negative values: Concave terrain (valleys, depressions)</li> <li>Near zero: Planar terrain</li> </ul> Example <pre><code>from darts_preprocessing import calculate_curvature\n\narcticdem_with_curv = calculate_curvature(arcticdem_ds)\n\n# Identify ridges (convex areas)\nridges = arcticdem_with_curv.curvature &gt; 0.1\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating curvature\", printer=logger.debug)\ndef calculate_curvature(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate curvature of the terrain surface from an ArcticDEM Dataset.\n\n    Curvature measures the rate of change of slope, indicating terrain convexity or concavity.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - curvature (float32): Curvature values.\n\n            - long_name: \"Curvature\"\n            - description: Rate of change of slope\n            - source: \"ArcticDEM\"\n\n    Note:\n        Curvature interpretation:\n\n        - Positive values: Convex terrain (hills, ridges)\n        - Negative values: Concave terrain (valleys, depressions)\n        - Near zero: Planar terrain\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_curvature\n\n        arcticdem_with_curv = calculate_curvature(arcticdem_ds)\n\n        # Identify ridges (convex areas)\n        ridges = arcticdem_with_curv.curvature &gt; 0.1\n        ```\n\n    \"\"\"\n    curvature_da = curvature(arcticdem_ds.dem)\n    curvature_da.attrs = {\n        \"long_name\": \"Curvature\",\n        \"units\": \"\",\n        \"description\": \"The curvature of the terrain surface.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"curvature\"] = curvature_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_dissection_index","title":"calculate_dissection_index","text":"<pre><code>calculate_dissection_index(\n    arcticdem_ds: xarray.Dataset, neighborhood_size: int\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Dissection Index (DI) from an ArcticDEM Dataset.</p> <p>DI measures the degree to which a landscape has been cut by valleys and ravines. Values range from 0 (smooth, undissected) to 1 (highly dissected).</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> <li> <code>neighborhood_size</code>               (<code>int</code>)           \u2013            <p>Neighborhood window size for DI calculation. Can be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>di (float32): Dissection Index [0-1].</p> </li> <li> <p>long_name: \"Dissection Index\"</p> </li> <li>description: Documents neighborhood size used</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>The dissection index quantifies landscape dissection by comparing elevation ranges within the neighborhood window. Higher values indicate more deeply incised terrain with greater vertical relief.</p> <p>The neighborhood_size parameter is converted to pixels based on DEM resolution.</p> Example <pre><code>from darts_preprocessing import calculate_dissection_index\n\n# Calculate DI with 100m neighborhood\narcticdem_with_di = calculate_dissection_index(\n    arcticdem_ds=arcticdem,\n    neighborhood_size=100\n)\n\n# Identify highly dissected terrain\ndissected = arcticdem_with_di.di &gt; 0.5\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating Dissection Index\", printer=logger.debug)\ndef calculate_dissection_index(arcticdem_ds: xr.Dataset, neighborhood_size: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Dissection Index (DI) from an ArcticDEM Dataset.\n\n    DI measures the degree to which a landscape has been cut by valleys and ravines.\n    Values range from 0 (smooth, undissected) to 1 (highly dissected).\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n        neighborhood_size (int): Neighborhood window size for DI calculation.\n            Can be specified as string with units (e.g., \"100m\" or \"10px\").\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - di (float32): Dissection Index [0-1].\n\n            - long_name: \"Dissection Index\"\n            - description: Documents neighborhood size used\n            - source: \"ArcticDEM\"\n\n    Note:\n        The dissection index quantifies landscape dissection by comparing elevation\n        ranges within the neighborhood window. Higher values indicate more deeply\n        incised terrain with greater vertical relief.\n\n        The neighborhood_size parameter is converted to pixels based on DEM resolution.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_dissection_index\n\n        # Calculate DI with 100m neighborhood\n        arcticdem_with_di = calculate_dissection_index(\n            arcticdem_ds=arcticdem,\n            neighborhood_size=100\n        )\n\n        # Identify highly dissected terrain\n        dissected = arcticdem_with_di.di &gt; 0.5\n        ```\n\n    \"\"\"\n    # Get neighborhood_size in pixels\n    neighborhood_size: Distance = Distance.parse(neighborhood_size, arcticdem_ds.odc.geobox.resolution.x)\n\n    di = dissection_index(arcticdem_ds.dem, window_size=neighborhood_size.pixel)\n\n    di.attrs = {\n        \"long_name\": \"Dissection Index\",\n        \"units\": \"\",\n        \"description\": (\n            f\"Dissection index calculated using a {neighborhood_size} neighborhood. \"\n            \"Values range from 0 (smooth) to 1 (most rugged).\"\n        ),\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"di\"] = di.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_evi","title":"calculate_evi","text":"<pre><code>calculate_evi(\n    optical: xarray.Dataset,\n    g: float = 2.5,\n    c1: float = 6,\n    c2: float = 7.5,\n    l: float = 1,\n) -&gt; xarray.DataArray\n</code></pre> <p>Calculate EVI (Enhanced Vegetation Index) from spectral bands.</p> <p>EVI is optimized to enhance vegetation signal with improved sensitivity in high biomass regions and improved vegetation monitoring through decoupling of canopy background signal and reducing atmospheric influences.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - red (float32): Red reflectance [0-1] - blue (float32): Blue reflectance [0-1]</p> </li> <li> <code>g</code>               (<code>float</code>, default:                   <code>2.5</code> )           \u2013            <p>Gain factor. Defaults to 2.5.</p> </li> <li> <code>c1</code>               (<code>float</code>, default:                   <code>6</code> )           \u2013            <p>Aerosol resistance coefficient for red band. Defaults to 6.</p> </li> <li> <code>c2</code>               (<code>float</code>, default:                   <code>7.5</code> )           \u2013            <p>Aerosol resistance coefficient for blue band. Defaults to 7.5.</p> </li> <li> <code>l</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Canopy background adjustment. Defaults to 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: EVI values with attributes: - long_name: \"EVI\"</p> </li> </ul> Note <p>Formula: EVI = G * (NIR - Red) / (NIR + C1 * Red - C2 * Blue + L)</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>A Huete, K Didan, T Miura, E.P Rodriguez, X Gao, L.G Ferreira, Overview of the radiometric and biophysical performance of the MODIS vegetation indices, Remote Sensing of Environment, Volume 83, Issues 1-2, 2002, Pages 195-213, ISSN 0034-4257, https://doi.org/10.1016/S0034-4257(02)00096-2.</p> Example <pre><code>from darts_preprocessing import calculate_evi\n\nevi = calculate_evi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating EVI\", printer=logger.debug)\ndef calculate_evi(optical: xr.Dataset, g: float = 2.5, c1: float = 6, c2: float = 7.5, l: float = 1) -&gt; xr.DataArray:  # noqa: E741\n    \"\"\"Calculate EVI (Enhanced Vegetation Index) from spectral bands.\n\n    EVI is optimized to enhance vegetation signal with improved sensitivity in high biomass\n    regions and improved vegetation monitoring through decoupling of canopy background signal\n    and reducing atmospheric influences.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n            - blue (float32): Blue reflectance [0-1]\n        g (float, optional): Gain factor. Defaults to 2.5.\n        c1 (float, optional): Aerosol resistance coefficient for red band. Defaults to 6.\n        c2 (float, optional): Aerosol resistance coefficient for blue band. Defaults to 7.5.\n        l (float, optional): Canopy background adjustment. Defaults to 1.\n\n    Returns:\n        xr.DataArray: EVI values with attributes:\n            - long_name: \"EVI\"\n\n    Note:\n        Formula: EVI = G * (NIR - Red) / (NIR + C1 * Red - C2 * Blue + L)\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        A Huete, K Didan, T Miura, E.P Rodriguez, X Gao, L.G Ferreira,\n        Overview of the radiometric and biophysical performance of the MODIS vegetation indices,\n        Remote Sensing of Environment, Volume 83, Issues 1-2, 2002, Pages 195-213, ISSN 0034-4257,\n        https://doi.org/10.1016/S0034-4257(02)00096-2.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_evi\n\n        evi = calculate_evi(optical)\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    b = optical[\"blue\"].clip(0, 1)\n    evi = g * (nir - r) / (nir + c1 * r - c2 * b + l)\n    evi = evi.assign_attrs({\"long_name\": \"EVI\"}).rename(\"evi\")\n    return evi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_exg","title":"calculate_exg","text":"<pre><code>calculate_exg(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate EXG (Excess Green Index) from spectral bands.</p> <p>EXG highlights green vegetation by emphasizing the green band relative to red and blue. Widely used for crop/weed discrimination and precision agriculture.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - green (float32): Green reflectance [0-1] - red (float32): Red reflectance [0-1] - blue (float32): Blue reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: EXG values with attributes: - long_name: \"EXG\"</p> </li> </ul> Note <p>Formula: EXG = 2 * Green - Red - Blue</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>Upendar, K., Agrawal, K.N., Chandel, N.S. et al. Greenness identification using visible spectral colour indices for site specific weed management. Plant Physiol. Rep. 26, 179-187 (2021). https://doi.org/10.1007/s40502-020-00562-0</p> Example <pre><code>from darts_preprocessing import calculate_exg\n\nexg = calculate_exg(optical)\n\n# Threshold for vegetation detection\nvegetation = exg &gt; 0\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating EXG\", printer=logger.debug)\ndef calculate_exg(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate EXG (Excess Green Index) from spectral bands.\n\n    EXG highlights green vegetation by emphasizing the green band relative to red and blue.\n    Widely used for crop/weed discrimination and precision agriculture.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - green (float32): Green reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n            - blue (float32): Blue reflectance [0-1]\n\n    Returns:\n        xr.DataArray: EXG values with attributes:\n            - long_name: \"EXG\"\n\n    Note:\n        Formula: EXG = 2 * Green - Red - Blue\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        Upendar, K., Agrawal, K.N., Chandel, N.S. et al.\n        Greenness identification using visible spectral colour indices for site specific weed management.\n        Plant Physiol. Rep. 26, 179-187 (2021).\n        https://doi.org/10.1007/s40502-020-00562-0\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_exg\n\n        exg = calculate_exg(optical)\n\n        # Threshold for vegetation detection\n        vegetation = exg &gt; 0\n        ```\n\n    \"\"\"\n    g = optical[\"green\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    b = optical[\"blue\"].clip(0, 1)\n    exg = 2 * g - r - b\n    exg = exg.assign_attrs({\"long_name\": \"EXG\"}).rename(\"exg\")\n    return exg\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_gli","title":"calculate_gli","text":"<pre><code>calculate_gli(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate GLI (Green Leaf Index) from spectral bands.</p> <p>GLI emphasizes green reflectance for vegetation detection using only visible bands. Suitable for RGB sensors and aerial imagery.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - green (float32): Green reflectance - red (float32): Red reflectance - blue (float32): Blue reflectance</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: GLI values with attributes: - long_name: \"GLI\"</p> </li> </ul> Note <p>Formula: GLI = (2 * Green - Red - Blue) / (2 * Green + Red + Blue)</p> References <p>Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019. The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images. International Journal of Technology. Volume 10(7), pp. 1385-1394 https://doi.org/10.14716/ijtech.v10i7.3275</p> Example <pre><code>from darts_preprocessing import calculate_gli\n\ngli = calculate_gli(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating GLI\", printer=logger.debug)\ndef calculate_gli(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate GLI (Green Leaf Index) from spectral bands.\n\n    GLI emphasizes green reflectance for vegetation detection using only visible bands.\n    Suitable for RGB sensors and aerial imagery.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - green (float32): Green reflectance\n            - red (float32): Red reflectance\n            - blue (float32): Blue reflectance\n\n    Returns:\n        xr.DataArray: GLI values with attributes:\n            - long_name: \"GLI\"\n\n    Note:\n        Formula: GLI = (2 * Green - Red - Blue) / (2 * Green + Red + Blue)\n\n    References:\n        Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019.\n        The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images.\n        International Journal of Technology. Volume 10(7), pp. 1385-1394\n        https://doi.org/10.14716/ijtech.v10i7.3275\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_gli\n\n        gli = calculate_gli(optical)\n        ```\n\n    \"\"\"\n    g = optical[\"green\"]\n    r = optical[\"red\"]\n    b = optical[\"blue\"]\n    gli = (2 * g - r - b) / (2 * g + r + b)\n    gli = gli.assign_attrs({\"long_name\": \"GLI\"}).rename(\"gli\")\n    return gli\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_gndvi","title":"calculate_gndvi","text":"<pre><code>calculate_gndvi(\n    optical: xarray.Dataset,\n) -&gt; xarray.DataArray\n</code></pre> <p>Calculate GNDVI (Green Normalized Difference Vegetation Index) from spectral bands.</p> <p>GNDVI is similar to NDVI but uses the green band instead of red, making it more sensitive to chlorophyll content and useful for mid to late season vegetation monitoring.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - green (float32): Green reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: GNDVI values with attributes: - long_name: \"GNDVI\" - Values clipped to [-1, 1] range</p> </li> </ul> Note <p>Formula: GNDVI = (NIR - Green) / (NIR + Green)</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> Example <pre><code>from darts_preprocessing import calculate_gndvi\n\ngndvi = calculate_gndvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating GNDVI\", printer=logger.debug)\ndef calculate_gndvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate GNDVI (Green Normalized Difference Vegetation Index) from spectral bands.\n\n    GNDVI is similar to NDVI but uses the green band instead of red, making it more sensitive\n    to chlorophyll content and useful for mid to late season vegetation monitoring.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - green (float32): Green reflectance [0-1]\n\n    Returns:\n        xr.DataArray: GNDVI values with attributes:\n            - long_name: \"GNDVI\"\n            - Values clipped to [-1, 1] range\n\n    Note:\n        Formula: GNDVI = (NIR - Green) / (NIR + Green)\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_gndvi\n\n        gndvi = calculate_gndvi(optical)\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    g = optical[\"green\"].clip(0, 1)\n    gndvi = (nir - g) / (nir + g)\n    gndvi = gndvi.clip(-1, 1)\n    gndvi = gndvi.assign_attrs({\"long_name\": \"GNDVI\"}).rename(\"gndvi\")\n    return gndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_grvi","title":"calculate_grvi","text":"<pre><code>calculate_grvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate GRVI (Green Red Vegetation Index) from spectral bands.</p> <p>GRVI uses visible bands to detect vegetation, useful for high-resolution imagery where NIR may not be available or for specific vegetation discrimination tasks.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - green (float32): Green reflectance [0-1] - red (float32): Red reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: GRVI values with attributes: - long_name: \"GRVI\"</p> </li> </ul> Note <p>Formula: GRVI = (Green - Red) / (Green + Red)</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019. The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images. International Journal of Technology. Volume 10(7), pp. 1385-1394 https://doi.org/10.14716/ijtech.v10i7.3275</p> Example <pre><code>from darts_preprocessing import calculate_grvi\n\ngrvi = calculate_grvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating GRVI\", printer=logger.debug)\ndef calculate_grvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate GRVI (Green Red Vegetation Index) from spectral bands.\n\n    GRVI uses visible bands to detect vegetation, useful for high-resolution imagery\n    where NIR may not be available or for specific vegetation discrimination tasks.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - green (float32): Green reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n\n    Returns:\n        xr.DataArray: GRVI values with attributes:\n            - long_name: \"GRVI\"\n\n    Note:\n        Formula: GRVI = (Green - Red) / (Green + Red)\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019.\n        The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images.\n        International Journal of Technology. Volume 10(7), pp. 1385-1394\n        https://doi.org/10.14716/ijtech.v10i7.3275\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_grvi\n\n        grvi = calculate_grvi(optical)\n        ```\n\n    \"\"\"\n    g = optical[\"green\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    grvi = (g - r) / (g + r)\n    grvi = grvi.assign_attrs({\"long_name\": \"GRVI\"}).rename(\"grvi\")\n    return grvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_hillshade","title":"calculate_hillshade","text":"<pre><code>calculate_hillshade(\n    arcticdem_ds: xarray.Dataset,\n    azimuth: int = 225,\n    angle_altitude: int = 25,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate hillshade of the terrain surface from an ArcticDEM Dataset.</p> <p>Hillshade simulates illumination of terrain from a specified sun position, useful for visualization and terrain analysis.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> <li> <code>azimuth</code>               (<code>int</code>, default:                   <code>225</code> )           \u2013            <p>Light source azimuth in degrees clockwise from north [0-360]. Defaults to 225 (southwest).</p> </li> <li> <code>angle_altitude</code>               (<code>int</code>, default:                   <code>25</code> )           \u2013            <p>Light source altitude angle in degrees above horizon [0-90]. Defaults to 25.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>hillshade (float32): Illumination values [0-255], where 0 is shadow and 255 is fully lit.</p> </li> <li> <p>long_name: \"Hillshade\"</p> </li> <li>description: Documents azimuth and angle_altitude used</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Common azimuth/altitude combinations:</p> <ul> <li>315\u00b0/45\u00b0: Classic northwest illumination (default for many GIS applications)</li> <li>225\u00b0/25\u00b0: Southwest with low sun (better for visualizing subtle features)</li> </ul> <p>The hillshade calculation accounts for both slope and aspect of the terrain.</p> Example <pre><code>from darts_preprocessing import calculate_hillshade\n\n# Default southwest illumination\narcticdem_with_hs = calculate_hillshade(arcticdem_ds)\n\n# Custom sun position\narcticdem_custom = calculate_hillshade(\n    arcticdem_ds,\n    azimuth=315,\n    angle_altitude=45\n)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating hillshade\", printer=logger.debug, print_kwargs=[\"azimuth\", \"angle_altitude\"])\ndef calculate_hillshade(arcticdem_ds: xr.Dataset, azimuth: int = 225, angle_altitude: int = 25) -&gt; xr.Dataset:\n    \"\"\"Calculate hillshade of the terrain surface from an ArcticDEM Dataset.\n\n    Hillshade simulates illumination of terrain from a specified sun position, useful\n    for visualization and terrain analysis.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n        azimuth (int, optional): Light source azimuth in degrees clockwise from north [0-360].\n            Defaults to 225 (southwest).\n        angle_altitude (int, optional): Light source altitude angle in degrees above horizon [0-90].\n            Defaults to 25.\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - hillshade (float32): Illumination values [0-255], where 0 is shadow and 255 is fully lit.\n\n            - long_name: \"Hillshade\"\n            - description: Documents azimuth and angle_altitude used\n            - source: \"ArcticDEM\"\n\n    Note:\n        Common azimuth/altitude combinations:\n\n        - 315\u00b0/45\u00b0: Classic northwest illumination (default for many GIS applications)\n        - 225\u00b0/25\u00b0: Southwest with low sun (better for visualizing subtle features)\n\n        The hillshade calculation accounts for both slope and aspect of the terrain.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_hillshade\n\n        # Default southwest illumination\n        arcticdem_with_hs = calculate_hillshade(arcticdem_ds)\n\n        # Custom sun position\n        arcticdem_custom = calculate_hillshade(\n            arcticdem_ds,\n            azimuth=315,\n            angle_altitude=45\n        )\n        ```\n\n    \"\"\"\n    x, y = arcticdem_ds.x.mean().item(), arcticdem_ds.y.mean().item()\n    correction_offset = np.arctan2(x, y) * (180 / np.pi) + 90\n    azimuth_corrected = (azimuth - correction_offset + 360) % 360\n\n    hillshade_da = hillshade(arcticdem_ds.dem, azimuth=azimuth_corrected, angle_altitude=angle_altitude)\n    hillshade_da.attrs = {\n        \"long_name\": \"Hillshade\",\n        \"units\": \"\",\n        \"description\": f\"The hillshade based on azimuth {azimuth} and angle_altitude {angle_altitude}.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"hillshade\"] = hillshade_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_ndvi","title":"calculate_ndvi","text":"<pre><code>calculate_ndvi(optical: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Calculate NDVI (Normalized Difference Vegetation Index) from spectral bands.</p> <p>NDVI is a widely-used vegetation index that indicates photosynthetic activity and vegetation health. Values range from -1 to 1, with higher values indicating denser, healthier vegetation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - red (float32): Red reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.DataArray: NDVI values with attributes: - long_name: \"NDVI\"</p> </li> </ul> Note <p>Formula: NDVI = (NIR - Red) / (NIR + Red)</p> <p>Input bands are clipped to [0, 1] before calculation to avoid numerical instabilities from negative reflectance values or sensor artifacts. The final result is also clipped to ensure values remain in the valid [-1, 1] range.</p> Example <p>Calculate NDVI from optical data:</p> <pre><code>from darts_preprocessing import calculate_ndvi\n\n# optical contains 'nir' and 'red' bands\nndvi = calculate_ndvi(optical)\n\n# Mask vegetation\nvegetation_mask = ndvi &gt; 0.3\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating NDVI\", printer=logger.debug)\ndef calculate_ndvi(optical: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI (Normalized Difference Vegetation Index) from spectral bands.\n\n    NDVI is a widely-used vegetation index that indicates photosynthetic activity and\n    vegetation health. Values range from -1 to 1, with higher values indicating denser,\n    healthier vegetation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n\n    Returns:\n        xr.DataArray: NDVI values with attributes:\n            - long_name: \"NDVI\"\n\n    Note:\n        Formula: NDVI = (NIR - Red) / (NIR + Red)\n\n        Input bands are clipped to [0, 1] before calculation to avoid numerical instabilities\n        from negative reflectance values or sensor artifacts. The final result is also clipped\n        to ensure values remain in the valid [-1, 1] range.\n\n    Example:\n        Calculate NDVI from optical data:\n\n        ```python\n        from darts_preprocessing import calculate_ndvi\n\n        # optical contains 'nir' and 'red' bands\n        ndvi = calculate_ndvi(optical)\n\n        # Mask vegetation\n        vegetation_mask = ndvi &gt; 0.3\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    ndvi = (nir - r) / (nir + r)\n    ndvi = ndvi.clip(-1, 1)\n    ndvi = ndvi.assign_attrs({\"long_name\": \"NDVI\"}).rename(\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_nrvi","title":"calculate_nrvi","text":"<pre><code>calculate_nrvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate NRVI (Normalized Ratio Vegetation Index) from spectral bands.</p> <p>NRVI normalizes RVI to a range similar to NDVI, making it more comparable across different vegetation densities.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - rvi (float32): RVI values (will be calculated if not present) - nir, red (float32): Required if RVI not present</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: NRVI values with attributes: - long_name: \"NRVI\"</p> </li> </ul> Note <p>Formula: NRVI = (RVI - 1) / (RVI + 1) where RVI = Red / NIR</p> <p>If RVI is already in the dataset, it will be reused to avoid recalculation.</p> Example <pre><code>from darts_preprocessing import calculate_nrvi\n\nnrvi = calculate_nrvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating NRVI\", printer=logger.debug)\ndef calculate_nrvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate NRVI (Normalized Ratio Vegetation Index) from spectral bands.\n\n    NRVI normalizes RVI to a range similar to NDVI, making it more comparable across\n    different vegetation densities.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - rvi (float32): RVI values (will be calculated if not present)\n            - nir, red (float32): Required if RVI not present\n\n    Returns:\n        xr.DataArray: NRVI values with attributes:\n            - long_name: \"NRVI\"\n\n    Note:\n        Formula: NRVI = (RVI - 1) / (RVI + 1)\n        where RVI = Red / NIR\n\n        If RVI is already in the dataset, it will be reused to avoid recalculation.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_nrvi\n\n        nrvi = calculate_nrvi(optical)\n        ```\n\n    \"\"\"\n    rvi = optical[\"rvi\"] if \"rvi\" in optical else calculate_rvi(optical)\n    nrvi = (rvi - 1) / (rvi + 1)\n    nrvi = nrvi.assign_attrs({\"long_name\": \"NRVI\"}).rename(\"nrvi\")\n    return nrvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_rvi","title":"calculate_rvi","text":"<pre><code>calculate_rvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate RVI (Ratio Vegetation Index) from spectral bands.</p> <p>RVI is a simple ratio index sensitive to vegetation amount and biomass. Values typically range from 0 to over 30 for dense vegetation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - red (float32): Red reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: RVI values with attributes: - long_name: \"RVI\"</p> </li> </ul> Note <p>Formula: RVI = Red / NIR</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_rvi\n\nrvi = calculate_rvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating RVI\", printer=logger.debug)\ndef calculate_rvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate RVI (Ratio Vegetation Index) from spectral bands.\n\n    RVI is a simple ratio index sensitive to vegetation amount and biomass. Values typically\n    range from 0 to over 30 for dense vegetation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n\n    Returns:\n        xr.DataArray: RVI values with attributes:\n            - long_name: \"RVI\"\n\n    Note:\n        Formula: RVI = Red / NIR\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_rvi\n\n        rvi = calculate_rvi(optical)\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    rvi = r / nir\n    rvi = rvi.assign_attrs({\"long_name\": \"RVI\"}).rename(\"rvi\")\n    return rvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_savi","title":"calculate_savi","text":"<pre><code>calculate_savi(\n    optical: xarray.Dataset, s: float = 0.5\n) -&gt; xarray.DataArray\n</code></pre> <p>Calculate SAVI (Soil Adjusted Vegetation Index) from spectral bands.</p> <p>SAVI minimizes soil brightness influences using a soil-brightness correction factor. Useful in areas with sparse vegetation or exposed soil.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - ndvi (float32): NDVI values (will be calculated if not present) - nir, red (float32): Required if NDVI not present</p> </li> <li> <code>s</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Soil adjustment factor. Common values: - 0.5: moderate vegetation cover (default) - 0.25: high vegetation cover - 1.0: low vegetation cover</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: SAVI values with attributes: - long_name: \"SAVI\"</p> </li> </ul> Note <p>Formula: SAVI = NDVI * (1 + s)</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_savi\n\n# For sparse vegetation\nsavi = calculate_savi(optical, s=1.0)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating SAVI\", printer=logger.debug)\ndef calculate_savi(optical: xr.Dataset, s: float = 0.5) -&gt; xr.DataArray:\n    \"\"\"Calculate SAVI (Soil Adjusted Vegetation Index) from spectral bands.\n\n    SAVI minimizes soil brightness influences using a soil-brightness correction factor.\n    Useful in areas with sparse vegetation or exposed soil.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - ndvi (float32): NDVI values (will be calculated if not present)\n            - nir, red (float32): Required if NDVI not present\n        s (float, optional): Soil adjustment factor. Common values:\n            - 0.5: moderate vegetation cover (default)\n            - 0.25: high vegetation cover\n            - 1.0: low vegetation cover\n\n    Returns:\n        xr.DataArray: SAVI values with attributes:\n            - long_name: \"SAVI\"\n\n    Note:\n        Formula: SAVI = NDVI * (1 + s)\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_savi\n\n        # For sparse vegetation\n        savi = calculate_savi(optical, s=1.0)\n        ```\n\n    \"\"\"\n    ndvi = optical[\"ndvi\"] if \"ndvi\" in optical else calculate_ndvi(optical)\n    savi = ndvi * (1 + s)\n    savi = savi.assign_attrs({\"long_name\": \"SAVI\"}).rename(\"savi\")\n    return savi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_slope","title":"calculate_slope","text":"<pre><code>calculate_slope(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Slope represents the rate of change of elevation, indicating terrain steepness.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>slope (float32): Slope in degrees [0-90].</p> </li> <li> <p>long_name: \"Slope\"</p> </li> <li>units: \"degrees\"</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Slope is calculated using finite difference methods on the DEM. Values approaching 90\u00b0 indicate near-vertical terrain.</p> Example <pre><code>from darts_preprocessing import calculate_slope\n\narcticdem_with_slope = calculate_slope(arcticdem_ds)\n\n# Mask steep terrain\nsteep_areas = arcticdem_with_slope.slope &gt; 30\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate slope of the terrain surface from an ArcticDEM Dataset.\n\n    Slope represents the rate of change of elevation, indicating terrain steepness.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - slope (float32): Slope in degrees [0-90].\n\n            - long_name: \"Slope\"\n            - units: \"degrees\"\n            - source: \"ArcticDEM\"\n\n    Note:\n        Slope is calculated using finite difference methods on the DEM.\n        Values approaching 90\u00b0 indicate near-vertical terrain.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_slope\n\n        arcticdem_with_slope = calculate_slope(arcticdem_ds)\n\n        # Mask steep terrain\n        steep_areas = arcticdem_with_slope.slope &gt; 30\n        ```\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_spyndex","title":"calculate_spyndex","text":"<pre><code>calculate_spyndex(\n    ds_optical: xarray.Dataset,\n    index: str,\n    **kwargs: dict[str, typing.Any],\n) -&gt; xarray.DataArray\n</code></pre> <p>Compute a spectral index using the spyndex library.</p> <p>This wrapper provides access to 200+ spectral indices from the spyndex library with automatic band mapping and parameter handling.</p> <p>Parameters:</p> <ul> <li> <code>ds_optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands. Band names should match spyndex common names (e.g., 'red', 'nir', 'blue', 'green').</p> </li> <li> <code>index</code>               (<code>str</code>)           \u2013            <p>Name of the spectral index to compute. Run <code>spyndex.indices</code> to see all available indices (e.g., 'NDVI', 'EVI', 'SAVI').</p> </li> <li> <code>**kwargs</code>               (<code>dict[str, typing.Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters or band overrides: - Band values: Override bands from ds_optical with scalar or array values (e.g., red=0.2) - Constants: Override default values for index-specific constants (e.g., L=0.5 for SAVI)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Computed spectral index with attributes: - source: \"spyndex\" - long_name: Full name of the index - reference: Citation for the index - formula: Mathematical formula - author: Index contributor</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a required band is missing from both ds_optical and kwargs.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If all bands are provided as scalar values.</p> </li> </ul> Note <p>Band resolution priority:</p> <ol> <li>Bands in ds_optical (with common_name matching spyndex.bands)</li> <li>Values in kwargs (override ds_optical bands)</li> <li>Default values for constants (from spyndex.constants)</li> </ol> <p>All optical bands are automatically clipped to [0, 1] before calculation.</p> <p>At least one band must come from ds_optical as a DataArray (not all scalars).</p> Example <p>Compute various indices with spyndex:</p> <pre><code>from darts_preprocessing import calculate_spyndex\nimport spyndex\n\n# List all available indices\nprint(list(spyndex.indices.keys()))\n\n# Basic NDVI\nndvi = calculate_spyndex(ds_optical, \"NDVI\")\n\n# SAVI with custom soil adjustment factor\nsavi = calculate_spyndex(ds_optical, \"SAVI\", L=0.5)\n\n# EVI with custom parameters\nevi = calculate_spyndex(ds_optical, \"EVI\", g=2.5, C1=6, C2=7.5, L=1)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/spyndex.py</code> <pre><code>@stopwatch.f(\"Computing spectral index with spyndex\", printer=logger.debug, print_kwargs=[\"index\"])\ndef calculate_spyndex(ds_optical: xr.Dataset, index: str, **kwargs: dict[str, Any]) -&gt; xr.DataArray:\n    \"\"\"Compute a spectral index using the spyndex library.\n\n    This wrapper provides access to 200+ spectral indices from the spyndex library with\n    automatic band mapping and parameter handling.\n\n    Args:\n        ds_optical (xr.Dataset): Dataset containing spectral bands. Band names should match\n            spyndex common names (e.g., 'red', 'nir', 'blue', 'green').\n        index (str): Name of the spectral index to compute. Run `spyndex.indices` to see\n            all available indices (e.g., 'NDVI', 'EVI', 'SAVI').\n        **kwargs: Additional parameters or band overrides:\n            - Band values: Override bands from ds_optical with scalar or array values (e.g., red=0.2)\n            - Constants: Override default values for index-specific constants (e.g., L=0.5 for SAVI)\n\n    Returns:\n        xr.DataArray: Computed spectral index with attributes:\n            - source: \"spyndex\"\n            - long_name: Full name of the index\n            - reference: Citation for the index\n            - formula: Mathematical formula\n            - author: Index contributor\n\n    Raises:\n        ValueError: If a required band is missing from both ds_optical and kwargs.\n        ValueError: If all bands are provided as scalar values.\n\n    Note:\n        Band resolution priority:\n\n        1. Bands in ds_optical (with common_name matching spyndex.bands)\n        2. Values in kwargs (override ds_optical bands)\n        3. Default values for constants (from spyndex.constants)\n\n        All optical bands are automatically clipped to [0, 1] before calculation.\n\n        At least one band must come from ds_optical as a DataArray (not all scalars).\n\n    Example:\n        Compute various indices with spyndex:\n\n        ```python\n        from darts_preprocessing import calculate_spyndex\n        import spyndex\n\n        # List all available indices\n        print(list(spyndex.indices.keys()))\n\n        # Basic NDVI\n        ndvi = calculate_spyndex(ds_optical, \"NDVI\")\n\n        # SAVI with custom soil adjustment factor\n        savi = calculate_spyndex(ds_optical, \"SAVI\", L=0.5)\n\n        # EVI with custom parameters\n        evi = calculate_spyndex(ds_optical, \"EVI\", g=2.5, C1=6, C2=7.5, L=1)\n        ```\n\n    \"\"\"\n    index: spyndex.axioms.SpectralIndex = spyndex.indices[index]\n\n    params = {}\n    atleast_one_dataarray = False\n    for band in index.bands:\n        is_constant = band in spyndex.constants\n        is_in_kwargs = band in kwargs\n\n        is_in_optical = False\n        if not is_constant:\n            spyndex_band: spyndex.axioms.Band = spyndex.bands.get(band)\n            is_in_optical = spyndex_band is not None and spyndex_band.common_name in ds_optical\n\n        # Case 4: band is missing -&gt; error\n        if not (is_constant or is_in_kwargs or is_in_optical):\n            raise ValueError(f\"Band '{band}' is required for index '{index.short_name}' but not provided in {kwargs=}.\")\n        # Case 3: band is in kwargs\n        if is_in_kwargs:\n            params[band] = kwargs[band]\n            continue\n        # Case 2: band is a constant\n        if is_constant:\n            constant: spyndex.axioms.Constant = spyndex.constants[band]\n            params[band] = constant.default\n            continue\n        # Case 1: band is in optical\n        params[band] = ds_optical[spyndex_band.common_name].clip(0, 1)\n        atleast_one_dataarray = True\n\n    if not atleast_one_dataarray:\n        raise ValueError(f\"At least one band must be a DataArray, got {params=}. Did you pass all bands as scalars?\")\n\n    idx = index.compute(params)\n    assert isinstance(idx, xr.DataArray)\n\n    idx = idx.assign_attrs(\n        {\n            \"source\": \"spyndex\",\n            \"long_name\": index.long_name,\n            \"reference\": index.reference,\n            \"formula\": index.formula,\n            \"author\": index.contributor,\n        }\n    ).rename(index.short_name.lower())\n    return idx\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_terrain_ruggedness_index","title":"calculate_terrain_ruggedness_index","text":"<pre><code>calculate_terrain_ruggedness_index(\n    arcticdem_ds: xarray.Dataset, neighborhood_size: int\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Terrain Ruggedness Index (TRI) from an ArcticDEM Dataset.</p> <p>TRI quantifies topographic heterogeneity by measuring elevation differences between a cell and its surrounding cells. Higher values indicate more rugged terrain.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> <li> <code>neighborhood_size</code>               (<code>int</code>)           \u2013            <p>Neighborhood window size for TRI calculation. Can be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>tri (float32): Terrain Ruggedness Index in meters.</p> </li> <li> <p>long_name: \"Terrain Ruggedness Index\"</p> </li> <li>units: \"m\"</li> <li>description: Documents kernel size used</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>TRI methodology from Riley et al (1999):</p> <ol> <li>Measures elevation difference from center cell to 8 surrounding cells</li> <li>Squares and averages these differences</li> <li>Takes square root for final TRI value</li> </ol> <p>The neighborhood_size parameter controls the kernel size. A square kernel is used, with the actual size rounded to the nearest pixel based on DEM resolution.</p> References <p>Riley, S.J., DeGloria, S.D., Elliot, R., 1999. A Terrain Ruggedness Index That Quantifies Topographic Heterogeneity. Intermountain Journal of Sciences, vol. 5, No. 1-4, pp. 23-27.</p> Example <pre><code>from darts_preprocessing import calculate_terrain_ruggedness_index\n\n# Calculate TRI with 100m neighborhood\narcticdem_with_tri = calculate_terrain_ruggedness_index(\n    arcticdem_ds=arcticdem,\n    neighborhood_size=100\n)\n\n# Identify highly rugged terrain\nrugged = arcticdem_with_tri.tri &gt; 10\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating TRI\", printer=logger.debug)\ndef calculate_terrain_ruggedness_index(arcticdem_ds: xr.Dataset, neighborhood_size: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Terrain Ruggedness Index (TRI) from an ArcticDEM Dataset.\n\n    TRI quantifies topographic heterogeneity by measuring elevation differences between\n    a cell and its surrounding cells. Higher values indicate more rugged terrain.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n        neighborhood_size (int): Neighborhood window size for TRI calculation.\n            Can be specified as string with units (e.g., \"100m\" or \"10px\").\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - tri (float32): Terrain Ruggedness Index in meters.\n\n            - long_name: \"Terrain Ruggedness Index\"\n            - units: \"m\"\n            - description: Documents kernel size used\n            - source: \"ArcticDEM\"\n\n    Note:\n        TRI methodology from Riley et al (1999):\n\n        1. Measures elevation difference from center cell to 8 surrounding cells\n        2. Squares and averages these differences\n        3. Takes square root for final TRI value\n\n        The neighborhood_size parameter controls the kernel size. A square kernel is used,\n        with the actual size rounded to the nearest pixel based on DEM resolution.\n\n    References:\n        Riley, S.J., DeGloria, S.D., Elliot, R., 1999.\n        A Terrain Ruggedness Index That Quantifies Topographic Heterogeneity.\n        Intermountain Journal of Sciences, vol. 5, No. 1-4, pp. 23-27.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_terrain_ruggedness_index\n\n        # Calculate TRI with 100m neighborhood\n        arcticdem_with_tri = calculate_terrain_ruggedness_index(\n            arcticdem_ds=arcticdem,\n            neighborhood_size=100\n        )\n\n        # Identify highly rugged terrain\n        rugged = arcticdem_with_tri.tri &gt; 10\n        ```\n\n    \"\"\"\n    cellsize_x, _cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)\n\n    neighborhood_size: Distance = Distance.parse(neighborhood_size, cellsize_x)\n\n    kernel = np.ones((neighborhood_size.pixel, neighborhood_size.pixel), dtype=float)\n    kernel[neighborhood_size.pixel // 2, neighborhood_size.pixel // 2] = 0  # Set the center cell to 0\n    kernel = convolution.custom_kernel(kernel)\n    logger.debug(f\"Calculating Terrain Ruggedness Index with square kernel of radius {neighborhood_size} cells.\")\n\n    # Change dtype of kernel to float32 since we don't need the precision and f32 is faster\n    kernel = kernel.astype(\"float32\")\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    # Kernel compute of TRI as described here:\n    # https://sites.utexas.edu/utarima/files/2024/02/terrain_roughness_index.pdf\n    dem_squared = arcticdem_ds.dem**2\n    focal_sum = convolution.convolution_2d(arcticdem_ds.dem, kernel)\n    focal_sum_squared = convolution.convolution_2d(dem_squared, kernel)\n    tri = np.sqrt((kernel.size - 1) * dem_squared - 2 * arcticdem_ds.dem * focal_sum + focal_sum_squared)\n\n    tri.attrs = {\n        \"long_name\": \"Terrain Ruggedness Index\",\n        \"units\": \"m\",\n        \"description\": (\n            \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n            f\" cells within a square kernel of radius {neighborhood_size} cells.\"\n        ),\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"tri\"] = tri.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_tgi","title":"calculate_tgi","text":"<pre><code>calculate_tgi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate TGI (Triangular Greenness Index) from spectral bands.</p> <p>TGI is sensitive to chlorophyll content and can estimate leaf area index without calibration. Particularly useful for crop monitoring.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - red (float32): Red reflectance [0-1] - green (float32): Green reflectance [0-1] - blue (float32): Blue reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: TGI values with attributes: - long_name: \"TGI\"</p> </li> </ul> Note <p>Formula: TGI = -0.5 * [190 * (Red - Green) - 120 * (Red - Blue)]</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>E. Raymond Hunt, Paul C. Doraiswamy, James E. McMurtrey, Craig S.T. Daughtry, Eileen M. Perry, Bakhyt Akhmedov, A visible band index for remote sensing leaf chlorophyll content at the canopy scale, International Journal of Applied Earth Observation and Geoinformation, Volume 21, 2013, Pages 103-112, ISSN 1569-8432, https://doi.org/10.1016/j.jag.2012.07.020.</p> Example <pre><code>from darts_preprocessing import calculate_tgi\n\ntgi = calculate_tgi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating TGI\", printer=logger.debug)\ndef calculate_tgi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate TGI (Triangular Greenness Index) from spectral bands.\n\n    TGI is sensitive to chlorophyll content and can estimate leaf area index without\n    calibration. Particularly useful for crop monitoring.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - red (float32): Red reflectance [0-1]\n            - green (float32): Green reflectance [0-1]\n            - blue (float32): Blue reflectance [0-1]\n\n    Returns:\n        xr.DataArray: TGI values with attributes:\n            - long_name: \"TGI\"\n\n    Note:\n        Formula: TGI = -0.5 * [190 * (Red - Green) - 120 * (Red - Blue)]\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        E. Raymond Hunt, Paul C. Doraiswamy, James E. McMurtrey, Craig S.T. Daughtry, Eileen M. Perry, Bakhyt Akhmedov,\n        A visible band index for remote sensing leaf chlorophyll content at the canopy scale,\n        International Journal of Applied Earth Observation and Geoinformation,\n        Volume 21, 2013, Pages 103-112, ISSN 1569-8432,\n        https://doi.org/10.1016/j.jag.2012.07.020.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_tgi\n\n        tgi = calculate_tgi(optical)\n        ```\n\n    \"\"\"\n    r = optical[\"red\"].clip(0, 1)\n    g = optical[\"green\"].clip(0, 1)\n    b = optical[\"blue\"].clip(0, 1)\n    tgi = -0.5 * (190 * (r - g) - 120 * (r - b))\n    tgi = tgi.assign_attrs({\"long_name\": \"TGI\"}).rename(\"tgi\")\n    return tgi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_topographic_position_index","title":"calculate_topographic_position_index","text":"<pre><code>calculate_topographic_position_index(\n    arcticdem_ds: xarray.Dataset,\n    outer_radius: int,\n    inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>TPI measures the relative topographic position of a point by comparing its elevation to the mean elevation of the surrounding neighborhood. Positive values indicate higher positions (ridges), negative values indicate lower positions (valleys).</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable (float32).</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the neighborhood in meters. Can also be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in meters. If &gt; 0, creates an annulus (ring) instead of a circle. Set to 0 for a circular kernel. Can also be specified as string with units (e.g., \"50m\" or \"5px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with a new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>tpi (float32): Topographic Position Index values.</p> </li> <li> <p>long_name: \"Topographic Position Index (TPI)\"</p> </li> <li>description: Details about the kernel used</li> </ul> </li> </ul> Note <p>Kernel shape combinations:</p> <ul> <li>inner_radius=0: Circular kernel comparing each cell to all neighbors within outer_radius</li> <li>inner_radius&gt;0: Annulus kernel comparing each cell to neighbors in a ring between   inner_radius and outer_radius. Useful for multi-scale terrain analysis.</li> </ul> <p>The actual radii used are rounded to the nearest pixel based on the DEM resolution.</p> Example <p>Calculate TPI with circular and annulus kernels:</p> <pre><code>from darts_preprocessing import calculate_topographic_position_index\n\n# Circular kernel (100m radius)\narcticdem_with_tpi = calculate_topographic_position_index(\n    arcticdem_ds=arcticdem,\n    outer_radius=100,\n    inner_radius=0\n)\n\n# Annulus kernel (50-100m ring)\narcticdem_multi_scale = calculate_topographic_position_index(\n    arcticdem_ds=arcticdem,\n    outer_radius=100,\n    inner_radius=50\n)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    TPI measures the relative topographic position of a point by comparing its elevation to\n    the mean elevation of the surrounding neighborhood. Positive values indicate higher\n    positions (ridges), negative values indicate lower positions (valleys).\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable (float32).\n        outer_radius (int): The outer radius of the neighborhood in meters.\n            Can also be specified as string with units (e.g., \"100m\" or \"10px\").\n        inner_radius (int): The inner radius of the annulus kernel in meters.\n            If &gt; 0, creates an annulus (ring) instead of a circle. Set to 0 for a circular kernel.\n            Can also be specified as string with units (e.g., \"50m\" or \"5px\").\n\n    Returns:\n        xr.Dataset: The input Dataset with a new data variable added:\n\n        - tpi (float32): Topographic Position Index values.\n\n            - long_name: \"Topographic Position Index (TPI)\"\n            - description: Details about the kernel used\n\n    Note:\n        Kernel shape combinations:\n\n        - inner_radius=0: Circular kernel comparing each cell to all neighbors within outer_radius\n        - inner_radius&gt;0: Annulus kernel comparing each cell to neighbors in a ring between\n          inner_radius and outer_radius. Useful for multi-scale terrain analysis.\n\n        The actual radii used are rounded to the nearest pixel based on the DEM resolution.\n\n    Example:\n        Calculate TPI with circular and annulus kernels:\n\n        ```python\n        from darts_preprocessing import calculate_topographic_position_index\n\n        # Circular kernel (100m radius)\n        arcticdem_with_tpi = calculate_topographic_position_index(\n            arcticdem_ds=arcticdem,\n            outer_radius=100,\n            inner_radius=0\n        )\n\n        # Annulus kernel (50-100m ring)\n        arcticdem_multi_scale = calculate_topographic_position_index(\n            arcticdem_ds=arcticdem,\n            outer_radius=100,\n            inner_radius=50\n        )\n        ```\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius: Distance = Distance.parse(outer_radius, cellsize_x)\n    if inner_radius &gt; 0:\n        inner_radius: Distance = Distance.parse(inner_radius, cellsize_x)\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius.meter, inner_radius.meter)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius}-{outer_radius} cells away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of {inner_radius}-{outer_radius} cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius.meter)\n        attr_cell_description = f\"within a circle at a distance of {outer_radius} cells away from the focal cell.\"\n        logger.debug(f\"Calculating Topographic Position Index with circle kernel of {outer_radius} cells.\")\n\n    # Change dtype of kernel to float32 since we don't need the precision and f32 is faster\n    kernel = kernel.astype(\"float32\")\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_ttvi","title":"calculate_ttvi","text":"<pre><code>calculate_ttvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate TTVI (Thiam's Transformed Vegetation Index) from spectral bands.</p> <p>TTVI applies an absolute value transformation to NDVI before the square root, making it suitable for both positive and negative NDVI values.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - ndvi (float32): NDVI values (will be calculated if not present) - nir, red (float32): Required if NDVI not present</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: TTVI values with attributes: - long_name: \"TTVI\"</p> </li> </ul> Note <p>Formula: TTVI = sqrt(|NDVI| + 0.5)</p> <p>If NDVI is already in the dataset, it will be reused to avoid recalculation.</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_ttvi\n\nttvi = calculate_ttvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating TTVI\", printer=logger.debug)\ndef calculate_ttvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate TTVI (Thiam's Transformed Vegetation Index) from spectral bands.\n\n    TTVI applies an absolute value transformation to NDVI before the square root,\n    making it suitable for both positive and negative NDVI values.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - ndvi (float32): NDVI values (will be calculated if not present)\n            - nir, red (float32): Required if NDVI not present\n\n    Returns:\n        xr.DataArray: TTVI values with attributes:\n            - long_name: \"TTVI\"\n\n    Note:\n        Formula: TTVI = sqrt(|NDVI| + 0.5)\n\n        If NDVI is already in the dataset, it will be reused to avoid recalculation.\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_ttvi\n\n        ttvi = calculate_ttvi(optical)\n        ```\n\n    \"\"\"\n    ndvi = optical[\"ndvi\"] if \"ndvi\" in optical else calculate_ndvi(optical)\n    ttvi = np.sqrt(np.abs(ndvi) + 0.5)\n    ttvi = ttvi.assign_attrs({\"long_name\": \"TTVI\"}).rename(\"ttvi\")\n    return ttvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_tvi","title":"calculate_tvi","text":"<pre><code>calculate_tvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate TVI (Transformed Vegetation Index) from spectral bands.</p> <p>TVI applies a transformation to NDVI to enhance contrast and improve discrimination of vegetation conditions.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - ndvi (float32): NDVI values (will be calculated if not present) - nir, red (float32): Required if NDVI not present</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: TVI values with attributes: - long_name: \"TVI\"</p> </li> </ul> Note <p>Formula: TVI = sqrt(NDVI + 0.5)</p> <p>If NDVI is already in the dataset, it will be reused to avoid recalculation.</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_tvi\n\ntvi = calculate_tvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating TVI\", printer=logger.debug)\ndef calculate_tvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate TVI (Transformed Vegetation Index) from spectral bands.\n\n    TVI applies a transformation to NDVI to enhance contrast and improve discrimination\n    of vegetation conditions.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - ndvi (float32): NDVI values (will be calculated if not present)\n            - nir, red (float32): Required if NDVI not present\n\n    Returns:\n        xr.DataArray: TVI values with attributes:\n            - long_name: \"TVI\"\n\n    Note:\n        Formula: TVI = sqrt(NDVI + 0.5)\n\n        If NDVI is already in the dataset, it will be reused to avoid recalculation.\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_tvi\n\n        tvi = calculate_tvi(optical)\n        ```\n\n    \"\"\"\n    ndvi = optical[\"ndvi\"] if \"ndvi\" in optical else calculate_ndvi(optical)\n    tvi = np.sqrt(ndvi + 0.5)\n    tvi = tvi.assign_attrs({\"long_name\": \"TVI\"}).rename(\"tvi\")\n    return tvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_vari","title":"calculate_vari","text":"<pre><code>calculate_vari(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate VARI (Visible Atmospherically Resistant Index) from spectral bands.</p> <p>VARI uses only visible bands, designed to minimize atmospheric effects. Useful for RGB imagery without NIR band or for atmospheric correction validation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - green (float32): Green reflectance [0-1] - red (float32): Red reflectance [0-1] - blue (float32): Blue reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: VARI values with attributes: - long_name: \"VARI\"</p> </li> </ul> Note <p>Formula: VARI = (Green - Red) / (Green + Red - Blue)</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019. The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images. International Journal of Technology. Volume 10(7), pp. 1385-1394 https://doi.org/10.14716/ijtech.v10i7.3275</p> Example <pre><code>from darts_preprocessing import calculate_vari\n\nvari = calculate_vari(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating VARI\", printer=logger.debug)\ndef calculate_vari(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate VARI (Visible Atmospherically Resistant Index) from spectral bands.\n\n    VARI uses only visible bands, designed to minimize atmospheric effects. Useful for\n    RGB imagery without NIR band or for atmospheric correction validation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - green (float32): Green reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n            - blue (float32): Blue reflectance [0-1]\n\n    Returns:\n        xr.DataArray: VARI values with attributes:\n            - long_name: \"VARI\"\n\n    Note:\n        Formula: VARI = (Green - Red) / (Green + Red - Blue)\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019.\n        The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images.\n        International Journal of Technology. Volume 10(7), pp. 1385-1394\n        https://doi.org/10.14716/ijtech.v10i7.3275\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_vari\n\n        vari = calculate_vari(optical)\n        ```\n\n    \"\"\"\n    g = optical[\"green\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    b = optical[\"blue\"].clip(0, 1)\n    vari = (g - r) / (g + r - b)\n    vari = vari.assign_attrs({\"long_name\": \"VARI\"}).rename(\"vari\")\n    return vari\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_vdvi","title":"calculate_vdvi","text":"<pre><code>calculate_vdvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Alias for GLI (Green Leaf Index) from an xarray Dataset containing spectral bands.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>def calculate_vdvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Alias for GLI (Green Leaf Index) from an xarray Dataset containing spectral bands.\"\"\"  # noqa: DOC201\n    logger.warning(\"VDVI is an alias for GLI, using GLI calculation.\")\n    return calculate_gli(optical)\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_vector_ruggedness_measure","title":"calculate_vector_ruggedness_measure","text":"<pre><code>calculate_vector_ruggedness_measure(\n    arcticdem_ds: xarray.Dataset, neighborhood_size: int\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Vector Ruggedness Measure (VRM) from an ArcticDEM Dataset.</p> <p>VRM quantifies terrain ruggedness using vector analysis of slope and aspect, providing a measure independent of absolute elevation. Values range from 0 (smooth) to 1 (rugged).</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model - slope (float32): Slope in degrees (will be calculated if not present) - aspect (float32): Aspect in degrees (will be calculated if not present)</p> </li> <li> <code>neighborhood_size</code>               (<code>int</code>)           \u2013            <p>Neighborhood window size for VRM calculation. Can be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>vrm (float32): Vector Ruggedness Measure [0-1].</p> </li> <li> <p>long_name: \"Vector Ruggedness Measure\"</p> </li> <li>description: Documents neighborhood size used</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>VRM calculation:</p> <ol> <li>Converts slope and aspect to 3D unit vectors (x, y, z components)</li> <li>Sums vectors in the neighborhood window</li> <li>Calculates magnitude of resultant vector</li> <li>VRM = 1 - resultant magnitude</li> </ol> <p>Flat areas (aspect = -1) are handled by setting aspect to 0.</p> <p>Requires slope and aspect to be already calculated on the dataset.</p> References <p>Sappington, J.M., K.M. Longshore, and D.B. Thomson. 2007. Quantifying Landscape Ruggedness for Animal Habitat Analysis: A case Study Using Bighorn Sheep in the Mojave Desert. Journal of Wildlife Management. 71(5): 1419-1426.</p> Example <pre><code>from darts_preprocessing import (\n    calculate_slope, calculate_aspect,\n    calculate_vector_ruggedness_measure\n)\n\n# VRM requires slope and aspect\narcticdem = calculate_slope(arcticdem)\narcticdem = calculate_aspect(arcticdem)\narcticdem_with_vrm = calculate_vector_ruggedness_measure(\n    arcticdem_ds=arcticdem,\n    neighborhood_size=100\n)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating Vector Ruggedness Measure\", printer=logger.debug)\ndef calculate_vector_ruggedness_measure(arcticdem_ds: xr.Dataset, neighborhood_size: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Vector Ruggedness Measure (VRM) from an ArcticDEM Dataset.\n\n    VRM quantifies terrain ruggedness using vector analysis of slope and aspect, providing\n    a measure independent of absolute elevation. Values range from 0 (smooth) to 1 (rugged).\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n            - slope (float32): Slope in degrees (will be calculated if not present)\n            - aspect (float32): Aspect in degrees (will be calculated if not present)\n        neighborhood_size (int): Neighborhood window size for VRM calculation.\n            Can be specified as string with units (e.g., \"100m\" or \"10px\").\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - vrm (float32): Vector Ruggedness Measure [0-1].\n\n            - long_name: \"Vector Ruggedness Measure\"\n            - description: Documents neighborhood size used\n            - source: \"ArcticDEM\"\n\n    Note:\n        VRM calculation:\n\n        1. Converts slope and aspect to 3D unit vectors (x, y, z components)\n        2. Sums vectors in the neighborhood window\n        3. Calculates magnitude of resultant vector\n        4. VRM = 1 - resultant magnitude\n\n        Flat areas (aspect = -1) are handled by setting aspect to 0.\n\n        Requires slope and aspect to be already calculated on the dataset.\n\n    References:\n        Sappington, J.M., K.M. Longshore, and D.B. Thomson. 2007.\n        Quantifying Landscape Ruggedness for Animal Habitat Analysis: A case Study Using Bighorn Sheep\n        in the Mojave Desert. Journal of Wildlife Management. 71(5): 1419-1426.\n\n    Example:\n        ```python\n        from darts_preprocessing import (\n            calculate_slope, calculate_aspect,\n            calculate_vector_ruggedness_measure\n        )\n\n        # VRM requires slope and aspect\n        arcticdem = calculate_slope(arcticdem)\n        arcticdem = calculate_aspect(arcticdem)\n        arcticdem_with_vrm = calculate_vector_ruggedness_measure(\n            arcticdem_ds=arcticdem,\n            neighborhood_size=100\n        )\n        ```\n\n    \"\"\"\n    # Calculate slope and aspect\n    slope_rad = arcticdem_ds.slope * (np.pi / 180)  # Convert to radians\n    aspect_rad = arcticdem_ds.aspect * (np.pi / 180)  # Convert to radians\n\n    # Calculate x, y, and z components of unit vectors\n    xy = np.sin(slope_rad)\n    z = np.cos(slope_rad)\n\n    # Handle flat areas (where aspect = -1)\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        aspect_rad.variable._data = cp.where(aspect_rad.variable._data == -1, 0, aspect_rad.variable._data)\n        # aspect_rad = aspect_rad.copy(data=aspect_rad_raw)\n    else:\n        aspect_rad = xr.where(aspect_rad == -1, 0, aspect_rad)\n    x = np.sin(aspect_rad) * xy\n    y = np.cos(aspect_rad) * xy\n\n    # Get neighborhood_size in pixels\n    neighborhood_size: Distance = Distance.parse(neighborhood_size, arcticdem_ds.odc.geobox.resolution.x)\n    # Create convolution kernel for focal sum\n    kernel = np.ones((neighborhood_size.pixel, neighborhood_size.pixel), dtype=float) / neighborhood_size.pixel**2\n    kernel = convolution.custom_kernel(kernel)\n\n    # Change dtype of kernel to float32 since we don't need the precision and f32 is faster\n    kernel = kernel.astype(\"float32\")\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    logger.debug(f\"Calculating Vector Ruggedness Measure with square kernel of size {neighborhood_size} cells.\")\n\n    # Calculate sums of x, y, and z components in the neighborhood\n    x_sum = convolution.convolution_2d(x, kernel)\n    y_sum = convolution.convolution_2d(y, kernel)\n    z_sum = convolution.convolution_2d(z, kernel)\n\n    # Calculate the resultant vector magnitude\n    vrm = 1 - np.sqrt(x_sum**2 + y_sum**2 + z_sum**2)\n\n    vrm.attrs = {\n        \"long_name\": \"Vector Ruggedness Measure\",\n        \"units\": \"\",\n        \"description\": (\n            f\"Vector ruggedness measure calculated using a {neighborhood_size} neighborhood. \"\n            \"Values range from 0 (smooth) to 1 (most rugged).\"\n        ),\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"vrm\"] = vrm.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_vigreen","title":"calculate_vigreen","text":"<pre><code>calculate_vigreen(\n    optical: xarray.Dataset,\n) -&gt; xarray.DataArray\n</code></pre> <p>Alias for VIGREEN (Vegetation Index Green) from an xarray Dataset containing spectral bands.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>def calculate_vigreen(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Alias for VIGREEN (Vegetation Index Green) from an xarray Dataset containing spectral bands.\"\"\"  # noqa: DOC201\n    logger.warning(\"VIGREEN is an alias for GRVI, using GRVI calculation.\")\n    return calculate_grvi(optical)\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.preprocess_legacy_fast","title":"preprocess_legacy_fast","text":"<pre><code>preprocess_legacy_fast(\n    ds_optical: xarray.Dataset,\n    ds_arcticdem: xarray.Dataset,\n    ds_tcvis: xarray.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_utils.cuda.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>The main difference to preprocess_legacy is the new data concept of the arcticdem. Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw arcticdem data and calculates slope and relative elevation on the fly.</p> <p>Parameters:</p> <ul> <li> <code>ds_optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The TCVIS dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_utils.cuda.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/legacy.py</code> <pre><code>@stopwatch(\"Preprocessing\", printer=logger.debug)\ndef preprocess_legacy_fast(\n    ds_optical: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    The main difference to preprocess_legacy is the new data concept of the arcticdem.\n    Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw\n    arcticdem data and calculates slope and relative elevation on the fly.\n\n    Args:\n        ds_optical (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Move to GPU for faster calculations\n    ds_optical = move_to_device(ds_optical, device)\n    # Calculate NDVI\n    ds_optical[\"ndvi\"] = calculate_ndvi(ds_optical)\n    ds_optical = move_to_host(ds_optical)\n\n    # Reproject TCVIS to optical data\n    with stopwatch(\"Reprojecting TCVIS\", printer=logger.debug):\n        ds_tcvis = ds_tcvis.odc.reproject(ds_optical.odc.geobox, resampling=\"cubic\")\n\n    ds_optical[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_optical[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_optical[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    with stopwatch(\"Reprojecting ArcticDEM\", printer=logger.debug):\n        ds_arcticdem = ds_arcticdem.odc.reproject(ds_optical.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n    # Move to same device as optical\n    ds_arcticdem = move_to_device(ds_arcticdem, device)\n    ds_arcticdem = preprocess_legacy_arcticdem_fast(ds_arcticdem, tpi_outer_radius, tpi_inner_radius)\n    ds_arcticdem = move_to_host(ds_arcticdem)\n\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_optical.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_optical)\n\n    ds_optical[\"dem\"] = ds_arcticdem.dem\n    ds_optical[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_optical[\"slope\"] = ds_arcticdem.slope\n    ds_optical[\"arcticdem_data_mask\"] = ds_arcticdem.arcticdem_data_mask\n\n    return ds_optical\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.preprocess_v2","title":"preprocess_v2","text":"<pre><code>preprocess_v2(\n    ds_optical: xarray.Dataset,\n    ds_arcticdem: xarray.Dataset | None,\n    ds_tcvis: xarray.Dataset | None,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_utils.cuda.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess optical data with modern (DARTS v2) preprocessing steps.</p> <p>This function combines optical imagery with terrain (ArcticDEM) and temporal vegetation indices (TCVIS) to create a multi-source feature dataset for segmentation. All auxiliary data sources are reprojected and cropped to match the optical data's extent and resolution.</p> Processing steps <ol> <li>Calculate NDVI from optical bands</li> <li>If TCVIS provided: Reproject and merge Tasseled Cap trends</li> <li>If ArcticDEM provided: Calculate terrain features (TPI, slope, hillshade, aspect, curvature)    using solar geometry from optical data attributes</li> </ol> <p>Parameters:</p> <ul> <li> <code>ds_optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Optical imagery dataset (PlanetScope or Sentinel-2) containing: - Required variables: blue, green, red, nir (float32, reflectance values) - Required variables: quality_data_mask, valid_data_mask (uint8) - Required attributes: azimuth (float), elevation (float) for hillshade calculation</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset | None</code>)           \u2013            <p>ArcticDEM dataset containing 'dem' (float32) and 'arcticdem_data_mask' (uint8). If None, terrain features are skipped.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset | None</code>)           \u2013            <p>TCVIS dataset containing tc_brightness, tc_greenness, tc_wetness (float). If None, TCVIS features are skipped.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Outer radius for TPI calculation in meters. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Inner radius for TPI annulus kernel in meters. Set to 0 for circular kernel. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_utils.cuda.DEFAULT_DEVICE</code> )           \u2013            <p>Device for GPU-accelerated computations (NDVI, TPI, slope). Use \"cuda\" for first GPU, int for specific GPU, or \"cpu\". Defaults to \"cuda\" if available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Preprocessed dataset with all input optical variables plus:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Added from optical processing: - ndvi (float32): Normalized Difference Vegetation Index   Attributes: long_name=\"NDVI\"</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Added from TCVIS (if ds_tcvis provided): - tc_brightness (float): Tasseled Cap brightness trend - tc_greenness (float): Tasseled Cap greenness trend - tc_wetness (float): Tasseled Cap wetness trend</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Added from ArcticDEM (if ds_arcticdem provided): - dem (float32): Elevation in meters - relative_elevation (float32): Topographic Position Index (TPI)   Attributes: long_name=\"Topographic Position Index (TPI)\" - slope (float32): Slope in degrees [0-90]   Attributes: long_name=\"Slope\" - hillshade (uint8): Hillshade values [0-255]   Attributes: long_name=\"Hillshade\" - aspect (float32): Aspect in degrees [0-360]   Attributes: long_name=\"Aspect\" - curvature (float32): Surface curvature   Attributes: long_name=\"Curvature\" - arcticdem_data_mask (uint8): DEM validity mask</p> </li> </ul> Note <p>Attribute usage: - <code>azimuth</code> attribute from ds_optical: Used for hillshade calculation (solar azimuth angle).   Falls back to 225\u00b0 if missing or invalid. - <code>elevation</code> attribute from ds_optical: Used for hillshade calculation (solar elevation angle).   Falls back to 25\u00b0 if missing or invalid.</p> <p>Processing behavior: - If both ds_tcvis and ds_arcticdem are None, only NDVI is calculated. - ArcticDEM is buffered by tpi_outer_radius before reprojection to avoid edge effects,   then cropped back to optical extent after terrain feature calculation. - Reprojection uses cubic resampling for smooth terrain features. - GPU acceleration (if device=\"cuda\") significantly speeds up TPI and slope calculations.</p> Example <p>Complete preprocessing with all data sources:</p> <pre><code>from darts_preprocessing import preprocess_v2\nfrom darts_acquisition import load_cdse_s2_sr_scene, load_arcticdem, load_tcvis\n\n# Load optical data\noptical = load_cdse_s2_sr_scene(s2_scene_id, ...)\n\n# Load auxiliary data\narcticdem = load_arcticdem(optical.odc.geobox, ...)\ntcvis = load_tcvis(optical.odc.geobox, ...)\n\n# Preprocess\npreprocessed = preprocess_v2(\n    ds_optical=optical,\n    ds_arcticdem=arcticdem,\n    ds_tcvis=tcvis,\n    tpi_outer_radius=100,\n    tpi_inner_radius=0,\n    device=\"cuda\"\n)\n\n# Result contains: blue, green, red, nir, ndvi, tc_brightness, tc_greenness,\n# tc_wetness, dem, relative_elevation, slope, hillshade, aspect, curvature\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/v2.py</code> <pre><code>@stopwatch(\"Preprocessing\", printer=logger.debug)\ndef preprocess_v2(\n    ds_optical: xr.Dataset,\n    ds_arcticdem: xr.Dataset | None,\n    ds_tcvis: xr.Dataset | None,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with modern (DARTS v2) preprocessing steps.\n\n    This function combines optical imagery with terrain (ArcticDEM) and temporal vegetation\n    indices (TCVIS) to create a multi-source feature dataset for segmentation. All auxiliary\n    data sources are reprojected and cropped to match the optical data's extent and resolution.\n\n    Processing steps:\n        1. Calculate NDVI from optical bands\n        2. If TCVIS provided: Reproject and merge Tasseled Cap trends\n        3. If ArcticDEM provided: Calculate terrain features (TPI, slope, hillshade, aspect, curvature)\n           using solar geometry from optical data attributes\n\n    Args:\n        ds_optical (xr.Dataset): Optical imagery dataset (PlanetScope or Sentinel-2) containing:\n            - Required variables: blue, green, red, nir (float32, reflectance values)\n            - Required variables: quality_data_mask, valid_data_mask (uint8)\n            - Required attributes: azimuth (float), elevation (float) for hillshade calculation\n        ds_arcticdem (xr.Dataset | None): ArcticDEM dataset containing 'dem' (float32) and\n            'arcticdem_data_mask' (uint8). If None, terrain features are skipped.\n        ds_tcvis (xr.Dataset | None): TCVIS dataset containing tc_brightness, tc_greenness,\n            tc_wetness (float). If None, TCVIS features are skipped.\n        tpi_outer_radius (int, optional): Outer radius for TPI calculation in meters.\n            Defaults to 100m.\n        tpi_inner_radius (int, optional): Inner radius for TPI annulus kernel in meters.\n            Set to 0 for circular kernel. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): Device for GPU-accelerated computations\n            (NDVI, TPI, slope). Use \"cuda\" for first GPU, int for specific GPU, or \"cpu\".\n            Defaults to \"cuda\" if available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: Preprocessed dataset with all input optical variables plus:\n\n        Added from optical processing:\n            - ndvi (float32): Normalized Difference Vegetation Index\n              Attributes: long_name=\"NDVI\"\n\n        Added from TCVIS (if ds_tcvis provided):\n            - tc_brightness (float): Tasseled Cap brightness trend\n            - tc_greenness (float): Tasseled Cap greenness trend\n            - tc_wetness (float): Tasseled Cap wetness trend\n\n        Added from ArcticDEM (if ds_arcticdem provided):\n            - dem (float32): Elevation in meters\n            - relative_elevation (float32): Topographic Position Index (TPI)\n              Attributes: long_name=\"Topographic Position Index (TPI)\"\n            - slope (float32): Slope in degrees [0-90]\n              Attributes: long_name=\"Slope\"\n            - hillshade (uint8): Hillshade values [0-255]\n              Attributes: long_name=\"Hillshade\"\n            - aspect (float32): Aspect in degrees [0-360]\n              Attributes: long_name=\"Aspect\"\n            - curvature (float32): Surface curvature\n              Attributes: long_name=\"Curvature\"\n            - arcticdem_data_mask (uint8): DEM validity mask\n\n    Note:\n        Attribute usage:\n        - `azimuth` attribute from ds_optical: Used for hillshade calculation (solar azimuth angle).\n          Falls back to 225\u00b0 if missing or invalid.\n        - `elevation` attribute from ds_optical: Used for hillshade calculation (solar elevation angle).\n          Falls back to 25\u00b0 if missing or invalid.\n\n        Processing behavior:\n        - If both ds_tcvis and ds_arcticdem are None, only NDVI is calculated.\n        - ArcticDEM is buffered by tpi_outer_radius before reprojection to avoid edge effects,\n          then cropped back to optical extent after terrain feature calculation.\n        - Reprojection uses cubic resampling for smooth terrain features.\n        - GPU acceleration (if device=\"cuda\") significantly speeds up TPI and slope calculations.\n\n    Example:\n        Complete preprocessing with all data sources:\n\n        ```python\n        from darts_preprocessing import preprocess_v2\n        from darts_acquisition import load_cdse_s2_sr_scene, load_arcticdem, load_tcvis\n\n        # Load optical data\n        optical = load_cdse_s2_sr_scene(s2_scene_id, ...)\n\n        # Load auxiliary data\n        arcticdem = load_arcticdem(optical.odc.geobox, ...)\n        tcvis = load_tcvis(optical.odc.geobox, ...)\n\n        # Preprocess\n        preprocessed = preprocess_v2(\n            ds_optical=optical,\n            ds_arcticdem=arcticdem,\n            ds_tcvis=tcvis,\n            tpi_outer_radius=100,\n            tpi_inner_radius=0,\n            device=\"cuda\"\n        )\n\n        # Result contains: blue, green, red, nir, ndvi, tc_brightness, tc_greenness,\n        # tc_wetness, dem, relative_elevation, slope, hillshade, aspect, curvature\n        ```\n\n    \"\"\"\n    # Move to GPU for faster calculations\n    ds_optical = move_to_device(ds_optical, device)\n    # Calculate NDVI\n    ds_optical[\"ndvi\"] = calculate_ndvi(ds_optical)\n    ds_optical = move_to_host(ds_optical)\n\n    if ds_tcvis is None and ds_arcticdem is None:\n        logger.debug(\"No auxiliary data provided. Only NDVI was calculated.\")\n        return ds_optical\n\n    if ds_tcvis is not None:\n        # Reproject TCVIS to optical data\n        with stopwatch(\"Reprojecting TCVIS\", printer=logger.debug):\n            # *: Reprojecting this way will not alter the datatype of the data!\n            # Should be uint8 before and after reprojection.\n            ds_tcvis = ds_tcvis.odc.reproject(ds_optical.odc.geobox, resampling=\"cubic\")\n\n        # !: Reprojecting with f64 coordinates and values behind the decimal point can result in a coordinate missmatch:\n        # E.g. ds_optical has x coordinates [2.123, ...] then is can happen that the\n        # reprojected ds_tcvis coordinates are [2.12300001, ...]\n        # This results is all-nan assigments later when adding the variables of the reprojected dataset to the original\n        assert (ds_optical.x == ds_tcvis.x).all(), \"x coordinates do not match! See code comment above\"\n        assert (ds_optical.y == ds_tcvis.y).all(), \"y coordinates do not match! See code comment above\"\n\n        # ?: Do ds_tcvis and ds_optical now share the same memory on the GPU?\n        #  or do I need to delete ds_tcvis to free memory?\n        # Same question for ArcticDEM\n        ds_optical[\"tc_brightness\"] = ds_tcvis.tc_brightness\n        ds_optical[\"tc_greenness\"] = ds_tcvis.tc_greenness\n        ds_optical[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    if ds_arcticdem is not None:\n        # Calculate TPI and slope from ArcticDEM\n        with stopwatch(\"Reprojecting ArcticDEM\", printer=logger.debug):\n            ds_arcticdem = ds_arcticdem.odc.reproject(\n                ds_optical.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\"\n            )\n        # Move to same device as optical\n        ds_arcticdem = move_to_device(ds_arcticdem, device)\n\n        assert (ds_optical.x == ds_arcticdem.x).all(), \"x coordinates do not match! See code comment above\"\n        assert (ds_optical.y == ds_arcticdem.y).all(), \"y coordinates do not match! See code comment above\"\n\n        azimuth, angle_altitude = get_azimuth_and_elevation(ds_optical)\n        ds_arcticdem = preprocess_arcticdem(\n            ds_arcticdem,\n            tpi_outer_radius,\n            tpi_inner_radius,\n            azimuth,\n            angle_altitude,\n        )\n        ds_arcticdem = move_to_host(ds_arcticdem)\n\n        # TODO: Check if crop can be done with apply_mask = False\n        # -&gt; Then the type conversion of the arcticdem data mask would not be necessary anymore\n        # -&gt; And this would also allow to keep the data on the GPU\n        ds_arcticdem = ds_arcticdem.odc.crop(ds_optical.odc.geobox.extent)\n        # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n        # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n        ds_arcticdem[\"x\"] = ds_optical.x\n        ds_arcticdem[\"y\"] = ds_optical.y\n\n        ds_optical[\"dem\"] = ds_arcticdem.dem\n        ds_optical[\"relative_elevation\"] = ds_arcticdem.tpi\n        ds_optical[\"slope\"] = ds_arcticdem.slope\n        ds_optical[\"hillshade\"] = ds_arcticdem.hillshade\n        ds_optical[\"aspect\"] = ds_arcticdem.aspect\n        ds_optical[\"curvature\"] = ds_arcticdem.curvature\n        ds_optical[\"arcticdem_data_mask\"] = ds_arcticdem.arcticdem_data_mask.astype(\"uint8\")\n\n    return ds_optical\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/","title":"engineering","text":""},{"location":"reference/darts_preprocessing/engineering/#darts_preprocessing.engineering","title":"darts_preprocessing.engineering","text":"<p>Engineered (Pre-Calculated) features.</p>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/","title":"arcticdem","text":""},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem","title":"darts_preprocessing.engineering.arcticdem","text":"<p>Computation of ArcticDEM derived products.</p>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.Distance","title":"Distance  <code>dataclass</code>","text":"<pre><code>Distance(pixel: int, meter: float)\n</code></pre> <p>Convenience class to represent a distance in pixels and meters.</p>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.Distance.meter","title":"meter  <code>instance-attribute</code>","text":"<pre><code>meter: float\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.Distance.pixel","title":"pixel  <code>instance-attribute</code>","text":"<pre><code>pixel: int\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.Distance.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>def __repr__(self):  # noqa: D105\n    return f\"{self.pixel}px ({self.meter}m)\"\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.Distance.parse","title":"parse  <code>classmethod</code>","text":"<pre><code>parse(\n    v: int | float | str, res: float\n) -&gt; darts_preprocessing.engineering.arcticdem.Distance\n</code></pre> <p>Parse a distance from a string or numeric value.</p> <p>If the input is a string, it can be in the format of \"10px\" or \"10m\". If it is a numeric value, it is interpreted as meters and converted to pixels based on the resolution.</p> <p>Parameters:</p> <ul> <li> <code>v</code>               (<code>int | float | str</code>)           \u2013            <p>The input distance value.</p> </li> <li> <code>res</code>               (<code>float</code>)           \u2013            <p>The resolution in meters per pixel.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input distance is negative.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input distance is not a valid string format.</p> </li> <li> <code>TypeError</code>             \u2013            <p>If the input distance is not a string, int, or float.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Distance</code> (              <code>darts_preprocessing.engineering.arcticdem.Distance</code> )          \u2013            <p>The parsed distance in pixels and meters.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@classmethod\ndef parse(cls, v: int | float | str, res: float) -&gt; \"Distance\":\n    \"\"\"Parse a distance from a string or numeric value.\n\n    If the input is a string, it can be in the format of \"10px\" or \"10m\".\n    If it is a numeric value, it is interpreted as meters and converted to pixels based on the resolution.\n\n    Args:\n        v (int | float | str): The input distance value.\n        res (float): The resolution in meters per pixel.\n\n    Raises:\n        ValueError: If the input distance is negative.\n        ValueError: If the input distance is not a valid string format.\n        TypeError: If the input distance is not a string, int, or float.\n\n    Returns:\n        Distance: The parsed distance in pixels and meters.\n\n    \"\"\"\n    if isinstance(v, str):\n        if v.endswith(\"px\"):\n            pixel = int(v[:-2])\n            meter = pixel * res\n        elif v.endswith(\"m\"):\n            meter = float(v[:-1])\n            pixel = ceil(meter / res)\n        else:\n            raise ValueError(f\"Invalid distance format: {v}\")\n    elif isinstance(v, (int, float)):\n        if v &lt; 0:\n            raise ValueError(\"Distance cannot be negative\")\n        pixel = ceil(v / res)\n        meter = pixel * res\n    else:\n        raise TypeError(f\"Invalid type for distance: {type(v)}\")\n    return cls(pixel=pixel, meter=meter)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_aspect","title":"calculate_aspect","text":"<pre><code>calculate_aspect(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate aspect (compass direction) of the terrain surface from an ArcticDEM Dataset.</p> <p>Aspect indicates the downslope direction of the maximum rate of change in elevation.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>aspect (float32): Aspect in degrees clockwise from north [0-360], or -1 for flat areas.</p> </li> <li> <p>long_name: \"Aspect\"</p> </li> <li>units: \"degrees\"</li> <li>description: Compass direction of slope</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Aspect values:</p> <ul> <li>0\u00b0 or 360\u00b0: North-facing</li> <li>90\u00b0: East-facing</li> <li>180\u00b0: South-facing</li> <li>270\u00b0: West-facing</li> <li>-1: Flat (no dominant direction)</li> </ul> Example <pre><code>from darts_preprocessing import calculate_aspect\n\narcticdem_with_aspect = calculate_aspect(arcticdem_ds)\n\n# Identify south-facing slopes (135-225 degrees)\nsouth_facing = (arcticdem_with_aspect.aspect &gt; 135) &amp; (arcticdem_with_aspect.aspect &lt; 225)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating aspect\", printer=logger.debug)\ndef calculate_aspect(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate aspect (compass direction) of the terrain surface from an ArcticDEM Dataset.\n\n    Aspect indicates the downslope direction of the maximum rate of change in elevation.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - aspect (float32): Aspect in degrees clockwise from north [0-360], or -1 for flat areas.\n\n            - long_name: \"Aspect\"\n            - units: \"degrees\"\n            - description: Compass direction of slope\n            - source: \"ArcticDEM\"\n\n    Note:\n        Aspect values:\n\n        - 0\u00b0 or 360\u00b0: North-facing\n        - 90\u00b0: East-facing\n        - 180\u00b0: South-facing\n        - 270\u00b0: West-facing\n        - -1: Flat (no dominant direction)\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_aspect\n\n        arcticdem_with_aspect = calculate_aspect(arcticdem_ds)\n\n        # Identify south-facing slopes (135-225 degrees)\n        south_facing = (arcticdem_with_aspect.aspect &gt; 135) &amp; (arcticdem_with_aspect.aspect &lt; 225)\n        ```\n\n    \"\"\"\n    aspect_deg = aspect(arcticdem_ds.dem)\n\n    # Aspect is always calculated in the projection - thus \"north\" is rather an \"up\"\n    # To get the true north, we need to correct the aspect based on the coordinates\n    x = arcticdem_ds.x.expand_dims({\"y\": arcticdem_ds.y})\n    y = arcticdem_ds.y.expand_dims({\"x\": arcticdem_ds.x})\n    if arcticdem_ds.cupy.is_cupy:\n        x = cp.asarray(x)\n        y = cp.asarray(y)\n        correction_offset = cp.arctan2(x, y) * (180 / np.pi) + 90\n    else:\n        correction_offset = np.arctan2(x, y) * (180 / np.pi) + 90\n    aspect_deg = (aspect_deg + correction_offset) % 360\n\n    aspect_deg.attrs = {\n        \"long_name\": \"Aspect\",\n        \"units\": \"degrees\",\n        \"description\": \"The compass direction that the slope faces, in degrees clockwise from north.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"aspect\"] = aspect_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_curvature","title":"calculate_curvature","text":"<pre><code>calculate_curvature(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate curvature of the terrain surface from an ArcticDEM Dataset.</p> <p>Curvature measures the rate of change of slope, indicating terrain convexity or concavity.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>curvature (float32): Curvature values.</p> </li> <li> <p>long_name: \"Curvature\"</p> </li> <li>description: Rate of change of slope</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Curvature interpretation:</p> <ul> <li>Positive values: Convex terrain (hills, ridges)</li> <li>Negative values: Concave terrain (valleys, depressions)</li> <li>Near zero: Planar terrain</li> </ul> Example <pre><code>from darts_preprocessing import calculate_curvature\n\narcticdem_with_curv = calculate_curvature(arcticdem_ds)\n\n# Identify ridges (convex areas)\nridges = arcticdem_with_curv.curvature &gt; 0.1\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating curvature\", printer=logger.debug)\ndef calculate_curvature(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate curvature of the terrain surface from an ArcticDEM Dataset.\n\n    Curvature measures the rate of change of slope, indicating terrain convexity or concavity.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - curvature (float32): Curvature values.\n\n            - long_name: \"Curvature\"\n            - description: Rate of change of slope\n            - source: \"ArcticDEM\"\n\n    Note:\n        Curvature interpretation:\n\n        - Positive values: Convex terrain (hills, ridges)\n        - Negative values: Concave terrain (valleys, depressions)\n        - Near zero: Planar terrain\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_curvature\n\n        arcticdem_with_curv = calculate_curvature(arcticdem_ds)\n\n        # Identify ridges (convex areas)\n        ridges = arcticdem_with_curv.curvature &gt; 0.1\n        ```\n\n    \"\"\"\n    curvature_da = curvature(arcticdem_ds.dem)\n    curvature_da.attrs = {\n        \"long_name\": \"Curvature\",\n        \"units\": \"\",\n        \"description\": \"The curvature of the terrain surface.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"curvature\"] = curvature_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_dissection_index","title":"calculate_dissection_index","text":"<pre><code>calculate_dissection_index(\n    arcticdem_ds: xarray.Dataset, neighborhood_size: int\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Dissection Index (DI) from an ArcticDEM Dataset.</p> <p>DI measures the degree to which a landscape has been cut by valleys and ravines. Values range from 0 (smooth, undissected) to 1 (highly dissected).</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> <li> <code>neighborhood_size</code>               (<code>int</code>)           \u2013            <p>Neighborhood window size for DI calculation. Can be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>di (float32): Dissection Index [0-1].</p> </li> <li> <p>long_name: \"Dissection Index\"</p> </li> <li>description: Documents neighborhood size used</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>The dissection index quantifies landscape dissection by comparing elevation ranges within the neighborhood window. Higher values indicate more deeply incised terrain with greater vertical relief.</p> <p>The neighborhood_size parameter is converted to pixels based on DEM resolution.</p> Example <pre><code>from darts_preprocessing import calculate_dissection_index\n\n# Calculate DI with 100m neighborhood\narcticdem_with_di = calculate_dissection_index(\n    arcticdem_ds=arcticdem,\n    neighborhood_size=100\n)\n\n# Identify highly dissected terrain\ndissected = arcticdem_with_di.di &gt; 0.5\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating Dissection Index\", printer=logger.debug)\ndef calculate_dissection_index(arcticdem_ds: xr.Dataset, neighborhood_size: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Dissection Index (DI) from an ArcticDEM Dataset.\n\n    DI measures the degree to which a landscape has been cut by valleys and ravines.\n    Values range from 0 (smooth, undissected) to 1 (highly dissected).\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n        neighborhood_size (int): Neighborhood window size for DI calculation.\n            Can be specified as string with units (e.g., \"100m\" or \"10px\").\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - di (float32): Dissection Index [0-1].\n\n            - long_name: \"Dissection Index\"\n            - description: Documents neighborhood size used\n            - source: \"ArcticDEM\"\n\n    Note:\n        The dissection index quantifies landscape dissection by comparing elevation\n        ranges within the neighborhood window. Higher values indicate more deeply\n        incised terrain with greater vertical relief.\n\n        The neighborhood_size parameter is converted to pixels based on DEM resolution.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_dissection_index\n\n        # Calculate DI with 100m neighborhood\n        arcticdem_with_di = calculate_dissection_index(\n            arcticdem_ds=arcticdem,\n            neighborhood_size=100\n        )\n\n        # Identify highly dissected terrain\n        dissected = arcticdem_with_di.di &gt; 0.5\n        ```\n\n    \"\"\"\n    # Get neighborhood_size in pixels\n    neighborhood_size: Distance = Distance.parse(neighborhood_size, arcticdem_ds.odc.geobox.resolution.x)\n\n    di = dissection_index(arcticdem_ds.dem, window_size=neighborhood_size.pixel)\n\n    di.attrs = {\n        \"long_name\": \"Dissection Index\",\n        \"units\": \"\",\n        \"description\": (\n            f\"Dissection index calculated using a {neighborhood_size} neighborhood. \"\n            \"Values range from 0 (smooth) to 1 (most rugged).\"\n        ),\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"di\"] = di.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_hillshade","title":"calculate_hillshade","text":"<pre><code>calculate_hillshade(\n    arcticdem_ds: xarray.Dataset,\n    azimuth: int = 225,\n    angle_altitude: int = 25,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate hillshade of the terrain surface from an ArcticDEM Dataset.</p> <p>Hillshade simulates illumination of terrain from a specified sun position, useful for visualization and terrain analysis.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> <li> <code>azimuth</code>               (<code>int</code>, default:                   <code>225</code> )           \u2013            <p>Light source azimuth in degrees clockwise from north [0-360]. Defaults to 225 (southwest).</p> </li> <li> <code>angle_altitude</code>               (<code>int</code>, default:                   <code>25</code> )           \u2013            <p>Light source altitude angle in degrees above horizon [0-90]. Defaults to 25.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>hillshade (float32): Illumination values [0-255], where 0 is shadow and 255 is fully lit.</p> </li> <li> <p>long_name: \"Hillshade\"</p> </li> <li>description: Documents azimuth and angle_altitude used</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Common azimuth/altitude combinations:</p> <ul> <li>315\u00b0/45\u00b0: Classic northwest illumination (default for many GIS applications)</li> <li>225\u00b0/25\u00b0: Southwest with low sun (better for visualizing subtle features)</li> </ul> <p>The hillshade calculation accounts for both slope and aspect of the terrain.</p> Example <pre><code>from darts_preprocessing import calculate_hillshade\n\n# Default southwest illumination\narcticdem_with_hs = calculate_hillshade(arcticdem_ds)\n\n# Custom sun position\narcticdem_custom = calculate_hillshade(\n    arcticdem_ds,\n    azimuth=315,\n    angle_altitude=45\n)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating hillshade\", printer=logger.debug, print_kwargs=[\"azimuth\", \"angle_altitude\"])\ndef calculate_hillshade(arcticdem_ds: xr.Dataset, azimuth: int = 225, angle_altitude: int = 25) -&gt; xr.Dataset:\n    \"\"\"Calculate hillshade of the terrain surface from an ArcticDEM Dataset.\n\n    Hillshade simulates illumination of terrain from a specified sun position, useful\n    for visualization and terrain analysis.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n        azimuth (int, optional): Light source azimuth in degrees clockwise from north [0-360].\n            Defaults to 225 (southwest).\n        angle_altitude (int, optional): Light source altitude angle in degrees above horizon [0-90].\n            Defaults to 25.\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - hillshade (float32): Illumination values [0-255], where 0 is shadow and 255 is fully lit.\n\n            - long_name: \"Hillshade\"\n            - description: Documents azimuth and angle_altitude used\n            - source: \"ArcticDEM\"\n\n    Note:\n        Common azimuth/altitude combinations:\n\n        - 315\u00b0/45\u00b0: Classic northwest illumination (default for many GIS applications)\n        - 225\u00b0/25\u00b0: Southwest with low sun (better for visualizing subtle features)\n\n        The hillshade calculation accounts for both slope and aspect of the terrain.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_hillshade\n\n        # Default southwest illumination\n        arcticdem_with_hs = calculate_hillshade(arcticdem_ds)\n\n        # Custom sun position\n        arcticdem_custom = calculate_hillshade(\n            arcticdem_ds,\n            azimuth=315,\n            angle_altitude=45\n        )\n        ```\n\n    \"\"\"\n    x, y = arcticdem_ds.x.mean().item(), arcticdem_ds.y.mean().item()\n    correction_offset = np.arctan2(x, y) * (180 / np.pi) + 90\n    azimuth_corrected = (azimuth - correction_offset + 360) % 360\n\n    hillshade_da = hillshade(arcticdem_ds.dem, azimuth=azimuth_corrected, angle_altitude=angle_altitude)\n    hillshade_da.attrs = {\n        \"long_name\": \"Hillshade\",\n        \"units\": \"\",\n        \"description\": f\"The hillshade based on azimuth {azimuth} and angle_altitude {angle_altitude}.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"hillshade\"] = hillshade_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_slope","title":"calculate_slope","text":"<pre><code>calculate_slope(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Slope represents the rate of change of elevation, indicating terrain steepness.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>slope (float32): Slope in degrees [0-90].</p> </li> <li> <p>long_name: \"Slope\"</p> </li> <li>units: \"degrees\"</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Slope is calculated using finite difference methods on the DEM. Values approaching 90\u00b0 indicate near-vertical terrain.</p> Example <pre><code>from darts_preprocessing import calculate_slope\n\narcticdem_with_slope = calculate_slope(arcticdem_ds)\n\n# Mask steep terrain\nsteep_areas = arcticdem_with_slope.slope &gt; 30\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate slope of the terrain surface from an ArcticDEM Dataset.\n\n    Slope represents the rate of change of elevation, indicating terrain steepness.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - slope (float32): Slope in degrees [0-90].\n\n            - long_name: \"Slope\"\n            - units: \"degrees\"\n            - source: \"ArcticDEM\"\n\n    Note:\n        Slope is calculated using finite difference methods on the DEM.\n        Values approaching 90\u00b0 indicate near-vertical terrain.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_slope\n\n        arcticdem_with_slope = calculate_slope(arcticdem_ds)\n\n        # Mask steep terrain\n        steep_areas = arcticdem_with_slope.slope &gt; 30\n        ```\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_terrain_ruggedness_index","title":"calculate_terrain_ruggedness_index","text":"<pre><code>calculate_terrain_ruggedness_index(\n    arcticdem_ds: xarray.Dataset, neighborhood_size: int\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Terrain Ruggedness Index (TRI) from an ArcticDEM Dataset.</p> <p>TRI quantifies topographic heterogeneity by measuring elevation differences between a cell and its surrounding cells. Higher values indicate more rugged terrain.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> <li> <code>neighborhood_size</code>               (<code>int</code>)           \u2013            <p>Neighborhood window size for TRI calculation. Can be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>tri (float32): Terrain Ruggedness Index in meters.</p> </li> <li> <p>long_name: \"Terrain Ruggedness Index\"</p> </li> <li>units: \"m\"</li> <li>description: Documents kernel size used</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>TRI methodology from Riley et al (1999):</p> <ol> <li>Measures elevation difference from center cell to 8 surrounding cells</li> <li>Squares and averages these differences</li> <li>Takes square root for final TRI value</li> </ol> <p>The neighborhood_size parameter controls the kernel size. A square kernel is used, with the actual size rounded to the nearest pixel based on DEM resolution.</p> References <p>Riley, S.J., DeGloria, S.D., Elliot, R., 1999. A Terrain Ruggedness Index That Quantifies Topographic Heterogeneity. Intermountain Journal of Sciences, vol. 5, No. 1-4, pp. 23-27.</p> Example <pre><code>from darts_preprocessing import calculate_terrain_ruggedness_index\n\n# Calculate TRI with 100m neighborhood\narcticdem_with_tri = calculate_terrain_ruggedness_index(\n    arcticdem_ds=arcticdem,\n    neighborhood_size=100\n)\n\n# Identify highly rugged terrain\nrugged = arcticdem_with_tri.tri &gt; 10\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating TRI\", printer=logger.debug)\ndef calculate_terrain_ruggedness_index(arcticdem_ds: xr.Dataset, neighborhood_size: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Terrain Ruggedness Index (TRI) from an ArcticDEM Dataset.\n\n    TRI quantifies topographic heterogeneity by measuring elevation differences between\n    a cell and its surrounding cells. Higher values indicate more rugged terrain.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n        neighborhood_size (int): Neighborhood window size for TRI calculation.\n            Can be specified as string with units (e.g., \"100m\" or \"10px\").\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - tri (float32): Terrain Ruggedness Index in meters.\n\n            - long_name: \"Terrain Ruggedness Index\"\n            - units: \"m\"\n            - description: Documents kernel size used\n            - source: \"ArcticDEM\"\n\n    Note:\n        TRI methodology from Riley et al (1999):\n\n        1. Measures elevation difference from center cell to 8 surrounding cells\n        2. Squares and averages these differences\n        3. Takes square root for final TRI value\n\n        The neighborhood_size parameter controls the kernel size. A square kernel is used,\n        with the actual size rounded to the nearest pixel based on DEM resolution.\n\n    References:\n        Riley, S.J., DeGloria, S.D., Elliot, R., 1999.\n        A Terrain Ruggedness Index That Quantifies Topographic Heterogeneity.\n        Intermountain Journal of Sciences, vol. 5, No. 1-4, pp. 23-27.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_terrain_ruggedness_index\n\n        # Calculate TRI with 100m neighborhood\n        arcticdem_with_tri = calculate_terrain_ruggedness_index(\n            arcticdem_ds=arcticdem,\n            neighborhood_size=100\n        )\n\n        # Identify highly rugged terrain\n        rugged = arcticdem_with_tri.tri &gt; 10\n        ```\n\n    \"\"\"\n    cellsize_x, _cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)\n\n    neighborhood_size: Distance = Distance.parse(neighborhood_size, cellsize_x)\n\n    kernel = np.ones((neighborhood_size.pixel, neighborhood_size.pixel), dtype=float)\n    kernel[neighborhood_size.pixel // 2, neighborhood_size.pixel // 2] = 0  # Set the center cell to 0\n    kernel = convolution.custom_kernel(kernel)\n    logger.debug(f\"Calculating Terrain Ruggedness Index with square kernel of radius {neighborhood_size} cells.\")\n\n    # Change dtype of kernel to float32 since we don't need the precision and f32 is faster\n    kernel = kernel.astype(\"float32\")\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    # Kernel compute of TRI as described here:\n    # https://sites.utexas.edu/utarima/files/2024/02/terrain_roughness_index.pdf\n    dem_squared = arcticdem_ds.dem**2\n    focal_sum = convolution.convolution_2d(arcticdem_ds.dem, kernel)\n    focal_sum_squared = convolution.convolution_2d(dem_squared, kernel)\n    tri = np.sqrt((kernel.size - 1) * dem_squared - 2 * arcticdem_ds.dem * focal_sum + focal_sum_squared)\n\n    tri.attrs = {\n        \"long_name\": \"Terrain Ruggedness Index\",\n        \"units\": \"m\",\n        \"description\": (\n            \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n            f\" cells within a square kernel of radius {neighborhood_size} cells.\"\n        ),\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"tri\"] = tri.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_topographic_position_index","title":"calculate_topographic_position_index","text":"<pre><code>calculate_topographic_position_index(\n    arcticdem_ds: xarray.Dataset,\n    outer_radius: int,\n    inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>TPI measures the relative topographic position of a point by comparing its elevation to the mean elevation of the surrounding neighborhood. Positive values indicate higher positions (ridges), negative values indicate lower positions (valleys).</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable (float32).</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the neighborhood in meters. Can also be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in meters. If &gt; 0, creates an annulus (ring) instead of a circle. Set to 0 for a circular kernel. Can also be specified as string with units (e.g., \"50m\" or \"5px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with a new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>tpi (float32): Topographic Position Index values.</p> </li> <li> <p>long_name: \"Topographic Position Index (TPI)\"</p> </li> <li>description: Details about the kernel used</li> </ul> </li> </ul> Note <p>Kernel shape combinations:</p> <ul> <li>inner_radius=0: Circular kernel comparing each cell to all neighbors within outer_radius</li> <li>inner_radius&gt;0: Annulus kernel comparing each cell to neighbors in a ring between   inner_radius and outer_radius. Useful for multi-scale terrain analysis.</li> </ul> <p>The actual radii used are rounded to the nearest pixel based on the DEM resolution.</p> Example <p>Calculate TPI with circular and annulus kernels:</p> <pre><code>from darts_preprocessing import calculate_topographic_position_index\n\n# Circular kernel (100m radius)\narcticdem_with_tpi = calculate_topographic_position_index(\n    arcticdem_ds=arcticdem,\n    outer_radius=100,\n    inner_radius=0\n)\n\n# Annulus kernel (50-100m ring)\narcticdem_multi_scale = calculate_topographic_position_index(\n    arcticdem_ds=arcticdem,\n    outer_radius=100,\n    inner_radius=50\n)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    TPI measures the relative topographic position of a point by comparing its elevation to\n    the mean elevation of the surrounding neighborhood. Positive values indicate higher\n    positions (ridges), negative values indicate lower positions (valleys).\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable (float32).\n        outer_radius (int): The outer radius of the neighborhood in meters.\n            Can also be specified as string with units (e.g., \"100m\" or \"10px\").\n        inner_radius (int): The inner radius of the annulus kernel in meters.\n            If &gt; 0, creates an annulus (ring) instead of a circle. Set to 0 for a circular kernel.\n            Can also be specified as string with units (e.g., \"50m\" or \"5px\").\n\n    Returns:\n        xr.Dataset: The input Dataset with a new data variable added:\n\n        - tpi (float32): Topographic Position Index values.\n\n            - long_name: \"Topographic Position Index (TPI)\"\n            - description: Details about the kernel used\n\n    Note:\n        Kernel shape combinations:\n\n        - inner_radius=0: Circular kernel comparing each cell to all neighbors within outer_radius\n        - inner_radius&gt;0: Annulus kernel comparing each cell to neighbors in a ring between\n          inner_radius and outer_radius. Useful for multi-scale terrain analysis.\n\n        The actual radii used are rounded to the nearest pixel based on the DEM resolution.\n\n    Example:\n        Calculate TPI with circular and annulus kernels:\n\n        ```python\n        from darts_preprocessing import calculate_topographic_position_index\n\n        # Circular kernel (100m radius)\n        arcticdem_with_tpi = calculate_topographic_position_index(\n            arcticdem_ds=arcticdem,\n            outer_radius=100,\n            inner_radius=0\n        )\n\n        # Annulus kernel (50-100m ring)\n        arcticdem_multi_scale = calculate_topographic_position_index(\n            arcticdem_ds=arcticdem,\n            outer_radius=100,\n            inner_radius=50\n        )\n        ```\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius: Distance = Distance.parse(outer_radius, cellsize_x)\n    if inner_radius &gt; 0:\n        inner_radius: Distance = Distance.parse(inner_radius, cellsize_x)\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius.meter, inner_radius.meter)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius}-{outer_radius} cells away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of {inner_radius}-{outer_radius} cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius.meter)\n        attr_cell_description = f\"within a circle at a distance of {outer_radius} cells away from the focal cell.\"\n        logger.debug(f\"Calculating Topographic Position Index with circle kernel of {outer_radius} cells.\")\n\n    # Change dtype of kernel to float32 since we don't need the precision and f32 is faster\n    kernel = kernel.astype(\"float32\")\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_vector_ruggedness_measure","title":"calculate_vector_ruggedness_measure","text":"<pre><code>calculate_vector_ruggedness_measure(\n    arcticdem_ds: xarray.Dataset, neighborhood_size: int\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Vector Ruggedness Measure (VRM) from an ArcticDEM Dataset.</p> <p>VRM quantifies terrain ruggedness using vector analysis of slope and aspect, providing a measure independent of absolute elevation. Values range from 0 (smooth) to 1 (rugged).</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model - slope (float32): Slope in degrees (will be calculated if not present) - aspect (float32): Aspect in degrees (will be calculated if not present)</p> </li> <li> <code>neighborhood_size</code>               (<code>int</code>)           \u2013            <p>Neighborhood window size for VRM calculation. Can be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>vrm (float32): Vector Ruggedness Measure [0-1].</p> </li> <li> <p>long_name: \"Vector Ruggedness Measure\"</p> </li> <li>description: Documents neighborhood size used</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>VRM calculation:</p> <ol> <li>Converts slope and aspect to 3D unit vectors (x, y, z components)</li> <li>Sums vectors in the neighborhood window</li> <li>Calculates magnitude of resultant vector</li> <li>VRM = 1 - resultant magnitude</li> </ol> <p>Flat areas (aspect = -1) are handled by setting aspect to 0.</p> <p>Requires slope and aspect to be already calculated on the dataset.</p> References <p>Sappington, J.M., K.M. Longshore, and D.B. Thomson. 2007. Quantifying Landscape Ruggedness for Animal Habitat Analysis: A case Study Using Bighorn Sheep in the Mojave Desert. Journal of Wildlife Management. 71(5): 1419-1426.</p> Example <pre><code>from darts_preprocessing import (\n    calculate_slope, calculate_aspect,\n    calculate_vector_ruggedness_measure\n)\n\n# VRM requires slope and aspect\narcticdem = calculate_slope(arcticdem)\narcticdem = calculate_aspect(arcticdem)\narcticdem_with_vrm = calculate_vector_ruggedness_measure(\n    arcticdem_ds=arcticdem,\n    neighborhood_size=100\n)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating Vector Ruggedness Measure\", printer=logger.debug)\ndef calculate_vector_ruggedness_measure(arcticdem_ds: xr.Dataset, neighborhood_size: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Vector Ruggedness Measure (VRM) from an ArcticDEM Dataset.\n\n    VRM quantifies terrain ruggedness using vector analysis of slope and aspect, providing\n    a measure independent of absolute elevation. Values range from 0 (smooth) to 1 (rugged).\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n            - slope (float32): Slope in degrees (will be calculated if not present)\n            - aspect (float32): Aspect in degrees (will be calculated if not present)\n        neighborhood_size (int): Neighborhood window size for VRM calculation.\n            Can be specified as string with units (e.g., \"100m\" or \"10px\").\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - vrm (float32): Vector Ruggedness Measure [0-1].\n\n            - long_name: \"Vector Ruggedness Measure\"\n            - description: Documents neighborhood size used\n            - source: \"ArcticDEM\"\n\n    Note:\n        VRM calculation:\n\n        1. Converts slope and aspect to 3D unit vectors (x, y, z components)\n        2. Sums vectors in the neighborhood window\n        3. Calculates magnitude of resultant vector\n        4. VRM = 1 - resultant magnitude\n\n        Flat areas (aspect = -1) are handled by setting aspect to 0.\n\n        Requires slope and aspect to be already calculated on the dataset.\n\n    References:\n        Sappington, J.M., K.M. Longshore, and D.B. Thomson. 2007.\n        Quantifying Landscape Ruggedness for Animal Habitat Analysis: A case Study Using Bighorn Sheep\n        in the Mojave Desert. Journal of Wildlife Management. 71(5): 1419-1426.\n\n    Example:\n        ```python\n        from darts_preprocessing import (\n            calculate_slope, calculate_aspect,\n            calculate_vector_ruggedness_measure\n        )\n\n        # VRM requires slope and aspect\n        arcticdem = calculate_slope(arcticdem)\n        arcticdem = calculate_aspect(arcticdem)\n        arcticdem_with_vrm = calculate_vector_ruggedness_measure(\n            arcticdem_ds=arcticdem,\n            neighborhood_size=100\n        )\n        ```\n\n    \"\"\"\n    # Calculate slope and aspect\n    slope_rad = arcticdem_ds.slope * (np.pi / 180)  # Convert to radians\n    aspect_rad = arcticdem_ds.aspect * (np.pi / 180)  # Convert to radians\n\n    # Calculate x, y, and z components of unit vectors\n    xy = np.sin(slope_rad)\n    z = np.cos(slope_rad)\n\n    # Handle flat areas (where aspect = -1)\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        aspect_rad.variable._data = cp.where(aspect_rad.variable._data == -1, 0, aspect_rad.variable._data)\n        # aspect_rad = aspect_rad.copy(data=aspect_rad_raw)\n    else:\n        aspect_rad = xr.where(aspect_rad == -1, 0, aspect_rad)\n    x = np.sin(aspect_rad) * xy\n    y = np.cos(aspect_rad) * xy\n\n    # Get neighborhood_size in pixels\n    neighborhood_size: Distance = Distance.parse(neighborhood_size, arcticdem_ds.odc.geobox.resolution.x)\n    # Create convolution kernel for focal sum\n    kernel = np.ones((neighborhood_size.pixel, neighborhood_size.pixel), dtype=float) / neighborhood_size.pixel**2\n    kernel = convolution.custom_kernel(kernel)\n\n    # Change dtype of kernel to float32 since we don't need the precision and f32 is faster\n    kernel = kernel.astype(\"float32\")\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    logger.debug(f\"Calculating Vector Ruggedness Measure with square kernel of size {neighborhood_size} cells.\")\n\n    # Calculate sums of x, y, and z components in the neighborhood\n    x_sum = convolution.convolution_2d(x, kernel)\n    y_sum = convolution.convolution_2d(y, kernel)\n    z_sum = convolution.convolution_2d(z, kernel)\n\n    # Calculate the resultant vector magnitude\n    vrm = 1 - np.sqrt(x_sum**2 + y_sum**2 + z_sum**2)\n\n    vrm.attrs = {\n        \"long_name\": \"Vector Ruggedness Measure\",\n        \"units\": \"\",\n        \"description\": (\n            f\"Vector ruggedness measure calculated using a {neighborhood_size} neighborhood. \"\n            \"Values range from 0 (smooth) to 1 (most rugged).\"\n        ),\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"vrm\"] = vrm.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.dissection_index","title":"dissection_index","text":"<pre><code>dissection_index(\n    agg: xarray.DataArray,\n    window_size: int = 3,\n    name: str | None = \"dissection_index\",\n) -&gt; xarray.DataArray\n</code></pre> <p>Compute the dissection index of a 2D array.</p> <p>Parameters:</p> <ul> <li> <code>agg</code>               (<code>xarray.DataArray</code>)           \u2013            <p>The input data array.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The size of the window to use for the computation. Defaults to 3.</p> </li> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>'dissection_index'</code> )           \u2013            <p>The name of the output data array. Defaults to \"dissection_index\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: The dissection index of the input data array.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/dissection_index.py</code> <pre><code>def dissection_index(agg: xr.DataArray, window_size: int = 3, name: str | None = \"dissection_index\") -&gt; xr.DataArray:\n    \"\"\"Compute the dissection index of a 2D array.\n\n    Args:\n        agg (xr.DataArray): The input data array.\n        window_size (int, optional): The size of the window to use for the computation. Defaults to 3.\n        name (str | None, optional): The name of the output data array. Defaults to \"dissection_index\".\n\n    Returns:\n        xr.DataArray: The dissection index of the input data array.\n\n    \"\"\"\n    mapper = ArrayTypeFunctionMapping(\n        numpy_func=_run_numpy,\n        dask_func=_run_dask_numpy,\n        cupy_func=_run_cupy,\n        dask_cupy_func=lambda *args: not_implemented_func(\n            *args, messages=\"dissection_index() does not support dask with cupy backed DataArray\"\n        ),\n    )\n\n    out = mapper(agg)(agg.data, (window_size - 1) // 2)\n\n    return xr.DataArray(out, name=name, coords=agg.coords, dims=agg.dims, attrs=agg.attrs)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/dissection_index/","title":"dissection_index","text":""},{"location":"reference/darts_preprocessing/engineering/dissection_index/#darts_preprocessing.engineering.dissection_index","title":"darts_preprocessing.engineering.dissection_index","text":""},{"location":"reference/darts_preprocessing/engineering/dissection_index/#darts_preprocessing.engineering.dissection_index.RADIAN","title":"RADIAN  <code>module-attribute</code>","text":"<pre><code>RADIAN = 180 / numpy.pi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/dissection_index/#darts_preprocessing.engineering.dissection_index._run_cupy","title":"_run_cupy","text":"<pre><code>_run_cupy(data, d: int)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/dissection_index.py</code> <pre><code>def _run_cupy(data, d: int):\n    data = data.astype(cupy.float32)\n    griddim, blockdim = cuda_args(data.shape)\n    out = cupy.empty(data.shape, dtype=\"float32\")\n    out[:] = cupy.nan\n    _run_gpu[griddim, blockdim](data, d, out)\n    return out\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/dissection_index/#darts_preprocessing.engineering.dissection_index._run_dask_numpy","title":"_run_dask_numpy","text":"<pre><code>_run_dask_numpy(\n    data: dask.array.Array, d: int\n) -&gt; dask.array.Array\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/dissection_index.py</code> <pre><code>def _run_dask_numpy(data: da.Array, d: int) -&gt; da.Array:\n    data = data.astype(np.float32)\n    _func = partial(_run_numpy, d=d)  # noqa: RUF052\n    out = data.map_overlap(_func, depth=(d, d), boundary=np.nan, meta=np.array(()))\n    return out\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/dissection_index/#darts_preprocessing.engineering.dissection_index._run_gpu","title":"_run_gpu","text":"<pre><code>_run_gpu(arr, d, out)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/dissection_index.py</code> <pre><code>@cuda.jit\ndef _run_gpu(arr, d, out):\n    i, j = cuda.grid(2)\n    di = d\n    dj = d\n    if i - di &gt;= 0 and i + di &lt; out.shape[0] and j - dj &gt;= 0 and j + dj &lt; out.shape[1]:\n        # aoi = arr[i - di : i + di + 1, j - dj : j + dj + 1]\n        aoi_min = np.inf\n        aoi_max = -np.inf\n        for y in range(i - di, i + di + 1):\n            for x in range(j - dj, j + dj + 1):\n                v = arr[y, x]\n                if v &lt; aoi_min:\n                    aoi_min = v\n                if v &gt; aoi_max:\n                    aoi_max = v\n        if aoi_max != 0:\n            out[i, j] = (aoi_max - aoi_min) / aoi_max\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/dissection_index/#darts_preprocessing.engineering.dissection_index._run_numpy","title":"_run_numpy","text":"<pre><code>_run_numpy(data: numpy.ndarray, d: int)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/dissection_index.py</code> <pre><code>@ngjit\ndef _run_numpy(data: np.ndarray, d: int):\n    data = data.astype(np.float32)\n    out = np.zeros_like(data, dtype=np.float32)\n    out[:] = np.nan\n    rows, cols = data.shape\n    for y in range(d, rows - d):\n        for x in range(d, cols - d):\n            aoi = data[y - d : y + d + 1, x - d : x + d + 1]\n            aoi_min = aoi.min()\n            aoi_max = aoi.max()\n            if aoi_max == aoi_min:\n                out[y, x] = 0\n            elif aoi_max == 0:\n                out[y, x] = float(\"nan\")\n            else:\n                out[y, x] = (aoi_max - aoi_min) / (aoi_max)\n    return out\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/dissection_index/#darts_preprocessing.engineering.dissection_index.dissection_index","title":"dissection_index","text":"<pre><code>dissection_index(\n    agg: xarray.DataArray,\n    window_size: int = 3,\n    name: str | None = \"dissection_index\",\n) -&gt; xarray.DataArray\n</code></pre> <p>Compute the dissection index of a 2D array.</p> <p>Parameters:</p> <ul> <li> <code>agg</code>               (<code>xarray.DataArray</code>)           \u2013            <p>The input data array.</p> </li> <li> <code>window_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The size of the window to use for the computation. Defaults to 3.</p> </li> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>'dissection_index'</code> )           \u2013            <p>The name of the output data array. Defaults to \"dissection_index\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: The dissection index of the input data array.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/dissection_index.py</code> <pre><code>def dissection_index(agg: xr.DataArray, window_size: int = 3, name: str | None = \"dissection_index\") -&gt; xr.DataArray:\n    \"\"\"Compute the dissection index of a 2D array.\n\n    Args:\n        agg (xr.DataArray): The input data array.\n        window_size (int, optional): The size of the window to use for the computation. Defaults to 3.\n        name (str | None, optional): The name of the output data array. Defaults to \"dissection_index\".\n\n    Returns:\n        xr.DataArray: The dissection index of the input data array.\n\n    \"\"\"\n    mapper = ArrayTypeFunctionMapping(\n        numpy_func=_run_numpy,\n        dask_func=_run_dask_numpy,\n        cupy_func=_run_cupy,\n        dask_cupy_func=lambda *args: not_implemented_func(\n            *args, messages=\"dissection_index() does not support dask with cupy backed DataArray\"\n        ),\n    )\n\n    out = mapper(agg)(agg.data, (window_size - 1) // 2)\n\n    return xr.DataArray(out, name=name, coords=agg.coords, dims=agg.dims, attrs=agg.attrs)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/","title":"indices","text":""},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices","title":"darts_preprocessing.engineering.indices","text":"<p>Calculation of spectral indices from optical data.</p>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_ctvi","title":"calculate_ctvi","text":"<pre><code>calculate_ctvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate CTVI (Corrected Transformed Vegetation Index) from spectral bands.</p> <p>CTVI is a corrected version of TVI that maintains the sign of the original NDVI values while applying the transformation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - ndvi (float32): NDVI values (will be calculated if not present) - nir, red (float32): Required if NDVI not present</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: CTVI values with attributes: - long_name: \"CTVI\"</p> </li> </ul> Note <p>Formula: CTVI = (NDVI + 0.5) / |NDVI + 0.5| * sqrt(|NDVI + 0.5|)</p> <p>If NDVI is already in the dataset, it will be reused to avoid recalculation.</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_ctvi\n\nctvi = calculate_ctvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating CTVI\", printer=logger.debug)\ndef calculate_ctvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate CTVI (Corrected Transformed Vegetation Index) from spectral bands.\n\n    CTVI is a corrected version of TVI that maintains the sign of the original NDVI values\n    while applying the transformation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - ndvi (float32): NDVI values (will be calculated if not present)\n            - nir, red (float32): Required if NDVI not present\n\n    Returns:\n        xr.DataArray: CTVI values with attributes:\n            - long_name: \"CTVI\"\n\n    Note:\n        Formula: CTVI = (NDVI + 0.5) / |NDVI + 0.5| * sqrt(|NDVI + 0.5|)\n\n        If NDVI is already in the dataset, it will be reused to avoid recalculation.\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_ctvi\n\n        ctvi = calculate_ctvi(optical)\n        ```\n\n    \"\"\"\n    ndvi = optical[\"ndvi\"] if \"ndvi\" in optical else calculate_ndvi(optical)\n    ctvi = (ndvi + 0.5) / np.abs(ndvi + 0.5) * np.sqrt(np.abs(ndvi + 0.5))\n    ctvi = ctvi.assign_attrs({\"long_name\": \"CTVI\"}).rename(\"ctvi\")\n    return ctvi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_evi","title":"calculate_evi","text":"<pre><code>calculate_evi(\n    optical: xarray.Dataset,\n    g: float = 2.5,\n    c1: float = 6,\n    c2: float = 7.5,\n    l: float = 1,\n) -&gt; xarray.DataArray\n</code></pre> <p>Calculate EVI (Enhanced Vegetation Index) from spectral bands.</p> <p>EVI is optimized to enhance vegetation signal with improved sensitivity in high biomass regions and improved vegetation monitoring through decoupling of canopy background signal and reducing atmospheric influences.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - red (float32): Red reflectance [0-1] - blue (float32): Blue reflectance [0-1]</p> </li> <li> <code>g</code>               (<code>float</code>, default:                   <code>2.5</code> )           \u2013            <p>Gain factor. Defaults to 2.5.</p> </li> <li> <code>c1</code>               (<code>float</code>, default:                   <code>6</code> )           \u2013            <p>Aerosol resistance coefficient for red band. Defaults to 6.</p> </li> <li> <code>c2</code>               (<code>float</code>, default:                   <code>7.5</code> )           \u2013            <p>Aerosol resistance coefficient for blue band. Defaults to 7.5.</p> </li> <li> <code>l</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Canopy background adjustment. Defaults to 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: EVI values with attributes: - long_name: \"EVI\"</p> </li> </ul> Note <p>Formula: EVI = G * (NIR - Red) / (NIR + C1 * Red - C2 * Blue + L)</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>A Huete, K Didan, T Miura, E.P Rodriguez, X Gao, L.G Ferreira, Overview of the radiometric and biophysical performance of the MODIS vegetation indices, Remote Sensing of Environment, Volume 83, Issues 1-2, 2002, Pages 195-213, ISSN 0034-4257, https://doi.org/10.1016/S0034-4257(02)00096-2.</p> Example <pre><code>from darts_preprocessing import calculate_evi\n\nevi = calculate_evi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating EVI\", printer=logger.debug)\ndef calculate_evi(optical: xr.Dataset, g: float = 2.5, c1: float = 6, c2: float = 7.5, l: float = 1) -&gt; xr.DataArray:  # noqa: E741\n    \"\"\"Calculate EVI (Enhanced Vegetation Index) from spectral bands.\n\n    EVI is optimized to enhance vegetation signal with improved sensitivity in high biomass\n    regions and improved vegetation monitoring through decoupling of canopy background signal\n    and reducing atmospheric influences.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n            - blue (float32): Blue reflectance [0-1]\n        g (float, optional): Gain factor. Defaults to 2.5.\n        c1 (float, optional): Aerosol resistance coefficient for red band. Defaults to 6.\n        c2 (float, optional): Aerosol resistance coefficient for blue band. Defaults to 7.5.\n        l (float, optional): Canopy background adjustment. Defaults to 1.\n\n    Returns:\n        xr.DataArray: EVI values with attributes:\n            - long_name: \"EVI\"\n\n    Note:\n        Formula: EVI = G * (NIR - Red) / (NIR + C1 * Red - C2 * Blue + L)\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        A Huete, K Didan, T Miura, E.P Rodriguez, X Gao, L.G Ferreira,\n        Overview of the radiometric and biophysical performance of the MODIS vegetation indices,\n        Remote Sensing of Environment, Volume 83, Issues 1-2, 2002, Pages 195-213, ISSN 0034-4257,\n        https://doi.org/10.1016/S0034-4257(02)00096-2.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_evi\n\n        evi = calculate_evi(optical)\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    b = optical[\"blue\"].clip(0, 1)\n    evi = g * (nir - r) / (nir + c1 * r - c2 * b + l)\n    evi = evi.assign_attrs({\"long_name\": \"EVI\"}).rename(\"evi\")\n    return evi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_exg","title":"calculate_exg","text":"<pre><code>calculate_exg(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate EXG (Excess Green Index) from spectral bands.</p> <p>EXG highlights green vegetation by emphasizing the green band relative to red and blue. Widely used for crop/weed discrimination and precision agriculture.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - green (float32): Green reflectance [0-1] - red (float32): Red reflectance [0-1] - blue (float32): Blue reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: EXG values with attributes: - long_name: \"EXG\"</p> </li> </ul> Note <p>Formula: EXG = 2 * Green - Red - Blue</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>Upendar, K., Agrawal, K.N., Chandel, N.S. et al. Greenness identification using visible spectral colour indices for site specific weed management. Plant Physiol. Rep. 26, 179-187 (2021). https://doi.org/10.1007/s40502-020-00562-0</p> Example <pre><code>from darts_preprocessing import calculate_exg\n\nexg = calculate_exg(optical)\n\n# Threshold for vegetation detection\nvegetation = exg &gt; 0\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating EXG\", printer=logger.debug)\ndef calculate_exg(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate EXG (Excess Green Index) from spectral bands.\n\n    EXG highlights green vegetation by emphasizing the green band relative to red and blue.\n    Widely used for crop/weed discrimination and precision agriculture.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - green (float32): Green reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n            - blue (float32): Blue reflectance [0-1]\n\n    Returns:\n        xr.DataArray: EXG values with attributes:\n            - long_name: \"EXG\"\n\n    Note:\n        Formula: EXG = 2 * Green - Red - Blue\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        Upendar, K., Agrawal, K.N., Chandel, N.S. et al.\n        Greenness identification using visible spectral colour indices for site specific weed management.\n        Plant Physiol. Rep. 26, 179-187 (2021).\n        https://doi.org/10.1007/s40502-020-00562-0\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_exg\n\n        exg = calculate_exg(optical)\n\n        # Threshold for vegetation detection\n        vegetation = exg &gt; 0\n        ```\n\n    \"\"\"\n    g = optical[\"green\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    b = optical[\"blue\"].clip(0, 1)\n    exg = 2 * g - r - b\n    exg = exg.assign_attrs({\"long_name\": \"EXG\"}).rename(\"exg\")\n    return exg\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_gli","title":"calculate_gli","text":"<pre><code>calculate_gli(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate GLI (Green Leaf Index) from spectral bands.</p> <p>GLI emphasizes green reflectance for vegetation detection using only visible bands. Suitable for RGB sensors and aerial imagery.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - green (float32): Green reflectance - red (float32): Red reflectance - blue (float32): Blue reflectance</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: GLI values with attributes: - long_name: \"GLI\"</p> </li> </ul> Note <p>Formula: GLI = (2 * Green - Red - Blue) / (2 * Green + Red + Blue)</p> References <p>Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019. The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images. International Journal of Technology. Volume 10(7), pp. 1385-1394 https://doi.org/10.14716/ijtech.v10i7.3275</p> Example <pre><code>from darts_preprocessing import calculate_gli\n\ngli = calculate_gli(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating GLI\", printer=logger.debug)\ndef calculate_gli(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate GLI (Green Leaf Index) from spectral bands.\n\n    GLI emphasizes green reflectance for vegetation detection using only visible bands.\n    Suitable for RGB sensors and aerial imagery.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - green (float32): Green reflectance\n            - red (float32): Red reflectance\n            - blue (float32): Blue reflectance\n\n    Returns:\n        xr.DataArray: GLI values with attributes:\n            - long_name: \"GLI\"\n\n    Note:\n        Formula: GLI = (2 * Green - Red - Blue) / (2 * Green + Red + Blue)\n\n    References:\n        Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019.\n        The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images.\n        International Journal of Technology. Volume 10(7), pp. 1385-1394\n        https://doi.org/10.14716/ijtech.v10i7.3275\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_gli\n\n        gli = calculate_gli(optical)\n        ```\n\n    \"\"\"\n    g = optical[\"green\"]\n    r = optical[\"red\"]\n    b = optical[\"blue\"]\n    gli = (2 * g - r - b) / (2 * g + r + b)\n    gli = gli.assign_attrs({\"long_name\": \"GLI\"}).rename(\"gli\")\n    return gli\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_gndvi","title":"calculate_gndvi","text":"<pre><code>calculate_gndvi(\n    optical: xarray.Dataset,\n) -&gt; xarray.DataArray\n</code></pre> <p>Calculate GNDVI (Green Normalized Difference Vegetation Index) from spectral bands.</p> <p>GNDVI is similar to NDVI but uses the green band instead of red, making it more sensitive to chlorophyll content and useful for mid to late season vegetation monitoring.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - green (float32): Green reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: GNDVI values with attributes: - long_name: \"GNDVI\" - Values clipped to [-1, 1] range</p> </li> </ul> Note <p>Formula: GNDVI = (NIR - Green) / (NIR + Green)</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> Example <pre><code>from darts_preprocessing import calculate_gndvi\n\ngndvi = calculate_gndvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating GNDVI\", printer=logger.debug)\ndef calculate_gndvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate GNDVI (Green Normalized Difference Vegetation Index) from spectral bands.\n\n    GNDVI is similar to NDVI but uses the green band instead of red, making it more sensitive\n    to chlorophyll content and useful for mid to late season vegetation monitoring.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - green (float32): Green reflectance [0-1]\n\n    Returns:\n        xr.DataArray: GNDVI values with attributes:\n            - long_name: \"GNDVI\"\n            - Values clipped to [-1, 1] range\n\n    Note:\n        Formula: GNDVI = (NIR - Green) / (NIR + Green)\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_gndvi\n\n        gndvi = calculate_gndvi(optical)\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    g = optical[\"green\"].clip(0, 1)\n    gndvi = (nir - g) / (nir + g)\n    gndvi = gndvi.clip(-1, 1)\n    gndvi = gndvi.assign_attrs({\"long_name\": \"GNDVI\"}).rename(\"gndvi\")\n    return gndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_grvi","title":"calculate_grvi","text":"<pre><code>calculate_grvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate GRVI (Green Red Vegetation Index) from spectral bands.</p> <p>GRVI uses visible bands to detect vegetation, useful for high-resolution imagery where NIR may not be available or for specific vegetation discrimination tasks.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - green (float32): Green reflectance [0-1] - red (float32): Red reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: GRVI values with attributes: - long_name: \"GRVI\"</p> </li> </ul> Note <p>Formula: GRVI = (Green - Red) / (Green + Red)</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019. The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images. International Journal of Technology. Volume 10(7), pp. 1385-1394 https://doi.org/10.14716/ijtech.v10i7.3275</p> Example <pre><code>from darts_preprocessing import calculate_grvi\n\ngrvi = calculate_grvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating GRVI\", printer=logger.debug)\ndef calculate_grvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate GRVI (Green Red Vegetation Index) from spectral bands.\n\n    GRVI uses visible bands to detect vegetation, useful for high-resolution imagery\n    where NIR may not be available or for specific vegetation discrimination tasks.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - green (float32): Green reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n\n    Returns:\n        xr.DataArray: GRVI values with attributes:\n            - long_name: \"GRVI\"\n\n    Note:\n        Formula: GRVI = (Green - Red) / (Green + Red)\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019.\n        The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images.\n        International Journal of Technology. Volume 10(7), pp. 1385-1394\n        https://doi.org/10.14716/ijtech.v10i7.3275\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_grvi\n\n        grvi = calculate_grvi(optical)\n        ```\n\n    \"\"\"\n    g = optical[\"green\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    grvi = (g - r) / (g + r)\n    grvi = grvi.assign_attrs({\"long_name\": \"GRVI\"}).rename(\"grvi\")\n    return grvi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_ndvi","title":"calculate_ndvi","text":"<pre><code>calculate_ndvi(optical: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Calculate NDVI (Normalized Difference Vegetation Index) from spectral bands.</p> <p>NDVI is a widely-used vegetation index that indicates photosynthetic activity and vegetation health. Values range from -1 to 1, with higher values indicating denser, healthier vegetation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - red (float32): Red reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.DataArray: NDVI values with attributes: - long_name: \"NDVI\"</p> </li> </ul> Note <p>Formula: NDVI = (NIR - Red) / (NIR + Red)</p> <p>Input bands are clipped to [0, 1] before calculation to avoid numerical instabilities from negative reflectance values or sensor artifacts. The final result is also clipped to ensure values remain in the valid [-1, 1] range.</p> Example <p>Calculate NDVI from optical data:</p> <pre><code>from darts_preprocessing import calculate_ndvi\n\n# optical contains 'nir' and 'red' bands\nndvi = calculate_ndvi(optical)\n\n# Mask vegetation\nvegetation_mask = ndvi &gt; 0.3\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating NDVI\", printer=logger.debug)\ndef calculate_ndvi(optical: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI (Normalized Difference Vegetation Index) from spectral bands.\n\n    NDVI is a widely-used vegetation index that indicates photosynthetic activity and\n    vegetation health. Values range from -1 to 1, with higher values indicating denser,\n    healthier vegetation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n\n    Returns:\n        xr.DataArray: NDVI values with attributes:\n            - long_name: \"NDVI\"\n\n    Note:\n        Formula: NDVI = (NIR - Red) / (NIR + Red)\n\n        Input bands are clipped to [0, 1] before calculation to avoid numerical instabilities\n        from negative reflectance values or sensor artifacts. The final result is also clipped\n        to ensure values remain in the valid [-1, 1] range.\n\n    Example:\n        Calculate NDVI from optical data:\n\n        ```python\n        from darts_preprocessing import calculate_ndvi\n\n        # optical contains 'nir' and 'red' bands\n        ndvi = calculate_ndvi(optical)\n\n        # Mask vegetation\n        vegetation_mask = ndvi &gt; 0.3\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    ndvi = (nir - r) / (nir + r)\n    ndvi = ndvi.clip(-1, 1)\n    ndvi = ndvi.assign_attrs({\"long_name\": \"NDVI\"}).rename(\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_nrvi","title":"calculate_nrvi","text":"<pre><code>calculate_nrvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate NRVI (Normalized Ratio Vegetation Index) from spectral bands.</p> <p>NRVI normalizes RVI to a range similar to NDVI, making it more comparable across different vegetation densities.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - rvi (float32): RVI values (will be calculated if not present) - nir, red (float32): Required if RVI not present</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: NRVI values with attributes: - long_name: \"NRVI\"</p> </li> </ul> Note <p>Formula: NRVI = (RVI - 1) / (RVI + 1) where RVI = Red / NIR</p> <p>If RVI is already in the dataset, it will be reused to avoid recalculation.</p> Example <pre><code>from darts_preprocessing import calculate_nrvi\n\nnrvi = calculate_nrvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating NRVI\", printer=logger.debug)\ndef calculate_nrvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate NRVI (Normalized Ratio Vegetation Index) from spectral bands.\n\n    NRVI normalizes RVI to a range similar to NDVI, making it more comparable across\n    different vegetation densities.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - rvi (float32): RVI values (will be calculated if not present)\n            - nir, red (float32): Required if RVI not present\n\n    Returns:\n        xr.DataArray: NRVI values with attributes:\n            - long_name: \"NRVI\"\n\n    Note:\n        Formula: NRVI = (RVI - 1) / (RVI + 1)\n        where RVI = Red / NIR\n\n        If RVI is already in the dataset, it will be reused to avoid recalculation.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_nrvi\n\n        nrvi = calculate_nrvi(optical)\n        ```\n\n    \"\"\"\n    rvi = optical[\"rvi\"] if \"rvi\" in optical else calculate_rvi(optical)\n    nrvi = (rvi - 1) / (rvi + 1)\n    nrvi = nrvi.assign_attrs({\"long_name\": \"NRVI\"}).rename(\"nrvi\")\n    return nrvi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_rvi","title":"calculate_rvi","text":"<pre><code>calculate_rvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate RVI (Ratio Vegetation Index) from spectral bands.</p> <p>RVI is a simple ratio index sensitive to vegetation amount and biomass. Values typically range from 0 to over 30 for dense vegetation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - red (float32): Red reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: RVI values with attributes: - long_name: \"RVI\"</p> </li> </ul> Note <p>Formula: RVI = Red / NIR</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_rvi\n\nrvi = calculate_rvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating RVI\", printer=logger.debug)\ndef calculate_rvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate RVI (Ratio Vegetation Index) from spectral bands.\n\n    RVI is a simple ratio index sensitive to vegetation amount and biomass. Values typically\n    range from 0 to over 30 for dense vegetation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n\n    Returns:\n        xr.DataArray: RVI values with attributes:\n            - long_name: \"RVI\"\n\n    Note:\n        Formula: RVI = Red / NIR\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_rvi\n\n        rvi = calculate_rvi(optical)\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    rvi = r / nir\n    rvi = rvi.assign_attrs({\"long_name\": \"RVI\"}).rename(\"rvi\")\n    return rvi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_savi","title":"calculate_savi","text":"<pre><code>calculate_savi(\n    optical: xarray.Dataset, s: float = 0.5\n) -&gt; xarray.DataArray\n</code></pre> <p>Calculate SAVI (Soil Adjusted Vegetation Index) from spectral bands.</p> <p>SAVI minimizes soil brightness influences using a soil-brightness correction factor. Useful in areas with sparse vegetation or exposed soil.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - ndvi (float32): NDVI values (will be calculated if not present) - nir, red (float32): Required if NDVI not present</p> </li> <li> <code>s</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Soil adjustment factor. Common values: - 0.5: moderate vegetation cover (default) - 0.25: high vegetation cover - 1.0: low vegetation cover</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: SAVI values with attributes: - long_name: \"SAVI\"</p> </li> </ul> Note <p>Formula: SAVI = NDVI * (1 + s)</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_savi\n\n# For sparse vegetation\nsavi = calculate_savi(optical, s=1.0)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating SAVI\", printer=logger.debug)\ndef calculate_savi(optical: xr.Dataset, s: float = 0.5) -&gt; xr.DataArray:\n    \"\"\"Calculate SAVI (Soil Adjusted Vegetation Index) from spectral bands.\n\n    SAVI minimizes soil brightness influences using a soil-brightness correction factor.\n    Useful in areas with sparse vegetation or exposed soil.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - ndvi (float32): NDVI values (will be calculated if not present)\n            - nir, red (float32): Required if NDVI not present\n        s (float, optional): Soil adjustment factor. Common values:\n            - 0.5: moderate vegetation cover (default)\n            - 0.25: high vegetation cover\n            - 1.0: low vegetation cover\n\n    Returns:\n        xr.DataArray: SAVI values with attributes:\n            - long_name: \"SAVI\"\n\n    Note:\n        Formula: SAVI = NDVI * (1 + s)\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_savi\n\n        # For sparse vegetation\n        savi = calculate_savi(optical, s=1.0)\n        ```\n\n    \"\"\"\n    ndvi = optical[\"ndvi\"] if \"ndvi\" in optical else calculate_ndvi(optical)\n    savi = ndvi * (1 + s)\n    savi = savi.assign_attrs({\"long_name\": \"SAVI\"}).rename(\"savi\")\n    return savi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_tgi","title":"calculate_tgi","text":"<pre><code>calculate_tgi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate TGI (Triangular Greenness Index) from spectral bands.</p> <p>TGI is sensitive to chlorophyll content and can estimate leaf area index without calibration. Particularly useful for crop monitoring.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - red (float32): Red reflectance [0-1] - green (float32): Green reflectance [0-1] - blue (float32): Blue reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: TGI values with attributes: - long_name: \"TGI\"</p> </li> </ul> Note <p>Formula: TGI = -0.5 * [190 * (Red - Green) - 120 * (Red - Blue)]</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>E. Raymond Hunt, Paul C. Doraiswamy, James E. McMurtrey, Craig S.T. Daughtry, Eileen M. Perry, Bakhyt Akhmedov, A visible band index for remote sensing leaf chlorophyll content at the canopy scale, International Journal of Applied Earth Observation and Geoinformation, Volume 21, 2013, Pages 103-112, ISSN 1569-8432, https://doi.org/10.1016/j.jag.2012.07.020.</p> Example <pre><code>from darts_preprocessing import calculate_tgi\n\ntgi = calculate_tgi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating TGI\", printer=logger.debug)\ndef calculate_tgi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate TGI (Triangular Greenness Index) from spectral bands.\n\n    TGI is sensitive to chlorophyll content and can estimate leaf area index without\n    calibration. Particularly useful for crop monitoring.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - red (float32): Red reflectance [0-1]\n            - green (float32): Green reflectance [0-1]\n            - blue (float32): Blue reflectance [0-1]\n\n    Returns:\n        xr.DataArray: TGI values with attributes:\n            - long_name: \"TGI\"\n\n    Note:\n        Formula: TGI = -0.5 * [190 * (Red - Green) - 120 * (Red - Blue)]\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        E. Raymond Hunt, Paul C. Doraiswamy, James E. McMurtrey, Craig S.T. Daughtry, Eileen M. Perry, Bakhyt Akhmedov,\n        A visible band index for remote sensing leaf chlorophyll content at the canopy scale,\n        International Journal of Applied Earth Observation and Geoinformation,\n        Volume 21, 2013, Pages 103-112, ISSN 1569-8432,\n        https://doi.org/10.1016/j.jag.2012.07.020.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_tgi\n\n        tgi = calculate_tgi(optical)\n        ```\n\n    \"\"\"\n    r = optical[\"red\"].clip(0, 1)\n    g = optical[\"green\"].clip(0, 1)\n    b = optical[\"blue\"].clip(0, 1)\n    tgi = -0.5 * (190 * (r - g) - 120 * (r - b))\n    tgi = tgi.assign_attrs({\"long_name\": \"TGI\"}).rename(\"tgi\")\n    return tgi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_ttvi","title":"calculate_ttvi","text":"<pre><code>calculate_ttvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate TTVI (Thiam's Transformed Vegetation Index) from spectral bands.</p> <p>TTVI applies an absolute value transformation to NDVI before the square root, making it suitable for both positive and negative NDVI values.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - ndvi (float32): NDVI values (will be calculated if not present) - nir, red (float32): Required if NDVI not present</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: TTVI values with attributes: - long_name: \"TTVI\"</p> </li> </ul> Note <p>Formula: TTVI = sqrt(|NDVI| + 0.5)</p> <p>If NDVI is already in the dataset, it will be reused to avoid recalculation.</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_ttvi\n\nttvi = calculate_ttvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating TTVI\", printer=logger.debug)\ndef calculate_ttvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate TTVI (Thiam's Transformed Vegetation Index) from spectral bands.\n\n    TTVI applies an absolute value transformation to NDVI before the square root,\n    making it suitable for both positive and negative NDVI values.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - ndvi (float32): NDVI values (will be calculated if not present)\n            - nir, red (float32): Required if NDVI not present\n\n    Returns:\n        xr.DataArray: TTVI values with attributes:\n            - long_name: \"TTVI\"\n\n    Note:\n        Formula: TTVI = sqrt(|NDVI| + 0.5)\n\n        If NDVI is already in the dataset, it will be reused to avoid recalculation.\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_ttvi\n\n        ttvi = calculate_ttvi(optical)\n        ```\n\n    \"\"\"\n    ndvi = optical[\"ndvi\"] if \"ndvi\" in optical else calculate_ndvi(optical)\n    ttvi = np.sqrt(np.abs(ndvi) + 0.5)\n    ttvi = ttvi.assign_attrs({\"long_name\": \"TTVI\"}).rename(\"ttvi\")\n    return ttvi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_tvi","title":"calculate_tvi","text":"<pre><code>calculate_tvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate TVI (Transformed Vegetation Index) from spectral bands.</p> <p>TVI applies a transformation to NDVI to enhance contrast and improve discrimination of vegetation conditions.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - ndvi (float32): NDVI values (will be calculated if not present) - nir, red (float32): Required if NDVI not present</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: TVI values with attributes: - long_name: \"TVI\"</p> </li> </ul> Note <p>Formula: TVI = sqrt(NDVI + 0.5)</p> <p>If NDVI is already in the dataset, it will be reused to avoid recalculation.</p> References <p>Lemenkova, Polina. \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\" Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78. https://doi.org/10.2478/arls-2020-0021</p> Example <pre><code>from darts_preprocessing import calculate_tvi\n\ntvi = calculate_tvi(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating TVI\", printer=logger.debug)\ndef calculate_tvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate TVI (Transformed Vegetation Index) from spectral bands.\n\n    TVI applies a transformation to NDVI to enhance contrast and improve discrimination\n    of vegetation conditions.\n\n    Args:\n        optical (xr.Dataset): Dataset containing:\n            - ndvi (float32): NDVI values (will be calculated if not present)\n            - nir, red (float32): Required if NDVI not present\n\n    Returns:\n        xr.DataArray: TVI values with attributes:\n            - long_name: \"TVI\"\n\n    Note:\n        Formula: TVI = sqrt(NDVI + 0.5)\n\n        If NDVI is already in the dataset, it will be reused to avoid recalculation.\n\n    References:\n        Lemenkova, Polina.\n        \"Hyperspectral Vegetation Indices Calculated by Qgis Using Landsat Tm Image: a Case Study of Northern Iceland\"\n        Advanced Research in Life Sciences, vol. 4, no. 1, Sciendo, 2020, pp. 70-78.\n        https://doi.org/10.2478/arls-2020-0021\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_tvi\n\n        tvi = calculate_tvi(optical)\n        ```\n\n    \"\"\"\n    ndvi = optical[\"ndvi\"] if \"ndvi\" in optical else calculate_ndvi(optical)\n    tvi = np.sqrt(ndvi + 0.5)\n    tvi = tvi.assign_attrs({\"long_name\": \"TVI\"}).rename(\"tvi\")\n    return tvi\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_vari","title":"calculate_vari","text":"<pre><code>calculate_vari(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Calculate VARI (Visible Atmospherically Resistant Index) from spectral bands.</p> <p>VARI uses only visible bands, designed to minimize atmospheric effects. Useful for RGB imagery without NIR band or for atmospheric correction validation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - green (float32): Green reflectance [0-1] - red (float32): Red reflectance [0-1] - blue (float32): Blue reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: VARI values with attributes: - long_name: \"VARI\"</p> </li> </ul> Note <p>Formula: VARI = (Green - Red) / (Green + Red - Blue)</p> <p>Input bands are clipped to [0, 1] to avoid numerical instabilities.</p> References <p>Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019. The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images. International Journal of Technology. Volume 10(7), pp. 1385-1394 https://doi.org/10.14716/ijtech.v10i7.3275</p> Example <pre><code>from darts_preprocessing import calculate_vari\n\nvari = calculate_vari(optical)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating VARI\", printer=logger.debug)\ndef calculate_vari(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Calculate VARI (Visible Atmospherically Resistant Index) from spectral bands.\n\n    VARI uses only visible bands, designed to minimize atmospheric effects. Useful for\n    RGB imagery without NIR band or for atmospheric correction validation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - green (float32): Green reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n            - blue (float32): Blue reflectance [0-1]\n\n    Returns:\n        xr.DataArray: VARI values with attributes:\n            - long_name: \"VARI\"\n\n    Note:\n        Formula: VARI = (Green - Red) / (Green + Red - Blue)\n\n        Input bands are clipped to [0, 1] to avoid numerical instabilities.\n\n    References:\n        Eng, L.S., Ismail, R., Hashim, W., Baharum, A., 2019.\n        The Use of VARI, GLI, and VIgreen Formulas in Detecting Vegetation In aerial Images.\n        International Journal of Technology. Volume 10(7), pp. 1385-1394\n        https://doi.org/10.14716/ijtech.v10i7.3275\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_vari\n\n        vari = calculate_vari(optical)\n        ```\n\n    \"\"\"\n    g = optical[\"green\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    b = optical[\"blue\"].clip(0, 1)\n    vari = (g - r) / (g + r - b)\n    vari = vari.assign_attrs({\"long_name\": \"VARI\"}).rename(\"vari\")\n    return vari\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_vdvi","title":"calculate_vdvi","text":"<pre><code>calculate_vdvi(optical: xarray.Dataset) -&gt; xarray.DataArray\n</code></pre> <p>Alias for GLI (Green Leaf Index) from an xarray Dataset containing spectral bands.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>def calculate_vdvi(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Alias for GLI (Green Leaf Index) from an xarray Dataset containing spectral bands.\"\"\"  # noqa: DOC201\n    logger.warning(\"VDVI is an alias for GLI, using GLI calculation.\")\n    return calculate_gli(optical)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_vigreen","title":"calculate_vigreen","text":"<pre><code>calculate_vigreen(\n    optical: xarray.Dataset,\n) -&gt; xarray.DataArray\n</code></pre> <p>Alias for VIGREEN (Vegetation Index Green) from an xarray Dataset containing spectral bands.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>def calculate_vigreen(optical: xr.Dataset) -&gt; xr.DataArray:\n    \"\"\"Alias for VIGREEN (Vegetation Index Green) from an xarray Dataset containing spectral bands.\"\"\"  # noqa: DOC201\n    logger.warning(\"VIGREEN is an alias for GRVI, using GRVI calculation.\")\n    return calculate_grvi(optical)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/spyndex/","title":"spyndex","text":""},{"location":"reference/darts_preprocessing/engineering/spyndex/#darts_preprocessing.engineering.spyndex","title":"darts_preprocessing.engineering.spyndex","text":"<p>Calculation of spectral indices from optical data with the spyndex library.</p>"},{"location":"reference/darts_preprocessing/engineering/spyndex/#darts_preprocessing.engineering.spyndex.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/spyndex/#darts_preprocessing.engineering.spyndex.calculate_spyndex","title":"calculate_spyndex","text":"<pre><code>calculate_spyndex(\n    ds_optical: xarray.Dataset,\n    index: str,\n    **kwargs: dict[str, typing.Any],\n) -&gt; xarray.DataArray\n</code></pre> <p>Compute a spectral index using the spyndex library.</p> <p>This wrapper provides access to 200+ spectral indices from the spyndex library with automatic band mapping and parameter handling.</p> <p>Parameters:</p> <ul> <li> <code>ds_optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands. Band names should match spyndex common names (e.g., 'red', 'nir', 'blue', 'green').</p> </li> <li> <code>index</code>               (<code>str</code>)           \u2013            <p>Name of the spectral index to compute. Run <code>spyndex.indices</code> to see all available indices (e.g., 'NDVI', 'EVI', 'SAVI').</p> </li> <li> <code>**kwargs</code>               (<code>dict[str, typing.Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional parameters or band overrides: - Band values: Override bands from ds_optical with scalar or array values (e.g., red=0.2) - Constants: Override default values for index-specific constants (e.g., L=0.5 for SAVI)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Computed spectral index with attributes: - source: \"spyndex\" - long_name: Full name of the index - reference: Citation for the index - formula: Mathematical formula - author: Index contributor</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a required band is missing from both ds_optical and kwargs.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If all bands are provided as scalar values.</p> </li> </ul> Note <p>Band resolution priority:</p> <ol> <li>Bands in ds_optical (with common_name matching spyndex.bands)</li> <li>Values in kwargs (override ds_optical bands)</li> <li>Default values for constants (from spyndex.constants)</li> </ol> <p>All optical bands are automatically clipped to [0, 1] before calculation.</p> <p>At least one band must come from ds_optical as a DataArray (not all scalars).</p> Example <p>Compute various indices with spyndex:</p> <pre><code>from darts_preprocessing import calculate_spyndex\nimport spyndex\n\n# List all available indices\nprint(list(spyndex.indices.keys()))\n\n# Basic NDVI\nndvi = calculate_spyndex(ds_optical, \"NDVI\")\n\n# SAVI with custom soil adjustment factor\nsavi = calculate_spyndex(ds_optical, \"SAVI\", L=0.5)\n\n# EVI with custom parameters\nevi = calculate_spyndex(ds_optical, \"EVI\", g=2.5, C1=6, C2=7.5, L=1)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/spyndex.py</code> <pre><code>@stopwatch.f(\"Computing spectral index with spyndex\", printer=logger.debug, print_kwargs=[\"index\"])\ndef calculate_spyndex(ds_optical: xr.Dataset, index: str, **kwargs: dict[str, Any]) -&gt; xr.DataArray:\n    \"\"\"Compute a spectral index using the spyndex library.\n\n    This wrapper provides access to 200+ spectral indices from the spyndex library with\n    automatic band mapping and parameter handling.\n\n    Args:\n        ds_optical (xr.Dataset): Dataset containing spectral bands. Band names should match\n            spyndex common names (e.g., 'red', 'nir', 'blue', 'green').\n        index (str): Name of the spectral index to compute. Run `spyndex.indices` to see\n            all available indices (e.g., 'NDVI', 'EVI', 'SAVI').\n        **kwargs: Additional parameters or band overrides:\n            - Band values: Override bands from ds_optical with scalar or array values (e.g., red=0.2)\n            - Constants: Override default values for index-specific constants (e.g., L=0.5 for SAVI)\n\n    Returns:\n        xr.DataArray: Computed spectral index with attributes:\n            - source: \"spyndex\"\n            - long_name: Full name of the index\n            - reference: Citation for the index\n            - formula: Mathematical formula\n            - author: Index contributor\n\n    Raises:\n        ValueError: If a required band is missing from both ds_optical and kwargs.\n        ValueError: If all bands are provided as scalar values.\n\n    Note:\n        Band resolution priority:\n\n        1. Bands in ds_optical (with common_name matching spyndex.bands)\n        2. Values in kwargs (override ds_optical bands)\n        3. Default values for constants (from spyndex.constants)\n\n        All optical bands are automatically clipped to [0, 1] before calculation.\n\n        At least one band must come from ds_optical as a DataArray (not all scalars).\n\n    Example:\n        Compute various indices with spyndex:\n\n        ```python\n        from darts_preprocessing import calculate_spyndex\n        import spyndex\n\n        # List all available indices\n        print(list(spyndex.indices.keys()))\n\n        # Basic NDVI\n        ndvi = calculate_spyndex(ds_optical, \"NDVI\")\n\n        # SAVI with custom soil adjustment factor\n        savi = calculate_spyndex(ds_optical, \"SAVI\", L=0.5)\n\n        # EVI with custom parameters\n        evi = calculate_spyndex(ds_optical, \"EVI\", g=2.5, C1=6, C2=7.5, L=1)\n        ```\n\n    \"\"\"\n    index: spyndex.axioms.SpectralIndex = spyndex.indices[index]\n\n    params = {}\n    atleast_one_dataarray = False\n    for band in index.bands:\n        is_constant = band in spyndex.constants\n        is_in_kwargs = band in kwargs\n\n        is_in_optical = False\n        if not is_constant:\n            spyndex_band: spyndex.axioms.Band = spyndex.bands.get(band)\n            is_in_optical = spyndex_band is not None and spyndex_band.common_name in ds_optical\n\n        # Case 4: band is missing -&gt; error\n        if not (is_constant or is_in_kwargs or is_in_optical):\n            raise ValueError(f\"Band '{band}' is required for index '{index.short_name}' but not provided in {kwargs=}.\")\n        # Case 3: band is in kwargs\n        if is_in_kwargs:\n            params[band] = kwargs[band]\n            continue\n        # Case 2: band is a constant\n        if is_constant:\n            constant: spyndex.axioms.Constant = spyndex.constants[band]\n            params[band] = constant.default\n            continue\n        # Case 1: band is in optical\n        params[band] = ds_optical[spyndex_band.common_name].clip(0, 1)\n        atleast_one_dataarray = True\n\n    if not atleast_one_dataarray:\n        raise ValueError(f\"At least one band must be a DataArray, got {params=}. Did you pass all bands as scalars?\")\n\n    idx = index.compute(params)\n    assert isinstance(idx, xr.DataArray)\n\n    idx = idx.assign_attrs(\n        {\n            \"source\": \"spyndex\",\n            \"long_name\": index.long_name,\n            \"reference\": index.reference,\n            \"formula\": index.formula,\n            \"author\": index.contributor,\n        }\n    ).rename(index.short_name.lower())\n    return idx\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/","title":"legacy","text":""},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy","title":"darts_preprocessing.legacy","text":"<p>PLANET scene based preprocessing.</p>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.calculate_ndvi","title":"calculate_ndvi","text":"<pre><code>calculate_ndvi(optical: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Calculate NDVI (Normalized Difference Vegetation Index) from spectral bands.</p> <p>NDVI is a widely-used vegetation index that indicates photosynthetic activity and vegetation health. Values range from -1 to 1, with higher values indicating denser, healthier vegetation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - red (float32): Red reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.DataArray: NDVI values with attributes: - long_name: \"NDVI\"</p> </li> </ul> Note <p>Formula: NDVI = (NIR - Red) / (NIR + Red)</p> <p>Input bands are clipped to [0, 1] before calculation to avoid numerical instabilities from negative reflectance values or sensor artifacts. The final result is also clipped to ensure values remain in the valid [-1, 1] range.</p> Example <p>Calculate NDVI from optical data:</p> <pre><code>from darts_preprocessing import calculate_ndvi\n\n# optical contains 'nir' and 'red' bands\nndvi = calculate_ndvi(optical)\n\n# Mask vegetation\nvegetation_mask = ndvi &gt; 0.3\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating NDVI\", printer=logger.debug)\ndef calculate_ndvi(optical: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI (Normalized Difference Vegetation Index) from spectral bands.\n\n    NDVI is a widely-used vegetation index that indicates photosynthetic activity and\n    vegetation health. Values range from -1 to 1, with higher values indicating denser,\n    healthier vegetation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n\n    Returns:\n        xr.DataArray: NDVI values with attributes:\n            - long_name: \"NDVI\"\n\n    Note:\n        Formula: NDVI = (NIR - Red) / (NIR + Red)\n\n        Input bands are clipped to [0, 1] before calculation to avoid numerical instabilities\n        from negative reflectance values or sensor artifacts. The final result is also clipped\n        to ensure values remain in the valid [-1, 1] range.\n\n    Example:\n        Calculate NDVI from optical data:\n\n        ```python\n        from darts_preprocessing import calculate_ndvi\n\n        # optical contains 'nir' and 'red' bands\n        ndvi = calculate_ndvi(optical)\n\n        # Mask vegetation\n        vegetation_mask = ndvi &gt; 0.3\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    ndvi = (nir - r) / (nir + r)\n    ndvi = ndvi.clip(-1, 1)\n    ndvi = ndvi.assign_attrs({\"long_name\": \"NDVI\"}).rename(\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.calculate_slope","title":"calculate_slope","text":"<pre><code>calculate_slope(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Slope represents the rate of change of elevation, indicating terrain steepness.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>slope (float32): Slope in degrees [0-90].</p> </li> <li> <p>long_name: \"Slope\"</p> </li> <li>units: \"degrees\"</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Slope is calculated using finite difference methods on the DEM. Values approaching 90\u00b0 indicate near-vertical terrain.</p> Example <pre><code>from darts_preprocessing import calculate_slope\n\narcticdem_with_slope = calculate_slope(arcticdem_ds)\n\n# Mask steep terrain\nsteep_areas = arcticdem_with_slope.slope &gt; 30\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate slope of the terrain surface from an ArcticDEM Dataset.\n\n    Slope represents the rate of change of elevation, indicating terrain steepness.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - slope (float32): Slope in degrees [0-90].\n\n            - long_name: \"Slope\"\n            - units: \"degrees\"\n            - source: \"ArcticDEM\"\n\n    Note:\n        Slope is calculated using finite difference methods on the DEM.\n        Values approaching 90\u00b0 indicate near-vertical terrain.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_slope\n\n        arcticdem_with_slope = calculate_slope(arcticdem_ds)\n\n        # Mask steep terrain\n        steep_areas = arcticdem_with_slope.slope &gt; 30\n        ```\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.calculate_topographic_position_index","title":"calculate_topographic_position_index","text":"<pre><code>calculate_topographic_position_index(\n    arcticdem_ds: xarray.Dataset,\n    outer_radius: int,\n    inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>TPI measures the relative topographic position of a point by comparing its elevation to the mean elevation of the surrounding neighborhood. Positive values indicate higher positions (ridges), negative values indicate lower positions (valleys).</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable (float32).</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the neighborhood in meters. Can also be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in meters. If &gt; 0, creates an annulus (ring) instead of a circle. Set to 0 for a circular kernel. Can also be specified as string with units (e.g., \"50m\" or \"5px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with a new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>tpi (float32): Topographic Position Index values.</p> </li> <li> <p>long_name: \"Topographic Position Index (TPI)\"</p> </li> <li>description: Details about the kernel used</li> </ul> </li> </ul> Note <p>Kernel shape combinations:</p> <ul> <li>inner_radius=0: Circular kernel comparing each cell to all neighbors within outer_radius</li> <li>inner_radius&gt;0: Annulus kernel comparing each cell to neighbors in a ring between   inner_radius and outer_radius. Useful for multi-scale terrain analysis.</li> </ul> <p>The actual radii used are rounded to the nearest pixel based on the DEM resolution.</p> Example <p>Calculate TPI with circular and annulus kernels:</p> <pre><code>from darts_preprocessing import calculate_topographic_position_index\n\n# Circular kernel (100m radius)\narcticdem_with_tpi = calculate_topographic_position_index(\n    arcticdem_ds=arcticdem,\n    outer_radius=100,\n    inner_radius=0\n)\n\n# Annulus kernel (50-100m ring)\narcticdem_multi_scale = calculate_topographic_position_index(\n    arcticdem_ds=arcticdem,\n    outer_radius=100,\n    inner_radius=50\n)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    TPI measures the relative topographic position of a point by comparing its elevation to\n    the mean elevation of the surrounding neighborhood. Positive values indicate higher\n    positions (ridges), negative values indicate lower positions (valleys).\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable (float32).\n        outer_radius (int): The outer radius of the neighborhood in meters.\n            Can also be specified as string with units (e.g., \"100m\" or \"10px\").\n        inner_radius (int): The inner radius of the annulus kernel in meters.\n            If &gt; 0, creates an annulus (ring) instead of a circle. Set to 0 for a circular kernel.\n            Can also be specified as string with units (e.g., \"50m\" or \"5px\").\n\n    Returns:\n        xr.Dataset: The input Dataset with a new data variable added:\n\n        - tpi (float32): Topographic Position Index values.\n\n            - long_name: \"Topographic Position Index (TPI)\"\n            - description: Details about the kernel used\n\n    Note:\n        Kernel shape combinations:\n\n        - inner_radius=0: Circular kernel comparing each cell to all neighbors within outer_radius\n        - inner_radius&gt;0: Annulus kernel comparing each cell to neighbors in a ring between\n          inner_radius and outer_radius. Useful for multi-scale terrain analysis.\n\n        The actual radii used are rounded to the nearest pixel based on the DEM resolution.\n\n    Example:\n        Calculate TPI with circular and annulus kernels:\n\n        ```python\n        from darts_preprocessing import calculate_topographic_position_index\n\n        # Circular kernel (100m radius)\n        arcticdem_with_tpi = calculate_topographic_position_index(\n            arcticdem_ds=arcticdem,\n            outer_radius=100,\n            inner_radius=0\n        )\n\n        # Annulus kernel (50-100m ring)\n        arcticdem_multi_scale = calculate_topographic_position_index(\n            arcticdem_ds=arcticdem,\n            outer_radius=100,\n            inner_radius=50\n        )\n        ```\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius: Distance = Distance.parse(outer_radius, cellsize_x)\n    if inner_radius &gt; 0:\n        inner_radius: Distance = Distance.parse(inner_radius, cellsize_x)\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius.meter, inner_radius.meter)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius}-{outer_radius} cells away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of {inner_radius}-{outer_radius} cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius.meter)\n        attr_cell_description = f\"within a circle at a distance of {outer_radius} cells away from the focal cell.\"\n        logger.debug(f\"Calculating Topographic Position Index with circle kernel of {outer_radius} cells.\")\n\n    # Change dtype of kernel to float32 since we don't need the precision and f32 is faster\n    kernel = kernel.astype(\"float32\")\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.preprocess_legacy_arcticdem_fast","title":"preprocess_legacy_arcticdem_fast","text":"<pre><code>preprocess_legacy_arcticdem_fast(\n    ds_arcticdem: xarray.Dataset,\n    tpi_outer_radius: int,\n    tpi_inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess the ArcticDEM data with legacy (DARTS v1) preprocessing steps.</p> <p>Parameters:</p> <ul> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in number of cells.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in number of cells.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed ArcticDEM dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/legacy.py</code> <pre><code>@stopwatch.f(\"Preprocessing arcticdem\", printer=logger.debug, print_kwargs=[\"tpi_outer_radius\", \"tpi_inner_radius\"])\ndef preprocess_legacy_arcticdem_fast(\n    ds_arcticdem: xr.Dataset,\n    tpi_outer_radius: int,\n    tpi_inner_radius: int,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess the ArcticDEM data with legacy (DARTS v1) preprocessing steps.\n\n    Args:\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        tpi_outer_radius (int): The outer radius of the annulus kernel for the tpi calculation in number of cells.\n        tpi_inner_radius (int): The inner radius of the annulus kernel for the tpi calculation in number of cells.\n\n    Returns:\n        xr.Dataset: The preprocessed ArcticDEM dataset.\n\n    \"\"\"\n    ds_arcticdem = calculate_topographic_position_index(ds_arcticdem, tpi_outer_radius, tpi_inner_radius)\n    ds_arcticdem = calculate_slope(ds_arcticdem)\n\n    return ds_arcticdem\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.preprocess_legacy_fast","title":"preprocess_legacy_fast","text":"<pre><code>preprocess_legacy_fast(\n    ds_optical: xarray.Dataset,\n    ds_arcticdem: xarray.Dataset,\n    ds_tcvis: xarray.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_utils.cuda.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>The main difference to preprocess_legacy is the new data concept of the arcticdem. Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw arcticdem data and calculates slope and relative elevation on the fly.</p> <p>Parameters:</p> <ul> <li> <code>ds_optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The TCVIS dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_utils.cuda.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/legacy.py</code> <pre><code>@stopwatch(\"Preprocessing\", printer=logger.debug)\ndef preprocess_legacy_fast(\n    ds_optical: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    The main difference to preprocess_legacy is the new data concept of the arcticdem.\n    Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw\n    arcticdem data and calculates slope and relative elevation on the fly.\n\n    Args:\n        ds_optical (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Move to GPU for faster calculations\n    ds_optical = move_to_device(ds_optical, device)\n    # Calculate NDVI\n    ds_optical[\"ndvi\"] = calculate_ndvi(ds_optical)\n    ds_optical = move_to_host(ds_optical)\n\n    # Reproject TCVIS to optical data\n    with stopwatch(\"Reprojecting TCVIS\", printer=logger.debug):\n        ds_tcvis = ds_tcvis.odc.reproject(ds_optical.odc.geobox, resampling=\"cubic\")\n\n    ds_optical[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_optical[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_optical[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    with stopwatch(\"Reprojecting ArcticDEM\", printer=logger.debug):\n        ds_arcticdem = ds_arcticdem.odc.reproject(ds_optical.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n    # Move to same device as optical\n    ds_arcticdem = move_to_device(ds_arcticdem, device)\n    ds_arcticdem = preprocess_legacy_arcticdem_fast(ds_arcticdem, tpi_outer_radius, tpi_inner_radius)\n    ds_arcticdem = move_to_host(ds_arcticdem)\n\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_optical.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_optical)\n\n    ds_optical[\"dem\"] = ds_arcticdem.dem\n    ds_optical[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_optical[\"slope\"] = ds_arcticdem.slope\n    ds_optical[\"arcticdem_data_mask\"] = ds_arcticdem.arcticdem_data_mask\n\n    return ds_optical\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/","title":"v2","text":""},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2","title":"darts_preprocessing.v2","text":"<p>PLANET scene based preprocessing.</p>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_aspect","title":"calculate_aspect","text":"<pre><code>calculate_aspect(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate aspect (compass direction) of the terrain surface from an ArcticDEM Dataset.</p> <p>Aspect indicates the downslope direction of the maximum rate of change in elevation.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>aspect (float32): Aspect in degrees clockwise from north [0-360], or -1 for flat areas.</p> </li> <li> <p>long_name: \"Aspect\"</p> </li> <li>units: \"degrees\"</li> <li>description: Compass direction of slope</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Aspect values:</p> <ul> <li>0\u00b0 or 360\u00b0: North-facing</li> <li>90\u00b0: East-facing</li> <li>180\u00b0: South-facing</li> <li>270\u00b0: West-facing</li> <li>-1: Flat (no dominant direction)</li> </ul> Example <pre><code>from darts_preprocessing import calculate_aspect\n\narcticdem_with_aspect = calculate_aspect(arcticdem_ds)\n\n# Identify south-facing slopes (135-225 degrees)\nsouth_facing = (arcticdem_with_aspect.aspect &gt; 135) &amp; (arcticdem_with_aspect.aspect &lt; 225)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating aspect\", printer=logger.debug)\ndef calculate_aspect(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate aspect (compass direction) of the terrain surface from an ArcticDEM Dataset.\n\n    Aspect indicates the downslope direction of the maximum rate of change in elevation.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - aspect (float32): Aspect in degrees clockwise from north [0-360], or -1 for flat areas.\n\n            - long_name: \"Aspect\"\n            - units: \"degrees\"\n            - description: Compass direction of slope\n            - source: \"ArcticDEM\"\n\n    Note:\n        Aspect values:\n\n        - 0\u00b0 or 360\u00b0: North-facing\n        - 90\u00b0: East-facing\n        - 180\u00b0: South-facing\n        - 270\u00b0: West-facing\n        - -1: Flat (no dominant direction)\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_aspect\n\n        arcticdem_with_aspect = calculate_aspect(arcticdem_ds)\n\n        # Identify south-facing slopes (135-225 degrees)\n        south_facing = (arcticdem_with_aspect.aspect &gt; 135) &amp; (arcticdem_with_aspect.aspect &lt; 225)\n        ```\n\n    \"\"\"\n    aspect_deg = aspect(arcticdem_ds.dem)\n\n    # Aspect is always calculated in the projection - thus \"north\" is rather an \"up\"\n    # To get the true north, we need to correct the aspect based on the coordinates\n    x = arcticdem_ds.x.expand_dims({\"y\": arcticdem_ds.y})\n    y = arcticdem_ds.y.expand_dims({\"x\": arcticdem_ds.x})\n    if arcticdem_ds.cupy.is_cupy:\n        x = cp.asarray(x)\n        y = cp.asarray(y)\n        correction_offset = cp.arctan2(x, y) * (180 / np.pi) + 90\n    else:\n        correction_offset = np.arctan2(x, y) * (180 / np.pi) + 90\n    aspect_deg = (aspect_deg + correction_offset) % 360\n\n    aspect_deg.attrs = {\n        \"long_name\": \"Aspect\",\n        \"units\": \"degrees\",\n        \"description\": \"The compass direction that the slope faces, in degrees clockwise from north.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"aspect\"] = aspect_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_curvature","title":"calculate_curvature","text":"<pre><code>calculate_curvature(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate curvature of the terrain surface from an ArcticDEM Dataset.</p> <p>Curvature measures the rate of change of slope, indicating terrain convexity or concavity.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>curvature (float32): Curvature values.</p> </li> <li> <p>long_name: \"Curvature\"</p> </li> <li>description: Rate of change of slope</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Curvature interpretation:</p> <ul> <li>Positive values: Convex terrain (hills, ridges)</li> <li>Negative values: Concave terrain (valleys, depressions)</li> <li>Near zero: Planar terrain</li> </ul> Example <pre><code>from darts_preprocessing import calculate_curvature\n\narcticdem_with_curv = calculate_curvature(arcticdem_ds)\n\n# Identify ridges (convex areas)\nridges = arcticdem_with_curv.curvature &gt; 0.1\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating curvature\", printer=logger.debug)\ndef calculate_curvature(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate curvature of the terrain surface from an ArcticDEM Dataset.\n\n    Curvature measures the rate of change of slope, indicating terrain convexity or concavity.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - curvature (float32): Curvature values.\n\n            - long_name: \"Curvature\"\n            - description: Rate of change of slope\n            - source: \"ArcticDEM\"\n\n    Note:\n        Curvature interpretation:\n\n        - Positive values: Convex terrain (hills, ridges)\n        - Negative values: Concave terrain (valleys, depressions)\n        - Near zero: Planar terrain\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_curvature\n\n        arcticdem_with_curv = calculate_curvature(arcticdem_ds)\n\n        # Identify ridges (convex areas)\n        ridges = arcticdem_with_curv.curvature &gt; 0.1\n        ```\n\n    \"\"\"\n    curvature_da = curvature(arcticdem_ds.dem)\n    curvature_da.attrs = {\n        \"long_name\": \"Curvature\",\n        \"units\": \"\",\n        \"description\": \"The curvature of the terrain surface.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"curvature\"] = curvature_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_hillshade","title":"calculate_hillshade","text":"<pre><code>calculate_hillshade(\n    arcticdem_ds: xarray.Dataset,\n    azimuth: int = 225,\n    angle_altitude: int = 25,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate hillshade of the terrain surface from an ArcticDEM Dataset.</p> <p>Hillshade simulates illumination of terrain from a specified sun position, useful for visualization and terrain analysis.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> <li> <code>azimuth</code>               (<code>int</code>, default:                   <code>225</code> )           \u2013            <p>Light source azimuth in degrees clockwise from north [0-360]. Defaults to 225 (southwest).</p> </li> <li> <code>angle_altitude</code>               (<code>int</code>, default:                   <code>25</code> )           \u2013            <p>Light source altitude angle in degrees above horizon [0-90]. Defaults to 25.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>hillshade (float32): Illumination values [0-255], where 0 is shadow and 255 is fully lit.</p> </li> <li> <p>long_name: \"Hillshade\"</p> </li> <li>description: Documents azimuth and angle_altitude used</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Common azimuth/altitude combinations:</p> <ul> <li>315\u00b0/45\u00b0: Classic northwest illumination (default for many GIS applications)</li> <li>225\u00b0/25\u00b0: Southwest with low sun (better for visualizing subtle features)</li> </ul> <p>The hillshade calculation accounts for both slope and aspect of the terrain.</p> Example <pre><code>from darts_preprocessing import calculate_hillshade\n\n# Default southwest illumination\narcticdem_with_hs = calculate_hillshade(arcticdem_ds)\n\n# Custom sun position\narcticdem_custom = calculate_hillshade(\n    arcticdem_ds,\n    azimuth=315,\n    angle_altitude=45\n)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating hillshade\", printer=logger.debug, print_kwargs=[\"azimuth\", \"angle_altitude\"])\ndef calculate_hillshade(arcticdem_ds: xr.Dataset, azimuth: int = 225, angle_altitude: int = 25) -&gt; xr.Dataset:\n    \"\"\"Calculate hillshade of the terrain surface from an ArcticDEM Dataset.\n\n    Hillshade simulates illumination of terrain from a specified sun position, useful\n    for visualization and terrain analysis.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n        azimuth (int, optional): Light source azimuth in degrees clockwise from north [0-360].\n            Defaults to 225 (southwest).\n        angle_altitude (int, optional): Light source altitude angle in degrees above horizon [0-90].\n            Defaults to 25.\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - hillshade (float32): Illumination values [0-255], where 0 is shadow and 255 is fully lit.\n\n            - long_name: \"Hillshade\"\n            - description: Documents azimuth and angle_altitude used\n            - source: \"ArcticDEM\"\n\n    Note:\n        Common azimuth/altitude combinations:\n\n        - 315\u00b0/45\u00b0: Classic northwest illumination (default for many GIS applications)\n        - 225\u00b0/25\u00b0: Southwest with low sun (better for visualizing subtle features)\n\n        The hillshade calculation accounts for both slope and aspect of the terrain.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_hillshade\n\n        # Default southwest illumination\n        arcticdem_with_hs = calculate_hillshade(arcticdem_ds)\n\n        # Custom sun position\n        arcticdem_custom = calculate_hillshade(\n            arcticdem_ds,\n            azimuth=315,\n            angle_altitude=45\n        )\n        ```\n\n    \"\"\"\n    x, y = arcticdem_ds.x.mean().item(), arcticdem_ds.y.mean().item()\n    correction_offset = np.arctan2(x, y) * (180 / np.pi) + 90\n    azimuth_corrected = (azimuth - correction_offset + 360) % 360\n\n    hillshade_da = hillshade(arcticdem_ds.dem, azimuth=azimuth_corrected, angle_altitude=angle_altitude)\n    hillshade_da.attrs = {\n        \"long_name\": \"Hillshade\",\n        \"units\": \"\",\n        \"description\": f\"The hillshade based on azimuth {azimuth} and angle_altitude {angle_altitude}.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"hillshade\"] = hillshade_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_ndvi","title":"calculate_ndvi","text":"<pre><code>calculate_ndvi(optical: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Calculate NDVI (Normalized Difference Vegetation Index) from spectral bands.</p> <p>NDVI is a widely-used vegetation index that indicates photosynthetic activity and vegetation health. Values range from -1 to 1, with higher values indicating denser, healthier vegetation.</p> <p>Parameters:</p> <ul> <li> <code>optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing spectral bands: - nir (float32): Near-infrared reflectance [0-1] - red (float32): Red reflectance [0-1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.DataArray: NDVI values with attributes: - long_name: \"NDVI\"</p> </li> </ul> Note <p>Formula: NDVI = (NIR - Red) / (NIR + Red)</p> <p>Input bands are clipped to [0, 1] before calculation to avoid numerical instabilities from negative reflectance values or sensor artifacts. The final result is also clipped to ensure values remain in the valid [-1, 1] range.</p> Example <p>Calculate NDVI from optical data:</p> <pre><code>from darts_preprocessing import calculate_ndvi\n\n# optical contains 'nir' and 'red' bands\nndvi = calculate_ndvi(optical)\n\n# Mask vegetation\nvegetation_mask = ndvi &gt; 0.3\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch(\"Calculating NDVI\", printer=logger.debug)\ndef calculate_ndvi(optical: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI (Normalized Difference Vegetation Index) from spectral bands.\n\n    NDVI is a widely-used vegetation index that indicates photosynthetic activity and\n    vegetation health. Values range from -1 to 1, with higher values indicating denser,\n    healthier vegetation.\n\n    Args:\n        optical (xr.Dataset): Dataset containing spectral bands:\n            - nir (float32): Near-infrared reflectance [0-1]\n            - red (float32): Red reflectance [0-1]\n\n    Returns:\n        xr.DataArray: NDVI values with attributes:\n            - long_name: \"NDVI\"\n\n    Note:\n        Formula: NDVI = (NIR - Red) / (NIR + Red)\n\n        Input bands are clipped to [0, 1] before calculation to avoid numerical instabilities\n        from negative reflectance values or sensor artifacts. The final result is also clipped\n        to ensure values remain in the valid [-1, 1] range.\n\n    Example:\n        Calculate NDVI from optical data:\n\n        ```python\n        from darts_preprocessing import calculate_ndvi\n\n        # optical contains 'nir' and 'red' bands\n        ndvi = calculate_ndvi(optical)\n\n        # Mask vegetation\n        vegetation_mask = ndvi &gt; 0.3\n        ```\n\n    \"\"\"\n    nir = optical[\"nir\"].clip(0, 1)\n    r = optical[\"red\"].clip(0, 1)\n    ndvi = (nir - r) / (nir + r)\n    ndvi = ndvi.clip(-1, 1)\n    ndvi = ndvi.assign_attrs({\"long_name\": \"NDVI\"}).rename(\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_slope","title":"calculate_slope","text":"<pre><code>calculate_slope(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Slope represents the rate of change of elevation, indicating terrain steepness.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset containing: - dem (float32): Digital Elevation Model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input Dataset with new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>slope (float32): Slope in degrees [0-90].</p> </li> <li> <p>long_name: \"Slope\"</p> </li> <li>units: \"degrees\"</li> <li>source: \"ArcticDEM\"</li> </ul> </li> </ul> Note <p>Slope is calculated using finite difference methods on the DEM. Values approaching 90\u00b0 indicate near-vertical terrain.</p> Example <pre><code>from darts_preprocessing import calculate_slope\n\narcticdem_with_slope = calculate_slope(arcticdem_ds)\n\n# Mask steep terrain\nsteep_areas = arcticdem_with_slope.slope &gt; 30\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate slope of the terrain surface from an ArcticDEM Dataset.\n\n    Slope represents the rate of change of elevation, indicating terrain steepness.\n\n    Args:\n        arcticdem_ds (xr.Dataset): Dataset containing:\n            - dem (float32): Digital Elevation Model\n\n    Returns:\n        xr.Dataset: Input Dataset with new data variable added:\n\n        - slope (float32): Slope in degrees [0-90].\n\n            - long_name: \"Slope\"\n            - units: \"degrees\"\n            - source: \"ArcticDEM\"\n\n    Note:\n        Slope is calculated using finite difference methods on the DEM.\n        Values approaching 90\u00b0 indicate near-vertical terrain.\n\n    Example:\n        ```python\n        from darts_preprocessing import calculate_slope\n\n        arcticdem_with_slope = calculate_slope(arcticdem_ds)\n\n        # Mask steep terrain\n        steep_areas = arcticdem_with_slope.slope &gt; 30\n        ```\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_topographic_position_index","title":"calculate_topographic_position_index","text":"<pre><code>calculate_topographic_position_index(\n    arcticdem_ds: xarray.Dataset,\n    outer_radius: int,\n    inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>TPI measures the relative topographic position of a point by comparing its elevation to the mean elevation of the surrounding neighborhood. Positive values indicate higher positions (ridges), negative values indicate lower positions (valleys).</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable (float32).</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the neighborhood in meters. Can also be specified as string with units (e.g., \"100m\" or \"10px\").</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in meters. If &gt; 0, creates an annulus (ring) instead of a circle. Set to 0 for a circular kernel. Can also be specified as string with units (e.g., \"50m\" or \"5px\").</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with a new data variable added:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <ul> <li> <p>tpi (float32): Topographic Position Index values.</p> </li> <li> <p>long_name: \"Topographic Position Index (TPI)\"</p> </li> <li>description: Details about the kernel used</li> </ul> </li> </ul> Note <p>Kernel shape combinations:</p> <ul> <li>inner_radius=0: Circular kernel comparing each cell to all neighbors within outer_radius</li> <li>inner_radius&gt;0: Annulus kernel comparing each cell to neighbors in a ring between   inner_radius and outer_radius. Useful for multi-scale terrain analysis.</li> </ul> <p>The actual radii used are rounded to the nearest pixel based on the DEM resolution.</p> Example <p>Calculate TPI with circular and annulus kernels:</p> <pre><code>from darts_preprocessing import calculate_topographic_position_index\n\n# Circular kernel (100m radius)\narcticdem_with_tpi = calculate_topographic_position_index(\n    arcticdem_ds=arcticdem,\n    outer_radius=100,\n    inner_radius=0\n)\n\n# Annulus kernel (50-100m ring)\narcticdem_multi_scale = calculate_topographic_position_index(\n    arcticdem_ds=arcticdem,\n    outer_radius=100,\n    inner_radius=50\n)\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    TPI measures the relative topographic position of a point by comparing its elevation to\n    the mean elevation of the surrounding neighborhood. Positive values indicate higher\n    positions (ridges), negative values indicate lower positions (valleys).\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable (float32).\n        outer_radius (int): The outer radius of the neighborhood in meters.\n            Can also be specified as string with units (e.g., \"100m\" or \"10px\").\n        inner_radius (int): The inner radius of the annulus kernel in meters.\n            If &gt; 0, creates an annulus (ring) instead of a circle. Set to 0 for a circular kernel.\n            Can also be specified as string with units (e.g., \"50m\" or \"5px\").\n\n    Returns:\n        xr.Dataset: The input Dataset with a new data variable added:\n\n        - tpi (float32): Topographic Position Index values.\n\n            - long_name: \"Topographic Position Index (TPI)\"\n            - description: Details about the kernel used\n\n    Note:\n        Kernel shape combinations:\n\n        - inner_radius=0: Circular kernel comparing each cell to all neighbors within outer_radius\n        - inner_radius&gt;0: Annulus kernel comparing each cell to neighbors in a ring between\n          inner_radius and outer_radius. Useful for multi-scale terrain analysis.\n\n        The actual radii used are rounded to the nearest pixel based on the DEM resolution.\n\n    Example:\n        Calculate TPI with circular and annulus kernels:\n\n        ```python\n        from darts_preprocessing import calculate_topographic_position_index\n\n        # Circular kernel (100m radius)\n        arcticdem_with_tpi = calculate_topographic_position_index(\n            arcticdem_ds=arcticdem,\n            outer_radius=100,\n            inner_radius=0\n        )\n\n        # Annulus kernel (50-100m ring)\n        arcticdem_multi_scale = calculate_topographic_position_index(\n            arcticdem_ds=arcticdem,\n            outer_radius=100,\n            inner_radius=50\n        )\n        ```\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius: Distance = Distance.parse(outer_radius, cellsize_x)\n    if inner_radius &gt; 0:\n        inner_radius: Distance = Distance.parse(inner_radius, cellsize_x)\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius.meter, inner_radius.meter)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius}-{outer_radius} cells away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of {inner_radius}-{outer_radius} cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius.meter)\n        attr_cell_description = f\"within a circle at a distance of {outer_radius} cells away from the focal cell.\"\n        logger.debug(f\"Calculating Topographic Position Index with circle kernel of {outer_radius} cells.\")\n\n    # Change dtype of kernel to float32 since we don't need the precision and f32 is faster\n    kernel = kernel.astype(\"float32\")\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.get_azimuth_and_elevation","title":"get_azimuth_and_elevation","text":"<pre><code>get_azimuth_and_elevation(\n    ds_optical: xarray.Dataset,\n) -&gt; tuple[float, float]\n</code></pre> <p>Get the azimuth and elevation from the optical dataset attributes.</p> <p>Parameters:</p> <ul> <li> <code>ds_optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The optical dataset.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[float, float]</code>           \u2013            <p>tuple[float, float]: The azimuth and elevation.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/v2.py</code> <pre><code>def get_azimuth_and_elevation(ds_optical: xr.Dataset) -&gt; tuple[float, float]:\n    \"\"\"Get the azimuth and elevation from the optical dataset attributes.\n\n    Args:\n        ds_optical (xr.Dataset): The optical dataset.\n\n    Returns:\n        tuple[float, float]: The azimuth and elevation.\n\n    \"\"\"\n    azimuth = ds_optical.attrs.get(\"azimuth\", float(\"nan\"))\n    elevation = ds_optical.attrs.get(\"elevation\", float(\"nan\"))\n    if isnan(azimuth):\n        azimuth = 225\n        logger.warning(\"No azimuth found in optical dataset attributes. Using default value of 225 degrees.\")\n    if isnan(elevation):\n        elevation = 25\n        logger.warning(\"No sun elevation found in optical dataset attributes. Using default value of 25 degrees.\")\n    if not isinstance(azimuth, (int, float)):\n        azimuth = 225\n        logger.warning(\n            f\"Azimuth found in optical dataset is {azimuth}, which is not a number. Using default value of 225 degrees.\"\n        )\n    if not isinstance(elevation, (int, float)):\n        elevation = 25\n        logger.warning(\n            f\"Sun elevation found in optical dataset is {elevation}, which is not a number.\"\n            \" Using default value of 25 degrees.\"\n        )\n\n    azimuth = round(azimuth)\n    elevation = round(elevation)\n    return azimuth, elevation\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.preprocess_arcticdem","title":"preprocess_arcticdem","text":"<pre><code>preprocess_arcticdem(\n    ds_arcticdem: xarray.Dataset,\n    tpi_outer_radius: int,\n    tpi_inner_radius: int,\n    azimuth: int,\n    angle_altitude: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess the ArcticDEM data with mdoern (DARTS v2) preprocessing steps.</p> <p>Parameters:</p> <ul> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in number of cells.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in number of cells.</p> </li> <li> <code>azimuth</code>               (<code>int</code>)           \u2013            <p>The azimuth angle of the light source in degrees for hillshade calculation.</p> </li> <li> <code>angle_altitude</code>               (<code>int</code>)           \u2013            <p>The altitude angle of the light source in degrees for hillshade</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed ArcticDEM dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/v2.py</code> <pre><code>@stopwatch.f(\n    \"Preprocessing arcticdem\",\n    printer=logger.debug,\n    print_kwargs=[\"tpi_outer_radius\", \"tpi_inner_radius\", \"azimuth\", \"angle_altitude\"],\n)\ndef preprocess_arcticdem(\n    ds_arcticdem: xr.Dataset,\n    tpi_outer_radius: int,\n    tpi_inner_radius: int,\n    azimuth: int,\n    angle_altitude: int,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess the ArcticDEM data with mdoern (DARTS v2) preprocessing steps.\n\n    Args:\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        tpi_outer_radius (int): The outer radius of the annulus kernel for the tpi calculation in number of cells.\n        tpi_inner_radius (int): The inner radius of the annulus kernel for the tpi calculation in number of cells.\n        azimuth (int): The azimuth angle of the light source in degrees for hillshade calculation.\n        angle_altitude (int): The altitude angle of the light source in degrees for hillshade\n\n    Returns:\n        xr.Dataset: The preprocessed ArcticDEM dataset.\n\n    \"\"\"\n    ds_arcticdem = calculate_topographic_position_index(ds_arcticdem, tpi_outer_radius, tpi_inner_radius)\n    ds_arcticdem = calculate_slope(ds_arcticdem)\n    ds_arcticdem = calculate_hillshade(ds_arcticdem, azimuth=azimuth, angle_altitude=angle_altitude)\n    ds_arcticdem = calculate_aspect(ds_arcticdem)\n    ds_arcticdem = calculate_curvature(ds_arcticdem)\n\n    return ds_arcticdem\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.preprocess_v2","title":"preprocess_v2","text":"<pre><code>preprocess_v2(\n    ds_optical: xarray.Dataset,\n    ds_arcticdem: xarray.Dataset | None,\n    ds_tcvis: xarray.Dataset | None,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_utils.cuda.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess optical data with modern (DARTS v2) preprocessing steps.</p> <p>This function combines optical imagery with terrain (ArcticDEM) and temporal vegetation indices (TCVIS) to create a multi-source feature dataset for segmentation. All auxiliary data sources are reprojected and cropped to match the optical data's extent and resolution.</p> Processing steps <ol> <li>Calculate NDVI from optical bands</li> <li>If TCVIS provided: Reproject and merge Tasseled Cap trends</li> <li>If ArcticDEM provided: Calculate terrain features (TPI, slope, hillshade, aspect, curvature)    using solar geometry from optical data attributes</li> </ol> <p>Parameters:</p> <ul> <li> <code>ds_optical</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Optical imagery dataset (PlanetScope or Sentinel-2) containing: - Required variables: blue, green, red, nir (float32, reflectance values) - Required variables: quality_data_mask, valid_data_mask (uint8) - Required attributes: azimuth (float), elevation (float) for hillshade calculation</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset | None</code>)           \u2013            <p>ArcticDEM dataset containing 'dem' (float32) and 'arcticdem_data_mask' (uint8). If None, terrain features are skipped.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset | None</code>)           \u2013            <p>TCVIS dataset containing tc_brightness, tc_greenness, tc_wetness (float). If None, TCVIS features are skipped.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Outer radius for TPI calculation in meters. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Inner radius for TPI annulus kernel in meters. Set to 0 for circular kernel. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_utils.cuda.DEFAULT_DEVICE</code> )           \u2013            <p>Device for GPU-accelerated computations (NDVI, TPI, slope). Use \"cuda\" for first GPU, int for specific GPU, or \"cpu\". Defaults to \"cuda\" if available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Preprocessed dataset with all input optical variables plus:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Added from optical processing: - ndvi (float32): Normalized Difference Vegetation Index   Attributes: long_name=\"NDVI\"</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Added from TCVIS (if ds_tcvis provided): - tc_brightness (float): Tasseled Cap brightness trend - tc_greenness (float): Tasseled Cap greenness trend - tc_wetness (float): Tasseled Cap wetness trend</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Added from ArcticDEM (if ds_arcticdem provided): - dem (float32): Elevation in meters - relative_elevation (float32): Topographic Position Index (TPI)   Attributes: long_name=\"Topographic Position Index (TPI)\" - slope (float32): Slope in degrees [0-90]   Attributes: long_name=\"Slope\" - hillshade (uint8): Hillshade values [0-255]   Attributes: long_name=\"Hillshade\" - aspect (float32): Aspect in degrees [0-360]   Attributes: long_name=\"Aspect\" - curvature (float32): Surface curvature   Attributes: long_name=\"Curvature\" - arcticdem_data_mask (uint8): DEM validity mask</p> </li> </ul> Note <p>Attribute usage: - <code>azimuth</code> attribute from ds_optical: Used for hillshade calculation (solar azimuth angle).   Falls back to 225\u00b0 if missing or invalid. - <code>elevation</code> attribute from ds_optical: Used for hillshade calculation (solar elevation angle).   Falls back to 25\u00b0 if missing or invalid.</p> <p>Processing behavior: - If both ds_tcvis and ds_arcticdem are None, only NDVI is calculated. - ArcticDEM is buffered by tpi_outer_radius before reprojection to avoid edge effects,   then cropped back to optical extent after terrain feature calculation. - Reprojection uses cubic resampling for smooth terrain features. - GPU acceleration (if device=\"cuda\") significantly speeds up TPI and slope calculations.</p> Example <p>Complete preprocessing with all data sources:</p> <pre><code>from darts_preprocessing import preprocess_v2\nfrom darts_acquisition import load_cdse_s2_sr_scene, load_arcticdem, load_tcvis\n\n# Load optical data\noptical = load_cdse_s2_sr_scene(s2_scene_id, ...)\n\n# Load auxiliary data\narcticdem = load_arcticdem(optical.odc.geobox, ...)\ntcvis = load_tcvis(optical.odc.geobox, ...)\n\n# Preprocess\npreprocessed = preprocess_v2(\n    ds_optical=optical,\n    ds_arcticdem=arcticdem,\n    ds_tcvis=tcvis,\n    tpi_outer_radius=100,\n    tpi_inner_radius=0,\n    device=\"cuda\"\n)\n\n# Result contains: blue, green, red, nir, ndvi, tc_brightness, tc_greenness,\n# tc_wetness, dem, relative_elevation, slope, hillshade, aspect, curvature\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/v2.py</code> <pre><code>@stopwatch(\"Preprocessing\", printer=logger.debug)\ndef preprocess_v2(\n    ds_optical: xr.Dataset,\n    ds_arcticdem: xr.Dataset | None,\n    ds_tcvis: xr.Dataset | None,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with modern (DARTS v2) preprocessing steps.\n\n    This function combines optical imagery with terrain (ArcticDEM) and temporal vegetation\n    indices (TCVIS) to create a multi-source feature dataset for segmentation. All auxiliary\n    data sources are reprojected and cropped to match the optical data's extent and resolution.\n\n    Processing steps:\n        1. Calculate NDVI from optical bands\n        2. If TCVIS provided: Reproject and merge Tasseled Cap trends\n        3. If ArcticDEM provided: Calculate terrain features (TPI, slope, hillshade, aspect, curvature)\n           using solar geometry from optical data attributes\n\n    Args:\n        ds_optical (xr.Dataset): Optical imagery dataset (PlanetScope or Sentinel-2) containing:\n            - Required variables: blue, green, red, nir (float32, reflectance values)\n            - Required variables: quality_data_mask, valid_data_mask (uint8)\n            - Required attributes: azimuth (float), elevation (float) for hillshade calculation\n        ds_arcticdem (xr.Dataset | None): ArcticDEM dataset containing 'dem' (float32) and\n            'arcticdem_data_mask' (uint8). If None, terrain features are skipped.\n        ds_tcvis (xr.Dataset | None): TCVIS dataset containing tc_brightness, tc_greenness,\n            tc_wetness (float). If None, TCVIS features are skipped.\n        tpi_outer_radius (int, optional): Outer radius for TPI calculation in meters.\n            Defaults to 100m.\n        tpi_inner_radius (int, optional): Inner radius for TPI annulus kernel in meters.\n            Set to 0 for circular kernel. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): Device for GPU-accelerated computations\n            (NDVI, TPI, slope). Use \"cuda\" for first GPU, int for specific GPU, or \"cpu\".\n            Defaults to \"cuda\" if available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: Preprocessed dataset with all input optical variables plus:\n\n        Added from optical processing:\n            - ndvi (float32): Normalized Difference Vegetation Index\n              Attributes: long_name=\"NDVI\"\n\n        Added from TCVIS (if ds_tcvis provided):\n            - tc_brightness (float): Tasseled Cap brightness trend\n            - tc_greenness (float): Tasseled Cap greenness trend\n            - tc_wetness (float): Tasseled Cap wetness trend\n\n        Added from ArcticDEM (if ds_arcticdem provided):\n            - dem (float32): Elevation in meters\n            - relative_elevation (float32): Topographic Position Index (TPI)\n              Attributes: long_name=\"Topographic Position Index (TPI)\"\n            - slope (float32): Slope in degrees [0-90]\n              Attributes: long_name=\"Slope\"\n            - hillshade (uint8): Hillshade values [0-255]\n              Attributes: long_name=\"Hillshade\"\n            - aspect (float32): Aspect in degrees [0-360]\n              Attributes: long_name=\"Aspect\"\n            - curvature (float32): Surface curvature\n              Attributes: long_name=\"Curvature\"\n            - arcticdem_data_mask (uint8): DEM validity mask\n\n    Note:\n        Attribute usage:\n        - `azimuth` attribute from ds_optical: Used for hillshade calculation (solar azimuth angle).\n          Falls back to 225\u00b0 if missing or invalid.\n        - `elevation` attribute from ds_optical: Used for hillshade calculation (solar elevation angle).\n          Falls back to 25\u00b0 if missing or invalid.\n\n        Processing behavior:\n        - If both ds_tcvis and ds_arcticdem are None, only NDVI is calculated.\n        - ArcticDEM is buffered by tpi_outer_radius before reprojection to avoid edge effects,\n          then cropped back to optical extent after terrain feature calculation.\n        - Reprojection uses cubic resampling for smooth terrain features.\n        - GPU acceleration (if device=\"cuda\") significantly speeds up TPI and slope calculations.\n\n    Example:\n        Complete preprocessing with all data sources:\n\n        ```python\n        from darts_preprocessing import preprocess_v2\n        from darts_acquisition import load_cdse_s2_sr_scene, load_arcticdem, load_tcvis\n\n        # Load optical data\n        optical = load_cdse_s2_sr_scene(s2_scene_id, ...)\n\n        # Load auxiliary data\n        arcticdem = load_arcticdem(optical.odc.geobox, ...)\n        tcvis = load_tcvis(optical.odc.geobox, ...)\n\n        # Preprocess\n        preprocessed = preprocess_v2(\n            ds_optical=optical,\n            ds_arcticdem=arcticdem,\n            ds_tcvis=tcvis,\n            tpi_outer_radius=100,\n            tpi_inner_radius=0,\n            device=\"cuda\"\n        )\n\n        # Result contains: blue, green, red, nir, ndvi, tc_brightness, tc_greenness,\n        # tc_wetness, dem, relative_elevation, slope, hillshade, aspect, curvature\n        ```\n\n    \"\"\"\n    # Move to GPU for faster calculations\n    ds_optical = move_to_device(ds_optical, device)\n    # Calculate NDVI\n    ds_optical[\"ndvi\"] = calculate_ndvi(ds_optical)\n    ds_optical = move_to_host(ds_optical)\n\n    if ds_tcvis is None and ds_arcticdem is None:\n        logger.debug(\"No auxiliary data provided. Only NDVI was calculated.\")\n        return ds_optical\n\n    if ds_tcvis is not None:\n        # Reproject TCVIS to optical data\n        with stopwatch(\"Reprojecting TCVIS\", printer=logger.debug):\n            # *: Reprojecting this way will not alter the datatype of the data!\n            # Should be uint8 before and after reprojection.\n            ds_tcvis = ds_tcvis.odc.reproject(ds_optical.odc.geobox, resampling=\"cubic\")\n\n        # !: Reprojecting with f64 coordinates and values behind the decimal point can result in a coordinate missmatch:\n        # E.g. ds_optical has x coordinates [2.123, ...] then is can happen that the\n        # reprojected ds_tcvis coordinates are [2.12300001, ...]\n        # This results is all-nan assigments later when adding the variables of the reprojected dataset to the original\n        assert (ds_optical.x == ds_tcvis.x).all(), \"x coordinates do not match! See code comment above\"\n        assert (ds_optical.y == ds_tcvis.y).all(), \"y coordinates do not match! See code comment above\"\n\n        # ?: Do ds_tcvis and ds_optical now share the same memory on the GPU?\n        #  or do I need to delete ds_tcvis to free memory?\n        # Same question for ArcticDEM\n        ds_optical[\"tc_brightness\"] = ds_tcvis.tc_brightness\n        ds_optical[\"tc_greenness\"] = ds_tcvis.tc_greenness\n        ds_optical[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    if ds_arcticdem is not None:\n        # Calculate TPI and slope from ArcticDEM\n        with stopwatch(\"Reprojecting ArcticDEM\", printer=logger.debug):\n            ds_arcticdem = ds_arcticdem.odc.reproject(\n                ds_optical.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\"\n            )\n        # Move to same device as optical\n        ds_arcticdem = move_to_device(ds_arcticdem, device)\n\n        assert (ds_optical.x == ds_arcticdem.x).all(), \"x coordinates do not match! See code comment above\"\n        assert (ds_optical.y == ds_arcticdem.y).all(), \"y coordinates do not match! See code comment above\"\n\n        azimuth, angle_altitude = get_azimuth_and_elevation(ds_optical)\n        ds_arcticdem = preprocess_arcticdem(\n            ds_arcticdem,\n            tpi_outer_radius,\n            tpi_inner_radius,\n            azimuth,\n            angle_altitude,\n        )\n        ds_arcticdem = move_to_host(ds_arcticdem)\n\n        # TODO: Check if crop can be done with apply_mask = False\n        # -&gt; Then the type conversion of the arcticdem data mask would not be necessary anymore\n        # -&gt; And this would also allow to keep the data on the GPU\n        ds_arcticdem = ds_arcticdem.odc.crop(ds_optical.odc.geobox.extent)\n        # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n        # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n        ds_arcticdem[\"x\"] = ds_optical.x\n        ds_arcticdem[\"y\"] = ds_optical.y\n\n        ds_optical[\"dem\"] = ds_arcticdem.dem\n        ds_optical[\"relative_elevation\"] = ds_arcticdem.tpi\n        ds_optical[\"slope\"] = ds_arcticdem.slope\n        ds_optical[\"hillshade\"] = ds_arcticdem.hillshade\n        ds_optical[\"aspect\"] = ds_arcticdem.aspect\n        ds_optical[\"curvature\"] = ds_arcticdem.curvature\n        ds_optical[\"arcticdem_data_mask\"] = ds_arcticdem.arcticdem_data_mask.astype(\"uint8\")\n\n    return ds_optical\n</code></pre>"},{"location":"reference/darts_postprocessing/","title":"darts_postprocessing","text":""},{"location":"reference/darts_postprocessing/#darts_postprocessing","title":"darts_postprocessing","text":"<p>Postprocessing steps for the DARTS dataset.</p>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.binarize","title":"binarize","text":"<pre><code>binarize(\n    probs: xarray.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xarray.DataArray,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.DataArray\n</code></pre> <p>Binarize segmentation probabilities with quality-based filtering.</p> <p>This function converts continuous probability predictions to binary segmentation masks by applying thresholding, removing edge artifacts, and filtering small objects.</p> Processing steps <ol> <li>Threshold probabilities (prob &gt; threshold \u2192 1, else \u2192 0)</li> <li>Identify and remove objects touching invalid regions or tile edges</li> <li>Remove objects smaller than min_object_size</li> </ol> <p>Parameters:</p> <ul> <li> <code>probs</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Segmentation probabilities (float32, range [0-1]). NaN values are treated as 0 (no detection).</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Probability threshold for binarization. Typical values: 0.3-0.7.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>)           \u2013            <p>Minimum object size in pixels. Objects with fewer pixels are removed.</p> </li> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Quality mask (uint8, 1=valid, 0=invalid). Objects overlapping invalid regions are removed to avoid artifacts at data boundaries.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>Device for processing. GPU acceleration recommended for large tiles.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Binary segmentation mask (bool, True=object, False=background).</p> </li> </ul> Note <p>Edge and boundary handling: - Objects touching tile edges or invalid data regions (mask==0) are completely removed - This prevents partial objects at boundaries from being misclassified - Uses connected component analysis (8-connectivity) to identify touching objects</p> <p>GPU acceleration: - Object removal operations are significantly faster on GPU - Automatically handles dask arrays by persisting before GPU operations</p> Example <p>Binarize with standard parameters:</p> <pre><code>from darts_postprocessing import binarize, erode_mask\n\n# Erode quality mask first\neroded_mask = erode_mask(\n    tile[\"quality_data_mask\"] == 2,  # High quality only\n    size=10,\n    device=\"cuda\"\n)\n\n# Binarize predictions\nbinary_mask = binarize(\n    probs=tile[\"probabilities\"],\n    threshold=0.5,\n    min_object_size=32,  # Remove objects &lt; 32 pixels\n    mask=eroded_mask,\n    device=\"cuda\"\n)\n</code></pre> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\"Binarizing probabilities\", printer=logger.debug, print_kwargs=[\"threshold\", \"min_object_size\"])\ndef binarize(\n    probs: xr.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xr.DataArray,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xr.DataArray:\n    \"\"\"Binarize segmentation probabilities with quality-based filtering.\n\n    This function converts continuous probability predictions to binary segmentation masks\n    by applying thresholding, removing edge artifacts, and filtering small objects.\n\n    Processing steps:\n        1. Threshold probabilities (prob &gt; threshold \u2192 1, else \u2192 0)\n        2. Identify and remove objects touching invalid regions or tile edges\n        3. Remove objects smaller than min_object_size\n\n    Args:\n        probs (xr.DataArray): Segmentation probabilities (float32, range [0-1]).\n            NaN values are treated as 0 (no detection).\n        threshold (float): Probability threshold for binarization. Typical values: 0.3-0.7.\n        min_object_size (int): Minimum object size in pixels. Objects with fewer pixels are removed.\n        mask (xr.DataArray): Quality mask (uint8, 1=valid, 0=invalid). Objects overlapping\n            invalid regions are removed to avoid artifacts at data boundaries.\n        device (Literal[\"cuda\", \"cpu\"] | int): Device for processing. GPU acceleration\n            recommended for large tiles.\n\n    Returns:\n        xr.DataArray: Binary segmentation mask (bool, True=object, False=background).\n\n    Note:\n        Edge and boundary handling:\n        - Objects touching tile edges or invalid data regions (mask==0) are completely removed\n        - This prevents partial objects at boundaries from being misclassified\n        - Uses connected component analysis (8-connectivity) to identify touching objects\n\n        GPU acceleration:\n        - Object removal operations are significantly faster on GPU\n        - Automatically handles dask arrays by persisting before GPU operations\n\n    Example:\n        Binarize with standard parameters:\n\n        ```python\n        from darts_postprocessing import binarize, erode_mask\n\n        # Erode quality mask first\n        eroded_mask = erode_mask(\n            tile[\"quality_data_mask\"] == 2,  # High quality only\n            size=10,\n            device=\"cuda\"\n        )\n\n        # Binarize predictions\n        binary_mask = binarize(\n            probs=tile[\"probabilities\"],\n            threshold=0.5,\n            min_object_size=32,  # Remove objects &lt; 32 pixels\n            mask=eroded_mask,\n            device=\"cuda\"\n        )\n        ```\n\n    \"\"\"\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Where the output from the ensemble / segmentation is nan turn it into 0, else threshold it\n    # Also, where there was no valid input data, turn it into 0\n    # Using uint8 for further processing\n    binarized = (probs.fillna(0) &gt; threshold).astype(\"uint8\")\n\n    # Remove objects at which overlap with either the edge of the tile or the noData mask\n    labels = binarized.copy(data=label(binarized, connectivity=2))\n    edge_label_ids = np.unique(xr.where(~mask, labels, 0))\n    binarized = ~labels.isin(edge_label_ids) &amp; binarized\n\n    # Remove small objects with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving binarized to GPU:{device}.\")\n        # Check if binarized is dask, if not persist it, since remove_small_objects_gpu can't be calculated from\n        # cupy-dask arrays\n        if binarized.chunks is not None:\n            binarized = binarized.persist()\n        with cp.cuda.Device(device_nr):\n            binarized = binarized.cupy.as_cupy()\n            binarized.values = remove_small_objects_gpu(\n                binarized.astype(bool).expand_dims(\"batch\", 0).data, min_size=min_object_size\n            )[0]\n            binarized = binarized.cupy.as_numpy()\n            free_cupy()\n    else:\n        binarized.values = remove_small_objects(\n            binarized.astype(bool).expand_dims(\"batch\", 0).values, min_size=min_object_size\n        )[0]\n\n    # Convert back to bool\n    binarized = binarized.astype(\"bool\")\n\n    return binarized\n</code></pre>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.erode_mask","title":"erode_mask","text":"<pre><code>erode_mask(\n    mask: xarray.DataArray,\n    size: int,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n    edge_size: int | None = None,\n) -&gt; xarray.DataArray\n</code></pre> <p>Erode a binary mask and invalidate edge regions.</p> <p>This function applies morphological erosion to shrink valid regions in a mask and additionally sets a border region around the entire mask to invalid. This is useful for removing unreliable predictions near tile edges and data boundaries.</p> <p>Parameters:</p> <ul> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Binary mask to erode (1=valid, 0=invalid). Will be converted to uint8.</p> </li> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>Radius of the disk structuring element for erosion in pixels. Also used as the width of the edge region to invalidate (unless edge_size is specified).</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>Device for processing. Use \"cuda\" for GPU acceleration, \"cpu\" for CPU processing, or an integer to specify a GPU device number.</p> </li> <li> <code>edge_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Width of the edge region to set to invalid in pixels. If None, uses the <code>size</code> parameter. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Eroded mask (uint8, 1=valid, 0=invalid) with edges invalidated.</p> </li> </ul> Note <p>GPU acceleration (requires cucim): - Significantly faster for large masks - Automatically falls back to CPU if cucim is not available - Handles both in-memory and dask arrays</p> <p>The erosion operation shrinks valid regions by removing pixels within <code>size</code> distance from invalid regions. Edge invalidation then sets the outermost <code>edge_size</code> pixels on all sides to 0.</p> Example <p>Erode mask to remove edge effects:</p> <pre><code>from darts_postprocessing import erode_mask\n\n# Erode by 10 pixels and invalidate 10-pixel edges\neroded = erode_mask(\n    mask=quality_mask,\n    size=10,\n    device=\"cuda\"\n)\n\n# Erode by 5 pixels but invalidate 20-pixel edges\neroded_custom = erode_mask(\n    mask=quality_mask,\n    size=5,\n    edge_size=20,\n    device=\"cpu\"\n)\n</code></pre> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\"Eroding mask\", printer=logger.debug, print_kwargs=[\"size\"])\ndef erode_mask(\n    mask: xr.DataArray, size: int, device: Literal[\"cuda\", \"cpu\"] | int, edge_size: int | None = None\n) -&gt; xr.DataArray:\n    \"\"\"Erode a binary mask and invalidate edge regions.\n\n    This function applies morphological erosion to shrink valid regions in a mask and\n    additionally sets a border region around the entire mask to invalid. This is useful\n    for removing unreliable predictions near tile edges and data boundaries.\n\n    Args:\n        mask (xr.DataArray): Binary mask to erode (1=valid, 0=invalid). Will be converted to uint8.\n        size (int): Radius of the disk structuring element for erosion in pixels.\n            Also used as the width of the edge region to invalidate (unless edge_size is specified).\n        device (Literal[\"cuda\", \"cpu\"] | int): Device for processing. Use \"cuda\" for GPU acceleration,\n            \"cpu\" for CPU processing, or an integer to specify a GPU device number.\n        edge_size (int | None, optional): Width of the edge region to set to invalid in pixels.\n            If None, uses the `size` parameter. Defaults to None.\n\n    Returns:\n        xr.DataArray: Eroded mask (uint8, 1=valid, 0=invalid) with edges invalidated.\n\n    Note:\n        GPU acceleration (requires cucim):\n        - Significantly faster for large masks\n        - Automatically falls back to CPU if cucim is not available\n        - Handles both in-memory and dask arrays\n\n        The erosion operation shrinks valid regions by removing pixels within `size` distance\n        from invalid regions. Edge invalidation then sets the outermost `edge_size` pixels\n        on all sides to 0.\n\n    Example:\n        Erode mask to remove edge effects:\n\n        ```python\n        from darts_postprocessing import erode_mask\n\n        # Erode by 10 pixels and invalidate 10-pixel edges\n        eroded = erode_mask(\n            mask=quality_mask,\n            size=10,\n            device=\"cuda\"\n        )\n\n        # Erode by 5 pixels but invalidate 20-pixel edges\n        eroded_custom = erode_mask(\n            mask=quality_mask,\n            size=5,\n            edge_size=20,\n            device=\"cpu\"\n        )\n        ```\n\n    \"\"\"\n    # Clone mask to avoid in-place operations\n    mask = mask.copy()\n\n    # Change to dtype uint8 for faster skimage operations\n    mask = mask.astype(\"uint8\")\n\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Dilate the mask with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving mask to GPU:{device}.\")\n        # Check if mask is dask, if not persist it, since dilation can't be calculated from cupy-dask arrays\n        if mask.chunks is not None:\n            mask = mask.persist()\n        with cp.cuda.Device(device_nr):\n            mask = mask.cupy.as_cupy()\n            mask.values = binary_erosion_gpu(mask.data, disk_gpu(size))\n            mask = mask.cupy.as_numpy()\n            free_cupy()\n    else:\n        mask.values = binary_erosion(mask.values, disk(size))\n\n    if edge_size is None:\n        edge_size = size\n\n    # Mask edges\n    mask[:edge_size, :] = 0\n    mask[-edge_size:, :] = 0\n    mask[:, :edge_size] = 0\n    mask[:, -edge_size:] = 0\n\n    return mask\n</code></pre>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.prepare_export","title":"prepare_export","text":"<pre><code>prepare_export(\n    tile: xarray.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 0,\n    ensemble_subsets: list[str] = [],\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_postprocessing.postprocess.DEFAULT_DEVICE,\n    edge_erosion_size: int | None = None,\n) -&gt; xarray.Dataset\n</code></pre> <p>Prepare segmentation results for export by applying quality filtering and binarization.</p> <p>This is a wrapper function that orchestrates the complete postprocessing pipeline: mask erosion, probability masking, and binarization. It processes both ensemble-averaged predictions and individual model outputs if present.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Input tile from inference containing: - probabilities (float32): Segmentation probabilities [0-1] - quality_data_mask (uint8): Quality mask (0=invalid, 1=low quality, 2=high quality) - probabilities-{subset} (float32): Optional individual model predictions</p> </li> <li> <code>bin_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability threshold for binarization. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Erosion radius for quality mask in pixels. Also used for edge invalidation unless edge_erosion_size is specified. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Minimum object size in pixels. Smaller objects are removed. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>0</code> )           \u2013            <p>Quality threshold for masking. Maps to quality_data_mask values: - \"high_quality\" or 2: Only use high quality pixels - \"low_quality\" or 1: Use low and high quality pixels - \"none\" or 0: No quality masking Defaults to 0 (no masking).</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>Names of individual models in the ensemble to process (e.g., [\"with_tcvis\", \"without_tcvis\"]). Defaults to [].</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_postprocessing.postprocess.DEFAULT_DEVICE</code> )           \u2013            <p>Device for processing. Defaults to GPU if available.</p> </li> <li> <code>edge_erosion_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Separate erosion width for tile edges in pixels. If None, uses mask_erosion_size. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input tile augmented with:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Added data variables: - extent (uint8): Valid extent mask after erosion (1=valid, 0=invalid).   Attributes: long_name=\"Extent of the segmentation\" - binarized_segmentation (bool): Binary segmentation mask (True=object).   Attributes: long_name=\"Binarized Segmentation\" - binarized_segmentation-{subset} (bool): Binary masks for each ensemble subset   (only if ensemble_subsets provided)</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Modified data variables: - probabilities (float32): Now masked to valid extent (NaN outside) - probabilities-{subset} (float32): Masked individual model predictions</p> </li> </ul> Note <p>Processing pipeline: 1. Filter quality mask to specified quality_level 2. Erode quality mask to remove boundary artifacts 3. Create extent mask from eroded quality mask 4. Mask probabilities to valid extent (NaN invalid regions) 5. Binarize masked probabilities with threshold and min object size filter 6. Repeat steps 4-5 for each ensemble subset if provided</p> <p>The extent variable defines the reliable prediction region and should be used to clip exported results.</p> Example <p>Complete postprocessing workflow:</p> <pre><code>from darts_postprocessing import prepare_export\n\n# After ensemble inference\nprocessed_tile = prepare_export(\n    tile=ensemble_result,\n    bin_threshold=0.5,\n    mask_erosion_size=10,\n    min_object_size=32,\n    quality_level=\"high_quality\",  # Only high quality pixels\n    ensemble_subsets=[\"with_tcvis\", \"without_tcvis\"],\n    device=\"cuda\"\n)\n\n# Now ready for export with:\n# - processed_tile[\"binarized_segmentation\"]: Main binary result\n# - processed_tile[\"extent\"]: Valid data extent\n# - processed_tile[\"probabilities\"]: Masked probabilities\n</code></pre> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\n    \"Preparing export\",\n    printer=logger.debug,\n    print_kwargs=[\n        \"bin_threshold\",\n        \"mask_erosion_size\",\n        \"edge_erosion_size\",\n        \"min_object_size\",\n        \"quality_level\",\n        \"ensemble_subsets\",\n    ],\n)\ndef prepare_export(\n    tile: xr.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int | Literal[\"high_quality\", \"low_quality\", \"none\"] = 0,\n    ensemble_subsets: list[str] = [],\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n    edge_erosion_size: int | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Prepare segmentation results for export by applying quality filtering and binarization.\n\n    This is a wrapper function that orchestrates the complete postprocessing pipeline:\n    mask erosion, probability masking, and binarization. It processes both ensemble-averaged\n    predictions and individual model outputs if present.\n\n    Args:\n        tile (xr.Dataset): Input tile from inference containing:\n            - probabilities (float32): Segmentation probabilities [0-1]\n            - quality_data_mask (uint8): Quality mask (0=invalid, 1=low quality, 2=high quality)\n            - probabilities-{subset} (float32): Optional individual model predictions\n        bin_threshold (float, optional): Probability threshold for binarization. Defaults to 0.5.\n        mask_erosion_size (int, optional): Erosion radius for quality mask in pixels. Also used\n            for edge invalidation unless edge_erosion_size is specified. Defaults to 10.\n        min_object_size (int, optional): Minimum object size in pixels. Smaller objects are removed.\n            Defaults to 32.\n        quality_level (int | Literal[\"high_quality\", \"low_quality\", \"none\"], optional):\n            Quality threshold for masking. Maps to quality_data_mask values:\n            - \"high_quality\" or 2: Only use high quality pixels\n            - \"low_quality\" or 1: Use low and high quality pixels\n            - \"none\" or 0: No quality masking\n            Defaults to 0 (no masking).\n        ensemble_subsets (list[str], optional): Names of individual models in the ensemble to\n            process (e.g., [\"with_tcvis\", \"without_tcvis\"]). Defaults to [].\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): Device for processing. Defaults to GPU if available.\n        edge_erosion_size (int | None, optional): Separate erosion width for tile edges in pixels.\n            If None, uses mask_erosion_size. Defaults to None.\n\n    Returns:\n        xr.Dataset: Input tile augmented with:\n\n        Added data variables:\n            - extent (uint8): Valid extent mask after erosion (1=valid, 0=invalid).\n              Attributes: long_name=\"Extent of the segmentation\"\n            - binarized_segmentation (bool): Binary segmentation mask (True=object).\n              Attributes: long_name=\"Binarized Segmentation\"\n            - binarized_segmentation-{subset} (bool): Binary masks for each ensemble subset\n              (only if ensemble_subsets provided)\n\n        Modified data variables:\n            - probabilities (float32): Now masked to valid extent (NaN outside)\n            - probabilities-{subset} (float32): Masked individual model predictions\n\n    Note:\n        Processing pipeline:\n        1. Filter quality mask to specified quality_level\n        2. Erode quality mask to remove boundary artifacts\n        3. Create extent mask from eroded quality mask\n        4. Mask probabilities to valid extent (NaN invalid regions)\n        5. Binarize masked probabilities with threshold and min object size filter\n        6. Repeat steps 4-5 for each ensemble subset if provided\n\n        The extent variable defines the reliable prediction region and should be used to\n        clip exported results.\n\n    Example:\n        Complete postprocessing workflow:\n\n        ```python\n        from darts_postprocessing import prepare_export\n\n        # After ensemble inference\n        processed_tile = prepare_export(\n            tile=ensemble_result,\n            bin_threshold=0.5,\n            mask_erosion_size=10,\n            min_object_size=32,\n            quality_level=\"high_quality\",  # Only high quality pixels\n            ensemble_subsets=[\"with_tcvis\", \"without_tcvis\"],\n            device=\"cuda\"\n        )\n\n        # Now ready for export with:\n        # - processed_tile[\"binarized_segmentation\"]: Main binary result\n        # - processed_tile[\"extent\"]: Valid data extent\n        # - processed_tile[\"probabilities\"]: Masked probabilities\n        ```\n\n    \"\"\"\n    quality_level = (\n        quality_level\n        if isinstance(quality_level, int)\n        else {\"high_quality\": 2, \"low_quality\": 1, \"none\": 0}[quality_level]\n    )\n    mask = tile[\"quality_data_mask\"] &gt;= quality_level\n    if quality_level &gt; 0:\n        mask = erode_mask(mask, mask_erosion_size, device, edge_size=edge_erosion_size)  # 0=positive, 1=negative\n    tile[\"extent\"] = mask.copy()\n    tile[\"extent\"].attrs = {\"long_name\": \"Extent of the segmentation\"}\n\n    def _prep_layer(tile: xr.Dataset, subset: str | None = None):\n        layername = \"probabilities\" if subset is None else f\"probabilities-{subset}\"\n        binarized_layername = \"binarized_segmentation\" if subset is None else f\"binarized_segmentation-{subset}\"\n\n        # Mask the segmentation\n        tile[layername] = tile[layername].where(mask)\n\n        # Binarize the segmentation\n        tile[binarized_layername] = binarize(tile[layername], bin_threshold, min_object_size, mask, device)\n        tile[binarized_layername].attrs = {\"long_name\": \"Binarized Segmentation\"}\n        return tile\n\n    tile = _prep_layer(tile)\n\n    # get the names of the model probabilities if available\n    # for example 'tcvis' from 'probabilities-tcvis'\n    for ensemble_subset in ensemble_subsets:\n        tile = _prep_layer(tile, ensemble_subset)\n\n    return tile\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/","title":"postprocess","text":""},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess","title":"darts_postprocessing.postprocess","text":"<p>Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.</p>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.CUCIM_AVAILABLE","title":"CUCIM_AVAILABLE  <code>module-attribute</code>","text":"<pre><code>CUCIM_AVAILABLE = True\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.DEFAULT_DEVICE","title":"DEFAULT_DEVICE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DEVICE = 'cuda'\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.binarize","title":"binarize","text":"<pre><code>binarize(\n    probs: xarray.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xarray.DataArray,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.DataArray\n</code></pre> <p>Binarize segmentation probabilities with quality-based filtering.</p> <p>This function converts continuous probability predictions to binary segmentation masks by applying thresholding, removing edge artifacts, and filtering small objects.</p> Processing steps <ol> <li>Threshold probabilities (prob &gt; threshold \u2192 1, else \u2192 0)</li> <li>Identify and remove objects touching invalid regions or tile edges</li> <li>Remove objects smaller than min_object_size</li> </ol> <p>Parameters:</p> <ul> <li> <code>probs</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Segmentation probabilities (float32, range [0-1]). NaN values are treated as 0 (no detection).</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Probability threshold for binarization. Typical values: 0.3-0.7.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>)           \u2013            <p>Minimum object size in pixels. Objects with fewer pixels are removed.</p> </li> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Quality mask (uint8, 1=valid, 0=invalid). Objects overlapping invalid regions are removed to avoid artifacts at data boundaries.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>Device for processing. GPU acceleration recommended for large tiles.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Binary segmentation mask (bool, True=object, False=background).</p> </li> </ul> Note <p>Edge and boundary handling: - Objects touching tile edges or invalid data regions (mask==0) are completely removed - This prevents partial objects at boundaries from being misclassified - Uses connected component analysis (8-connectivity) to identify touching objects</p> <p>GPU acceleration: - Object removal operations are significantly faster on GPU - Automatically handles dask arrays by persisting before GPU operations</p> Example <p>Binarize with standard parameters:</p> <pre><code>from darts_postprocessing import binarize, erode_mask\n\n# Erode quality mask first\neroded_mask = erode_mask(\n    tile[\"quality_data_mask\"] == 2,  # High quality only\n    size=10,\n    device=\"cuda\"\n)\n\n# Binarize predictions\nbinary_mask = binarize(\n    probs=tile[\"probabilities\"],\n    threshold=0.5,\n    min_object_size=32,  # Remove objects &lt; 32 pixels\n    mask=eroded_mask,\n    device=\"cuda\"\n)\n</code></pre> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\"Binarizing probabilities\", printer=logger.debug, print_kwargs=[\"threshold\", \"min_object_size\"])\ndef binarize(\n    probs: xr.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xr.DataArray,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xr.DataArray:\n    \"\"\"Binarize segmentation probabilities with quality-based filtering.\n\n    This function converts continuous probability predictions to binary segmentation masks\n    by applying thresholding, removing edge artifacts, and filtering small objects.\n\n    Processing steps:\n        1. Threshold probabilities (prob &gt; threshold \u2192 1, else \u2192 0)\n        2. Identify and remove objects touching invalid regions or tile edges\n        3. Remove objects smaller than min_object_size\n\n    Args:\n        probs (xr.DataArray): Segmentation probabilities (float32, range [0-1]).\n            NaN values are treated as 0 (no detection).\n        threshold (float): Probability threshold for binarization. Typical values: 0.3-0.7.\n        min_object_size (int): Minimum object size in pixels. Objects with fewer pixels are removed.\n        mask (xr.DataArray): Quality mask (uint8, 1=valid, 0=invalid). Objects overlapping\n            invalid regions are removed to avoid artifacts at data boundaries.\n        device (Literal[\"cuda\", \"cpu\"] | int): Device for processing. GPU acceleration\n            recommended for large tiles.\n\n    Returns:\n        xr.DataArray: Binary segmentation mask (bool, True=object, False=background).\n\n    Note:\n        Edge and boundary handling:\n        - Objects touching tile edges or invalid data regions (mask==0) are completely removed\n        - This prevents partial objects at boundaries from being misclassified\n        - Uses connected component analysis (8-connectivity) to identify touching objects\n\n        GPU acceleration:\n        - Object removal operations are significantly faster on GPU\n        - Automatically handles dask arrays by persisting before GPU operations\n\n    Example:\n        Binarize with standard parameters:\n\n        ```python\n        from darts_postprocessing import binarize, erode_mask\n\n        # Erode quality mask first\n        eroded_mask = erode_mask(\n            tile[\"quality_data_mask\"] == 2,  # High quality only\n            size=10,\n            device=\"cuda\"\n        )\n\n        # Binarize predictions\n        binary_mask = binarize(\n            probs=tile[\"probabilities\"],\n            threshold=0.5,\n            min_object_size=32,  # Remove objects &lt; 32 pixels\n            mask=eroded_mask,\n            device=\"cuda\"\n        )\n        ```\n\n    \"\"\"\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Where the output from the ensemble / segmentation is nan turn it into 0, else threshold it\n    # Also, where there was no valid input data, turn it into 0\n    # Using uint8 for further processing\n    binarized = (probs.fillna(0) &gt; threshold).astype(\"uint8\")\n\n    # Remove objects at which overlap with either the edge of the tile or the noData mask\n    labels = binarized.copy(data=label(binarized, connectivity=2))\n    edge_label_ids = np.unique(xr.where(~mask, labels, 0))\n    binarized = ~labels.isin(edge_label_ids) &amp; binarized\n\n    # Remove small objects with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving binarized to GPU:{device}.\")\n        # Check if binarized is dask, if not persist it, since remove_small_objects_gpu can't be calculated from\n        # cupy-dask arrays\n        if binarized.chunks is not None:\n            binarized = binarized.persist()\n        with cp.cuda.Device(device_nr):\n            binarized = binarized.cupy.as_cupy()\n            binarized.values = remove_small_objects_gpu(\n                binarized.astype(bool).expand_dims(\"batch\", 0).data, min_size=min_object_size\n            )[0]\n            binarized = binarized.cupy.as_numpy()\n            free_cupy()\n    else:\n        binarized.values = remove_small_objects(\n            binarized.astype(bool).expand_dims(\"batch\", 0).values, min_size=min_object_size\n        )[0]\n\n    # Convert back to bool\n    binarized = binarized.astype(\"bool\")\n\n    return binarized\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.erode_mask","title":"erode_mask","text":"<pre><code>erode_mask(\n    mask: xarray.DataArray,\n    size: int,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n    edge_size: int | None = None,\n) -&gt; xarray.DataArray\n</code></pre> <p>Erode a binary mask and invalidate edge regions.</p> <p>This function applies morphological erosion to shrink valid regions in a mask and additionally sets a border region around the entire mask to invalid. This is useful for removing unreliable predictions near tile edges and data boundaries.</p> <p>Parameters:</p> <ul> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Binary mask to erode (1=valid, 0=invalid). Will be converted to uint8.</p> </li> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>Radius of the disk structuring element for erosion in pixels. Also used as the width of the edge region to invalidate (unless edge_size is specified).</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>Device for processing. Use \"cuda\" for GPU acceleration, \"cpu\" for CPU processing, or an integer to specify a GPU device number.</p> </li> <li> <code>edge_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Width of the edge region to set to invalid in pixels. If None, uses the <code>size</code> parameter. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Eroded mask (uint8, 1=valid, 0=invalid) with edges invalidated.</p> </li> </ul> Note <p>GPU acceleration (requires cucim): - Significantly faster for large masks - Automatically falls back to CPU if cucim is not available - Handles both in-memory and dask arrays</p> <p>The erosion operation shrinks valid regions by removing pixels within <code>size</code> distance from invalid regions. Edge invalidation then sets the outermost <code>edge_size</code> pixels on all sides to 0.</p> Example <p>Erode mask to remove edge effects:</p> <pre><code>from darts_postprocessing import erode_mask\n\n# Erode by 10 pixels and invalidate 10-pixel edges\neroded = erode_mask(\n    mask=quality_mask,\n    size=10,\n    device=\"cuda\"\n)\n\n# Erode by 5 pixels but invalidate 20-pixel edges\neroded_custom = erode_mask(\n    mask=quality_mask,\n    size=5,\n    edge_size=20,\n    device=\"cpu\"\n)\n</code></pre> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\"Eroding mask\", printer=logger.debug, print_kwargs=[\"size\"])\ndef erode_mask(\n    mask: xr.DataArray, size: int, device: Literal[\"cuda\", \"cpu\"] | int, edge_size: int | None = None\n) -&gt; xr.DataArray:\n    \"\"\"Erode a binary mask and invalidate edge regions.\n\n    This function applies morphological erosion to shrink valid regions in a mask and\n    additionally sets a border region around the entire mask to invalid. This is useful\n    for removing unreliable predictions near tile edges and data boundaries.\n\n    Args:\n        mask (xr.DataArray): Binary mask to erode (1=valid, 0=invalid). Will be converted to uint8.\n        size (int): Radius of the disk structuring element for erosion in pixels.\n            Also used as the width of the edge region to invalidate (unless edge_size is specified).\n        device (Literal[\"cuda\", \"cpu\"] | int): Device for processing. Use \"cuda\" for GPU acceleration,\n            \"cpu\" for CPU processing, or an integer to specify a GPU device number.\n        edge_size (int | None, optional): Width of the edge region to set to invalid in pixels.\n            If None, uses the `size` parameter. Defaults to None.\n\n    Returns:\n        xr.DataArray: Eroded mask (uint8, 1=valid, 0=invalid) with edges invalidated.\n\n    Note:\n        GPU acceleration (requires cucim):\n        - Significantly faster for large masks\n        - Automatically falls back to CPU if cucim is not available\n        - Handles both in-memory and dask arrays\n\n        The erosion operation shrinks valid regions by removing pixels within `size` distance\n        from invalid regions. Edge invalidation then sets the outermost `edge_size` pixels\n        on all sides to 0.\n\n    Example:\n        Erode mask to remove edge effects:\n\n        ```python\n        from darts_postprocessing import erode_mask\n\n        # Erode by 10 pixels and invalidate 10-pixel edges\n        eroded = erode_mask(\n            mask=quality_mask,\n            size=10,\n            device=\"cuda\"\n        )\n\n        # Erode by 5 pixels but invalidate 20-pixel edges\n        eroded_custom = erode_mask(\n            mask=quality_mask,\n            size=5,\n            edge_size=20,\n            device=\"cpu\"\n        )\n        ```\n\n    \"\"\"\n    # Clone mask to avoid in-place operations\n    mask = mask.copy()\n\n    # Change to dtype uint8 for faster skimage operations\n    mask = mask.astype(\"uint8\")\n\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Dilate the mask with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving mask to GPU:{device}.\")\n        # Check if mask is dask, if not persist it, since dilation can't be calculated from cupy-dask arrays\n        if mask.chunks is not None:\n            mask = mask.persist()\n        with cp.cuda.Device(device_nr):\n            mask = mask.cupy.as_cupy()\n            mask.values = binary_erosion_gpu(mask.data, disk_gpu(size))\n            mask = mask.cupy.as_numpy()\n            free_cupy()\n    else:\n        mask.values = binary_erosion(mask.values, disk(size))\n\n    if edge_size is None:\n        edge_size = size\n\n    # Mask edges\n    mask[:edge_size, :] = 0\n    mask[-edge_size:, :] = 0\n    mask[:, :edge_size] = 0\n    mask[:, -edge_size:] = 0\n\n    return mask\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.prepare_export","title":"prepare_export","text":"<pre><code>prepare_export(\n    tile: xarray.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 0,\n    ensemble_subsets: list[str] = [],\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_postprocessing.postprocess.DEFAULT_DEVICE,\n    edge_erosion_size: int | None = None,\n) -&gt; xarray.Dataset\n</code></pre> <p>Prepare segmentation results for export by applying quality filtering and binarization.</p> <p>This is a wrapper function that orchestrates the complete postprocessing pipeline: mask erosion, probability masking, and binarization. It processes both ensemble-averaged predictions and individual model outputs if present.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Input tile from inference containing: - probabilities (float32): Segmentation probabilities [0-1] - quality_data_mask (uint8): Quality mask (0=invalid, 1=low quality, 2=high quality) - probabilities-{subset} (float32): Optional individual model predictions</p> </li> <li> <code>bin_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Probability threshold for binarization. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Erosion radius for quality mask in pixels. Also used for edge invalidation unless edge_erosion_size is specified. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>Minimum object size in pixels. Smaller objects are removed. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>0</code> )           \u2013            <p>Quality threshold for masking. Maps to quality_data_mask values: - \"high_quality\" or 2: Only use high quality pixels - \"low_quality\" or 1: Use low and high quality pixels - \"none\" or 0: No quality masking Defaults to 0 (no masking).</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>Names of individual models in the ensemble to process (e.g., [\"with_tcvis\", \"without_tcvis\"]). Defaults to [].</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_postprocessing.postprocess.DEFAULT_DEVICE</code> )           \u2013            <p>Device for processing. Defaults to GPU if available.</p> </li> <li> <code>edge_erosion_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Separate erosion width for tile edges in pixels. If None, uses mask_erosion_size. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input tile augmented with:</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Added data variables: - extent (uint8): Valid extent mask after erosion (1=valid, 0=invalid).   Attributes: long_name=\"Extent of the segmentation\" - binarized_segmentation (bool): Binary segmentation mask (True=object).   Attributes: long_name=\"Binarized Segmentation\" - binarized_segmentation-{subset} (bool): Binary masks for each ensemble subset   (only if ensemble_subsets provided)</p> </li> <li> <code>xarray.Dataset</code>           \u2013            <p>Modified data variables: - probabilities (float32): Now masked to valid extent (NaN outside) - probabilities-{subset} (float32): Masked individual model predictions</p> </li> </ul> Note <p>Processing pipeline: 1. Filter quality mask to specified quality_level 2. Erode quality mask to remove boundary artifacts 3. Create extent mask from eroded quality mask 4. Mask probabilities to valid extent (NaN invalid regions) 5. Binarize masked probabilities with threshold and min object size filter 6. Repeat steps 4-5 for each ensemble subset if provided</p> <p>The extent variable defines the reliable prediction region and should be used to clip exported results.</p> Example <p>Complete postprocessing workflow:</p> <pre><code>from darts_postprocessing import prepare_export\n\n# After ensemble inference\nprocessed_tile = prepare_export(\n    tile=ensemble_result,\n    bin_threshold=0.5,\n    mask_erosion_size=10,\n    min_object_size=32,\n    quality_level=\"high_quality\",  # Only high quality pixels\n    ensemble_subsets=[\"with_tcvis\", \"without_tcvis\"],\n    device=\"cuda\"\n)\n\n# Now ready for export with:\n# - processed_tile[\"binarized_segmentation\"]: Main binary result\n# - processed_tile[\"extent\"]: Valid data extent\n# - processed_tile[\"probabilities\"]: Masked probabilities\n</code></pre> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\n    \"Preparing export\",\n    printer=logger.debug,\n    print_kwargs=[\n        \"bin_threshold\",\n        \"mask_erosion_size\",\n        \"edge_erosion_size\",\n        \"min_object_size\",\n        \"quality_level\",\n        \"ensemble_subsets\",\n    ],\n)\ndef prepare_export(\n    tile: xr.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int | Literal[\"high_quality\", \"low_quality\", \"none\"] = 0,\n    ensemble_subsets: list[str] = [],\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n    edge_erosion_size: int | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Prepare segmentation results for export by applying quality filtering and binarization.\n\n    This is a wrapper function that orchestrates the complete postprocessing pipeline:\n    mask erosion, probability masking, and binarization. It processes both ensemble-averaged\n    predictions and individual model outputs if present.\n\n    Args:\n        tile (xr.Dataset): Input tile from inference containing:\n            - probabilities (float32): Segmentation probabilities [0-1]\n            - quality_data_mask (uint8): Quality mask (0=invalid, 1=low quality, 2=high quality)\n            - probabilities-{subset} (float32): Optional individual model predictions\n        bin_threshold (float, optional): Probability threshold for binarization. Defaults to 0.5.\n        mask_erosion_size (int, optional): Erosion radius for quality mask in pixels. Also used\n            for edge invalidation unless edge_erosion_size is specified. Defaults to 10.\n        min_object_size (int, optional): Minimum object size in pixels. Smaller objects are removed.\n            Defaults to 32.\n        quality_level (int | Literal[\"high_quality\", \"low_quality\", \"none\"], optional):\n            Quality threshold for masking. Maps to quality_data_mask values:\n            - \"high_quality\" or 2: Only use high quality pixels\n            - \"low_quality\" or 1: Use low and high quality pixels\n            - \"none\" or 0: No quality masking\n            Defaults to 0 (no masking).\n        ensemble_subsets (list[str], optional): Names of individual models in the ensemble to\n            process (e.g., [\"with_tcvis\", \"without_tcvis\"]). Defaults to [].\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): Device for processing. Defaults to GPU if available.\n        edge_erosion_size (int | None, optional): Separate erosion width for tile edges in pixels.\n            If None, uses mask_erosion_size. Defaults to None.\n\n    Returns:\n        xr.Dataset: Input tile augmented with:\n\n        Added data variables:\n            - extent (uint8): Valid extent mask after erosion (1=valid, 0=invalid).\n              Attributes: long_name=\"Extent of the segmentation\"\n            - binarized_segmentation (bool): Binary segmentation mask (True=object).\n              Attributes: long_name=\"Binarized Segmentation\"\n            - binarized_segmentation-{subset} (bool): Binary masks for each ensemble subset\n              (only if ensemble_subsets provided)\n\n        Modified data variables:\n            - probabilities (float32): Now masked to valid extent (NaN outside)\n            - probabilities-{subset} (float32): Masked individual model predictions\n\n    Note:\n        Processing pipeline:\n        1. Filter quality mask to specified quality_level\n        2. Erode quality mask to remove boundary artifacts\n        3. Create extent mask from eroded quality mask\n        4. Mask probabilities to valid extent (NaN invalid regions)\n        5. Binarize masked probabilities with threshold and min object size filter\n        6. Repeat steps 4-5 for each ensemble subset if provided\n\n        The extent variable defines the reliable prediction region and should be used to\n        clip exported results.\n\n    Example:\n        Complete postprocessing workflow:\n\n        ```python\n        from darts_postprocessing import prepare_export\n\n        # After ensemble inference\n        processed_tile = prepare_export(\n            tile=ensemble_result,\n            bin_threshold=0.5,\n            mask_erosion_size=10,\n            min_object_size=32,\n            quality_level=\"high_quality\",  # Only high quality pixels\n            ensemble_subsets=[\"with_tcvis\", \"without_tcvis\"],\n            device=\"cuda\"\n        )\n\n        # Now ready for export with:\n        # - processed_tile[\"binarized_segmentation\"]: Main binary result\n        # - processed_tile[\"extent\"]: Valid data extent\n        # - processed_tile[\"probabilities\"]: Masked probabilities\n        ```\n\n    \"\"\"\n    quality_level = (\n        quality_level\n        if isinstance(quality_level, int)\n        else {\"high_quality\": 2, \"low_quality\": 1, \"none\": 0}[quality_level]\n    )\n    mask = tile[\"quality_data_mask\"] &gt;= quality_level\n    if quality_level &gt; 0:\n        mask = erode_mask(mask, mask_erosion_size, device, edge_size=edge_erosion_size)  # 0=positive, 1=negative\n    tile[\"extent\"] = mask.copy()\n    tile[\"extent\"].attrs = {\"long_name\": \"Extent of the segmentation\"}\n\n    def _prep_layer(tile: xr.Dataset, subset: str | None = None):\n        layername = \"probabilities\" if subset is None else f\"probabilities-{subset}\"\n        binarized_layername = \"binarized_segmentation\" if subset is None else f\"binarized_segmentation-{subset}\"\n\n        # Mask the segmentation\n        tile[layername] = tile[layername].where(mask)\n\n        # Binarize the segmentation\n        tile[binarized_layername] = binarize(tile[layername], bin_threshold, min_object_size, mask, device)\n        tile[binarized_layername].attrs = {\"long_name\": \"Binarized Segmentation\"}\n        return tile\n\n    tile = _prep_layer(tile)\n\n    # get the names of the model probabilities if available\n    # for example 'tcvis' from 'probabilities-tcvis'\n    for ensemble_subset in ensemble_subsets:\n        tile = _prep_layer(tile, ensemble_subset)\n\n    return tile\n</code></pre>"},{"location":"reference/darts_segmentation/","title":"darts_segmentation","text":""},{"location":"reference/darts_segmentation/#darts_segmentation","title":"darts_segmentation","text":"<p>Image segmentation of thaw-slumps for the DARTS dataset.</p>"},{"location":"reference/darts_segmentation/#darts_segmentation.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_segmentation/inference/","title":"inference","text":""},{"location":"reference/darts_segmentation/inference/#darts_segmentation.inference","title":"darts_segmentation.inference","text":"<p>Shared utilities for the inference modules.</p>"},{"location":"reference/darts_segmentation/inference/#darts_segmentation.inference.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/inference/#darts_segmentation.inference.create_patches","title":"create_patches","text":"<pre><code>create_patches(\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    return_coords: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Create patches from a tensor.</p> <p>Parameters:</p> <ul> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>return_coords</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/inference.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"reference/darts_segmentation/inference/#darts_segmentation.inference.patch_coords","title":"patch_coords","text":"<pre><code>patch_coords(\n    h: int, w: int, patch_size: int, overlap: int\n) -&gt; collections.abc.Generator[\n    tuple[int, int, int, int], None, None\n]\n</code></pre> <p>Yield patch coordinates based on height, width, patch size and margin size.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>int</code>)           \u2013            <p>Height of the image.</p> </li> <li> <code>w</code>               (<code>int</code>)           \u2013            <p>Width of the image.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Patch size.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>Margin size.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[int, int, int, int]</code>           \u2013            <p>tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/inference.py</code> <pre><code>def patch_coords(h: int, w: int, patch_size: int, overlap: int) -&gt; Generator[tuple[int, int, int, int], None, None]:\n    \"\"\"Yield patch coordinates based on height, width, patch size and margin size.\n\n    Args:\n        h (int): Height of the image.\n        w (int): Width of the image.\n        patch_size (int): Patch size.\n        overlap (int): Margin size.\n\n    Yields:\n        tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.\n\n    \"\"\"\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            yield y, x, patch_idx_y, patch_idx_x\n</code></pre>"},{"location":"reference/darts_segmentation/inference/#darts_segmentation.inference.predict_in_patches","title":"predict_in_patches","text":"<pre><code>predict_in_patches(\n    model: torch.nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device: torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Predict on a tensor.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>torch.nn.Module</code>)           \u2013            <p>The model to use for prediction.</p> </li> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> </li> <li> <code>reflection</code>               (<code>int</code>)           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor.</p> </li> <li> <code>device</code>               (<code>torch.device</code>)           \u2013            <p>The device to use for the prediction.</p> </li> <li> <code>return_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>The predicted tensor.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/inference.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device: torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    n_skipped = 0\n    for i, batch in enumerate(patches):\n        # If batch contains only nans, skip it\n        if torch.isnan(batch).all(axis=0).any():\n            patched_probabilities[i * batch_size : (i + 1) * batch_size] = 0\n            n_skipped += 1\n            continue\n        # If batch contains some nans, replace them with zeros\n        batch[torch.isnan(batch)] = 0\n\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    if n_skipped &gt; 0:\n        logger.debug(f\"Skipped {n_skipped} batches because they only contained NaNs\")\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/","title":"metrics","text":""},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics","title":"darts_segmentation.metrics","text":"<p>Own metrics for segmentation tasks.</p>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU","title":"BinaryBoundaryIoU","text":"<pre><code>BinaryBoundaryIoU(\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Unpack[\n        darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs\n    ],\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Binary Boundary IoU metric for binary segmentation tasks.</p> <p>This metric is similar to the Binary Intersection over Union (IoU or Jaccard Index) metric, but instead of comparing all pixels it only compares the boundaries of each foreground object.</p> <p>Create a new instance of the BinaryBoundaryIoU metric.</p> <p>Please see the torchmetrics docs for more info about the **kwargs.</p> <p>Parameters:</p> <ul> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class.  Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>**kwargs</code>               (<code>typing.Unpack[darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the metric.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>zero_division</code>               (<code>int</code>)           \u2013            <p>Value to return when there is a zero division. Default is 0.</p> </li> <li> <code>compute_on_cpu</code>               (<code>bool</code>)           \u2013            <p>If metric state should be stored on CPU during computations. Only works for list states.</p> </li> <li> <code>dist_sync_on_step</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize on <code>forward()</code>. Default is <code>False</code>.</p> </li> <li> <code>process_group</code>               (<code>str</code>)           \u2013            <p>The process group on which the synchronization is called. Default is the world.</p> </li> <li> <code>dist_sync_fn</code>               (<code>callable</code>)           \u2013            <p>Function that performs the allgather option on the metric state. Default is a custom implementation that calls <code>torch.distributed.all_gather</code> internally.</p> </li> <li> <code>distributed_available_fn</code>               (<code>callable</code>)           \u2013            <p>Function that checks if the distributed backend is available. Defaults to a check of <code>torch.distributed.is_available()</code> and <code>torch.distributed.is_initialized()</code>.</p> </li> <li> <code>sync_on_compute</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize when <code>compute</code> is called. Default is <code>True</code>.</p> </li> <li> <code>compute_with_cache</code>               (<code>bool</code>)           \u2013            <p>If results from <code>compute</code> should be cached. Default is <code>True</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If dilation is not a float or int.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def __init__(\n    self,\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Unpack[BinaryBoundaryIoUKwargs],\n):\n    \"\"\"Create a new instance of the BinaryBoundaryIoU metric.\n\n    Please see the\n    [torchmetrics docs](https://lightning.ai/docs/torchmetrics/stable/pages/overview.html#metric-kwargs)\n    for more info about the **kwargs.\n\n    Args:\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class.  Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        **kwargs: Additional keyword arguments for the metric.\n\n    Keyword Args:\n        zero_division (int):\n            Value to return when there is a zero division. Default is 0.\n        compute_on_cpu (bool):\n            If metric state should be stored on CPU during computations. Only works for list states.\n        dist_sync_on_step (bool):\n            If metric state should synchronize on ``forward()``. Default is ``False``.\n        process_group (str):\n            The process group on which the synchronization is called. Default is the world.\n        dist_sync_fn (callable):\n            Function that performs the allgather option on the metric state. Default is a custom\n            implementation that calls ``torch.distributed.all_gather`` internally.\n        distributed_available_fn (callable):\n            Function that checks if the distributed backend is available. Defaults to a\n            check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n        sync_on_compute (bool):\n            If metric state should synchronize when ``compute`` is called. Default is ``True``.\n        compute_with_cache (bool):\n            If results from ``compute`` should be cached. Default is ``True``.\n\n    Raises:\n        ValueError: If dilation is not a float or int.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super().__init__(**kwargs)\n\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not isinstance(dilation, float | int):\n            raise ValueError(f\"Expected argument `dilation` to be a float or int, but got {dilation}.\")\n\n    self.dilation = dilation\n    self.threshold = threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if multidim_average == \"samplewise\":\n        self.add_state(\"intersection\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"union\", default=[], dist_reduce_fx=\"cat\")\n    else:\n        self.add_state(\"intersection\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.dilation","title":"dilation  <code>instance-attribute</code>","text":"<pre><code>dilation = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    dilation\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.intersection","title":"intersection  <code>instance-attribute</code>","text":"<pre><code>intersection: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.union","title":"union  <code>instance-attribute</code>","text":"<pre><code>union: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>torch.Tensor</code> )          \u2013            <p>The computed metric.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\n\n    Returns:\n        Tensor: The computed metric.\n\n    \"\"\"\n    if self.multidim_average == \"global\":\n        return self.intersection / self.union\n    else:\n        self.intersection = torch.tensor(self.intersection)\n        self.union = torch.tensor(self.union)\n        return self.intersection / self.union\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input arguments are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input shapes are invalid.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If the input arguments are invalid.\n        ValueError: If the input shapes are invalid.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.shape == target.shape:\n            raise ValueError(\n                f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\"\n            )\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.\")\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    target = target.to(torch.uint8)\n    preds = preds.to(torch.uint8)\n\n    target_boundary = get_boundary((target == 1).to(torch.uint8), self.dilation, self.validate_args)\n    preds_boundary = get_boundary(preds, self.dilation, self.validate_args)\n\n    intersection = target_boundary &amp; preds_boundary\n    union = target_boundary | preds_boundary\n\n    if self.ignore_index is not None:\n        # Important that this is NOT the boundary, but the original mask\n        valid_idx = target != self.ignore_index\n        intersection &amp;= valid_idx\n        union &amp;= valid_idx\n\n    intersection = intersection.sum().item()\n    union = union.sum().item()\n\n    if self.multidim_average == \"global\":\n        self.intersection += intersection\n        self.union += union\n    else:\n        self.intersection.append(intersection)\n        self.union.append(union)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy","title":"BinaryInstanceAccuracy","text":"<pre><code>BinaryInstanceAccuracy(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance accuracy metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _accuracy_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision","title":"BinaryInstanceAveragePrecision","text":"<pre><code>BinaryInstanceAveragePrecision(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve</code></p> <p>Compute the average precision for binary instance segmentation.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tensor:  # type: ignore[override]  # noqa: D102\n    return _binary_average_precision_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # type: ignore[override]  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix","title":"BinaryInstanceConfusionMatrix","text":"<pre><code>BinaryInstanceConfusionMatrix(\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance confusion matrix metric.</p> <p>Create a new instance of the BinaryInstanceConfusionMatrix metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, return the confusion matrix normalized by the number of instances. If False, return the confusion matrix without normalization. Defaults to None.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalize</code> is not a bool.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceConfusionMatrix metric.\n\n    Args:\n        normalize (bool, optional): If True, return the confusion matrix normalized by the number of instances.\n            If False, return the confusion matrix without normalization. Defaults to None.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `normalize` is not a bool.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        matching_threshold=matching_threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if normalize is not None and not isinstance(normalize, bool):\n        raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.normalize","title":"normalize  <code>instance-attribute</code>","text":"<pre><code>normalize = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix(\n    normalize\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    # tn is always 0\n    if self.normalize:\n        all = tp + fp + fn\n        return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n    else:\n        return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n    add_text: bool = True,\n    labels: list[str] | None = None,\n    cmap: torchmetrics.utilities.plot._CMAP_TYPE\n    | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n    add_text: bool = True,\n    labels: list[str] | None = None,  # type: ignore\n    cmap: _CMAP_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    val = val or self.compute()\n    if not isinstance(val, Tensor):\n        raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n    fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n    return fig, ax\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score","title":"BinaryInstanceF1Score","text":"<pre><code>BinaryInstanceF1Score(\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore</code></p> <p>Binary instance F1 score metric.</p> <p>Create a new instance of the BinaryInstanceF1Score metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceF1Score metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        beta=1.0,\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=validate_args,\n        zero_division=zero_division,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore","title":"BinaryInstanceFBetaScore","text":"<pre><code>BinaryInstanceFBetaScore(\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance F-beta score metric.</p> <p>Create a new instance of the BinaryInstanceFBetaScore metric.</p> <p>Parameters:</p> <ul> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The beta parameter for the F-beta score.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceFBetaScore metric.\n\n    Args:\n        beta (float): The beta parameter for the F-beta score.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if validate_args:\n        _binary_fbeta_score_arg_validation(beta, threshold, multidim_average, ignore_index, zero_division)\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n    self.beta = beta\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision","title":"BinaryInstancePrecision","text":"<pre><code>BinaryInstancePrecision(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance precision metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"precision\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve","title":"BinaryInstancePrecisionRecallCurve","text":"<pre><code>BinaryInstancePrecisionRecallCurve(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Compute the precision-recall curve for binary instance segmentation.</p> <p>This metric works similar to <code>torchmetrics.classification.PrecisionRecallCurve</code>, with two key differences: 1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.     Instead of calculating the values for each pixel. 2. The \"thresholds\" argument is required.     Calculating the thresholds at the compute stage would cost to much memory for this usecase.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.compute","title":"compute","text":"<pre><code>compute() -&gt; tuple[\n    torch.Tensor, torch.Tensor, torch.Tensor\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; tuple[Tensor, Tensor, Tensor]:  # noqa: D102\n    return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.plot","title":"plot","text":"<pre><code>plot(\n    curve: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n    | None = None,\n    score: torch.Tensor | bool | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    curve: tuple[Tensor, Tensor, Tensor] | None = None,\n    score: Tensor | bool | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    curve_computed = curve or self.compute()\n    # switch order as the standard way is recall along x-axis and precision along y-axis\n    curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n    score = (\n        _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n        if not curve and score is True\n        else None\n    )\n    return plot_curve(\n        curve_computed, score=score, ax=ax, label_names=(\"Recall\", \"Precision\"), name=self.__class__.__name__\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall","title":"BinaryInstanceRecall","text":"<pre><code>BinaryInstanceRecall(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance recall metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"recall\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores","title":"BinaryInstanceStatScores","text":"<pre><code>BinaryInstanceStatScores(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.classification.stat_scores._AbstractStatScores</code></p> <p>Base class for binary instance segmentation metrics.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _binary_stat_scores_compute(tp, fp, tn, fn, self.multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/","title":"binary_instance_prc","text":""},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc","title":"darts_segmentation.metrics.binary_instance_prc","text":"<p>Complex binary instance segmentation metrics.</p>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.MatchingMetric","title":"MatchingMetric  <code>module-attribute</code>","text":"<pre><code>MatchingMetric = typing.Literal['iou', 'boundary']\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision","title":"BinaryInstanceAveragePrecision","text":"<pre><code>BinaryInstanceAveragePrecision(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve</code></p> <p>Compute the average precision for binary instance segmentation.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tensor:  # type: ignore[override]  # noqa: D102\n    return _binary_average_precision_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # type: ignore[override]  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve","title":"BinaryInstancePrecisionRecallCurve","text":"<pre><code>BinaryInstancePrecisionRecallCurve(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Compute the precision-recall curve for binary instance segmentation.</p> <p>This metric works similar to <code>torchmetrics.classification.PrecisionRecallCurve</code>, with two key differences: 1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.     Instead of calculating the values for each pixel. 2. The \"thresholds\" argument is required.     Calculating the thresholds at the compute stage would cost to much memory for this usecase.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.compute","title":"compute","text":"<pre><code>compute() -&gt; tuple[\n    torch.Tensor, torch.Tensor, torch.Tensor\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; tuple[Tensor, Tensor, Tensor]:  # noqa: D102\n    return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.plot","title":"plot","text":"<pre><code>plot(\n    curve: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n    | None = None,\n    score: torch.Tensor | bool | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    curve: tuple[Tensor, Tensor, Tensor] | None = None,\n    score: Tensor | bool | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    curve_computed = curve or self.compute()\n    # switch order as the standard way is recall along x-axis and precision along y-axis\n    curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n    score = (\n        _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n        if not curve and score is True\n        else None\n    )\n    return plot_curve(\n        curve_computed, score=score, ax=ax, label_names=(\"Recall\", \"Precision\"), name=self.__class__.__name__\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.mask_to_instances","title":"mask_to_instances","text":"<pre><code>mask_to_instances(\n    x: torch.Tensor, validate_args: bool = False\n) -&gt; list[torch.Tensor]\n</code></pre> <p>Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.</p> <p>Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[torch.Tensor]</code>           \u2013            <p>list[torch.Tensor]: The instance masks. Length of list: batch_size. Shape of a tensor: (height, width), dtype: torch.uint8</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef mask_to_instances(x: torch.Tensor, validate_args: bool = False) -&gt; list[torch.Tensor]:\n    \"\"\"Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.\n\n    Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.\n\n    Args:\n        x (torch.Tensor): The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        list[torch.Tensor]: The instance masks. Length of list: batch_size.\n            Shape of a tensor: (height, width), dtype: torch.uint8\n\n    \"\"\"\n    if validate_args:\n        assert x.dim() == 3, f\"Expected 3 dimensions, got {x.dim()}\"\n        assert x.dtype == torch.uint8, f\"Expected torch.uint8, got {x.dtype}\"\n        assert x.min() &gt;= 0 and x.max() &lt;= 1, f\"Expected binary mask, got {x.min()} and {x.max()}\"\n\n    # A note on using lists as separation between instances instead of using a batched tensor:\n    # Using a batched tensor with instance numbers (1, 2, 3, ...) would indicate that the instances of the samples\n    # are identical. Using a list clearly separates the instances of the samples.\n\n    if CUCIM_AVAILABLE:\n        # Check if device is cuda\n        assert x.device.type == \"cuda\", f\"Expected device to be cuda, got {x.device.type}\"\n        x = cp.asarray(x).astype(cp.uint8)\n\n        instances = []\n        for x_i in x:\n            instances_i = label_gpu(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n\n    else:\n        instances = []\n        for x_i in x:\n            x_i = x_i.cpu().numpy()\n            instances_i = label(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.match_instances","title":"match_instances","text":"<pre><code>match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.</p> <p>Parameters:</p> <ul> <li> <code>instances_target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the target. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>instances_preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>match_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int, int]</code>           \u2013            <p>tuple[int, int, int]: True positives, false positives, false negatives</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]:\n    \"\"\"Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.\n\n    Args:\n        instances_target (torch.Tensor): The instance mask of the target. Shape: (height, width), dtype: torch.uint8\n        instances_preds (torch.Tensor): The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8\n        match_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        tuple[int, int, int]: True positives, false positives, false negatives\n\n    \"\"\"\n    if validate_args:\n        assert instances_target.dim() == 2, f\"Expected 2 dimensions, got {instances_target.dim()}\"\n        assert instances_preds.dim() == 2, f\"Expected 2 dimensions, got {instances_preds.dim()}\"\n        assert instances_target.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_target.dtype}\"\n        assert instances_preds.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_preds.dtype}\"\n        assert instances_target.shape == instances_preds.shape, (\n            f\"Shapes do not match: {instances_target.shape} and {instances_preds.shape}\"\n        )\n\n    height, width = instances_target.shape\n    ntargets = instances_target.max().item()\n    npreds = instances_preds.max().item()\n    # If target or predictions has no instances, return 0 for their respective metrics.\n    # If none of them has instances, return 0 for all metrics. (This is implied)\n    if ntargets == 0:\n        return 0, npreds, 0\n    if npreds == 0:\n        return 0, 0, ntargets\n\n    # TODO: These are old edge case filter that need revision.\n    # They are probably not necessary, since the instance metrics are meaningless for noisy predictions.\n    # If there are too many predictions, return all as false positives (this happens when the model is very noisy)\n    # if npreds &gt; ntargets * 5:\n    #     return 0, npreds, ntargets\n    # If there is only one prediction, return all as false negatives (this happens when the model is very noisy)\n    # if npreds == 1 and ntargets &gt; 1:\n    #     return 0, 1, ntargets\n\n    # Create one-hot encoding of instances, so that each instance is a channel\n    instances_target_onehot = torch.zeros((ntargets, height, width), dtype=torch.uint8, device=instances_target.device)\n    instances_preds_onehot = torch.zeros((npreds, height, width), dtype=torch.uint8, device=instances_target.device)\n    for i in range(ntargets):\n        instances_target_onehot[i, :, :] = instances_target == (i + 1)\n    for i in range(npreds):\n        instances_preds_onehot[i, :, :] = instances_preds == (i + 1)\n\n    # Now the instances are channels, hence tensors of shape (num_instances, height, width)\n\n    # Calculate IoU (we need to do a n-m intersection and union, therefore we need to broadcast)\n    intersection = (instances_target_onehot.unsqueeze(1) &amp; instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (instances_target_onehot.unsqueeze(1) | instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    # Match instances based on IoU\n    tp = (iou &gt;= match_threshold).sum().item()\n    fp = npreds - tp\n    fn = ntargets - tp\n\n    return tp, fp, fn\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/","title":"binary_instance_stat_scores","text":""},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores","title":"darts_segmentation.metrics.binary_instance_stat_scores","text":"<p>Binary instance segmentation metrics.</p>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy","title":"BinaryInstanceAccuracy","text":"<pre><code>BinaryInstanceAccuracy(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance accuracy metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _accuracy_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix","title":"BinaryInstanceConfusionMatrix","text":"<pre><code>BinaryInstanceConfusionMatrix(\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance confusion matrix metric.</p> <p>Create a new instance of the BinaryInstanceConfusionMatrix metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, return the confusion matrix normalized by the number of instances. If False, return the confusion matrix without normalization. Defaults to None.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalize</code> is not a bool.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceConfusionMatrix metric.\n\n    Args:\n        normalize (bool, optional): If True, return the confusion matrix normalized by the number of instances.\n            If False, return the confusion matrix without normalization. Defaults to None.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `normalize` is not a bool.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        matching_threshold=matching_threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if normalize is not None and not isinstance(normalize, bool):\n        raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.normalize","title":"normalize  <code>instance-attribute</code>","text":"<pre><code>normalize = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix(\n    normalize\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    # tn is always 0\n    if self.normalize:\n        all = tp + fp + fn\n        return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n    else:\n        return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n    add_text: bool = True,\n    labels: list[str] | None = None,\n    cmap: torchmetrics.utilities.plot._CMAP_TYPE\n    | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n    add_text: bool = True,\n    labels: list[str] | None = None,  # type: ignore\n    cmap: _CMAP_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    val = val or self.compute()\n    if not isinstance(val, Tensor):\n        raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n    fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n    return fig, ax\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score","title":"BinaryInstanceF1Score","text":"<pre><code>BinaryInstanceF1Score(\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore</code></p> <p>Binary instance F1 score metric.</p> <p>Create a new instance of the BinaryInstanceF1Score metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceF1Score metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        beta=1.0,\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=validate_args,\n        zero_division=zero_division,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore","title":"BinaryInstanceFBetaScore","text":"<pre><code>BinaryInstanceFBetaScore(\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance F-beta score metric.</p> <p>Create a new instance of the BinaryInstanceFBetaScore metric.</p> <p>Parameters:</p> <ul> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The beta parameter for the F-beta score.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceFBetaScore metric.\n\n    Args:\n        beta (float): The beta parameter for the F-beta score.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if validate_args:\n        _binary_fbeta_score_arg_validation(beta, threshold, multidim_average, ignore_index, zero_division)\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n    self.beta = beta\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision","title":"BinaryInstancePrecision","text":"<pre><code>BinaryInstancePrecision(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance precision metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"precision\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall","title":"BinaryInstanceRecall","text":"<pre><code>BinaryInstanceRecall(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance recall metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"recall\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores","title":"BinaryInstanceStatScores","text":"<pre><code>BinaryInstanceStatScores(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.classification.stat_scores._AbstractStatScores</code></p> <p>Base class for binary instance segmentation metrics.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _binary_stat_scores_compute(tp, fp, tn, fn, self.multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.mask_to_instances","title":"mask_to_instances","text":"<pre><code>mask_to_instances(\n    x: torch.Tensor, validate_args: bool = False\n) -&gt; list[torch.Tensor]\n</code></pre> <p>Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.</p> <p>Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[torch.Tensor]</code>           \u2013            <p>list[torch.Tensor]: The instance masks. Length of list: batch_size. Shape of a tensor: (height, width), dtype: torch.uint8</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef mask_to_instances(x: torch.Tensor, validate_args: bool = False) -&gt; list[torch.Tensor]:\n    \"\"\"Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.\n\n    Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.\n\n    Args:\n        x (torch.Tensor): The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        list[torch.Tensor]: The instance masks. Length of list: batch_size.\n            Shape of a tensor: (height, width), dtype: torch.uint8\n\n    \"\"\"\n    if validate_args:\n        assert x.dim() == 3, f\"Expected 3 dimensions, got {x.dim()}\"\n        assert x.dtype == torch.uint8, f\"Expected torch.uint8, got {x.dtype}\"\n        assert x.min() &gt;= 0 and x.max() &lt;= 1, f\"Expected binary mask, got {x.min()} and {x.max()}\"\n\n    # A note on using lists as separation between instances instead of using a batched tensor:\n    # Using a batched tensor with instance numbers (1, 2, 3, ...) would indicate that the instances of the samples\n    # are identical. Using a list clearly separates the instances of the samples.\n\n    if CUCIM_AVAILABLE:\n        # Check if device is cuda\n        assert x.device.type == \"cuda\", f\"Expected device to be cuda, got {x.device.type}\"\n        x = cp.asarray(x).astype(cp.uint8)\n\n        instances = []\n        for x_i in x:\n            instances_i = label_gpu(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n\n    else:\n        instances = []\n        for x_i in x:\n            x_i = x_i.cpu().numpy()\n            instances_i = label(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.match_instances","title":"match_instances","text":"<pre><code>match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.</p> <p>Parameters:</p> <ul> <li> <code>instances_target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the target. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>instances_preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>match_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int, int]</code>           \u2013            <p>tuple[int, int, int]: True positives, false positives, false negatives</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]:\n    \"\"\"Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.\n\n    Args:\n        instances_target (torch.Tensor): The instance mask of the target. Shape: (height, width), dtype: torch.uint8\n        instances_preds (torch.Tensor): The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8\n        match_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        tuple[int, int, int]: True positives, false positives, false negatives\n\n    \"\"\"\n    if validate_args:\n        assert instances_target.dim() == 2, f\"Expected 2 dimensions, got {instances_target.dim()}\"\n        assert instances_preds.dim() == 2, f\"Expected 2 dimensions, got {instances_preds.dim()}\"\n        assert instances_target.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_target.dtype}\"\n        assert instances_preds.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_preds.dtype}\"\n        assert instances_target.shape == instances_preds.shape, (\n            f\"Shapes do not match: {instances_target.shape} and {instances_preds.shape}\"\n        )\n\n    height, width = instances_target.shape\n    ntargets = instances_target.max().item()\n    npreds = instances_preds.max().item()\n    # If target or predictions has no instances, return 0 for their respective metrics.\n    # If none of them has instances, return 0 for all metrics. (This is implied)\n    if ntargets == 0:\n        return 0, npreds, 0\n    if npreds == 0:\n        return 0, 0, ntargets\n\n    # TODO: These are old edge case filter that need revision.\n    # They are probably not necessary, since the instance metrics are meaningless for noisy predictions.\n    # If there are too many predictions, return all as false positives (this happens when the model is very noisy)\n    # if npreds &gt; ntargets * 5:\n    #     return 0, npreds, ntargets\n    # If there is only one prediction, return all as false negatives (this happens when the model is very noisy)\n    # if npreds == 1 and ntargets &gt; 1:\n    #     return 0, 1, ntargets\n\n    # Create one-hot encoding of instances, so that each instance is a channel\n    instances_target_onehot = torch.zeros((ntargets, height, width), dtype=torch.uint8, device=instances_target.device)\n    instances_preds_onehot = torch.zeros((npreds, height, width), dtype=torch.uint8, device=instances_target.device)\n    for i in range(ntargets):\n        instances_target_onehot[i, :, :] = instances_target == (i + 1)\n    for i in range(npreds):\n        instances_preds_onehot[i, :, :] = instances_preds == (i + 1)\n\n    # Now the instances are channels, hence tensors of shape (num_instances, height, width)\n\n    # Calculate IoU (we need to do a n-m intersection and union, therefore we need to broadcast)\n    intersection = (instances_target_onehot.unsqueeze(1) &amp; instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (instances_target_onehot.unsqueeze(1) | instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    # Match instances based on IoU\n    tp = (iou &gt;= match_threshold).sum().item()\n    fp = npreds - tp\n    fn = ntargets - tp\n\n    return tp, fp, fn\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/","title":"boundary_helpers","text":""},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers","title":"darts_segmentation.metrics.boundary_helpers","text":"<p>Helper functions for boundary metrics.</p>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers.MatchingMetric","title":"MatchingMetric  <code>module-attribute</code>","text":"<pre><code>MatchingMetric = typing.Literal['iou', 'boundary']\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers._boundary_arg_validation","title":"_boundary_arg_validation","text":"<pre><code>_boundary_arg_validation(\n    matching_threshold: float = 0.5,\n    matching_metric: darts_segmentation.metrics.boundary_helpers.MatchingMetric = \"iou\",\n    boundary_dilation: float | int = 0.02,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>def _boundary_arg_validation(\n    matching_threshold: float = 0.5,\n    matching_metric: MatchingMetric = \"iou\",\n    boundary_dilation: float | int = 0.02,\n):\n    if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n        raise ValueError(\n            f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n        )\n    if not isinstance(matching_metric, MatchingMetric):\n        raise ValueError(\n            f'Expected argument `matching_metric` to be either \"iou\" or \"boundary\", but got {matching_metric}.'\n        )\n    if matching_metric not in get_args(MatchingMetric):\n        raise ValueError(\n            f'Expected argument `matching_metric` to be either \"iou\" or \"boundary\", but got {matching_metric}.'\n        )\n    if not isinstance(boundary_dilation, float | int) and matching_metric == \"boundary\":\n        raise ValueError(f\"Expected argument `boundary_dilation` to be a float or int, but got {boundary_dilation}.\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers.erode_pytorch","title":"erode_pytorch","text":"<pre><code>erode_pytorch(\n    mask: torch.Tensor,\n    iterations: int = 1,\n    validate_args: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Erodes a binary mask using a square kernel in PyTorch.</p> <p>Parameters:</p> <ul> <li> <code>mask</code>               (<code>torch.Tensor</code>)           \u2013            <p>The binary mask. Shape: (batch_size, height, width) or (batch_size, channels, height, width), dtype: torch.uint8</p> </li> <li> <code>iterations</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The size of the erosion. Defaults to 1.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The eroded mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef erode_pytorch(mask: torch.Tensor, iterations: int = 1, validate_args: bool = False) -&gt; torch.Tensor:\n    \"\"\"Erodes a binary mask using a square kernel in PyTorch.\n\n    Args:\n        mask (torch.Tensor): The binary mask.\n            Shape: (batch_size, height, width) or (batch_size, channels, height, width), dtype: torch.uint8\n        iterations (int, optional): The size of the erosion. Defaults to 1.\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        torch.Tensor: The eroded mask. Shape: (batch_size, height, width), dtype: torch.uint8\n\n    \"\"\"\n    if validate_args:\n        assert mask.dim() not in [3, 4], f\"Expected 3 or 4 dimensions, got {mask.dim()}\"\n        assert mask.dtype == torch.uint8, f\"Expected torch.uint8, got {mask.dtype}\"\n        assert mask.min() &gt;= 0 and mask.max() &lt;= 1, f\"Expected binary mask, got {mask.min()} and {mask.max()}\"\n\n    isbatched = mask.dim() == 4\n    if not isbatched:\n        mask = mask.unsqueeze(1)\n\n    _n, c, _h, _w = mask.shape\n\n    kernel = torch.ones(c, 1, 3, 3, device=mask.device)\n    erode = torch.nn.functional.conv2d(mask.float(), kernel, padding=1, stride=1, groups=c)\n\n    for _ in range(iterations - 1):\n        erode = torch.nn.functional.conv2d(erode, kernel, padding=1, stride=1, groups=c)\n\n    if isbatched:\n        eroded = (erode == erode.max()).to(torch.uint8)\n    else:\n        eroded = (erode == erode.max()).to(torch.uint8).squeeze(1)\n    return eroded\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers.get_boundary","title":"get_boundary","text":"<pre><code>get_boundary(\n    binary_instances: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Convert instance masks to instance boundaries.</p> <p>Parameters:</p> <ul> <li> <code>binary_instances</code>               (<code>torch.Tensor</code>)           \u2013            <p>Target instance masks. Must be binary. Can be batched, one-hot encoded or both. (3 or 4 dimensions). The last two dimensions must be height and width.</p> </li> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Weather arguments should be validated. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[torch.Tensor, torch.Tensor]</code>           \u2013            <p>tuple[torch.Tensor, torch.Tensor]: The boundaries of the instances.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef get_boundary(\n    binary_instances: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Convert instance masks to instance boundaries.\n\n    Args:\n        binary_instances (torch.Tensor): Target instance masks. Must be binary.\n            Can be batched, one-hot encoded or both. (3 or 4 dimensions).\n            The last two dimensions must be height and width.\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        validate_args (bool, optional): Weather arguments should be validated. Defaults to False.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: The boundaries of the instances.\n\n    \"\"\"\n    if validate_args:\n        assert binary_instances.dim() in [3, 4], f\"Expected 3 or 4 dimensions, got {binary_instances.dim()}\"\n        assert binary_instances.dtype == torch.uint8, f\"Expected torch.uint8, got {binary_instances.dtype}\"\n        assert binary_instances.min() &gt;= 0 and binary_instances.max() &lt;= 1, (\n            f\"Expected binary mask, got range between {binary_instances.min()} and {binary_instances.max()}\"\n        )\n        assert isinstance(dilation, float | int), f\"Expected float or int, got {type(dilation)}\"\n        assert dilation &gt;= 0, f\"Expected dilation &gt;= 0, got {dilation}\"\n\n    if binary_instances.dim() == 3:\n        _n, h, w = binary_instances.shape\n    else:\n        _n, _c, h, w = binary_instances.shape\n\n    if isinstance(dilation, float):\n        img_diag = sqrt(h**2 + w**2)\n        dilation = round(dilation * img_diag)\n        if dilation &lt; 1:\n            dilation = 1\n\n    # Pad the instances to avoid boundary issues\n    pad = torchvision.transforms.Pad(1)\n    binary_instances_padded = pad(binary_instances)\n\n    # Erode the instances to get the boundaries\n    eroded = erode_pytorch(binary_instances_padded, iterations=dilation, validate_args=validate_args)\n\n    # Remove the padding\n    if binary_instances.dim() == 3:\n        eroded = eroded[:, 1:-1, 1:-1]\n    else:\n        eroded = eroded[:, :, 1:-1, 1:-1]\n    # Calculate the boundary of the instances\n    boundaries = binary_instances - eroded\n\n    return boundaries\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers.instance_boundary_iou","title":"instance_boundary_iou","text":"<pre><code>instance_boundary_iou(\n    instances_target_onehot: torch.Tensor,\n    instances_preds_onehot: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Calculate the IoU of the boundaries of instances.</p> <p>Expects non-batched, one-hot encoded input from skimage.measure.label</p> <p>Parameters:</p> <ul> <li> <code>instances_target_onehot</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the target. Shape: (num_instances, height, width), dtype: torch.uint8</p> </li> <li> <code>instances_preds_onehot</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the prediction. Shape: (num_instances, height, width), dtype: torch.uint8</p> </li> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The IoU of the boundaries. Shape: (num_instances,)</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef instance_boundary_iou(\n    instances_target_onehot: torch.Tensor,\n    instances_preds_onehot: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the IoU of the boundaries of instances.\n\n    Expects non-batched, one-hot encoded input from skimage.measure.label\n\n    Args:\n        instances_target_onehot (torch.Tensor): The instance mask of the target.\n            Shape: (num_instances, height, width), dtype: torch.uint8\n        instances_preds_onehot (torch.Tensor): The instance mask of the prediction.\n            Shape: (num_instances, height, width), dtype: torch.uint8\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        torch.Tensor: The IoU of the boundaries. Shape: (num_instances,)\n\n    \"\"\"\n    # Calculate the boundary of the instances\n    boundaries_target = get_boundary(instances_target_onehot, dilation, validate_args)\n    boundaries_preds = get_boundary(instances_preds_onehot, dilation, validate_args)\n\n    # Calculate the IoU of the boundaries (broadcast because of the different number of instances)\n    intersection = (boundaries_target.unsqueeze(1) &amp; boundaries_preds.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (boundaries_target.unsqueeze(1) | boundaries_preds.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    return iou\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/","title":"boundary_iou","text":""},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou","title":"darts_segmentation.metrics.boundary_iou","text":"<p>Boundary IoU metric for binary segmentation tasks.</p>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.MatchingMetric","title":"MatchingMetric  <code>module-attribute</code>","text":"<pre><code>MatchingMetric = typing.Literal['iou', 'boundary']\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU","title":"BinaryBoundaryIoU","text":"<pre><code>BinaryBoundaryIoU(\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Unpack[\n        darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs\n    ],\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Binary Boundary IoU metric for binary segmentation tasks.</p> <p>This metric is similar to the Binary Intersection over Union (IoU or Jaccard Index) metric, but instead of comparing all pixels it only compares the boundaries of each foreground object.</p> <p>Create a new instance of the BinaryBoundaryIoU metric.</p> <p>Please see the torchmetrics docs for more info about the **kwargs.</p> <p>Parameters:</p> <ul> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class.  Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>**kwargs</code>               (<code>typing.Unpack[darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the metric.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>zero_division</code>               (<code>int</code>)           \u2013            <p>Value to return when there is a zero division. Default is 0.</p> </li> <li> <code>compute_on_cpu</code>               (<code>bool</code>)           \u2013            <p>If metric state should be stored on CPU during computations. Only works for list states.</p> </li> <li> <code>dist_sync_on_step</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize on <code>forward()</code>. Default is <code>False</code>.</p> </li> <li> <code>process_group</code>               (<code>str</code>)           \u2013            <p>The process group on which the synchronization is called. Default is the world.</p> </li> <li> <code>dist_sync_fn</code>               (<code>callable</code>)           \u2013            <p>Function that performs the allgather option on the metric state. Default is a custom implementation that calls <code>torch.distributed.all_gather</code> internally.</p> </li> <li> <code>distributed_available_fn</code>               (<code>callable</code>)           \u2013            <p>Function that checks if the distributed backend is available. Defaults to a check of <code>torch.distributed.is_available()</code> and <code>torch.distributed.is_initialized()</code>.</p> </li> <li> <code>sync_on_compute</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize when <code>compute</code> is called. Default is <code>True</code>.</p> </li> <li> <code>compute_with_cache</code>               (<code>bool</code>)           \u2013            <p>If results from <code>compute</code> should be cached. Default is <code>True</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If dilation is not a float or int.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def __init__(\n    self,\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Unpack[BinaryBoundaryIoUKwargs],\n):\n    \"\"\"Create a new instance of the BinaryBoundaryIoU metric.\n\n    Please see the\n    [torchmetrics docs](https://lightning.ai/docs/torchmetrics/stable/pages/overview.html#metric-kwargs)\n    for more info about the **kwargs.\n\n    Args:\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class.  Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        **kwargs: Additional keyword arguments for the metric.\n\n    Keyword Args:\n        zero_division (int):\n            Value to return when there is a zero division. Default is 0.\n        compute_on_cpu (bool):\n            If metric state should be stored on CPU during computations. Only works for list states.\n        dist_sync_on_step (bool):\n            If metric state should synchronize on ``forward()``. Default is ``False``.\n        process_group (str):\n            The process group on which the synchronization is called. Default is the world.\n        dist_sync_fn (callable):\n            Function that performs the allgather option on the metric state. Default is a custom\n            implementation that calls ``torch.distributed.all_gather`` internally.\n        distributed_available_fn (callable):\n            Function that checks if the distributed backend is available. Defaults to a\n            check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n        sync_on_compute (bool):\n            If metric state should synchronize when ``compute`` is called. Default is ``True``.\n        compute_with_cache (bool):\n            If results from ``compute`` should be cached. Default is ``True``.\n\n    Raises:\n        ValueError: If dilation is not a float or int.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super().__init__(**kwargs)\n\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not isinstance(dilation, float | int):\n            raise ValueError(f\"Expected argument `dilation` to be a float or int, but got {dilation}.\")\n\n    self.dilation = dilation\n    self.threshold = threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if multidim_average == \"samplewise\":\n        self.add_state(\"intersection\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"union\", default=[], dist_reduce_fx=\"cat\")\n    else:\n        self.add_state(\"intersection\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.dilation","title":"dilation  <code>instance-attribute</code>","text":"<pre><code>dilation = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    dilation\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.intersection","title":"intersection  <code>instance-attribute</code>","text":"<pre><code>intersection: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.union","title":"union  <code>instance-attribute</code>","text":"<pre><code>union: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>torch.Tensor</code> )          \u2013            <p>The computed metric.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\n\n    Returns:\n        Tensor: The computed metric.\n\n    \"\"\"\n    if self.multidim_average == \"global\":\n        return self.intersection / self.union\n    else:\n        self.intersection = torch.tensor(self.intersection)\n        self.union = torch.tensor(self.union)\n        return self.intersection / self.union\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input arguments are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input shapes are invalid.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If the input arguments are invalid.\n        ValueError: If the input shapes are invalid.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.shape == target.shape:\n            raise ValueError(\n                f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\"\n            )\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.\")\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    target = target.to(torch.uint8)\n    preds = preds.to(torch.uint8)\n\n    target_boundary = get_boundary((target == 1).to(torch.uint8), self.dilation, self.validate_args)\n    preds_boundary = get_boundary(preds, self.dilation, self.validate_args)\n\n    intersection = target_boundary &amp; preds_boundary\n    union = target_boundary | preds_boundary\n\n    if self.ignore_index is not None:\n        # Important that this is NOT the boundary, but the original mask\n        valid_idx = target != self.ignore_index\n        intersection &amp;= valid_idx\n        union &amp;= valid_idx\n\n    intersection = intersection.sum().item()\n    union = union.sum().item()\n\n    if self.multidim_average == \"global\":\n        self.intersection += intersection\n        self.union += union\n    else:\n        self.intersection.append(intersection)\n        self.union.append(union)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs","title":"BinaryBoundaryIoUKwargs","text":"<p>               Bases: <code>typing.TypedDict</code></p> <p>Keyword arguments for the BinaryBoundaryIoU metric.</p>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.compute_on_cpu","title":"compute_on_cpu  <code>instance-attribute</code>","text":"<pre><code>compute_on_cpu: bool\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.compute_with_cache","title":"compute_with_cache  <code>instance-attribute</code>","text":"<pre><code>compute_with_cache: bool\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.dist_sync_fn","title":"dist_sync_fn  <code>instance-attribute</code>","text":"<pre><code>dist_sync_fn: callable\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.dist_sync_on_step","title":"dist_sync_on_step  <code>instance-attribute</code>","text":"<pre><code>dist_sync_on_step: bool\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.distributed_available_fn","title":"distributed_available_fn  <code>instance-attribute</code>","text":"<pre><code>distributed_available_fn: callable\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.process_group","title":"process_group  <code>instance-attribute</code>","text":"<pre><code>process_group: str\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.sync_on_compute","title":"sync_on_compute  <code>instance-attribute</code>","text":"<pre><code>sync_on_compute: bool\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division: typing.Literal[0, 1]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.get_boundary","title":"get_boundary","text":"<pre><code>get_boundary(\n    binary_instances: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Convert instance masks to instance boundaries.</p> <p>Parameters:</p> <ul> <li> <code>binary_instances</code>               (<code>torch.Tensor</code>)           \u2013            <p>Target instance masks. Must be binary. Can be batched, one-hot encoded or both. (3 or 4 dimensions). The last two dimensions must be height and width.</p> </li> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Weather arguments should be validated. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[torch.Tensor, torch.Tensor]</code>           \u2013            <p>tuple[torch.Tensor, torch.Tensor]: The boundaries of the instances.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef get_boundary(\n    binary_instances: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Convert instance masks to instance boundaries.\n\n    Args:\n        binary_instances (torch.Tensor): Target instance masks. Must be binary.\n            Can be batched, one-hot encoded or both. (3 or 4 dimensions).\n            The last two dimensions must be height and width.\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        validate_args (bool, optional): Weather arguments should be validated. Defaults to False.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: The boundaries of the instances.\n\n    \"\"\"\n    if validate_args:\n        assert binary_instances.dim() in [3, 4], f\"Expected 3 or 4 dimensions, got {binary_instances.dim()}\"\n        assert binary_instances.dtype == torch.uint8, f\"Expected torch.uint8, got {binary_instances.dtype}\"\n        assert binary_instances.min() &gt;= 0 and binary_instances.max() &lt;= 1, (\n            f\"Expected binary mask, got range between {binary_instances.min()} and {binary_instances.max()}\"\n        )\n        assert isinstance(dilation, float | int), f\"Expected float or int, got {type(dilation)}\"\n        assert dilation &gt;= 0, f\"Expected dilation &gt;= 0, got {dilation}\"\n\n    if binary_instances.dim() == 3:\n        _n, h, w = binary_instances.shape\n    else:\n        _n, _c, h, w = binary_instances.shape\n\n    if isinstance(dilation, float):\n        img_diag = sqrt(h**2 + w**2)\n        dilation = round(dilation * img_diag)\n        if dilation &lt; 1:\n            dilation = 1\n\n    # Pad the instances to avoid boundary issues\n    pad = torchvision.transforms.Pad(1)\n    binary_instances_padded = pad(binary_instances)\n\n    # Erode the instances to get the boundaries\n    eroded = erode_pytorch(binary_instances_padded, iterations=dilation, validate_args=validate_args)\n\n    # Remove the padding\n    if binary_instances.dim() == 3:\n        eroded = eroded[:, 1:-1, 1:-1]\n    else:\n        eroded = eroded[:, :, 1:-1, 1:-1]\n    # Calculate the boundary of the instances\n    boundaries = binary_instances - eroded\n\n    return boundaries\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/instance_helpers/","title":"instance_helpers","text":""},{"location":"reference/darts_segmentation/metrics/instance_helpers/#darts_segmentation.metrics.instance_helpers","title":"darts_segmentation.metrics.instance_helpers","text":"<p>Helper functions for instance segmentation metrics.</p>"},{"location":"reference/darts_segmentation/metrics/instance_helpers/#darts_segmentation.metrics.instance_helpers.CUCIM_AVAILABLE","title":"CUCIM_AVAILABLE  <code>module-attribute</code>","text":"<pre><code>CUCIM_AVAILABLE = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/instance_helpers/#darts_segmentation.metrics.instance_helpers.mask_to_instances","title":"mask_to_instances","text":"<pre><code>mask_to_instances(\n    x: torch.Tensor, validate_args: bool = False\n) -&gt; list[torch.Tensor]\n</code></pre> <p>Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.</p> <p>Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[torch.Tensor]</code>           \u2013            <p>list[torch.Tensor]: The instance masks. Length of list: batch_size. Shape of a tensor: (height, width), dtype: torch.uint8</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef mask_to_instances(x: torch.Tensor, validate_args: bool = False) -&gt; list[torch.Tensor]:\n    \"\"\"Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.\n\n    Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.\n\n    Args:\n        x (torch.Tensor): The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        list[torch.Tensor]: The instance masks. Length of list: batch_size.\n            Shape of a tensor: (height, width), dtype: torch.uint8\n\n    \"\"\"\n    if validate_args:\n        assert x.dim() == 3, f\"Expected 3 dimensions, got {x.dim()}\"\n        assert x.dtype == torch.uint8, f\"Expected torch.uint8, got {x.dtype}\"\n        assert x.min() &gt;= 0 and x.max() &lt;= 1, f\"Expected binary mask, got {x.min()} and {x.max()}\"\n\n    # A note on using lists as separation between instances instead of using a batched tensor:\n    # Using a batched tensor with instance numbers (1, 2, 3, ...) would indicate that the instances of the samples\n    # are identical. Using a list clearly separates the instances of the samples.\n\n    if CUCIM_AVAILABLE:\n        # Check if device is cuda\n        assert x.device.type == \"cuda\", f\"Expected device to be cuda, got {x.device.type}\"\n        x = cp.asarray(x).astype(cp.uint8)\n\n        instances = []\n        for x_i in x:\n            instances_i = label_gpu(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n\n    else:\n        instances = []\n        for x_i in x:\n            x_i = x_i.cpu().numpy()\n            instances_i = label(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/instance_helpers/#darts_segmentation.metrics.instance_helpers.match_instances","title":"match_instances","text":"<pre><code>match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.</p> <p>Parameters:</p> <ul> <li> <code>instances_target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the target. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>instances_preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>match_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int, int]</code>           \u2013            <p>tuple[int, int, int]: True positives, false positives, false negatives</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]:\n    \"\"\"Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.\n\n    Args:\n        instances_target (torch.Tensor): The instance mask of the target. Shape: (height, width), dtype: torch.uint8\n        instances_preds (torch.Tensor): The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8\n        match_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        tuple[int, int, int]: True positives, false positives, false negatives\n\n    \"\"\"\n    if validate_args:\n        assert instances_target.dim() == 2, f\"Expected 2 dimensions, got {instances_target.dim()}\"\n        assert instances_preds.dim() == 2, f\"Expected 2 dimensions, got {instances_preds.dim()}\"\n        assert instances_target.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_target.dtype}\"\n        assert instances_preds.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_preds.dtype}\"\n        assert instances_target.shape == instances_preds.shape, (\n            f\"Shapes do not match: {instances_target.shape} and {instances_preds.shape}\"\n        )\n\n    height, width = instances_target.shape\n    ntargets = instances_target.max().item()\n    npreds = instances_preds.max().item()\n    # If target or predictions has no instances, return 0 for their respective metrics.\n    # If none of them has instances, return 0 for all metrics. (This is implied)\n    if ntargets == 0:\n        return 0, npreds, 0\n    if npreds == 0:\n        return 0, 0, ntargets\n\n    # TODO: These are old edge case filter that need revision.\n    # They are probably not necessary, since the instance metrics are meaningless for noisy predictions.\n    # If there are too many predictions, return all as false positives (this happens when the model is very noisy)\n    # if npreds &gt; ntargets * 5:\n    #     return 0, npreds, ntargets\n    # If there is only one prediction, return all as false negatives (this happens when the model is very noisy)\n    # if npreds == 1 and ntargets &gt; 1:\n    #     return 0, 1, ntargets\n\n    # Create one-hot encoding of instances, so that each instance is a channel\n    instances_target_onehot = torch.zeros((ntargets, height, width), dtype=torch.uint8, device=instances_target.device)\n    instances_preds_onehot = torch.zeros((npreds, height, width), dtype=torch.uint8, device=instances_target.device)\n    for i in range(ntargets):\n        instances_target_onehot[i, :, :] = instances_target == (i + 1)\n    for i in range(npreds):\n        instances_preds_onehot[i, :, :] = instances_preds == (i + 1)\n\n    # Now the instances are channels, hence tensors of shape (num_instances, height, width)\n\n    # Calculate IoU (we need to do a n-m intersection and union, therefore we need to broadcast)\n    intersection = (instances_target_onehot.unsqueeze(1) &amp; instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (instances_target_onehot.unsqueeze(1) | instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    # Match instances based on IoU\n    tp = (iou &gt;= match_threshold).sum().item()\n    fp = npreds - tp\n    fn = ntargets - tp\n\n    return tp, fp, fn\n</code></pre>"},{"location":"reference/darts_segmentation/segment/","title":"segment","text":""},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment","title":"darts_segmentation.segment","text":"<p>Functionality for segmenting tiles.</p>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.DEFAULT_DEVICE","title":"DEFAULT_DEVICE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DEVICE = torch.device(\n    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter","title":"SMPSegmenter","text":"<pre><code>SMPSegmenter(\n    model_checkpoint: pathlib.Path | str,\n    device: torch.device = darts_segmentation.segment.DEFAULT_DEVICE,\n)\n</code></pre> <p>Semantic segmentation model wrapper for RTS detection using Segmentation Models PyTorch.</p> <p>This class provides a stateful inference interface for semantic segmentation models trained with the DARTS pipeline. It handles model loading, normalization, patch-based inference, and memory management.</p> <p>Attributes:</p> <ul> <li> <code>config</code>               (<code>darts_segmentation.segment.SMPSegmenterConfig</code>)           \u2013            <p>Model configuration including architecture and required bands.</p> </li> <li> <code>model</code>               (<code>torch.nn.Module</code>)           \u2013            <p>The loaded PyTorch segmentation model.</p> </li> <li> <code>device</code>               (<code>torch.device</code>)           \u2013            <p>Device where the model is loaded (CPU or GPU).</p> </li> </ul> Note <p>The segmenter automatically: - Loads model weights from PyTorch Lightning or legacy checkpoints - Normalizes input data using band-specific statistics from darts_utils.bands - Handles memory cleanup after inference to prevent GPU memory leaks</p> Example <p>Basic segmentation workflow:</p> <pre><code>from darts_segmentation import SMPSegmenter\nimport torch\n\n# Initialize segmenter\nsegmenter = SMPSegmenter(\n    model_checkpoint=\"path/to/model.ckpt\",\n    device=torch.device(\"cuda\")\n)\n\n# Check required bands\nprint(segmenter.required_bands)\n# {'blue', 'green', 'red', 'nir', 'ndvi', 'slope', 'hillshade', ...}\n\n# Run inference on preprocessed tile\nresult = segmenter.segment_tile(\n    tile=preprocessed_tile,\n    patch_size=1024,\n    overlap=16,\n    batch_size=8\n)\n\n# Access predictions\nprobabilities = result[\"probabilities\"]  # float32, range [0, 1]\n</code></pre> <p>Initialize the segmenter with a trained model checkpoint.</p> <p>Parameters:</p> <ul> <li> <code>model_checkpoint</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>Path to the model checkpoint file (.ckpt). Supports both PyTorch Lightning checkpoints and legacy formats.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_segmentation.segment.DEFAULT_DEVICE</code> )           \u2013            <p>Device to load the model on. Defaults to CUDA if available, else CPU.</p> </li> </ul> Note <p>The checkpoint must contain: - Model architecture configuration (config or hyper_parameters) - Trained weights (state_dict or statedict) - Required input bands list Using lightning checkpoints from our training pipeline is recommended.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __init__(self, model_checkpoint: Path | str, device: torch.device = DEFAULT_DEVICE):\n    \"\"\"Initialize the segmenter with a trained model checkpoint.\n\n    Args:\n        model_checkpoint (Path | str): Path to the model checkpoint file (.ckpt).\n            Supports both PyTorch Lightning checkpoints and legacy formats.\n        device (torch.device, optional): Device to load the model on.\n            Defaults to CUDA if available, else CPU.\n\n    Note:\n        The checkpoint must contain:\n        - Model architecture configuration (config or hyper_parameters)\n        - Trained weights (state_dict or statedict)\n        - Required input bands list\n        Using lightning checkpoints from our training pipeline is recommended.\n\n    \"\"\"\n    if isinstance(model_checkpoint, str):\n        model_checkpoint = Path(model_checkpoint)\n    self.device = device\n    ckpt = torch.load(model_checkpoint, map_location=self.device, weights_only=False)\n    self.config = SMPSegmenterConfig.from_ckpt(ckpt)\n    # Overwrite the encoder weights with None, because we load our own\n    self.config[\"model\"] |= {\"encoder_weights\": None}\n    self.model = smp.create_model(**self.config[\"model\"])\n    self.model.to(self.device)\n\n    # Legacy version\n    if \"statedict\" in ckpt.keys():\n        statedict = ckpt[\"statedict\"]\n    else:\n        statedict = ckpt[\"state_dict\"]\n        # Lightning Checkpoints are prefixed with \"model.\" -&gt; we need to remove them. This is an in-place function\n        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n    self.model.load_state_dict(statedict)\n    self.model.eval()\n\n    logger.debug(f\"Successfully loaded model from {model_checkpoint.resolve()} with inputs: {self.config['bands']}\")\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: darts_segmentation.segment.SMPSegmenterConfig = (\n    darts_segmentation.segment.SMPSegmenterConfig.from_ckpt(\n        ckpt\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: torch.device = (\n    darts_segmentation.segment.SMPSegmenter(device)\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: torch.nn.Module = (\n    segmentation_models_pytorch.create_model(\n        **(\n            darts_segmentation.segment.SMPSegmenter(\n                self\n            ).config[\"model\"]\n        )\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.required_bands","title":"required_bands  <code>property</code>","text":"<pre><code>required_bands: set[str]\n</code></pre> <p>The bands required by this model.</p>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset | list[xarray.Dataset]\n</code></pre> <p>Run inference on a single tile or a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>A single tile or a list of tiles augmented by a predicted <code>probabilities</code> layer, depending on the input.</p> </li> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>Each <code>probability</code> has type float32 and range [0, 1].</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xr.Dataset | list[xr.Dataset]:\n    \"\"\"Run inference on a single tile or a list of tiles.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n        Each `probability` has type float32 and range [0, 1].\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    elif isinstance(input, list):\n        return NotImplementedError(\"Currently passing multiple datasets at once is not supported.\")\n    else:\n        raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run semantic segmentation inference on a single tile.</p> <p>This method performs patch-based inference with optional overlap and reflection padding to handle edge artifacts. The tile is automatically normalized using band-specific statistics before inference.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Input tile containing preprocessed data. Must include all bands specified in <code>self.required_bands</code>. Variables should be float32 reflectance or normalized feature values.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>Size of square patches for inference in pixels. Larger patches use more memory but may be faster. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>Overlap between adjacent patches in pixels. Helps reduce edge artifacts. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Number of patches to process simultaneously. Higher values use more GPU memory but may be faster. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection padding applied to tile edges in pixels. Reduces edge effects. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Input tile augmented with a new data variable: - probabilities (float32): Segmentation probabilities in range [0, 1].   Attributes: long_name=\"Probabilities\"</p> </li> </ul> Note <p>Processing pipeline: 1. Extract and reorder bands according to model requirements 2. Normalize using darts_utils.bands.manager 3. Convert to torch tensor 4. Run patch-based inference with overlap blending 5. Convert predictions back to xarray</p> <p>Memory management: - Automatically frees GPU memory after inference - Predictions are moved to CPU before returning</p> Example <p>Run inference with custom parameters:</p> <pre><code>result = segmenter.segment_tile(\n    tile=preprocessed_tile,\n    patch_size=512,  # Smaller patches for limited GPU memory\n    overlap=32,      # More overlap for smoother predictions\n    batch_size=4,    # Smaller batches for memory constraints\n    reflection=16    # Add padding to reduce edge artifacts\n)\n\n# Extract probabilities\nprobs = result[\"probabilities\"]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@stopwatch.f(\n    \"Segmenting tile\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\"],\n)\ndef segment_tile(\n    self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n) -&gt; xr.Dataset:\n    \"\"\"Run semantic segmentation inference on a single tile.\n\n    This method performs patch-based inference with optional overlap and reflection padding\n    to handle edge artifacts. The tile is automatically normalized using band-specific\n    statistics before inference.\n\n    Args:\n        tile (xr.Dataset): Input tile containing preprocessed data. Must include all bands\n            specified in `self.required_bands`. Variables should be float32 reflectance\n            or normalized feature values.\n        patch_size (int, optional): Size of square patches for inference in pixels.\n            Larger patches use more memory but may be faster. Defaults to 1024.\n        overlap (int, optional): Overlap between adjacent patches in pixels. Helps reduce\n            edge artifacts. Defaults to 16.\n        batch_size (int, optional): Number of patches to process simultaneously. Higher\n            values use more GPU memory but may be faster. Defaults to 8.\n        reflection (int, optional): Reflection padding applied to tile edges in pixels.\n            Reduces edge effects. Defaults to 0.\n\n    Returns:\n        xr.Dataset: Input tile augmented with a new data variable:\n            - probabilities (float32): Segmentation probabilities in range [0, 1].\n              Attributes: long_name=\"Probabilities\"\n\n    Note:\n        Processing pipeline:\n        1. Extract and reorder bands according to model requirements\n        2. Normalize using darts_utils.bands.manager\n        3. Convert to torch tensor\n        4. Run patch-based inference with overlap blending\n        5. Convert predictions back to xarray\n\n        Memory management:\n        - Automatically frees GPU memory after inference\n        - Predictions are moved to CPU before returning\n\n    Example:\n        Run inference with custom parameters:\n\n        ```python\n        result = segmenter.segment_tile(\n            tile=preprocessed_tile,\n            patch_size=512,  # Smaller patches for limited GPU memory\n            overlap=32,      # More overlap for smoother predictions\n            batch_size=4,    # Smaller batches for memory constraints\n            reflection=16    # Add padding to reduce edge artifacts\n        )\n\n        # Extract probabilities\n        probs = result[\"probabilities\"]\n        ```\n\n    \"\"\"\n    # Convert the tile to a tensor\n    tile = tile[self.config[\"bands\"]].transpose(\"y\", \"x\")\n    tile = manager.normalize(tile)\n    # ? The heavy operation is .to_dataarray()\n    tensor_tile = torch.as_tensor(tile.to_dataarray().data)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tile = tensor_tile.unsqueeze(0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n    ).squeeze(0)\n\n    # Highly sophisticated DL-based predictor\n    tile[\"probabilities\"] = ((\"y\", \"x\"), probabilities.cpu().numpy())\n    tile[\"probabilities\"].attrs = {\"long_name\": \"Probabilities\"}\n\n    # Cleanup cuda memory\n    del tensor_tile, probabilities\n    free_torch()\n\n    return tile\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenterConfig","title":"SMPSegmenterConfig","text":"<p>               Bases: <code>typing.TypedDict</code></p> <p>Configuration for the segmentor.</p>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenterConfig.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands: list[str]\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenterConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: dict[str, typing.Any]\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenterConfig.from_ckpt","title":"from_ckpt  <code>classmethod</code>","text":"<pre><code>from_ckpt(\n    ckpt: dict[str, typing.Any],\n) -&gt; darts_segmentation.segment.SMPSegmenterConfig\n</code></pre> <p>Load and validate the config from a checkpoint for the segmentor.</p> <p>Parameters:</p> <ul> <li> <code>ckpt</code>               (<code>dict[str, typing.Any]</code>)           \u2013            <p>The checkpoint to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>darts_segmentation.segment.SMPSegmenterConfig</code>           \u2013            <p>The configuration.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@classmethod\ndef from_ckpt(cls, ckpt: dict[str, Any]) -&gt; \"SMPSegmenterConfig\":\n    \"\"\"Load and validate the config from a checkpoint for the segmentor.\n\n    Args:\n        ckpt: The checkpoint to load.\n\n    Returns:\n        The configuration.\n\n    \"\"\"\n    # Legacy version: config and directly in ckpt\n    if \"config\" in ckpt:\n        config = ckpt[\"config\"]\n        # Handling legacy case that the config contains the old keys\n        if \"input_combination\" in config and \"norm_factors\" in config:\n            # Check if all input_combination features are in norm_factors\n            config[\"bands\"] = config[\"input_combination\"]\n            config.pop(\"norm_factors\")\n            config.pop(\"input_combination\")\n        # Another legacy case uses a deprecated \"Bands\" class, which is pickled into the config as dict\n        if isinstance(config[\"bands\"], dict):\n            config[\"bands\"] = config[\"bands\"][\"bands\"]\n    # New version: load directly from lightning checkpoint\n    else:\n        config = ckpt[\"hyper_parameters\"][\"config\"]\n\n    assert \"model\" in config, \"Model config is missing!\"\n    assert \"bands\" in config, \"Bands config is missing!\"\n    return config\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.predict_in_patches","title":"predict_in_patches","text":"<pre><code>predict_in_patches(\n    model: torch.nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device: torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Predict on a tensor.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>torch.nn.Module</code>)           \u2013            <p>The model to use for prediction.</p> </li> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> </li> <li> <code>reflection</code>               (<code>int</code>)           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor.</p> </li> <li> <code>device</code>               (<code>torch.device</code>)           \u2013            <p>The device to use for the prediction.</p> </li> <li> <code>return_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>The predicted tensor.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/inference.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device: torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    n_skipped = 0\n    for i, batch in enumerate(patches):\n        # If batch contains only nans, skip it\n        if torch.isnan(batch).all(axis=0).any():\n            patched_probabilities[i * batch_size : (i + 1) * batch_size] = 0\n            n_skipped += 1\n            continue\n        # If batch contains some nans, replace them with zeros\n        batch[torch.isnan(batch)] = 0\n\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    if n_skipped &gt; 0:\n        logger.debug(f\"Skipped {n_skipped} batches because they only contained NaNs\")\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"reference/darts_segmentation/training/","title":"training","text":""},{"location":"reference/darts_segmentation/training/#darts_segmentation.training","title":"darts_segmentation.training","text":"<p>Training related functions and classes for Image Segmentation.</p>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.convert_lightning_checkpoint","title":"convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.cross_validation_smp","title":"cross_validation_smp","text":"<pre><code>cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    cv: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n)\n</code></pre> <p>Perform cross-validation for a model with given hyperparameters.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This cross-validation function is designed to evaluate the performance of a single model configuration. It can be used by a tuning script to tune hyperparameters. It calls the training function, hence most functionality is the same as the training function. In general, it does perform this:</p> <pre><code>for seed in seeds:\n    for fold in folds:\n        train_model(seed=seed, fold=fold, ...)\n</code></pre> <p>and calculates a score from the results.</p> <p>To specify on which metric(s) the score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code> for tunes (meaning if <code>tune_name</code> is not None) else <code>{artifact_dir}/_cross_validation</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation. If None, a name is generated automatically. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>cv</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for the training.</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for the data.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for the devices to use.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable, and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no runs were performed, meaning the configuration is invalid or no data was found.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/cv.py</code> <pre><code>def cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    cv: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n    logging_config: LoggingConfig = LoggingConfig(),\n):\n    \"\"\"Perform cross-validation for a model with given hyperparameters.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This cross-validation function is designed to evaluate the performance of a single model configuration.\n    It can be used by a tuning script to tune hyperparameters.\n    It calls the training function, hence most functionality is the same as the training function.\n    In general, it does perform this:\n\n    ```py\n    for seed in seeds:\n        for fold in folds:\n            train_model(seed=seed, fold=fold, ...)\n    ```\n\n    and calculates a score from the results.\n\n    To specify on which metric(s) the score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}` for tunes (meaning if `tune_name` is not None)\n    else `{artifact_dir}/_cross_validation`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the cross-validation. If None, a name is generated automatically.\n            Defaults to None.\n        tune_name (str | None, optional): Name of the tuning. Should only be specified by a tuning script.\n            Defaults to None.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        cv (CrossValidationConfig): Configuration for cross-validation.\n        training_config (TrainingConfig): Configuration for the training.\n        data_config (DataConfig): Configuration for the data.\n        device_config (DeviceConfig): Configuration for the devices to use.\n        hparams (Hyperparameters): Hyperparameters for the training.\n        logging_config (LoggingConfig): Logging configuration.\n\n    Returns:\n        tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable,\n            and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)\n\n    Raises:\n        ValueError: If no runs were performed, meaning the configuration is invalid or no data was found.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.scoring import score_from_runs\n\n    tick_fstart = time.perf_counter()\n\n    paths.set_defaults(default_dirs)\n\n    artifact_dir = logging_config.artifact_dir_at_cv(tune_name)\n    cv_name = name or generate_counted_name(artifact_dir)\n    artifact_dir = artifact_dir / cv_name\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    n_folds = cv.n_folds or data_config.total_folds\n\n    logger.info(\n        f\"Starting cross-validation '{cv_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_randoms*n_folds = {cv.n_randoms}*{n_folds} = {cv.n_randoms * n_folds} experiments.\"\n    )\n\n    seeds = cv.rng_seeds\n    logger.debug(f\"Using seeds: {seeds}\")\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs: list[_ProcessInputs] = []\n    for i, seed in enumerate(seeds):\n        for fold in range(n_folds):\n            current = i * len(seeds) + fold\n            total = n_folds * len(seeds)\n            run = TrainRunConfig(\n                name=f\"{cv_name}-run-f{fold}s{seed}\",\n                cv_name=cv_name,\n                tune_name=tune_name,\n                fold=fold,\n                random_seed=seed,\n            )\n            process_inputs.append(\n                _ProcessInputs(\n                    current=current,\n                    total=total,\n                    seed=seed,\n                    fold=fold,\n                    cv=cv,\n                    run=run,\n                    training_config=training_config,\n                    logging_config=logging_config,\n                    data_config=data_config,\n                    device_config=device_config,\n                    hparams=hparams,\n                )\n            )\n\n    run_infos = []\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"cv-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_training,\n    ):\n        run_infos.append(output.run_info)\n\n    if len(run_infos) == 0:\n        raise ValueError(\n            \"No runs were performed. Please check your configuration and data.\"\n            \" If you are using a tuning script, make sure to specify the correct parameters.\"\n        )\n\n    logger.debug(f\"{run_infos=}\")\n    score = score_from_runs(run_infos, cv.scoring_metric, cv.multi_score_strategy)\n\n    run_infos = pd.DataFrame(run_infos)\n    run_infos[\"score\"] = score\n    is_unstable = run_infos[\"is_unstable\"].any()\n    run_infos[\"score_is_unstable\"] = is_unstable\n    if is_unstable:\n        logger.warning(\"Score is unstable, meaning at least one of the metrics is NaN, Inf, -Inf or 0.\")\n    run_infos.to_parquet(artifact_dir / \"run_infos.parquet\")\n    logger.debug(f\"Saved run infos to {artifact_dir / 'run_infos.parquet'}\")\n\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Finished cross-validation '{cv_name}' in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, is_unstable, run_infos\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.test_smp","title":"test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    artifact_dir: pathlib.Path | None = None,\n    num_workers: int = 0,\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be specified by providing a float between 0 and 1 to data_split_by. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str] | str | float | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which seed/regions/samples split. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory to save artifacts. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    artifact_dir: Path | None = None,\n    num_workers: int = 0,\n    device_config: DeviceConfig = DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): The path (top-level) to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int): Batch size for training and validation.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be\n            specified by providing a float between 0 and 1 to data_split_by.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str] | str | float | None, optional): Select by which seed/regions/samples split.\n            Defaults to None.\n        artifact_dir (Path | None, optional): Directory to save artifacts.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device_config (DeviceConfig, optional): Device and distributed strategy related parameters.\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar, ThroughputMonitor\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n\n    LoggingManager._overwrite_wandb_logger()\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\", level=logging.INFO)\n\n    tick_fstart = time.perf_counter()\n\n    # Further nest the artifact directory to avoid cluttering the root directory\n    artifact_dir = artifact_dir / \"_runs\"\n\n    logger.info(\n        f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device_config}\")\n\n    lovely_tensors.set_config(color=False)\n    lovely_tensors.monkey_patch()\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    dataset_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    logger.debug(f\"Using model checkpoint at {model_ckp.resolve()}\")\n    model = LitSMP.load_from_checkpoint(model_ckp)\n\n    available_bands: list[str] = dataset_config[\"bands\"]\n    bands = model.hparams[\"config\"][\"bands\"]\n    assert all(b in available_bands for b in bands), (\n        f\"Some specified bands ({bands}) do not exist in the dataset config ({available_bands})!\"\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir,\n        batch_size=batch_size,\n        data_split_method=data_split_method,\n        data_split_by=data_split_by,\n        bands=bands,\n        num_workers=num_workers,\n    )\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=\"none\",\n            job_type=\"none\",\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            nbands=len(bands),\n            batch_size=batch_size,\n            patch_size=dataset_config[\"patch_size\"],\n        ),\n        ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0)),\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=device_config.accelerator,\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        devices=device_config.devices,\n        deterministic=True,\n    )\n\n    trainer.test(model, datamodule, ckpt_path=model_ckp)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.train_smp","title":"train_smp","text":"<pre><code>train_smp(\n    *,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    run: darts_segmentation.training.train.TrainRunConfig = darts_segmentation.training.train.TrainRunConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n)\n</code></pre> <p>Run the training of the SMP model, specifically binary segmentation.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This training function is meant for single training runs but is also used for cross-validation and hyperparameter tuning by cv.py and tune.py. This strongly affects where artifacts are stored:</p> <ul> <li>Run was created by a tune: <code>{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}</code></li> <li>Run was created by a cross-validation: <code>{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}</code></li> <li>Single runs: <code>{artifact_dir}/_runs/{run_name}-{run_id}</code></li> </ul> <p><code>run_name</code> can be specified by the user, else it is generated automatically. In case of cross-validation, the run name is generated automatically by the cross-validation. <code>run_id</code> is generated automatically by the training function. Both are saved to the final checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Data related parameters for training.</p> </li> <li> <code>run</code>               (<code>darts_segmentation.training.train.TrainRunConfig</code>, default:                   <code>darts_segmentation.training.train.TrainRunConfig()</code> )           \u2013            <p>Run related parameters for training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging related parameters for training.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Training related parameters for training.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the model.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>pl.Trainer: The trainer object used for training. Contains also metrics.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def train_smp(  # noqa: C901\n    *,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    run: TrainRunConfig = TrainRunConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n):\n    \"\"\"Run the training of the SMP model, specifically binary segmentation.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This training function is meant for single training runs but is also used for cross-validation and hyperparameter\n    tuning by cv.py and tune.py.\n    This strongly affects where artifacts are stored:\n\n    - Run was created by a tune: `{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}`\n    - Run was created by a cross-validation: `{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}`\n    - Single runs: `{artifact_dir}/_runs/{run_name}-{run_id}`\n\n    `run_name` can be specified by the user, else it is generated automatically.\n    In case of cross-validation, the run name is generated automatically by the cross-validation.\n    `run_id` is generated automatically by the training function.\n    Both are saved to the final checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        data_config (DataConfig): Data related parameters for training.\n        run (TrainRunConfig): Run related parameters for training.\n        logging_config (LoggingConfig): Logging related parameters for training.\n        device_config (DeviceConfig): Device and distributed strategy related parameters.\n        training_config (TrainingConfig): Training related parameters for training.\n        hparams (Hyperparameters): Hyperparameters for the model.\n\n    Returns:\n        pl.Trainer: The trainer object used for training. Contains also metrics.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from darts_utils.namegen import generate_counted_name, generate_id\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics, BinarySegmentationPreview\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n\n    LoggingManager._overwrite_wandb_logger()\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\", level=logging.INFO)\n\n    tick_fstart = time.perf_counter()\n\n    paths.set_defaults(default_dirs)\n\n    # Get the right nesting of the artifact directory\n    artifact_dir = logging_config.artifact_dir_at_run(run.cv_name, run.tune_name)\n    train_data_dir = (data_config.train_data_dir or paths.training).resolve()\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run.name or generate_counted_name(artifact_dir)\n    run_id = generate_id()  # Needed for wandb\n\n    logger.info(\n        f\"Starting training '{run_name}' ('{run_id}') with data from {train_data_dir}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(\n        f\"Using config:\\n\\t{run}\\n\\t{training_config}\\n\\t{data_config}\\n\\t{logging_config}\\n\\t\"\n        f\"{device_config}\\n\\t{hparams}\"\n    )\n    if training_config.continue_from_checkpoint:\n        logger.debug(f\"Continuing from checkpoint '{training_config.continue_from_checkpoint.resolve()}'\")\n\n    lovely_tensors.monkey_patch()\n    lovely_tensors.set_config(color=False)\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(run.random_seed, workers=True, verbose=False)\n\n    dataset_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n    # Confusing thing about bands: There is\n    # (1) the available bands derived from the dataset config and\n    # (2) the bands specified in the hyperparameters\n    available_bands: list[str] = dataset_config[\"bands\"]\n    if hparams.bands:\n        # Assert that all specified bands exist in the dataset_config\n        assert all(b in available_bands for b in hparams.bands), (\n            f\"Some specified bands ({hparams.bands}) do not exist in the dataset config ({available_bands})!\"\n        )\n        bands = hparams.bands\n    else:\n        bands = available_bands\n\n    config = SMPSegmenterConfig(\n        bands=bands,\n        model={\n            \"arch\": hparams.model_arch,\n            \"encoder_name\": hparams.model_encoder,\n            \"encoder_weights\": hparams.model_encoder_weights,\n            \"in_channels\": len(bands),\n            \"classes\": 1,\n        },\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir,\n        batch_size=hparams.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        fold_method=data_config.fold_method,\n        total_folds=data_config.total_folds,\n        fold=run.fold,\n        subsample=data_config.subsample,\n        bands=bands,\n        augment=hparams.augment,\n        num_workers=training_config.num_workers,\n        in_memory=data_config.in_memory,\n    )\n    if training_config.weights_from_checkpoint:\n        logger.debug(f\"Loading model weights from checkpoint '{training_config.weights_from_checkpoint.resolve()}'\")\n        model = LitSMP.load_from_checkpoint(\n            training_config.weights_from_checkpoint,\n            map_location=\"cpu\",\n        )\n    else:\n        model = LitSMP(\n            config=config,\n            learning_rate=hparams.learning_rate,\n            gamma=hparams.gamma,\n            focal_loss_alpha=hparams.focal_loss_alpha,\n            focal_loss_gamma=hparams.focal_loss_gamma,\n            # Storing the model / checkpoint version in the hparams\n            model_version=\"2\",\n            # These are only stored in the hparams and are only used as metadata\n            run_id=run_id,\n            run_name=run_name,\n            cv_name=run.cv_name or \"none\",\n            tune_name=run.tune_name or \"none\",\n            random_seed=run.random_seed,\n            datetime=datetime.now(),\n            model_framework=\"smp\",\n        )\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=None, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        tags = [train_data_dir.stem]\n        if run.cv_name:\n            tags.append(run.cv_name)\n        if run.tune_name:\n            tags.append(run.tune_name)\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent.parent if run.tune_name or run.cv_name else artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=logging_config.wandb_project,\n            entity=logging_config.wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=run.tune_name or \"none\",\n            job_type=run.cv_name or \"none\",\n            # Using tags to quickly identify the run\n            tags=tags,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{logging_config.wandb_entity}' and project '{logging_config.wandb_project}'\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks and profiler\n    callbacks = [\n        # RichProgressBar(),\n        ModelCheckpoint(\n            filename=\"epoch={epoch}-step={step}-val_iou={val/JaccardIndex:.2f}\",\n            auto_insert_metric_name=False,\n            verbose=True,\n            monitor=\"val/JaccardIndex\",\n            mode=\"max\",\n            save_last=\"link\",\n            save_top_k=training_config.save_top_k,\n        ),\n        BinarySegmentationMetrics(\n            nbands=len(bands),\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            is_crossval=bool(run.cv_name),\n            batch_size=hparams.batch_size,\n            patch_size=dataset_config[\"patch_size\"],\n        ),\n        BinarySegmentationPreview(\n            bands=bands,\n            augmentations=hparams.augment,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n        ),\n        # Something does not work well here...\n        # ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0), window_size=log_every_n_steps),\n    ]\n    # ! Using rich progress bar can lead to issues sometimes\n    if training_config.continue_from_checkpoint is None:\n        callbacks.append(RichProgressBar())\n\n    if training_config.early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {training_config.early_stopping_patience}\")\n        early_stopping = EarlyStopping(\n            monitor=\"val/JaccardIndex\", mode=\"max\", patience=training_config.early_stopping_patience\n        )\n        callbacks.append(early_stopping)\n\n    if training_config.advanced_profiler:\n        logger.error(\n            \"Using the advanced profiler is not supported yet. Please see: https://github.com/Lightning-AI/pytorch-lightning/issues/21365\"\n        )\n        # profiler_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"profiler\"\n        # profiler_dir.mkdir(parents=True, exist_ok=True)\n        # profiler = AdvancedProfiler(dirpath=profiler_dir, filename=\"perf_logs\", dump_stats=True)\n        # logger.debug(f\"Using profiler with output to {profiler.dirpath.resolve()}\")\n        profiler = None\n    else:\n        profiler = None\n\n    logger.debug(\n        f\"Creating lightning-trainer on {device_config.accelerator} with devices {device_config.devices}\"\n        f\" and strategy '{device_config.lightning_strategy}'\"\n    )\n    # Train\n    trainer = L.Trainer(\n        max_epochs=training_config.max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=logging_config.log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=logging_config.check_val_every_n_epoch or None,  # Set 0 to None\n        accelerator=device_config.accelerator,\n        devices=device_config.devices if device_config.devices[0] != \"auto\" else \"auto\",\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        deterministic=False,  # True does not work for some reason\n        profiler=profiler,\n    )\n    trainer.fit(model, datamodule, ckpt_path=training_config.continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.tune_smp","title":"tune_smp","text":"<pre><code>tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | typing.Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    cv_config: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    hpconfig: pathlib.Path | None = None,\n    config_file: pathlib.Path | None = None,\n)\n</code></pre> <p>Tune the hyper-parameters of the model using cross-validation and random states.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This tuning script is designed to sweep over hyperparameters with a cross-validation used to evaluate each hyperparameter configuration. Optionally, by setting <code>retrain_and_test</code> to True, the best hyperparameters are then selected based on the cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.</p> <p>Hyperparameters can be configured using a <code>hpconfig</code> file (YAML or Toml). Please consult the training guide or the documentation of <code>darts_segmentation.training.hparams.parse_hyperparameters</code> to learn how such a file should be structured. Per default, a random search is performed, where the number of samples can be specified by <code>n_trials</code>. If <code>n_trials</code> is set to \"grid\", a grid search is performed instead. However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.</p> <p>To specify on which metric(s) the cv score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\". In such cases, the configuration is not considered for further evaluation.</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning run. Will be generated based on the number of existing directories in the artifact directory if None. Defaults to None.</p> </li> <li> <code>n_trials</code>               (<code>int | typing.Literal['grid']</code>, default:                   <code>100</code> )           \u2013            <p>Number of trials to perform in hyperparameter tuning. If \"grid\", span a grid search over all configured hyperparameters. In a grid search, only constant or choice hyperparameters are allowed. Defaults to 100.</p> </li> <li> <code>retrain_and_test</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to retrain the model with the best hyperparameters and test it. Defaults to False.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>cv_config</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation. Defaults to CrossValidationConfig().</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for training. Defaults to TrainingConfig().</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for data. Defaults to DataConfig().</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for device. Defaults to DeviceConfig().</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Configuration for logging. Defaults to LoggingConfig().</p> </li> <li> <code>hpconfig</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the hyperparameter configuration file. Please see the documentation of <code>hyperparameters</code> for more information. Defaults to None.</p> </li> <li> <code>config_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the configuration file. If provided, it will be used instead of <code>hpconfig</code> if <code>hpconfig</code> is None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no hyperparameter configuration file is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/tune.py</code> <pre><code>def tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    cv_config: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    hpconfig: Path | None = None,\n    config_file: Annotated[Path | None, cyclopts.Parameter(parse=False)] = None,\n):\n    \"\"\"Tune the hyper-parameters of the model using cross-validation and random states.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This tuning script is designed to sweep over hyperparameters with a cross-validation\n    used to evaluate each hyperparameter configuration.\n    Optionally, by setting `retrain_and_test` to True, the best hyperparameters are then selected based on the\n    cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.\n\n    Hyperparameters can be configured using a `hpconfig` file (YAML or Toml).\n    Please consult the training guide or the documentation of\n    `darts_segmentation.training.hparams.parse_hyperparameters` to learn how such a file should be structured.\n    Per default, a random search is performed, where the number of samples can be specified by `n_trials`.\n    If `n_trials` is set to \"grid\", a grid search is performed instead.\n    However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.\n\n    To specify on which metric(s) the cv score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n    In such cases, the configuration is not considered for further evaluation.\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the tuning run.\n            Will be generated based on the number of existing directories in the artifact directory if None.\n            Defaults to None.\n        n_trials (int | Literal[\"grid\"], optional): Number of trials to perform in hyperparameter tuning.\n            If \"grid\", span a grid search over all configured hyperparameters.\n            In a grid search, only constant or choice hyperparameters are allowed.\n            Defaults to 100.\n        retrain_and_test (bool, optional): Whether to retrain the model with the best hyperparameters and test it.\n            Defaults to False.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        cv_config (CrossValidationConfig, optional): Configuration for cross-validation.\n            Defaults to CrossValidationConfig().\n        training_config (TrainingConfig, optional): Configuration for training.\n            Defaults to TrainingConfig().\n        data_config (DataConfig, optional): Configuration for data.\n            Defaults to DataConfig().\n        device_config (DeviceConfig, optional): Configuration for device.\n            Defaults to DeviceConfig().\n        logging_config (LoggingConfig, optional): Configuration for logging.\n            Defaults to LoggingConfig().\n        hpconfig (Path | None, optional): Path to the hyperparameter configuration file.\n            Please see the documentation of `hyperparameters` for more information.\n            Defaults to None.\n        config_file (Path | None, optional): Path to the configuration file. If provided,\n            it will be used instead of `hpconfig` if `hpconfig` is None. Defaults to None.\n\n    Returns:\n        tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.\n\n    Raises:\n        ValueError: If no hyperparameter configuration file is provided.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.hparams import parse_hyperparameters, sample_hyperparameters\n    from darts_segmentation.training.scoring import score_from_single_run\n    from darts_segmentation.training.train import test_smp, train_smp\n\n    tick_fstart = time.perf_counter()\n\n    paths.set_defaults(default_dirs)\n\n    tune_name = name or generate_counted_name(logging_config.artifact_dir or paths.artifacts)\n    artifact_dir = (logging_config.artifact_dir or paths.artifacts) / tune_name\n    run_infos_file = artifact_dir / f\"{tune_name}.parquet\"\n\n    # Check if the artifact directory is empty\n    assert not artifact_dir.exists(), f\"{artifact_dir} already exists.\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    hpconfig = hpconfig or config_file\n    if hpconfig is None:\n        raise ValueError(\n            \"No hyperparameter configuration file provided. Please provide a valid file via the `--hpconfig` flag.\"\n        )\n    param_grid = parse_hyperparameters(hpconfig)\n    logger.debug(f\"Parsed hyperparameter grid: {param_grid}\")\n    param_list = sample_hyperparameters(param_grid, n_trials)\n\n    logger.info(\n        f\"Starting tune '{tune_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_trials*n_randoms*n_folds =\"\n        f\" {len(param_list)}*{cv_config.n_randoms}*{cv_config.n_folds} =\"\n        f\" {len(param_list) * cv_config.n_randoms * cv_config.n_folds} experiments.\"\n    )\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs = [\n        _ProcessInputs(\n            current=i,\n            total=len(param_list),\n            tune_name=tune_name,\n            cv=cv_config,\n            training_config=training_config,\n            logging_config=logging_config,\n            data_config=data_config,\n            device_config=device_config,\n            hparams=hparams,\n        )\n        for i, hparams in enumerate(param_list)\n    ]\n\n    run_infos: list[pd.DataFrame] = []\n    best_score = 0\n    best_hp = None\n\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"tune-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_cv,\n    ):\n        run_infos.append(output.run_infos)\n        if not output.is_unstable and output.score &gt; best_score:\n            best_score = output.score\n            best_hp = inp.hparams\n\n        # Save already here to prevent data loss if something goes wrong\n        pd.concat(run_infos).reset_index(drop=True).to_parquet(run_infos_file)\n        logger.debug(f\"Saved run infos to {run_infos_file}\")\n\n    if len(run_infos) == 0:\n        logger.error(\"No hyperparameters resulted in a valid score. Please check the logs for more information.\")\n        return 0, run_infos\n\n    run_infos = pd.concat(run_infos).reset_index(drop=True)\n\n    tick_fend = time.perf_counter()\n\n    if best_hp is None:\n        logger.warning(\n            f\"Tuning completed in {tick_fend - tick_fstart:.2f}s.\"\n            \" No hyperparameters resulted in a valid score. Please check the logs for more information.\"\n        )\n        return 0, run_infos\n    logger.info(\n        f\"Tuning completed in {tick_fend - tick_fstart:.2f}s. The best score was {best_score:.4f} with {best_hp}.\"\n    )\n\n    # =====================\n    # === End of tuning ===\n    # =====================\n\n    if not retrain_and_test:\n        return 0, run_infos\n\n    logger.info(\"Starting retraining with the best hyperparameters.\")\n\n    tick_fstart = time.perf_counter()\n    trainer = train_smp(\n        run=TrainRunConfig(name=f\"{tune_name}-retrain\"),\n        training_config=training_config,  # TODO: device and strategy\n        data_config=DataConfig(\n            train_data_dir=data_config.train_data_dir,\n            data_split_method=data_config.data_split_method,\n            data_split_by=data_config.data_split_by,\n            fold_method=None,  # No fold method for retraining\n            total_folds=None,  # No folds for retraining\n        ),\n        logging_config=LoggingConfig(\n            artifact_dir=artifact_dir,\n            log_every_n_steps=logging_config.log_every_n_steps,\n            check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            wandb_entity=logging_config.wandb_entity,\n            wandb_project=logging_config.wandb_project,\n        ),\n        hparams=best_hp,\n    )\n    run_id = trainer.lightning_module.hparams[\"run_id\"]\n    trainer = test_smp(\n        train_data_dir=data_config.train_data_dir,\n        run_id=run_id,\n        run_name=f\"{tune_name}-retrain\",\n        model_ckp=trainer.checkpoint_callback.best_model_path,\n        batch_size=best_hp.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        artifact_dir=artifact_dir,\n        num_workers=training_config.num_workers,\n        device_config=device_config,\n        wandb_entity=logging_config.wandb_entity,\n        wandb_project=logging_config.wandb_project,\n    )\n\n    run_info = {k: v.item() for k, v in trainer.callback_metrics.items()}\n    test_scoring_metric = (\n        cv_config.scoring_metric.replace(\"val/\", \"test/\")\n        if isinstance(cv_config.scoring_metric, str)\n        else [sm.replace(\"val/\", \"test/\") for sm in cv_config.scoring_metric]\n    )\n    score = score_from_single_run(run_info, test_scoring_metric, cv_config.multi_score_strategy)\n    is_unstable = check_score_is_unstable(run_info, cv_config.scoring_metric)\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Retraining and testing completed successfully in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, run_infos\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.validate_dataset","title":"validate_dataset","text":"<pre><code>validate_dataset(\n    train_data_dir: str | pathlib.Path,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n        \"none\",\n    ] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Validate a training dataset config based on a training dataset.</p> <p>Please see the DartsDataModule for more information.</p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified', 'none']</code>, default:                   <code>'kfold'</code> )           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> <li> <code>bands</code>               (<code>Bands | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of bands to use. Expects the data_dir to contain a config.toml with a \"darts.bands\" key, with which the indices of the bands will be mapped to. Defaults to None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/data_validate.py</code> <pre><code>def validate_dataset(\n    train_data_dir: str | Path,\n    # data_split is for the test split\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str | float] | None = None,\n    # fold is for cross-validation split (train/val)\n    fold_method: Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\", \"none\"] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n    bands: list[str] | None = None,\n):\n    \"\"\"Validate a training dataset config based on a training dataset.\n\n    Please see the DartsDataModule for more information.\n\n    Args:\n        train_data_dir (Path): The path to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the test size can be specified\n            by providing a list with a single a float between 0 and 1 to data_split_by\n            This will be the fraction of the data to be used for testing.\n            E.g. [0.2] will use 20% of the data for testing.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str | float] | None, optional): Select by which regions/samples to split or\n            the size of test set. Defaults to None.\n        fold_method (Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\", \"none\"], optional):\n            Method for cross-validation split. Defaults to \"kfold\".\n        total_folds (int, optional): Total number of folds in cross-validation. Defaults to 5.\n        subsample (int | None, optional): If set, will subsample the dataset to this number of samples.\n            This is useful for debugging and testing. Defaults to None.\n        bands (Bands | list[str] | None, optional): List of bands to use.\n            Expects the data_dir to contain a config.toml with a \"darts.bands\" key,\n            with which the indices of the bands will be mapped to.\n            Defaults to None.\n\n    \"\"\"\n    import geopandas as gpd\n    import toml\n\n    from darts_segmentation.training.data import _get_fold, _log_stats, _split_metadata\n\n    data_dir = Path(train_data_dir)\n\n    config_file = data_dir / \"config.toml\"\n    assert config_file.exists(), f\"Config file {config_file} not found!\"\n    config = toml.load(config_file)\n    assert \"darts\" in config and \"bands\" in config[\"darts\"], f\"Config file {config_file} is missing 'darts.bands' key!\"\n\n    if bands:\n        # Check if bands are in config\n        data_bands = config[\"darts\"][\"bands\"]\n        for b in bands:\n            assert b in data_bands, f\"Band {b} not found in config file {config_file}!\"\n\n    metadata_file = data_dir / \"metadata.parquet\"\n    assert metadata_file.exists(), f\"Metadata file {metadata_file} not found!\"\n    metadata = gpd.read_parquet(data_dir / \"metadata.parquet\")\n\n    if subsample is not None:\n        metadata = metadata.sample(n=subsample, random_state=42)\n    train_metadata, test_metadata = _split_metadata(metadata, data_split_method, data_split_by)\n\n    _log_stats(train_metadata, \"Non-Test (train-split)\", level=logging.INFO)\n    _log_stats(test_metadata, \"Test (test-split)\", level=logging.INFO)\n\n    for fold in range(total_folds):\n        train_index, val_index = _get_fold(train_metadata, fold_method, total_folds, fold)\n        _log_stats(metadata.loc[train_index], f\"Training fold {fold} (train-fold)\", level=logging.INFO)\n        _log_stats(metadata.loc[val_index], f\"Validation fold {fold} (val-fold)\", level=logging.INFO)\n</code></pre>"},{"location":"reference/darts_segmentation/training/adp/","title":"adp","text":""},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp","title":"darts_segmentation.training.adp","text":"<p>Abstract Data Parallelism (ADP) module for DARTS Segmentation.</p>"},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp.RunI","title":"RunI  <code>module-attribute</code>","text":"<pre><code>RunI = typing.TypeVar('I')\n</code></pre>"},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp.RunO","title":"RunO  <code>module-attribute</code>","text":"<pre><code>RunO = typing.TypeVar('O')\n</code></pre>"},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp._adp","title":"_adp","text":"<pre><code>_adp(\n    process_inputs: list[\n        darts_segmentation.training.adp.RunI\n    ],\n    is_parallel: bool,\n    devices: list[int],\n    available_devices: multiprocessing.Queue,\n    _run: collections.abc.Callable[\n        [darts_segmentation.training.adp.RunI],\n        darts_segmentation.training.adp.RunO,\n    ],\n) -&gt; collections.abc.Generator[\n    tuple[\n        darts_segmentation.training.adp.RunI,\n        darts_segmentation.training.adp.RunO,\n    ],\n    None,\n    None,\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/adp.py</code> <pre><code>def _adp(\n    process_inputs: list[RunI],\n    is_parallel: bool,\n    devices: list[int],\n    available_devices: Queue,\n    _run: Callable[[RunI], RunO],\n) -&gt; Generator[tuple[RunI, RunO], None, None]:\n    # Handling different parallelization strategies\n    if is_parallel:\n        logger.debug(\"Using parallel strategy for ADP\")\n        for device in devices:\n            logger.debug(f\"Adding device {device} to available devices queue\")\n            available_devices.put(device)\n        with ProcessPoolExecutor(max_workers=len(devices)) as executor:\n            futures = {executor.submit(_run, inp): inp for inp in process_inputs}\n\n            for future in as_completed(futures):\n                inp = futures[future]\n                try:\n                    output = future.result()\n                except (KeyboardInterrupt, SystemError, SystemExit):\n                    executor.shutdown(wait=False, cancel_futures=True)\n                    raise\n                except Exception as e:\n                    logger.error(f\"Error in {inp}: {e}\", exc_info=True)\n                    continue\n\n                yield inp, output\n    else:\n        logger.debug(\"Using serial strategy for ADP\")\n        for inp in process_inputs:\n            try:\n                output = _run(inp)\n            except Exception as e:\n                logger.error(f\"Error in {inp}: {e}\", exc_info=True)\n                continue\n            yield inp, output\n</code></pre>"},{"location":"reference/darts_segmentation/training/augmentations/","title":"augmentations","text":""},{"location":"reference/darts_segmentation/training/augmentations/#darts_segmentation.training.augmentations","title":"darts_segmentation.training.augmentations","text":"<p>Augmentations for segmentation tasks.</p>"},{"location":"reference/darts_segmentation/training/augmentations/#darts_segmentation.training.augmentations.Augmentation","title":"Augmentation  <code>module-attribute</code>","text":"<pre><code>Augmentation = typing.Literal[\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"D4\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\",\n    \"Posterize\",\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/augmentations/#darts_segmentation.training.augmentations.get_augmentation","title":"get_augmentation","text":"<pre><code>get_augmentation(\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None,\n    always_apply: bool = False,\n) -&gt; albumentations.Compose | None\n</code></pre> <p>Get augmentations for segmentation tasks.</p> <p>Parameters:</p> <ul> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply. If None or emtpy, no augmentations are applied. If not empty, augmentations are applied in the order they are listed. Available augmentations:     - D4 (Combination of HorizontalFlip, VerticalFlip, and RandomRotate90)     - Blur     - RandomBrightnessContrast     - MultiplicativeNoise     - Posterize (quantization to reduce number of bits per channel)</p> </li> <li> <code>always_apply</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, augmentations are always applied. This is useful for visualization/testing augmentations. Default is False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown augmentation is provided.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>albumentations.Compose | None</code>           \u2013            <p>A.Compose | None: A Compose object containing the augmentations. If no augmentations are provided, returns None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/augmentations.py</code> <pre><code>def get_augmentation(augment: list[Augmentation] | None, always_apply: bool = False) -&gt; \"A.Compose | None\":  # noqa: C901\n    \"\"\"Get augmentations for segmentation tasks.\n\n    Args:\n        augment (list[Augmentation] | None): List of augmentations to apply.\n            If None or emtpy, no augmentations are applied.\n            If not empty, augmentations are applied in the order they are listed.\n            Available augmentations:\n                - D4 (Combination of HorizontalFlip, VerticalFlip, and RandomRotate90)\n                - Blur\n                - RandomBrightnessContrast\n                - MultiplicativeNoise\n                - Posterize (quantization to reduce number of bits per channel)\n        always_apply (bool): If True, augmentations are always applied.\n            This is useful for visualization/testing augmentations.\n            Default is False.\n\n    Raises:\n        ValueError: If an unknown augmentation is provided.\n\n    Returns:\n        A.Compose | None: A Compose object containing the augmentations.\n            If no augmentations are provided, returns None.\n\n    \"\"\"\n    import albumentations as A  # noqa: N812\n\n    if not isinstance(augment, list) or len(augment) == 0:\n        return None\n\n    # Replace HorizontalFlip, VerticalFlip, RandomRotate90 with D4\n    if \"HorizontalFlip\" in augment and \"VerticalFlip\" in augment and \"RandomRotate90\" in augment:\n        augment = [aug for aug in augment if aug not in (\"HorizontalFlip\", \"VerticalFlip\", \"RandomRotate90\")]\n        augment.insert(0, \"D4\")\n\n    transforms = []\n    for aug in augment:\n        match aug:\n            case \"D4\":\n                transforms.append(A.D4())\n            case \"HorizontalFlip\":\n                transforms.append(A.HorizontalFlip(p=1.0 if always_apply else 0.5))\n            case \"VerticalFlip\":\n                transforms.append(A.VerticalFlip(p=1.0 if always_apply else 0.5))\n            case \"RandomRotate90\":\n                transforms.append(A.RandomRotate90())\n            case \"Blur\":\n                transforms.append(A.Blur(p=1.0 if always_apply else 0.5))\n            case \"RandomBrightnessContrast\":\n                transforms.append(A.RandomBrightnessContrast(p=1.0 if always_apply else 0.5))\n            case \"MultiplicativeNoise\":\n                transforms.append(\n                    A.MultiplicativeNoise(per_channel=True, elementwise=True, p=1.0 if always_apply else 0.5)\n                )\n            case \"Posterize\":\n                # First convert to uint8, then apply posterization, then convert back to float32\n                # * Note: This does only work for float32 images.\n                transforms += [\n                    A.FromFloat(dtype=\"uint8\"),\n                    A.Posterize(num_bits=6, p=1.0),\n                    A.ToFloat(),\n                ]\n            case _:\n                raise ValueError(f\"Unknown augmentation: {aug}\")\n    return A.Compose(transforms)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/","title":"callbacks","text":""},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks","title":"darts_segmentation.training.callbacks","text":"<p>PyTorch Lightning Callbacks for training and validation.</p>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Augmentation","title":"Augmentation  <code>module-attribute</code>","text":"<pre><code>Augmentation = typing.Literal[\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"D4\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\",\n    \"Posterize\",\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Stage","title":"Stage  <code>module-attribute</code>","text":"<pre><code>Stage = typing.Literal[\"fit\", \"validate\", \"test\", \"predict\"]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU","title":"BinaryBoundaryIoU","text":"<pre><code>BinaryBoundaryIoU(\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Unpack[\n        darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs\n    ],\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Binary Boundary IoU metric for binary segmentation tasks.</p> <p>This metric is similar to the Binary Intersection over Union (IoU or Jaccard Index) metric, but instead of comparing all pixels it only compares the boundaries of each foreground object.</p> <p>Create a new instance of the BinaryBoundaryIoU metric.</p> <p>Please see the torchmetrics docs for more info about the **kwargs.</p> <p>Parameters:</p> <ul> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class.  Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>**kwargs</code>               (<code>typing.Unpack[darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the metric.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>zero_division</code>               (<code>int</code>)           \u2013            <p>Value to return when there is a zero division. Default is 0.</p> </li> <li> <code>compute_on_cpu</code>               (<code>bool</code>)           \u2013            <p>If metric state should be stored on CPU during computations. Only works for list states.</p> </li> <li> <code>dist_sync_on_step</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize on <code>forward()</code>. Default is <code>False</code>.</p> </li> <li> <code>process_group</code>               (<code>str</code>)           \u2013            <p>The process group on which the synchronization is called. Default is the world.</p> </li> <li> <code>dist_sync_fn</code>               (<code>callable</code>)           \u2013            <p>Function that performs the allgather option on the metric state. Default is a custom implementation that calls <code>torch.distributed.all_gather</code> internally.</p> </li> <li> <code>distributed_available_fn</code>               (<code>callable</code>)           \u2013            <p>Function that checks if the distributed backend is available. Defaults to a check of <code>torch.distributed.is_available()</code> and <code>torch.distributed.is_initialized()</code>.</p> </li> <li> <code>sync_on_compute</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize when <code>compute</code> is called. Default is <code>True</code>.</p> </li> <li> <code>compute_with_cache</code>               (<code>bool</code>)           \u2013            <p>If results from <code>compute</code> should be cached. Default is <code>True</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If dilation is not a float or int.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def __init__(\n    self,\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Unpack[BinaryBoundaryIoUKwargs],\n):\n    \"\"\"Create a new instance of the BinaryBoundaryIoU metric.\n\n    Please see the\n    [torchmetrics docs](https://lightning.ai/docs/torchmetrics/stable/pages/overview.html#metric-kwargs)\n    for more info about the **kwargs.\n\n    Args:\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class.  Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        **kwargs: Additional keyword arguments for the metric.\n\n    Keyword Args:\n        zero_division (int):\n            Value to return when there is a zero division. Default is 0.\n        compute_on_cpu (bool):\n            If metric state should be stored on CPU during computations. Only works for list states.\n        dist_sync_on_step (bool):\n            If metric state should synchronize on ``forward()``. Default is ``False``.\n        process_group (str):\n            The process group on which the synchronization is called. Default is the world.\n        dist_sync_fn (callable):\n            Function that performs the allgather option on the metric state. Default is a custom\n            implementation that calls ``torch.distributed.all_gather`` internally.\n        distributed_available_fn (callable):\n            Function that checks if the distributed backend is available. Defaults to a\n            check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n        sync_on_compute (bool):\n            If metric state should synchronize when ``compute`` is called. Default is ``True``.\n        compute_with_cache (bool):\n            If results from ``compute`` should be cached. Default is ``True``.\n\n    Raises:\n        ValueError: If dilation is not a float or int.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super().__init__(**kwargs)\n\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not isinstance(dilation, float | int):\n            raise ValueError(f\"Expected argument `dilation` to be a float or int, but got {dilation}.\")\n\n    self.dilation = dilation\n    self.threshold = threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if multidim_average == \"samplewise\":\n        self.add_state(\"intersection\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"union\", default=[], dist_reduce_fx=\"cat\")\n    else:\n        self.add_state(\"intersection\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.dilation","title":"dilation  <code>instance-attribute</code>","text":"<pre><code>dilation = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    dilation\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.intersection","title":"intersection  <code>instance-attribute</code>","text":"<pre><code>intersection: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.union","title":"union  <code>instance-attribute</code>","text":"<pre><code>union: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>torch.Tensor</code> )          \u2013            <p>The computed metric.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\n\n    Returns:\n        Tensor: The computed metric.\n\n    \"\"\"\n    if self.multidim_average == \"global\":\n        return self.intersection / self.union\n    else:\n        self.intersection = torch.tensor(self.intersection)\n        self.union = torch.tensor(self.union)\n        return self.intersection / self.union\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input arguments are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input shapes are invalid.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If the input arguments are invalid.\n        ValueError: If the input shapes are invalid.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.shape == target.shape:\n            raise ValueError(\n                f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\"\n            )\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.\")\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    target = target.to(torch.uint8)\n    preds = preds.to(torch.uint8)\n\n    target_boundary = get_boundary((target == 1).to(torch.uint8), self.dilation, self.validate_args)\n    preds_boundary = get_boundary(preds, self.dilation, self.validate_args)\n\n    intersection = target_boundary &amp; preds_boundary\n    union = target_boundary | preds_boundary\n\n    if self.ignore_index is not None:\n        # Important that this is NOT the boundary, but the original mask\n        valid_idx = target != self.ignore_index\n        intersection &amp;= valid_idx\n        union &amp;= valid_idx\n\n    intersection = intersection.sum().item()\n    union = union.sum().item()\n\n    if self.multidim_average == \"global\":\n        self.intersection += intersection\n        self.union += union\n    else:\n        self.intersection.append(intersection)\n        self.union.append(union)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy","title":"BinaryInstanceAccuracy","text":"<pre><code>BinaryInstanceAccuracy(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance accuracy metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _accuracy_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision","title":"BinaryInstanceAveragePrecision","text":"<pre><code>BinaryInstanceAveragePrecision(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve</code></p> <p>Compute the average precision for binary instance segmentation.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tensor:  # type: ignore[override]  # noqa: D102\n    return _binary_average_precision_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # type: ignore[override]  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix","title":"BinaryInstanceConfusionMatrix","text":"<pre><code>BinaryInstanceConfusionMatrix(\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance confusion matrix metric.</p> <p>Create a new instance of the BinaryInstanceConfusionMatrix metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, return the confusion matrix normalized by the number of instances. If False, return the confusion matrix without normalization. Defaults to None.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalize</code> is not a bool.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceConfusionMatrix metric.\n\n    Args:\n        normalize (bool, optional): If True, return the confusion matrix normalized by the number of instances.\n            If False, return the confusion matrix without normalization. Defaults to None.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `normalize` is not a bool.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        matching_threshold=matching_threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if normalize is not None and not isinstance(normalize, bool):\n        raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.normalize","title":"normalize  <code>instance-attribute</code>","text":"<pre><code>normalize = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix(\n    normalize\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    # tn is always 0\n    if self.normalize:\n        all = tp + fp + fn\n        return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n    else:\n        return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n    add_text: bool = True,\n    labels: list[str] | None = None,\n    cmap: torchmetrics.utilities.plot._CMAP_TYPE\n    | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n    add_text: bool = True,\n    labels: list[str] | None = None,  # type: ignore\n    cmap: _CMAP_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    val = val or self.compute()\n    if not isinstance(val, Tensor):\n        raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n    fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n    return fig, ax\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score","title":"BinaryInstanceF1Score","text":"<pre><code>BinaryInstanceF1Score(\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore</code></p> <p>Binary instance F1 score metric.</p> <p>Create a new instance of the BinaryInstanceF1Score metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceF1Score metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        beta=1.0,\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=validate_args,\n        zero_division=zero_division,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision","title":"BinaryInstancePrecision","text":"<pre><code>BinaryInstancePrecision(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance precision metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"precision\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve","title":"BinaryInstancePrecisionRecallCurve","text":"<pre><code>BinaryInstancePrecisionRecallCurve(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Compute the precision-recall curve for binary instance segmentation.</p> <p>This metric works similar to <code>torchmetrics.classification.PrecisionRecallCurve</code>, with two key differences: 1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.     Instead of calculating the values for each pixel. 2. The \"thresholds\" argument is required.     Calculating the thresholds at the compute stage would cost to much memory for this usecase.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.compute","title":"compute","text":"<pre><code>compute() -&gt; tuple[\n    torch.Tensor, torch.Tensor, torch.Tensor\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; tuple[Tensor, Tensor, Tensor]:  # noqa: D102\n    return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.plot","title":"plot","text":"<pre><code>plot(\n    curve: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n    | None = None,\n    score: torch.Tensor | bool | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    curve: tuple[Tensor, Tensor, Tensor] | None = None,\n    score: Tensor | bool | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    curve_computed = curve or self.compute()\n    # switch order as the standard way is recall along x-axis and precision along y-axis\n    curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n    score = (\n        _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n        if not curve and score is True\n        else None\n    )\n    return plot_curve(\n        curve_computed, score=score, ax=ax, label_names=(\"Recall\", \"Precision\"), name=self.__class__.__name__\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall","title":"BinaryInstanceRecall","text":"<pre><code>BinaryInstanceRecall(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance recall metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"recall\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics","title":"BinarySegmentationMetrics","text":"<pre><code>BinarySegmentationMetrics(\n    *,\n    nbands: int,\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n    is_crossval: bool = False,\n    batch_size: int = 8,\n    patch_size: int = 512,\n)\n</code></pre> <p>               Bases: <code>lightning.pytorch.callbacks.Callback</code></p> <p>Callback for validation metrics and visualizations.</p> <p>Initialize the ValidationCallback.</p> <p>Parameters:</p> <ul> <li> <code>nbands</code>               (<code>int</code>)           \u2013            <p>Number of bands used create compute estimates.</p> </li> <li> <code>val_set</code>               (<code>str</code>, default:                   <code>'val'</code> )           \u2013            <p>Name of the validation set. Only used for naming the validation metrics. Defaults to \"val\".</p> </li> <li> <code>test_set</code>               (<code>str</code>, default:                   <code>'test'</code> )           \u2013            <p>Name of the test set. Only used for naming the test metrics. Defaults to \"test\".</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>is_crossval</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether the training is done with cross-validation. This will change the logging behavior of scalar metrics from logging to {val_set} to just \"val\". The logging behaviour of the samples is not affected. Defaults to False.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size. Needed for throughput measurements. Defaults to 8.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Patch size. Needed for throughput measurements. Defaults to 512.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    *,\n    nbands: int,\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n    is_crossval: bool = False,\n    batch_size: int = 8,\n    patch_size: int = 512,\n):\n    \"\"\"Initialize the ValidationCallback.\n\n    Args:\n        nbands (int): Number of bands used create compute estimates.\n        val_set (str, optional): Name of the validation set. Only used for naming the validation metrics.\n            Defaults to \"val\".\n        test_set (str, optional): Name of the test set. Only used for naming the test metrics. Defaults to \"test\".\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        is_crossval (bool, optional): Whether the training is done with cross-validation.\n            This will change the logging behavior of scalar metrics from logging to {val_set} to just \"val\".\n            The logging behaviour of the samples is not affected.\n            Defaults to False.\n        batch_size (int, optional): Batch size. Needed for throughput measurements. Defaults to 8.\n        patch_size (int, optional): Patch size. Needed for throughput measurements. Defaults to 512.\n\n    \"\"\"\n    assert \"/\" not in val_set, \"val_set must not contain '/'\"\n    assert \"/\" not in test_set, \"test_set must not contain '/'\"\n    self.val_set = val_set\n    self.test_set = test_set\n    self.plot_every_n_val_epochs = plot_every_n_val_epochs\n    self.nbands = nbands\n    self.is_crossval = is_crossval\n    self.batch_size = batch_size\n    self.patch_size = patch_size\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    batch_size\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.is_crossval","title":"is_crossval  <code>instance-attribute</code>","text":"<pre><code>is_crossval = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    is_crossval\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.nbands","title":"nbands  <code>instance-attribute</code>","text":"<pre><code>nbands = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    nbands\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    patch_size\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.pl_module","title":"pl_module  <code>instance-attribute</code>","text":"<pre><code>pl_module: lightning.LightningModule\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    plot_every_n_val_epochs\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.stage","title":"stage  <code>instance-attribute</code>","text":"<pre><code>stage: darts_segmentation.training.callbacks.Stage\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_cmx","title":"test_cmx  <code>instance-attribute</code>","text":"<pre><code>test_cmx: torchmetrics.ConfusionMatrix\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_instance_cmx","title":"test_instance_cmx  <code>instance-attribute</code>","text":"<pre><code>test_instance_cmx: (\n    darts_segmentation.metrics.BinaryInstanceConfusionMatrix\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_instance_prc","title":"test_instance_prc  <code>instance-attribute</code>","text":"<pre><code>test_instance_prc: darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_metrics","title":"test_metrics  <code>instance-attribute</code>","text":"<pre><code>test_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_prc","title":"test_prc  <code>instance-attribute</code>","text":"<pre><code>test_prc: torchmetrics.PrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_roc","title":"test_roc  <code>instance-attribute</code>","text":"<pre><code>test_roc: torchmetrics.ROC\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_set","title":"test_set  <code>instance-attribute</code>","text":"<pre><code>test_set = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    test_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.train_metrics","title":"train_metrics  <code>instance-attribute</code>","text":"<pre><code>train_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: lightning.Trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_cmx","title":"val_cmx  <code>instance-attribute</code>","text":"<pre><code>val_cmx: torchmetrics.ConfusionMatrix\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_metrics","title":"val_metrics  <code>instance-attribute</code>","text":"<pre><code>val_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_prc","title":"val_prc  <code>instance-attribute</code>","text":"<pre><code>val_prc: torchmetrics.PrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_roc","title":"val_roc  <code>instance-attribute</code>","text":"<pre><code>val_roc: torchmetrics.ROC\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_set","title":"val_set  <code>instance-attribute</code>","text":"<pre><code>val_set = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    val_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.is_val_plot_epoch","title":"is_val_plot_epoch","text":"<pre><code>is_val_plot_epoch(\n    current_epoch: int, check_val_every_n_epoch: int | None\n) -&gt; bool\n</code></pre> <p>Check if the current epoch is an epoch where validation samples should be plotted.</p> <p>Parameters:</p> <ul> <li> <code>current_epoch</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int | None</code>)           \u2013            <p>The number of epochs to check for plotting. If None, no plotting is done.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the current epoch is a plot epoch, False otherwise.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def is_val_plot_epoch(self, current_epoch: int, check_val_every_n_epoch: int | None) -&gt; bool:\n    \"\"\"Check if the current epoch is an epoch where validation samples should be plotted.\n\n    Args:\n        current_epoch (int): The current epoch.\n        check_val_every_n_epoch (int | None): The number of epochs to check for plotting.\n            If None, no plotting is done.\n\n    Returns:\n        bool: True if the current epoch is a plot epoch, False otherwise.\n\n    \"\"\"\n    if check_val_every_n_epoch is None or check_val_every_n_epoch &lt;= 0 or self.plot_every_n_val_epochs &lt;= 0:\n        return False\n    n = self.plot_every_n_val_epochs * check_val_every_n_epoch\n    return ((current_epoch + 1) % n) == 0 or current_epoch == 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_test_batch_end","title":"on_test_batch_end","text":"<pre><code>on_test_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    pl_module.log(f\"{self.test_set}/loss\", outputs[\"loss\"])\n    _x, y = batch\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'test_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    pl_module.test_metrics.update(y_hat, y)\n    pl_module.test_roc.update(y_hat, y)\n    pl_module.test_prc.update(y_hat, y)\n    pl_module.test_cmx.update(y_hat, y)\n    pl_module.test_instance_prc.update(y_hat, y)\n    pl_module.test_instance_cmx.update(y_hat, y)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    pl_module.test_cmx.compute()\n    pl_module.test_roc.compute()\n    pl_module.test_prc.compute()\n    pl_module.test_instance_prc.compute()\n    pl_module.test_instance_cmx.compute()\n\n    # Plot roc, prc and confusion matrix to disk and wandb\n    fig_cmx, _ = pl_module.test_cmx.plot(cmap=\"Blues\")\n    fig_roc, _ = pl_module.test_roc.plot(score=True)\n    fig_prc, _ = pl_module.test_prc.plot(score=True)\n    fig_instance_cmx, _ = pl_module.test_instance_cmx.plot(cmap=\"Blues\")\n    fig_instance_prc, _ = pl_module.test_instance_prc.plot(score=True)\n\n    # Check for a wandb or csv logger to log the images\n    for pllogger in pl_module.loggers:\n        if isinstance(pllogger, CSVLogger):\n            fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.test_set}-samples\"\n            fig_dir.mkdir(exist_ok=True, parents=True)\n            fig_cmx.savefig(fig_dir / f\"cmx_{pl_module.global_step}.png\")\n            fig_roc.savefig(fig_dir / f\"roc_{pl_module.global_step}.png\")\n            fig_prc.savefig(fig_dir / f\"prc_{pl_module.global_step}.png\")\n            fig_instance_cmx.savefig(fig_dir / f\"instance_cmx_{pl_module.global_step}.png\")\n            fig_instance_prc.savefig(fig_dir / f\"instance_prc_{pl_module.global_step}.png\")\n        if isinstance(pllogger, WandbLogger):\n            wandb_run: Run = pllogger.experiment\n            wandb_run.log({f\"{self.test_set}/cmx\": wandb.Image(fig_cmx)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/roc\": wandb.Image(fig_roc)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/prc\": wandb.Image(fig_prc)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/instance_cmx\": wandb.Image(fig_instance_cmx)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/instance_prc\": wandb.Image(fig_instance_prc)}, commit=False)\n\n    fig_cmx.clear()\n    fig_roc.clear()\n    fig_prc.clear()\n    fig_instance_cmx.clear()\n    fig_instance_prc.clear()\n    plt.close(\"all\")\n\n    # This will also commit the accumulated plots\n    pl_module.log_dict(pl_module.test_metrics.compute())\n\n    pl_module.test_metrics.reset()\n    pl_module.test_roc.reset()\n    pl_module.test_prc.reset()\n    pl_module.test_cmx.reset()\n    pl_module.test_instance_prc.reset()\n    pl_module.test_instance_cmx.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_train_batch_end","title":"on_train_batch_end","text":"<pre><code>on_train_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_train_batch_end(self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx):  # noqa: D102\n    pl_module.log(\"train/loss\", outputs[\"loss\"])\n    _, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'training_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n    pl_module.train_metrics(y_hat, y)\n    pl_module.log_dict(pl_module.train_metrics, on_step=True, on_epoch=False)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    pl_module.train_metrics.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_validation_batch_end","title":"on_validation_batch_end","text":"<pre><code>on_validation_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    pl_module.log(f\"{self._val_prefix}/loss\", outputs[\"loss\"])\n    _x, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'validation_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    pl_module.val_metrics.update(y_hat, y)\n    pl_module.val_roc.update(y_hat, y)\n    pl_module.val_prc.update(y_hat, y)\n    pl_module.val_cmx.update(y_hat, y)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    # Only do this every self.plot_every_n_val_epochs epochs\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if is_val_plot_epoch and trainer.state.stage != \"sanity_check\":\n        pl_module.val_cmx.compute()\n        pl_module.val_roc.compute()\n        pl_module.val_prc.compute()\n\n        # Plot roc, prc and confusion matrix to disk and wandb\n        fig_cmx, _ = pl_module.val_cmx.plot(cmap=\"Blues\")\n        fig_roc, _ = pl_module.val_roc.plot(score=True)\n        fig_prc, _ = pl_module.val_prc.plot(score=True)\n\n        # Check for a wandb or csv logger to log the images\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self._val_prefix}-samples\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig_cmx.savefig(fig_dir / f\"cmx_{pl_module.global_step}png\")\n                fig_roc.savefig(fig_dir / f\"roc_{pl_module.global_step}png\")\n                fig_prc.savefig(fig_dir / f\"prc_{pl_module.global_step}.png\")\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                wandb_run.log({f\"{self._val_prefix}/cmx\": wandb.Image(fig_cmx)}, commit=False)\n                wandb_run.log({f\"{self._val_prefix}/roc\": wandb.Image(fig_roc)}, commit=False)\n                wandb_run.log({f\"{self._val_prefix}/prc\": wandb.Image(fig_prc)}, commit=False)\n\n        fig_cmx.clear()\n        fig_roc.clear()\n        fig_prc.clear()\n        plt.close(\"all\")\n\n    # This will also commit the accumulated plots\n    pl_module.log_dict(pl_module.val_metrics.compute())\n\n    pl_module.val_metrics.reset()\n    pl_module.val_roc.reset()\n    pl_module.val_prc.reset()\n    pl_module.val_cmx.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.setup","title":"setup","text":"<pre><code>setup(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> <p>Setups the callback.</p> <p>Creates metrics required for the specific stage:</p> <ul> <li>For the \"fit\" stage, creates training and validation metrics and visualizations.</li> <li>For the \"validate\" stage, only creates validation metrics and visualizations.</li> <li>For the \"test\" stage, only creates test metrics and visualizations.</li> <li>For the \"predict\" stage, no metrics or visualizations are created.</li> </ul> <p>Always maps the trainer and pl_module to the callback.</p> <p>Training and validation metrics are \"simple\" metrics from torchmetrics. The validation visualizations are more complex metrics from torchmetrics. The test metrics and vsiualizations are the same as the validation ones, and also include custom \"Instance\" metrics.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>lightning.Trainer</code>)           \u2013            <p>The lightning trainer.</p> </li> <li> <code>pl_module</code>               (<code>lightning.LightningModule</code>)           \u2013            <p>The lightning module.</p> </li> <li> <code>stage</code>               (<code>typing.Literal['fit', 'validate', 'test', 'predict']</code>)           \u2013            <p>The current stage. One of: \"fit\", \"validate\", \"test\", \"predict\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):\n    \"\"\"Setups the callback.\n\n    Creates metrics required for the specific stage:\n\n    - For the \"fit\" stage, creates training and validation metrics and visualizations.\n    - For the \"validate\" stage, only creates validation metrics and visualizations.\n    - For the \"test\" stage, only creates test metrics and visualizations.\n    - For the \"predict\" stage, no metrics or visualizations are created.\n\n    Always maps the trainer and pl_module to the callback.\n\n    Training and validation metrics are \"simple\" metrics from torchmetrics.\n    The validation visualizations are more complex metrics from torchmetrics.\n    The test metrics and vsiualizations are the same as the validation ones,\n    and also include custom \"Instance\" metrics.\n\n    Args:\n        trainer (Trainer): The lightning trainer.\n        pl_module (LightningModule): The lightning module.\n        stage (Literal[\"fit\", \"validate\", \"test\", \"predict\"]): The current stage.\n            One of: \"fit\", \"validate\", \"test\", \"predict\".\n\n    \"\"\"\n    # Save references to the trainer and pl_module\n    self.trainer = trainer\n    self.pl_module = pl_module\n    self.stage = stage\n\n    # We don't want to use memory in the predict stage\n    if stage == \"predict\":\n        return\n\n    # Add throughput metric, meant to be consumed by the ThroughputMonitor callback\n    # ! This will assume that the batch size does not change during training!\n    with torch.device(\"meta\"):\n        model: torch.Module = copy.deepcopy(self.pl_module.model).to(device=\"meta\")\n\n        def sample_forward():\n            batch = torch.randn(\n                self.batch_size,\n                self.nbands,\n                self.patch_size,\n                self.patch_size,\n                device=\"meta\",\n            )\n            return model(batch)\n\n        if stage == \"fit\":\n            # We use sum as a dummy loss function because we don't have a second input available\n            self.pl_module.flops_per_batch = measure_flops(model, sample_forward, loss_fn=torch.Tensor.sum)\n        else:\n            # Don't compute backward pass for validation and test\n            self.pl_module.flops_per_batch = measure_flops(model, sample_forward)\n        logger.debug(f\"FLOPS per batch: {self.pl_module.flops_per_batch}\")\n\n    metric_kwargs = {\"task\": \"binary\", \"validate_args\": False, \"ignore_index\": 2}\n    metrics = MetricCollection(\n        {\n            \"Accuracy\": Accuracy(**metric_kwargs),\n            \"Precision\": Precision(**metric_kwargs),\n            \"Specificity\": Specificity(**metric_kwargs),\n            \"Recall\": Recall(**metric_kwargs),\n            \"F1Score\": F1Score(**metric_kwargs),\n            \"JaccardIndex\": JaccardIndex(**metric_kwargs),\n            \"CohenKappa\": CohenKappa(**metric_kwargs),\n            \"HammingDistance\": HammingDistance(**metric_kwargs),\n        }\n    )\n\n    added_metrics: defaultdict[str] = defaultdict(list)\n\n    # Train metrics only for the fit stage\n    if stage == \"fit\":\n        pl_module.train_metrics = metrics.clone(prefix=\"train/\")\n        added_metrics[\"train\"] += list(pl_module.train_metrics.keys(keep_base=True))\n    # Validation metrics and visualizations for the fit and validate stages\n    if stage == \"fit\" or stage == \"validate\":\n        pl_module.val_metrics = metrics.clone(prefix=f\"{self._val_prefix}/\")\n        pl_module.val_metrics.add_metrics(\n            {\n                \"AUROC\": AUROC(thresholds=20, **metric_kwargs),\n                \"AveragePrecision\": AveragePrecision(thresholds=20, **metric_kwargs),\n            }\n        )\n        pl_module.val_roc = ROC(thresholds=20, **metric_kwargs)\n        pl_module.val_prc = PrecisionRecallCurve(thresholds=20, **metric_kwargs)\n        pl_module.val_cmx = ConfusionMatrix(normalize=\"true\", **metric_kwargs)\n        added_metrics[self._val_prefix] += list(pl_module.val_metrics.keys(keep_base=True))\n        added_metrics[self._val_prefix] += [\"roc\", \"prc\", \"cmx\"]\n\n    # Test metrics and visualizations for the test stage\n    if stage == \"test\":\n        pl_module.test_metrics = metrics.clone(prefix=f\"{self.test_set}/\")\n        pl_module.test_metrics.add_metrics(\n            {\n                \"AUROC\": AUROC(thresholds=20, **metric_kwargs),\n                \"AveragePrecision\": AveragePrecision(thresholds=20, **metric_kwargs),\n            }\n        )\n        pl_module.test_roc = ROC(thresholds=20, **metric_kwargs)\n        pl_module.test_prc = PrecisionRecallCurve(thresholds=20, **metric_kwargs)\n        pl_module.test_cmx = ConfusionMatrix(normalize=\"true\", **metric_kwargs)\n\n        # Instance Metrics\n        instance_metric_kwargs = {\"validate_args\": False, \"ignore_index\": 2, \"matching_threshold\": 0.3}\n        pl_module.test_metrics.add_metrics(\n            {\n                \"InstanceAccuracy\": BinaryInstanceAccuracy(**instance_metric_kwargs),\n                \"InstancePrecision\": BinaryInstancePrecision(**instance_metric_kwargs),\n                \"InstanceRecall\": BinaryInstanceRecall(**instance_metric_kwargs),\n                \"InstanceF1Score\": BinaryInstanceF1Score(**instance_metric_kwargs),\n                \"InstanceAveragePrecision\": BinaryInstanceAveragePrecision(thresholds=20, **instance_metric_kwargs),\n            }\n        )\n        boundary_metric_kwargs = {\"validate_args\": False, \"ignore_index\": 2}\n        pl_module.test_metrics.add_metrics(\n            {\n                \"InstanceBoundaryIoU\": BinaryBoundaryIoU(**boundary_metric_kwargs),\n            }\n        )\n        pl_module.test_instance_prc = BinaryInstancePrecisionRecallCurve(thresholds=20, **instance_metric_kwargs)\n        pl_module.test_instance_cmx = BinaryInstanceConfusionMatrix(normalize=True, **instance_metric_kwargs)\n\n        added_metrics[self.test_set] += list(pl_module.test_metrics.keys(keep_base=True))\n        added_metrics[self.test_set] += [\"roc\", \"prc\", \"cmx\", \"instance_prc\", \"instance_cmx\"]\n\n    # Log the added metrics\n    added_metrics = {k: str(sorted(v)) for k, v in added_metrics.items()}\n    logger.debug(f\"Added metrics:{added_metrics}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.teardown","title":"teardown","text":"<pre><code>teardown(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def teardown(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):  # noqa: D102\n    # Delete the references to the trainer and pl_module\n    del self.trainer\n    del self.pl_module\n    del self.stage\n\n    # No need to delete anything if we are in the predict stage\n    if stage == \"predict\":\n        return\n\n    if stage == \"fit\":\n        del pl_module.train_metrics\n\n    if stage == \"fit\" or stage == \"validate\":\n        del pl_module.val_metrics\n        del pl_module.val_roc\n        del pl_module.val_prc\n        del pl_module.val_cmx\n\n    if stage == \"test\":\n        del pl_module.test_metrics\n        del pl_module.test_roc\n        del pl_module.test_prc\n        del pl_module.test_cmx\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview","title":"BinarySegmentationPreview","text":"<pre><code>BinarySegmentationPreview(\n    *,\n    bands: list[str],\n    augmentations: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n)\n</code></pre> <p>               Bases: <code>lightning.pytorch.callbacks.Callback</code></p> <p>Callback for validation metrics and visualizations.</p> <p>Initialize the ValidationCallback.</p> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>Bands</code>)           \u2013            <p>List of bands to combine for the visualization.</p> </li> <li> <code>augmentations</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of augmentations to apply. Default to None.</p> </li> <li> <code>val_set</code>               (<code>str</code>, default:                   <code>'val'</code> )           \u2013            <p>Name of the validation set. Only used for naming the validation metrics. Defaults to \"val\".</p> </li> <li> <code>test_set</code>               (<code>str</code>, default:                   <code>'test'</code> )           \u2013            <p>Name of the test set. Only used for naming the test metrics. Defaults to \"test\".</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    *,\n    bands: list[str],\n    augmentations: list[Augmentation] | None = None,\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n):\n    \"\"\"Initialize the ValidationCallback.\n\n    Args:\n        bands (Bands): List of bands to combine for the visualization.\n        augmentations (list[Augmentation] | None): List of augmentations to apply. Default to None.\n        val_set (str, optional): Name of the validation set. Only used for naming the validation metrics.\n            Defaults to \"val\".\n        test_set (str, optional): Name of the test set. Only used for naming the test metrics. Defaults to \"test\".\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n\n    \"\"\"\n    assert \"/\" not in val_set, \"val_set must not contain '/'\"\n    assert \"/\" not in test_set, \"test_set must not contain '/'\"\n    self.val_set = val_set\n    self.test_set = test_set\n    self.plot_every_n_val_epochs = plot_every_n_val_epochs\n    self.band_names = bands\n    self.augmentations = augmentations\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.augmentations","title":"augmentations  <code>instance-attribute</code>","text":"<pre><code>augmentations = darts_segmentation.training.callbacks.BinarySegmentationPreview(\n    augmentations\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.band_names","title":"band_names  <code>instance-attribute</code>","text":"<pre><code>band_names = darts_segmentation.training.callbacks.BinarySegmentationPreview(\n    bands\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.pl_module","title":"pl_module  <code>instance-attribute</code>","text":"<pre><code>pl_module: lightning.LightningModule\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs = darts_segmentation.training.callbacks.BinarySegmentationPreview(\n    plot_every_n_val_epochs\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.stage","title":"stage  <code>instance-attribute</code>","text":"<pre><code>stage: darts_segmentation.training.callbacks.Stage\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.test_set","title":"test_set  <code>instance-attribute</code>","text":"<pre><code>test_set = darts_segmentation.training.callbacks.BinarySegmentationPreview(\n    test_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: lightning.Trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.val_set","title":"val_set  <code>instance-attribute</code>","text":"<pre><code>val_set = darts_segmentation.training.callbacks.BinarySegmentationPreview(\n    val_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.is_val_plot_epoch","title":"is_val_plot_epoch","text":"<pre><code>is_val_plot_epoch(\n    current_epoch: int, check_val_every_n_epoch: int | None\n) -&gt; bool\n</code></pre> <p>Check if the current epoch is an epoch where validation samples should be plotted.</p> <p>Parameters:</p> <ul> <li> <code>current_epoch</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int | None</code>)           \u2013            <p>The number of epochs to check for plotting. If None, no plotting is done.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the current epoch is a plot epoch, False otherwise.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def is_val_plot_epoch(self, current_epoch: int, check_val_every_n_epoch: int | None) -&gt; bool:\n    \"\"\"Check if the current epoch is an epoch where validation samples should be plotted.\n\n    Args:\n        current_epoch (int): The current epoch.\n        check_val_every_n_epoch (int | None): The number of epochs to check for plotting.\n            If None, no plotting is done.\n\n    Returns:\n        bool: True if the current epoch is a plot epoch, False otherwise.\n\n    \"\"\"\n    if check_val_every_n_epoch is None or check_val_every_n_epoch &lt;= 0 or self.plot_every_n_val_epochs &lt;= 0:\n        return False\n\n    n = self.plot_every_n_val_epochs * check_val_every_n_epoch\n    return ((current_epoch + 1) % n) == 0 or current_epoch == 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.on_test_batch_end","title":"on_test_batch_end","text":"<pre><code>on_test_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    # Only do this every self.plot_every_n_val_epochs epochs\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if not is_val_plot_epoch:\n        return\n\n    x, y = batch\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'test_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    # Create figures for the samples (plot at maximum 30)\n    # We want to plot at max 20 POSITIVE samples and 10 NEGATIVE samples in a single epoch\n    # These should also be the same over all epochs\n    max_pos_samples = 20\n    max_neg_samples = 10\n    for i in range(x.shape[0]):\n        if self._test_pos_visualizations &gt;= max_pos_samples and self._test_neg_visualizations &gt;= max_neg_samples:\n            break\n\n        # Plot positive sample\n        if y[i].sum() &gt; 0 and self._test_pos_visualizations &lt; max_pos_samples:\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.band_names)\n            self._test_pos_visualizations += 1\n        # Plot negative sample\n        elif y[i].sum() == 0 and self._test_neg_visualizations &lt; max_neg_samples:\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.band_names)\n            self._test_neg_visualizations += 1\n        # Either the number of positive or negative samples is already full\n        else:\n            continue\n\n        sample_on_disk: Path | None = None\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.test_set}-samples\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig_file = fig_dir / f\"sample_{pl_module.global_step}_{batch_idx}_{i}.png\"\n                fig.savefig(fig_file, dpi=100)\n                sample_on_disk = fig_file\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                # We don't commit the log yet, so that the step is increased with the next lightning log\n                # Which happens at the end of the validation epoch\n                img_name = f\"{self.test_set}-samples/sample_{batch_idx}_{i}\"\n                if sample_on_disk is not None:\n                    wandb_img = wandb.Image(str(sample_on_disk))\n                else:\n                    wandb_img = wandb.Image(fig)\n                wandb_run.log({img_name: wandb_img}, commit=False)\n        fig.clear()\n        plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    self._test_pos_visualizations = 0\n    self._test_neg_visualizations = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.on_train_batch_end","title":"on_train_batch_end","text":"<pre><code>on_train_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_train_batch_end(self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx):  # noqa: D102\n    # Do an augmentation visualization at the start of the training\n    if (\n        pl_module.current_epoch == 0\n        and batch_idx == 0\n        and self.augmentations is not None\n        and len(self.augmentations) &gt; 0\n    ):\n        logger.debug(\"Creating augmentation visualization, this may take a while...\")\n        x, _ = batch\n        fig, _ = plot_augmentations(x, augmentations=self.augmentations, band_names=self.band_names)\n\n        aug_fig_on_disk: Path | None = None\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / \"data\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig.savefig(fig_dir / f\"augmentation_{pl_module.global_step}_{batch_idx}.png\", dpi=100)\n                aug_fig_on_disk = fig_dir / f\"augmentation_{pl_module.global_step}_{batch_idx}.png\"\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                img_name = f\"data/augmentation_{pl_module.global_step}_{batch_idx}\"\n                # Runtime optimization: If we have already saved the figure to disk, use that\n                if aug_fig_on_disk is not None:\n                    wandb_img = wandb.Image(str(aug_fig_on_disk))\n                else:\n                    pil_image = _uplt_to_pil(fig, dpi=100)\n                    wandb_img = wandb.Image(pil_image)\n                # We don't commit the log yet, so that the step is increased with the next lightning log\n                # Which happens at the end of the validation epoch\n                wandb_run.log({img_name: wandb_img}, commit=False)\n        fig.clear()\n        plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.on_validation_batch_end","title":"on_validation_batch_end","text":"<pre><code>on_validation_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_batch_end(  # noqa: C901, D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    # Don't plot in sanity check\n    if trainer.state.stage == \"sanity_check\":\n        return\n\n    # Only do this every self.plot_every_n_val_epochs epochs\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if not is_val_plot_epoch:\n        return\n\n    x, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'validation_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    # Create figures for the samples (plot at maximum 30)\n    # We want to plot at max 20 POSITIVE samples and 10 NEGATIVE samples in a single epoch\n    # These should also be the same over all epochs\n    max_pos_samples = 20\n    max_neg_samples = 10\n    for i in range(x.shape[0]):\n        if self._val_pos_visualizations &gt;= max_pos_samples and self._val_neg_visualizations &gt;= max_neg_samples:\n            break\n\n        # Plot positive sample\n        is_postive = (y[i] == 1).sum() &gt; 0\n        if is_postive and self._val_pos_visualizations &lt; max_pos_samples:\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.band_names)\n            self._val_pos_visualizations += 1\n        # Plot negative sample\n        elif not is_postive and self._val_neg_visualizations &lt; max_neg_samples:\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.band_names)\n            self._val_neg_visualizations += 1\n        # Either the number of positive or negative samples is already full\n        else:\n            continue\n\n        sample_on_disk: Path | None = None\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.val_set}-samples\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig_file = fig_dir / f\"sample_{pl_module.global_step}_{batch_idx}_{i}.png\"\n                fig.savefig(fig_file, dpi=100)\n                sample_on_disk = fig_file\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                # We don't commit the log yet, so that the step is increased with the next lightning log\n                # Which happens at the end of the validation epoch\n                img_name = f\"{self.val_set}-samples/sample_{batch_idx}_{i}\"\n                if sample_on_disk is not None:\n                    wandb_img = wandb.Image(str(sample_on_disk))\n                else:\n                    wandb_img = wandb.Image(fig)\n                wandb_run.log({img_name: wandb_img}, commit=False)\n        fig.clear()\n        plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    self._val_pos_visualizations = 0\n    self._val_neg_visualizations = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.setup","title":"setup","text":"<pre><code>setup(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> <p>Setups the callback.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>lightning.Trainer</code>)           \u2013            <p>The lightning trainer.</p> </li> <li> <code>pl_module</code>               (<code>lightning.LightningModule</code>)           \u2013            <p>The lightning module.</p> </li> <li> <code>stage</code>               (<code>typing.Literal['fit', 'validate', 'test', 'predict']</code>)           \u2013            <p>The current stage. One of: \"fit\", \"validate\", \"test\", \"predict\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):\n    \"\"\"Setups the callback.\n\n    Args:\n        trainer (Trainer): The lightning trainer.\n        pl_module (LightningModule): The lightning module.\n        stage (Literal[\"fit\", \"validate\", \"test\", \"predict\"]): The current stage.\n            One of: \"fit\", \"validate\", \"test\", \"predict\".\n\n    \"\"\"\n    # We don't want to use memory in the predict stage\n    if stage == \"predict\":\n        return\n\n    # Validation metrics and visualizations for the fit and validate stages\n    if stage == \"fit\" or stage == \"validate\":\n        # Internal state to track how many visualizations have been generated in an epoch\n        self._val_pos_visualizations = 0\n        self._val_neg_visualizations = 0\n\n    # Test metrics and visualizations for the test stage\n    if stage == \"test\":\n        # Internal state to track how many visualizations have been generated in an epoch\n        self._test_pos_visualizations = 0\n        self._test_neg_visualizations = 0\n\n    # Plot the data distribution\n    if hasattr(trainer, \"datamodule\") and hasattr(trainer.datamodule, \"plot\"):\n        logger.debug(\"Creating training data distribution plot...\")\n        fig, _ = trainer.datamodule.plot()\n        dist_fig_on_disk: Path | None = None\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / \"data\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig.savefig(fig_dir / \"data_distribution.png\", dpi=150)\n                dist_fig_on_disk = fig_dir / \"data_distribution.png\"\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                if dist_fig_on_disk is not None:\n                    wandb_img = wandb.Image(str(dist_fig_on_disk))\n                else:\n                    pil_image = _uplt_to_pil(fig, dpi=150)\n                    wandb_img = wandb.Image(pil_image)\n                # We don't commit the log yet, so that the step is increased with the next lightning log\n                # Which happens at the end of the validation epoch\n                wandb_run.log({\"data/data_distribution\": wandb_img}, commit=False)\n        fig.clear()\n        plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks._uplt_to_pil","title":"_uplt_to_pil","text":"<pre><code>_uplt_to_pil(\n    fig: ultraplot.Figure, dpi: int = 100\n) -&gt; PIL.Image.Image\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def _uplt_to_pil(fig: uplt.Figure, dpi: int = 100) -&gt; PIL.Image.Image:\n    buf = io.BytesIO()\n    fig.savefig(buf, format=\"png\", dpi=dpi)\n    image = PIL.Image.open(buf)\n    return image\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.plot_augmentations","title":"plot_augmentations","text":"<pre><code>plot_augmentations(\n    x: torch.Tensor,\n    augmentations: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ],\n    band_names: list[str],\n) -&gt; tuple[\n    ultraplot.Figure, ultraplot.gridspec.SubplotGrid\n]\n</code></pre> <p>Plot augmentations applied to a sample image.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>Input tensor [N, C, H, W] (float).</p> </li> <li> <code>augmentations</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation]</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>List of band names corresponding to the channels in x.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ultraplot.Figure</code>           \u2013            <p>matplotlib.figure.Figure: The figure object containing the plots.</p> </li> <li> <code>ultraplot.gridspec.SubplotGrid</code>           \u2013            <p>ultraplot.gridspec.SubplotGrid: The axes of the plot.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/viz.py</code> <pre><code>def plot_augmentations(\n    x: torch.Tensor, augmentations: list[Augmentation], band_names: list[str]\n) -&gt; tuple[uplt.Figure, uplt.gridspec.SubplotGrid]:\n    \"\"\"Plot augmentations applied to a sample image.\n\n    Args:\n        x (torch.Tensor): Input tensor [N, C, H, W] (float).\n        augmentations (list[Augmentation]): List of augmentations to apply.\n        band_names (list[str]): List of band names corresponding to the channels in x.\n\n    Returns:\n        matplotlib.figure.Figure: The figure object containing the plots.\n        ultraplot.gridspec.SubplotGrid: The axes of the plot.\n\n    \"\"\"\n    compose = get_augmentation(augmentations)\n    augmentations: dict[str, A.BasicTransform] = {aug: get_augmentation([aug], True) for aug in augmentations}\n\n    rgb_idx = [band_names.index(band) for band in [\"red\", \"green\", \"blue\"]]\n\n    nrows = 1 + len(augmentations) + 4\n    ncols = x.shape[0]\n    fig, axs = uplt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 5, nrows * 5))\n    for i in range(ncols):\n        img = x[i, rgb_idx].permute(1, 2, 0).cpu().numpy()\n        axs[0, i].imshow(img, vmin=0, vmax=0.1)\n        axs[0, i].set_title(\"Original Image\")\n        for j, (aug_name, aug_fn) in enumerate(augmentations.items()):\n            augmented = aug_fn(image=img)\n            aug_img = augmented[\"image\"]\n            axs[j + 1, i].imshow(aug_img, vmin=0, vmax=0.1)\n            axs[j + 1, i].set_title(f\"Augmented: {aug_name}\")\n\n        # Apply full compose\n        for j in range(4):\n            augmented = compose(image=img)\n            aug_img = augmented[\"image\"]\n            axs[j + 1 + len(augmentations), i].imshow(aug_img, vmin=0, vmax=0.1)\n            axs[j + 1 + len(augmentations), i].set_title(f\"Compose Augmentation {j + 1}\")\n    return fig, axs\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.plot_sample","title":"plot_sample","text":"<pre><code>plot_sample(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    y_pred: torch.Tensor,\n    band_names: list[str],\n) -&gt; tuple[\n    matplotlib.pyplot.Figure,\n    dict[str, matplotlib.pyplot.Axes],\n]\n</code></pre> <p>Plot a single sample with the input, the ground truth and the prediction.</p> <p>This function does a few expections on the input: - The input is expected to be normalized to 0-1. - The prediction is expected to be converted from logits to prediction. - The target is expected to be a int or long tensor with values of:     0 (negative class)     1 (positive class) and     2 (invalid pixels).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor [C, H, W] (float).</p> </li> <li> <code>y</code>               (<code>torch.Tensor</code>)           \u2013            <p>The ground truth tensor [H, W] (int).</p> </li> <li> <code>y_pred</code>               (<code>torch.Tensor</code>)           \u2013            <p>The prediction tensor [H, W] (float).</p> </li> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>The combinations of the input bands.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[matplotlib.pyplot.Figure, dict[str, matplotlib.pyplot.Axes]]</code>           \u2013            <p>tuple[Figure, dict[str, Axes]]: The figure and the axes of the plot.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/viz.py</code> <pre><code>def plot_sample(\n    x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor, band_names: list[str]\n) -&gt; tuple[plt.Figure, dict[str, plt.Axes]]:\n    \"\"\"Plot a single sample with the input, the ground truth and the prediction.\n\n    This function does a few expections on the input:\n    - The input is expected to be normalized to 0-1.\n    - The prediction is expected to be converted from logits to prediction.\n    - The target is expected to be a int or long tensor with values of:\n        0 (negative class)\n        1 (positive class) and\n        2 (invalid pixels).\n\n    Args:\n        x (torch.Tensor): The input tensor [C, H, W] (float).\n        y (torch.Tensor): The ground truth tensor [H, W] (int).\n        y_pred (torch.Tensor): The prediction tensor [H, W] (float).\n        band_names (list[str]): The combinations of the input bands.\n\n    Returns:\n        tuple[Figure, dict[str, Axes]]: The figure and the axes of the plot.\n\n    \"\"\"\n    x = x.cpu()\n    y = y.cpu()\n    y_pred = y_pred.detach().cpu()\n\n    # Make y class 2 invalids (replace 2 with nan)\n    x = x.where(y != 2, torch.nan)\n    y_pred = y_pred.where(y != 2, torch.nan)\n    y = y.where(y != 2, torch.nan)\n\n    # pred == 0, y == 0 -&gt; 0 (true negative)\n    # pred == 1, y == 0 -&gt; 1 (false positive)\n    # pred == 0, y == 1 -&gt; 2 (false negative)\n    # pred == 1, y == 1 -&gt; 3 (true positive)\n    classification_labels = (y_pred &gt; 0.5).int() + y * 2\n    classification_labels = classification_labels.where(classification_labels != 0, torch.nan)\n\n    # Calculate f1 and iou\n    true_positive = (classification_labels == 3).sum()\n    false_positive = (classification_labels == 1).sum()\n    false_negative = (classification_labels == 2).sum()\n    true_negative = (classification_labels == 0).sum()\n    acc = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n    f1 = 2 * true_positive / (2 * true_positive + false_positive + false_negative)\n    iou = true_positive / (true_positive + false_positive + false_negative)\n\n    cmap = mcolors.ListedColormap([\"#cd43b2\", \"#3e0f2f\", \"#6cd875\"])\n    fig, axs = plt.subplot_mosaic(\n        # [[\"rgb\", \"rgb\", \"ndvi\", \"tcvis\", \"stats\"], [\"rgb\", \"rgb\", \"pred\", \"slope\", \"elev\"]],\n        [[\"rgb\", \"rgb\", \"pred\", \"tcvis\"], [\"rgb\", \"rgb\", \"ndvi\", \"slope\"], [\"none\", \"stats\", \"stats\", \"stats\"]],\n        # layout=\"constrained\",\n        figsize=(11, 8),\n    )\n\n    # Disable none plot\n    axs[\"none\"].axis(\"off\")\n\n    # RGB Plot\n    ax_rgb = axs[\"rgb\"]\n    # disable axis\n    ax_rgb.axis(\"off\")\n    is_rgb = \"red\" in band_names and \"green\" in band_names and \"blue\" in band_names\n    if is_rgb:\n        red_band = band_names.index(\"red\")\n        green_band = band_names.index(\"green\")\n        blue_band = band_names.index(\"blue\")\n        rgb = x[[red_band, green_band, blue_band]].transpose(0, 2).transpose(0, 1)\n        ax_rgb.imshow(rgb ** (1 / 1.4))\n        ax_rgb.set_title(f\"Acc: {acc:.1%} F1: {f1:.1%} IoU: {iou:.1%}\")\n    else:\n        # Plot empty with message that RGB is not provided\n        ax_rgb.set_title(\"No RGB values are provided!\")\n    ax_rgb.imshow(classification_labels, alpha=0.6, cmap=cmap, vmin=1, vmax=3)\n    # Add a legend\n    patches = [\n        mpatches.Patch(color=\"#6cd875\", label=\"True Positive\"),\n        mpatches.Patch(color=\"#3e0f2f\", label=\"False Negative\"),\n        mpatches.Patch(color=\"#cd43b2\", label=\"False Positive\"),\n    ]\n    ax_rgb.legend(handles=patches, loc=\"upper left\")\n\n    # NDVI Plot\n    ax_ndvi = axs[\"ndvi\"]\n    ax_ndvi.axis(\"off\")\n    is_ndvi = \"ndvi\" in band_names\n    if is_ndvi:\n        ndvi_band = band_names.index(\"ndvi\")\n        ndvi = x[ndvi_band]\n        ax_ndvi.imshow(ndvi, vmin=0, vmax=1, cmap=\"RdYlGn\")\n        ax_ndvi.set_title(\"NDVI\")\n    else:\n        # Plot empty with message that NDVI is not provided\n        ax_ndvi.set_title(\"No NDVI values are provided!\")\n\n    # TCVIS Plot\n    ax_tcv = axs[\"tcvis\"]\n    ax_tcv.axis(\"off\")\n    is_tcvis = \"tc_brightness\" in band_names and \"tc_greenness\" in band_names and \"tc_wetness\" in band_names\n    if is_tcvis:\n        tcb_band = band_names.index(\"tc_brightness\")\n        tcg_band = band_names.index(\"tc_greenness\")\n        tcw_band = band_names.index(\"tc_wetness\")\n        tcvis = x[[tcb_band, tcg_band, tcw_band]].transpose(0, 2).transpose(0, 1)\n        ax_tcv.imshow(tcvis)\n        ax_tcv.set_title(\"TCVIS\")\n    else:\n        ax_tcv.set_title(\"No TCVIS values are provided!\")\n\n    # Statistics Plot\n    ax_stat = axs[\"stats\"]\n    if (y == 1).sum() &gt; 0:\n        n_bands = x.shape[0]\n        n_pixel = x.shape[1] * x.shape[2]\n        x_flat = x.flatten().cpu()\n        y_flat = y.flatten().repeat(n_bands).cpu()\n        bands = list(itertools.chain.from_iterable([band_names[i]] * n_pixel for i in range(n_bands)))\n        plot_data = pd.DataFrame({\"x\": x_flat, \"y\": y_flat, \"band\": bands})\n        if len(plot_data) &gt; 50000:\n            plot_data = plot_data.sample(50000)\n        plot_data = plot_data.sort_values(\"band\")\n        sns.violinplot(\n            x=\"x\",\n            y=\"band\",\n            hue=\"y\",\n            data=plot_data,\n            split=True,\n            inner=\"quart\",\n            fill=False,\n            palette={1: \"g\", 0: \".35\"},\n            density_norm=\"width\",\n            ax=ax_stat,\n        )\n        ax_stat.set_title(\"Band Statistics\")\n    else:\n        ax_stat.set_title(\"No positive labels in this sample!\")\n        ax_stat.axis(\"off\")\n\n    # Prediction Plot\n    ax_mask = axs[\"pred\"]\n    ax_mask.imshow(y_pred, vmin=0, vmax=1)\n    ax_mask.axis(\"off\")\n    ax_mask.set_title(\"Model Output\")\n\n    # Slope Plot\n    ax_slope = axs[\"slope\"]\n    ax_slope.axis(\"off\")\n    is_slope = \"slope\" in band_names\n    if is_slope:\n        slope_band = band_names.index(\"slope\")\n        slope = x[slope_band]\n        ax_slope.imshow(slope, cmap=\"cividis\")\n        # Add TPI as contour lines\n        is_rel_elev = \"relative_elevation\" in band_names\n        if is_rel_elev:\n            rel_elev_band = band_names.index(\"relative_elevation\")\n            rel_elev = x[rel_elev_band]\n            cs = ax_slope.contour(rel_elev, [0], colors=\"red\", linewidths=0.3, alpha=0.6)\n            ax_slope.clabel(cs, inline=True, fontsize=5, fmt=\"%.1f\")\n\n        ax_slope.set_title(\"Slope\")\n    else:\n        # Plot empty with message that slope is not provided\n        ax_slope.set_title(\"No Slope values are provided!\")\n\n    # Relative Elevation Plot\n    # rel_elev_band = band_names.index(\"relative_elevation\")\n    # rel_elev = x[rel_elev_band]\n    # ax_rel_elev = axs[\"elev\"]\n    # ax_rel_elev.imshow(rel_elev, cmap=\"cividis\")\n    # ax_rel_elev.axis(\"off\")\n    # ax_rel_elev.set_title(\"Relative Elevation\")\n\n    return fig, axs\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/","title":"cv","text":""},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv","title":"darts_segmentation.training.cv","text":"<p>Cross-validation implementation for binary segmentation.</p>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.available_devices","title":"available_devices  <code>module-attribute</code>","text":"<pre><code>available_devices = multiprocessing.Queue()\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig","title":"CrossValidationConfig  <code>dataclass</code>","text":"<pre><code>CrossValidationConfig(\n    n_folds: int | None = None,\n    n_randoms: int = 3,\n    scoring_metric: list[str] = (\n        lambda: [\"val/JaccardIndex\", \"val/Recall\"]\n    )(),\n    multi_score_strategy: typing.Literal[\n        \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n    ] = \"harmonic\",\n)\n</code></pre> <p>Configuration for cross-validation.</p> <p>This is used to configure the cross-validation process. It is used by the <code>cross_validation_smp</code> function.</p> <p>Attributes:</p> <ul> <li> <code>n_folds</code>               (<code>int | None</code>)           \u2013            <p>Number of folds to perform in cross-validation. If None, all folds (total_folds) will be used. Defaults to None.</p> </li> <li> <code>n_randoms</code>               (<code>int</code>)           \u2013            <p>Number of random seeds to perform in cross-validation. First three seeds are always 42, 21, 69, further seeds are deterministic generated. Defaults to 3.</p> </li> <li> <code>scoring_metric</code>               (<code>list[str]</code>)           \u2013            <p>Metric(s) to use for scoring. Defaults to [\"val/JaccardIndex\", \"val/Recall\"].</p> </li> <li> <code>multi_score_strategy</code>               (<code>typing.Literal['harmonic', 'arithmetic', 'geometric', 'min']</code>)           \u2013            <p>Strategy for combining multiple metrics. Defaults to \"harmonic\".</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.multi_score_strategy","title":"multi_score_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>multi_score_strategy: typing.Literal[\n    \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n] = \"harmonic\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.n_folds","title":"n_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_folds: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.n_randoms","title":"n_randoms  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_randoms: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.rng_seeds","title":"rng_seeds  <code>property</code>","text":"<pre><code>rng_seeds: list[int]\n</code></pre> <p>Generate a list of seeds for cross-validation.</p> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>list[int]: A list of seeds for cross-validation.</p> </li> <li> <code>list[int]</code>           \u2013            <p>The first three seeds are always 42, 21, 69, further seeds are deterministically generated.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.scoring_metric","title":"scoring_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scoring_metric: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"val/JaccardIndex\",\n        \"val/Recall\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig","title":"DataConfig  <code>dataclass</code>","text":"<pre><code>DataConfig(\n    train_data_dir: pathlib.Path | None = None,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n        \"none\",\n    ] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n    in_memory: bool = False,\n)\n</code></pre> <p>Data related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>)           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>)           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified', 'none']</code>)           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>)           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>)           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> <li> <code>in_memory</code>               (<code>bool</code>)           \u2013            <p>If True, the dataset will be loaded into memory.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.data_split_by","title":"data_split_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_by: list[str | float] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.data_split_method","title":"data_split_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_method: (\n    typing.Literal[\"random\", \"region\", \"sample\"] | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.fold_method","title":"fold_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold_method: typing.Literal[\n    \"kfold\",\n    \"shuffle\",\n    \"stratified\",\n    \"region\",\n    \"region-stratified\",\n    \"none\",\n] = \"kfold\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.in_memory","title":"in_memory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>in_memory: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.subsample","title":"subsample  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subsample: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.total_folds","title":"total_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_folds: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.train_data_dir","title":"train_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig","title":"DeviceConfig  <code>dataclass</code>","text":"<pre><code>DeviceConfig(\n    accelerator: typing.Literal[\n        \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n    ] = \"auto\",\n    strategy: typing.Literal[\n        \"auto\",\n        \"ddp\",\n        \"ddp_fork\",\n        \"ddp_notebook\",\n        \"fsdp\",\n        \"cv-parallel\",\n        \"tune-parallel\",\n    ] = \"auto\",\n    devices: list[int | str] = (lambda: [\"auto\"])(),\n    num_nodes: int = 1,\n)\n</code></pre> <p>Device and Distributed Strategy related parameters.</p> <p>Attributes:</p> <ul> <li> <code>accelerator</code>               (<code>typing.Literal['auto', 'cpu', 'gpu', 'mps', 'tpu']</code>)           \u2013            <p>Accelerator to use. Defaults to \"auto\".</p> </li> <li> <code>strategy</code>               (<code>typing.Literal['auto', 'ddp', 'ddp_fork', 'ddp_notebook', 'fsdp', 'cv-parallel', 'tune-parallel', 'cv-parallel', 'tune-parallel']</code>)           \u2013            <p>Distributed strategy to use. Defaults to \"auto\".</p> </li> <li> <code>devices</code>               (<code>list[int | str]</code>)           \u2013            <p>List of devices to use. Defaults to [\"auto\"].</p> </li> <li> <code>num_nodes</code>               (<code>int</code>)           \u2013            <p>Number of nodes to use for distributed training. Defaults to 1.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.accelerator","title":"accelerator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>accelerator: typing.Literal[\n    \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int | str] = dataclasses.field(\n    default_factory=lambda: [\"auto\"]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.lightning_strategy","title":"lightning_strategy  <code>property</code>","text":"<pre><code>lightning_strategy: str\n</code></pre> <p>Get the Lightning strategy for the current configuration.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The Lightning strategy to use.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.num_nodes","title":"num_nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_nodes: int = 1\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.strategy","title":"strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strategy: typing.Literal[\n    \"auto\",\n    \"ddp\",\n    \"ddp_fork\",\n    \"ddp_notebook\",\n    \"fsdp\",\n    \"cv-parallel\",\n    \"tune-parallel\",\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.in_parallel","title":"in_parallel","text":"<pre><code>in_parallel(\n    device: int | str | None = None,\n) -&gt; darts_segmentation.training.train.DeviceConfig\n</code></pre> <p>Turn the current configuration into a suitable configuration for parallel training.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to use for parallel training. If None, assumes non-multiprocessing parallel training and propagate all devices. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceConfig</code> (              <code>darts_segmentation.training.train.DeviceConfig</code> )          \u2013            <p>A new DeviceConfig instance that is suitable for parallel training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def in_parallel(self, device: int | str | None = None) -&gt; \"DeviceConfig\":\n    \"\"\"Turn the current configuration into a suitable configuration for parallel training.\n\n    Args:\n        device (int | str | None, optional): The device to use for parallel training.\n            If None, assumes non-multiprocessing parallel training and propagate all devices.\n            Defaults to None.\n\n    Returns:\n        DeviceConfig: A new DeviceConfig instance that is suitable for parallel training.\n\n    \"\"\"\n    # In case of parallel training via multiprocessing, only few strategies are allowed.\n    if self.strategy in [\"ddp\", \"ddp_fork\", \"ddp_notebook\", \"fsdp\"]:\n        logger.warning(\"Using 'ddp_fork' instead of 'ddp' for multiprocessing.\")\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=\"ddp_fork\",  # Fork is the only supported strategy for multiprocessing\n            devices=self.devices,\n            num_nodes=self.num_nodes,\n        )\n    elif device is not None:\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=self.strategy,\n            # If a device is specified, we assume that we want to run on a single device\n            devices=[device],\n            num_nodes=1,\n        )\n    else:\n        return self\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters","title":"Hyperparameters  <code>dataclass</code>","text":"<pre><code>Hyperparameters(\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Hyperparameters for Cyclopts CLI.</p> <p>Attributes:</p> <ul> <li> <code>model_arch</code>               (<code>str</code>)           \u2013            <p>Architecture of the model to use.</p> </li> <li> <code>model_encoder</code>               (<code>str</code>)           \u2013            <p>Encoder type for the model.</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>)           \u2013            <p>Weights for the encoder, if any.</p> </li> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>)           \u2013            <p>Learning rate for training.</p> </li> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Decay factor for learning rate.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float | None</code>)           \u2013            <p>Alpha parameter for focal loss, if using.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>)           \u2013            <p>Gamma parameter for focal loss.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>)           \u2013            <p>List of bands to use. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.augment","title":"augment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>augment: (\n    list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.bands","title":"bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bands: list[str] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.focal_loss_alpha","title":"focal_loss_alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_alpha: float | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.focal_loss_gamma","title":"focal_loss_gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_gamma: float = 2.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.9\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.001\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.model_arch","title":"model_arch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_arch: str = 'Unet'\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.model_encoder","title":"model_encoder  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder: str = 'dpn107'\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.model_encoder_weights","title":"model_encoder_weights  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder_weights: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig","title":"LoggingConfig  <code>dataclass</code>","text":"<pre><code>LoggingConfig(\n    artifact_dir: pathlib.Path | None = None,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Logging related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>artifact_dir</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Top-level path to the training output directory. Will contain checkpoints and metrics. If None, will use the default artifact directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>)           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>)           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>)           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.artifact_dir","title":"artifact_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>artifact_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.check_val_every_n_epoch","title":"check_val_every_n_epoch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>check_val_every_n_epoch: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.log_every_n_steps","title":"log_every_n_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_every_n_steps: int = 10\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.wandb_entity","title":"wandb_entity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_entity: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.wandb_project","title":"wandb_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_project: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.artifact_dir_at_cv","title":"artifact_dir_at_cv","text":"<pre><code>artifact_dir_at_cv(tune_name: str | None) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory for cross-validation runs.</p> <p>Similar to <code>parse_artifact_dir_for_run</code>, but meant to be used by the cross-validation script.</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning, if applicable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path for cross-validation runs.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_cv(self, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory for cross-validation runs.\n\n    Similar to `parse_artifact_dir_for_run`, but meant to be used by the cross-validation script.\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        tune_name (str | None): Name of the tuning, if applicable.\n\n    Returns:\n        Path: The nested artifact directory path for cross-validation runs.\n\n    \"\"\"\n    artifact_dir = self.artifact_dir or paths.artifacts\n    artifact_dir = artifact_dir / tune_name if tune_name else artifact_dir / \"_cross_validations\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.artifact_dir_at_run","title":"artifact_dir_at_run","text":"<pre><code>artifact_dir_at_run(\n    cv_name: str | None, tune_name: str | None\n) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory to avoid cluttering the root directory.</p> <p>For cv it is expected that the cv function already nests the artifact directory Meaning for cv the artifact_dir of this function should be either {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If tune_name is specified, but cv_name is not, which is invalid.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_run(self, cv_name: str | None, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory to avoid cluttering the root directory.\n\n    For cv it is expected that the cv function already nests the artifact directory\n    Meaning for cv the artifact_dir of this function should be either\n    {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        cv_name (str | None): Name of the cross-validation.\n        tune_name (str | None): Name of the tuning.\n\n    Raises:\n        ValueError: If tune_name is specified, but cv_name is not, which is invalid.\n\n    Returns:\n        Path: The nested artifact directory path.\n\n    \"\"\"\n    artifact_dir = self.artifact_dir or paths.artifacts\n    # Run only\n    if cv_name is None and tune_name is None:\n        artifact_dir = artifact_dir / \"_runs\"\n    # Cross-validation only\n    elif cv_name is not None and tune_name is None:\n        artifact_dir = artifact_dir / \"_cross_validations\" / cv_name\n    # Cross-validation and tuning\n    elif cv_name is not None and tune_name is not None:\n        artifact_dir = artifact_dir / tune_name / cv_name\n    # Tuning only (invalid)\n    else:\n        raise ValueError(\n            \"Cannot parse artifact directory for cross-validation and tuning. \"\n            \"Please specify either cv_name or tune_name, but not both.\"\n        )\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig","title":"TrainRunConfig  <code>dataclass</code>","text":"<pre><code>TrainRunConfig(\n    name: str | None = None,\n    cv_name: str | None = None,\n    tune_name: str | None = None,\n    fold: int = 0,\n    random_seed: int = 42,\n)\n</code></pre> <p>Run related parameters for training.</p> <p>Defines the script inputs for the training script. Must be build by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str | None</code>)           \u2013            <p>Name of the run. If None is generated automatically. Defaults to None.</p> </li> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation. Should only be specified by a cross-validation script. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>fold</code>               (<code>int</code>)           \u2013            <p>Index of the current fold. Defaults to 0.</p> </li> <li> <code>random_seed</code>               (<code>int</code>)           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.cv_name","title":"cv_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cv_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.fold","title":"fold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: int = 42\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.tune_name","title":"tune_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<pre><code>TrainingConfig(\n    weights_from_checkpoint: pathlib.Path | None = None,\n    continue_from_checkpoint: pathlib.Path | None = None,\n    max_epochs: int = 100,\n    early_stopping_patience: int = 5,\n    num_workers: int = 0,\n    save_top_k: int = 1,\n    advanced_profiler: bool = False,\n)\n</code></pre> <p>Training related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>weights_from_checkpoint</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Path to the lightning checkpoint to load the model from. If None, the model will be trained from scratch. Defaults to None.</p> </li> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Path to a checkpoint to continue training from. Differs from <code>weights_from_checkpoint</code> in that it will continue training from this training state, hence all optimizer states, learning rate schedulers, etc. will be continued. Defaults to None.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>)           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>)           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>save_top_k</code>               (<code>int</code>)           \u2013            <p>Number of best checkpoints to save. Set to 0 to disable saving checkpoints. Set to -1 to save all checkpoints. Defaults to 1.</p> </li> <li> <code>advanced_profiler</code>               (<code>bool</code>)           \u2013            <p>Whether to use the advanced profiler. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.advanced_profiler","title":"advanced_profiler  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>advanced_profiler: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.continue_from_checkpoint","title":"continue_from_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>continue_from_checkpoint: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.early_stopping_patience","title":"early_stopping_patience  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>early_stopping_patience: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.max_epochs","title":"max_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_epochs: int = 100\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.num_workers","title":"num_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_workers: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.save_top_k","title":"save_top_k  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>save_top_k: int = 1\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.weights_from_checkpoint","title":"weights_from_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>weights_from_checkpoint: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs","title":"_ProcessInputs  <code>dataclass</code>","text":"<pre><code>_ProcessInputs(\n    current: int,\n    total: int,\n    seed: int,\n    fold: int,\n    cv: darts_segmentation.training.cv.CrossValidationConfig,\n    run: darts_segmentation.training.train.TrainRunConfig,\n    training_config: darts_segmentation.training.train.TrainingConfig,\n    logging_config: darts_segmentation.training.train.LoggingConfig,\n    data_config: darts_segmentation.training.train.DataConfig,\n    device_config: darts_segmentation.training.train.DeviceConfig,\n    hparams: darts_segmentation.training.hparams.Hyperparameters,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.current","title":"current  <code>instance-attribute</code>","text":"<pre><code>current: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.cv","title":"cv  <code>instance-attribute</code>","text":"<pre><code>cv: darts_segmentation.training.cv.CrossValidationConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.data_config","title":"data_config  <code>instance-attribute</code>","text":"<pre><code>data_config: darts_segmentation.training.train.DataConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.device_config","title":"device_config  <code>instance-attribute</code>","text":"<pre><code>device_config: (\n    darts_segmentation.training.train.DeviceConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.fold","title":"fold  <code>instance-attribute</code>","text":"<pre><code>fold: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.hparams","title":"hparams  <code>instance-attribute</code>","text":"<pre><code>hparams: darts_segmentation.training.hparams.Hyperparameters\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.logging_config","title":"logging_config  <code>instance-attribute</code>","text":"<pre><code>logging_config: (\n    darts_segmentation.training.train.LoggingConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.run","title":"run  <code>instance-attribute</code>","text":"<pre><code>run: darts_segmentation.training.train.TrainRunConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.seed","title":"seed  <code>instance-attribute</code>","text":"<pre><code>seed: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.total","title":"total  <code>instance-attribute</code>","text":"<pre><code>total: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.training_config","title":"training_config  <code>instance-attribute</code>","text":"<pre><code>training_config: (\n    darts_segmentation.training.train.TrainingConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessOutputs","title":"_ProcessOutputs  <code>dataclass</code>","text":"<pre><code>_ProcessOutputs(run_info: dict)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessOutputs.run_info","title":"run_info  <code>instance-attribute</code>","text":"<pre><code>run_info: dict\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._run_training","title":"_run_training","text":"<pre><code>_run_training(\n    inp: darts_segmentation.training.cv._ProcessInputs,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/cv.py</code> <pre><code>def _run_training(inp: _ProcessInputs):\n    # Wrapper function for handling parallel multiprocessing training runs.\n    import torch\n\n    from darts_segmentation.training.scoring import check_score_is_unstable\n    from darts_segmentation.training.train import train_smp\n\n    # Setup device configuration: If strategy is \"cv-parallel\" expect a mp scenario:\n    # Wait for a device to become available.\n    # Otherwise, expect a serial scenario, where the devices and strategy are set by the user.\n    is_parallel = inp.device_config.strategy == \"cv-parallel\"\n    if is_parallel:\n        device = available_devices.get()\n        device_config = inp.device_config.in_parallel(device)\n        logger.debug(f\"Starting run {inp.run.name} ({inp.current + 1}/{inp.total}) on device {device}.\")\n    else:\n        device = None\n        device_config = inp.device_config.in_parallel()\n        logger.debug(f\"Starting run {inp.run.name} ({inp.current + 1}/{inp.total}).\")\n\n    try:\n        tick_rstart = time.time()\n        trainer = train_smp(\n            run=inp.run,\n            training_config=inp.training_config,\n            data_config=inp.data_config,\n            device_config=device_config,\n            hparams=inp.hparams,\n            logging_config=inp.logging_config,\n        )\n        tick_rend = time.time()\n\n        run_info = {\n            \"run_name\": inp.run.name,\n            \"run_id\": trainer.lightning_module.hparams[\"run_id\"],\n            \"seed\": inp.seed,\n            \"fold\": inp.fold,\n            \"duration\": tick_rend - tick_rstart,\n        }\n        for metric, value in trainer.logged_metrics.items():\n            run_info[metric] = value.item() if isinstance(value, torch.Tensor) else value\n        if trainer.checkpoint_callback:\n            run_info[\"checkpoint\"] = trainer.checkpoint_callback.best_model_path\n        run_info[\"is_unstable\"] = check_score_is_unstable(run_info, inp.cv.scoring_metric)\n\n        logger.debug(f\"{run_info=}\")\n        output = _ProcessOutputs(run_info=run_info)\n    finally:\n        # If we are in parallel mode, we need to return the device to the queue.\n        if is_parallel:\n            logger.debug(f\"Free device {device} for cv {inp.run.name}\")\n            available_devices.put(device)\n    return output\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.cross_validation_smp","title":"cross_validation_smp","text":"<pre><code>cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    cv: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n)\n</code></pre> <p>Perform cross-validation for a model with given hyperparameters.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This cross-validation function is designed to evaluate the performance of a single model configuration. It can be used by a tuning script to tune hyperparameters. It calls the training function, hence most functionality is the same as the training function. In general, it does perform this:</p> <pre><code>for seed in seeds:\n    for fold in folds:\n        train_model(seed=seed, fold=fold, ...)\n</code></pre> <p>and calculates a score from the results.</p> <p>To specify on which metric(s) the score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code> for tunes (meaning if <code>tune_name</code> is not None) else <code>{artifact_dir}/_cross_validation</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation. If None, a name is generated automatically. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>cv</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for the training.</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for the data.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for the devices to use.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable, and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no runs were performed, meaning the configuration is invalid or no data was found.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/cv.py</code> <pre><code>def cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    cv: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n    logging_config: LoggingConfig = LoggingConfig(),\n):\n    \"\"\"Perform cross-validation for a model with given hyperparameters.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This cross-validation function is designed to evaluate the performance of a single model configuration.\n    It can be used by a tuning script to tune hyperparameters.\n    It calls the training function, hence most functionality is the same as the training function.\n    In general, it does perform this:\n\n    ```py\n    for seed in seeds:\n        for fold in folds:\n            train_model(seed=seed, fold=fold, ...)\n    ```\n\n    and calculates a score from the results.\n\n    To specify on which metric(s) the score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}` for tunes (meaning if `tune_name` is not None)\n    else `{artifact_dir}/_cross_validation`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the cross-validation. If None, a name is generated automatically.\n            Defaults to None.\n        tune_name (str | None, optional): Name of the tuning. Should only be specified by a tuning script.\n            Defaults to None.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        cv (CrossValidationConfig): Configuration for cross-validation.\n        training_config (TrainingConfig): Configuration for the training.\n        data_config (DataConfig): Configuration for the data.\n        device_config (DeviceConfig): Configuration for the devices to use.\n        hparams (Hyperparameters): Hyperparameters for the training.\n        logging_config (LoggingConfig): Logging configuration.\n\n    Returns:\n        tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable,\n            and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)\n\n    Raises:\n        ValueError: If no runs were performed, meaning the configuration is invalid or no data was found.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.scoring import score_from_runs\n\n    tick_fstart = time.perf_counter()\n\n    paths.set_defaults(default_dirs)\n\n    artifact_dir = logging_config.artifact_dir_at_cv(tune_name)\n    cv_name = name or generate_counted_name(artifact_dir)\n    artifact_dir = artifact_dir / cv_name\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    n_folds = cv.n_folds or data_config.total_folds\n\n    logger.info(\n        f\"Starting cross-validation '{cv_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_randoms*n_folds = {cv.n_randoms}*{n_folds} = {cv.n_randoms * n_folds} experiments.\"\n    )\n\n    seeds = cv.rng_seeds\n    logger.debug(f\"Using seeds: {seeds}\")\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs: list[_ProcessInputs] = []\n    for i, seed in enumerate(seeds):\n        for fold in range(n_folds):\n            current = i * len(seeds) + fold\n            total = n_folds * len(seeds)\n            run = TrainRunConfig(\n                name=f\"{cv_name}-run-f{fold}s{seed}\",\n                cv_name=cv_name,\n                tune_name=tune_name,\n                fold=fold,\n                random_seed=seed,\n            )\n            process_inputs.append(\n                _ProcessInputs(\n                    current=current,\n                    total=total,\n                    seed=seed,\n                    fold=fold,\n                    cv=cv,\n                    run=run,\n                    training_config=training_config,\n                    logging_config=logging_config,\n                    data_config=data_config,\n                    device_config=device_config,\n                    hparams=hparams,\n                )\n            )\n\n    run_infos = []\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"cv-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_training,\n    ):\n        run_infos.append(output.run_info)\n\n    if len(run_infos) == 0:\n        raise ValueError(\n            \"No runs were performed. Please check your configuration and data.\"\n            \" If you are using a tuning script, make sure to specify the correct parameters.\"\n        )\n\n    logger.debug(f\"{run_infos=}\")\n    score = score_from_runs(run_infos, cv.scoring_metric, cv.multi_score_strategy)\n\n    run_infos = pd.DataFrame(run_infos)\n    run_infos[\"score\"] = score\n    is_unstable = run_infos[\"is_unstable\"].any()\n    run_infos[\"score_is_unstable\"] = is_unstable\n    if is_unstable:\n        logger.warning(\"Score is unstable, meaning at least one of the metrics is NaN, Inf, -Inf or 0.\")\n    run_infos.to_parquet(artifact_dir / \"run_infos.parquet\")\n    logger.debug(f\"Saved run infos to {artifact_dir / 'run_infos.parquet'}\")\n\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Finished cross-validation '{cv_name}' in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, is_unstable, run_infos\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/","title":"data","text":""},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data","title":"darts_segmentation.training.data","text":"<p>Training script for DARTS segmentation.</p>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Augmentation","title":"Augmentation  <code>module-attribute</code>","text":"<pre><code>Augmentation = typing.Literal[\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"D4\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\",\n    \"Posterize\",\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule","title":"DartsDataModule","text":"<pre><code>DartsDataModule(\n    data_dir: pathlib.Path,\n    batch_size: int,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n    ]\n    | None = \"kfold\",\n    total_folds: int = 5,\n    fold: int = 0,\n    subsample: int | None = None,\n    bands: list[str] | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    num_workers: int = 0,\n    in_memory: bool = False,\n)\n</code></pre> <p>               Bases: <code>lightning.LightningDataModule</code></p> <p>Initialize the data module.</p> <p>Supports spliting the data into train and test set while also defining cv-folds. Folding only applies to the non-test set and splits this into a train and validation set.</p> Example <ol> <li> <p>Normal train-validate. (Can also be used for testing on the complete dataset) </p><pre><code>dm = DartsDataModule(data_dir, batch_size)\n</code></pre><p></p> </li> <li> <p>Specifying a test split by random (20% of the data will be used for testing) </p><pre><code>dm = DartsDataModule(data_dir, batch_size, data_split_method=\"random\")\n</code></pre><p></p> </li> <li> <p>Specific fold for cross-validation (On the complete dataset, because data_split_method is \"none\"). This will be take the third of a total of7 folds to determine the validation set. </p><pre><code>dm = DartsDataModule(data_dir, batch_size, fold_method=\"region-stratified\", fold=2, total_folds=7)\n</code></pre><p></p> </li> </ol> <p>In general this should be used in combination with a cross-validation loop. </p><pre><code>for fold in range(total_folds):\n    dm = DartsDataModule(\n        data_dir,\n        batch_size,\n        fold_method=\"region-stratified\",\n        fold=fold,\n        total_folds=total_folds)\n    ...\n</code></pre><p></p> <ol> <li>Don't split anything -&gt; only train <pre><code>dm = DartsDataModule(data_dir, batch_size, fold_method=None)\n</code></pre></li> </ol> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified'] | None</code>, default:                   <code>'kfold'</code> )           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>fold</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Index of the current fold. Defaults to 0.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> <li> <code>bands</code>               (<code>Bands | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of bands to use. Expects the data_dir to contain a config.toml with a \"darts.bands\" key, with which the indices of the bands will be mapped to. Defaults to None.</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>Whether to augment the data. Does nothing for testing. Defaults to True.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for data loading. See torch.utils.data.DataLoader. Defaults to 0.</p> </li> <li> <code>in_memory</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to load the data into memory. Defaults to False.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path,\n    batch_size: int,\n    # data_split is for the test split\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str | float] | None = None,\n    # fold is for cross-validation split (train/val)\n    fold_method: Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\"] | None = \"kfold\",\n    total_folds: int = 5,\n    fold: int = 0,\n    subsample: int | None = None,\n    bands: list[str] | None = None,\n    augment: list[Augmentation] | None = None,  # Not used for val or test\n    num_workers: int = 0,\n    in_memory: bool = False,\n):\n    \"\"\"Initialize the data module.\n\n    Supports spliting the data into train and test set while also defining cv-folds.\n    Folding only applies to the non-test set and splits this into a train and validation set.\n\n    Example:\n        1. Normal train-validate. (Can also be used for testing on the complete dataset)\n        ```py\n        dm = DartsDataModule(data_dir, batch_size)\n        ```\n\n        2. Specifying a test split by random (20% of the data will be used for testing)\n        ```py\n        dm = DartsDataModule(data_dir, batch_size, data_split_method=\"random\")\n        ```\n\n        3. Specific fold for cross-validation (On the complete dataset, because data_split_method is \"none\").\n        This will be take the third of a total of7 folds to determine the validation set.\n        ```py\n        dm = DartsDataModule(data_dir, batch_size, fold_method=\"region-stratified\", fold=2, total_folds=7)\n        ```\n\n        In general this should be used in combination with a cross-validation loop.\n        ```py\n        for fold in range(total_folds):\n            dm = DartsDataModule(\n                data_dir,\n                batch_size,\n                fold_method=\"region-stratified\",\n                fold=fold,\n                total_folds=total_folds)\n            ...\n        ```\n\n        4. Don't split anything -&gt; only train\n        ```py\n        dm = DartsDataModule(data_dir, batch_size, fold_method=None)\n        ```\n\n    Args:\n        data_dir (Path): The path to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        batch_size (int): Batch size for training and validation.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the test size can be specified\n            by providing a list with a single a float between 0 and 1 to data_split_by\n            This will be the fraction of the data to be used for testing.\n            E.g. [0.2] will use 20% of the data for testing.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str | float] | None, optional): Select by which regions/samples to split or\n            the size of test set. Defaults to None.\n        fold_method (Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\"] | None, optional):\n            Method for cross-validation split. Defaults to \"kfold\".\n        total_folds (int, optional): Total number of folds in cross-validation. Defaults to 5.\n        fold (int, optional): Index of the current fold. Defaults to 0.\n        subsample (int | None, optional): If set, will subsample the dataset to this number of samples.\n            This is useful for debugging and testing. Defaults to None.\n        bands (Bands | list[str] | None, optional): List of bands to use.\n            Expects the data_dir to contain a config.toml with a \"darts.bands\" key,\n            with which the indices of the bands will be mapped to.\n            Defaults to None.\n        augment (bool, optional): Whether to augment the data. Does nothing for testing. Defaults to True.\n        num_workers (int, optional): Number of workers for data loading. See torch.utils.data.DataLoader.\n            Defaults to 0.\n        in_memory (bool, optional): Whether to load the data into memory. Defaults to False.\n\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters(ignore=[\"num_workers\", \"in_memory\"])\n    self.data_dir = data_dir\n    self.batch_size = batch_size\n\n    self.fold = fold\n    self.data_split_method = data_split_method\n    self.data_split_by = data_split_by\n    self.fold_method = fold_method\n    self.total_folds = total_folds\n\n    self.subsample = subsample\n    self.augment = augment\n    self.num_workers = num_workers\n    self.in_memory = in_memory\n\n    data_dir = Path(data_dir)\n\n    metadata_file = data_dir / \"metadata.parquet\"\n    assert metadata_file.exists(), f\"Metadata file {metadata_file} not found!\"\n\n    config_file = data_dir / \"config.toml\"\n    assert config_file.exists(), f\"Config file {config_file} not found!\"\n    data_bands = toml.load(config_file)[\"darts\"][\"bands\"]\n    self.bands = [data_bands.index(b) for b in bands] if bands else None\n\n    zdir = data_dir / \"data.zarr\"\n    assert zdir.exists(), f\"Data directory {zdir} not found!\"\n    zroot = zarr.group(store=LocalStore(data_dir / \"data.zarr\"))\n    self.nsamples = zroot[\"x\"].shape[0]\n    logger.debug(f\"Data directory {zdir} found with {self.nsamples} samples.\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.augment","title":"augment  <code>instance-attribute</code>","text":"<pre><code>augment = darts_segmentation.training.data.DartsDataModule(\n    augment\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands = (\n    [\n        (data_bands.index(b))\n        for b in darts_segmentation.training.data.DartsDataModule(\n            bands\n        )\n    ]\n    if darts_segmentation.training.data.DartsDataModule(\n        bands\n    )\n    else None\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = (\n    darts_segmentation.training.data.DartsDataModule(\n        batch_size\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.data_dir","title":"data_dir  <code>instance-attribute</code>","text":"<pre><code>data_dir = darts_segmentation.training.data.DartsDataModule(\n    data_dir\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.data_split_by","title":"data_split_by  <code>instance-attribute</code>","text":"<pre><code>data_split_by = (\n    darts_segmentation.training.data.DartsDataModule(\n        data_split_by\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.data_split_method","title":"data_split_method  <code>instance-attribute</code>","text":"<pre><code>data_split_method = (\n    darts_segmentation.training.data.DartsDataModule(\n        data_split_method\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.fold","title":"fold  <code>instance-attribute</code>","text":"<pre><code>fold = darts_segmentation.training.data.DartsDataModule(\n    fold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.fold_method","title":"fold_method  <code>instance-attribute</code>","text":"<pre><code>fold_method = (\n    darts_segmentation.training.data.DartsDataModule(\n        fold_method\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.in_memory","title":"in_memory  <code>instance-attribute</code>","text":"<pre><code>in_memory = (\n    darts_segmentation.training.data.DartsDataModule(\n        in_memory\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.nsamples","title":"nsamples  <code>instance-attribute</code>","text":"<pre><code>nsamples = zroot['x'].shape[0]\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.num_workers","title":"num_workers  <code>instance-attribute</code>","text":"<pre><code>num_workers = (\n    darts_segmentation.training.data.DartsDataModule(\n        num_workers\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.subsample","title":"subsample  <code>instance-attribute</code>","text":"<pre><code>subsample = (\n    darts_segmentation.training.data.DartsDataModule(\n        subsample\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.total_folds","title":"total_folds  <code>instance-attribute</code>","text":"<pre><code>total_folds = (\n    darts_segmentation.training.data.DartsDataModule(\n        total_folds\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.plot","title":"plot","text":"<pre><code>plot()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def plot(self):\n    name = self.data_dir.name.replace(\"_\", \" \").capitalize()\n    metadata = gpd.read_parquet(self.data_dir / \"metadata.parquet\")\n    if self.subsample is not None:\n        metadata = metadata.sample(n=self.subsample, random_state=42)\n    trainval_metadata, test_metadata = _split_metadata(metadata, self.data_split_method, self.data_split_by)\n    train_index, val_index = _get_fold(trainval_metadata, self.fold_method, self.total_folds, self.fold)\n    train_metadata = trainval_metadata.loc[train_index]\n    val_metadata = trainval_metadata.loc[val_index]\n    test_metadata = test_metadata if self.data_split_method is not None else None\n    val_metadata = val_metadata if self.fold_method is not None else None\n    return plot_training_data_distribution(train_metadata, val_metadata, test_metadata, name)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.setup","title":"setup","text":"<pre><code>setup(\n    stage: typing.Literal[\n        \"fit\", \"validate\", \"test\", \"predict\"\n    ]\n    | None = None,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def setup(self, stage: Literal[\"fit\", \"validate\", \"test\", \"predict\"] | None = None):\n    if stage == \"predict\" or stage is None:\n        return\n\n    metadata = gpd.read_parquet(self.data_dir / \"metadata.parquet\")\n    if self.subsample is not None:\n        metadata = metadata.sample(n=self.subsample, random_state=42)\n    train_metadata, test_metadata = _split_metadata(metadata, self.data_split_method, self.data_split_by)\n\n    _log_stats(train_metadata, \"train-split\")\n    _log_stats(test_metadata, \"test-split\")\n\n    # Log stats about the data\n\n    if stage in [\"fit\", \"validate\"]:\n        train_index, val_index = _get_fold(train_metadata, self.fold_method, self.total_folds, self.fold)\n        _log_stats(metadata.loc[train_index], \"train-fold\")\n        _log_stats(metadata.loc[val_index], \"val-fold\")\n\n        dsclass = DartsDatasetInMemory if self.in_memory else DartsDatasetZarr\n        self.train = dsclass(self.data_dir / \"data.zarr\", self.augment, train_index, self.bands)\n        self.val = dsclass(self.data_dir / \"data.zarr\", None, val_index, self.bands)\n    if stage == \"test\":\n        test_index = test_metadata.index.tolist()\n        dsclass = DartsDatasetInMemory if self.in_memory else DartsDatasetZarr\n        self.test = dsclass(self.data_dir / \"data.zarr\", None, test_index, self.bands)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.test_dataloader","title":"test_dataloader","text":"<pre><code>test_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def test_dataloader(self):\n    return DataLoader(\n        self.test,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def train_dataloader(self):\n    return DataLoader(\n        self.train,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        shuffle=True,\n        drop_last=True,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.val_dataloader","title":"val_dataloader","text":"<pre><code>val_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def val_dataloader(self):\n    return DataLoader(\n        self.val,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory","title":"DartsDatasetInMemory","text":"<pre><code>DartsDatasetInMemory(\n    data_dir: pathlib.Path | str,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    indices: list[int] | None = None,\n    bands: list[int] | None = None,\n)\n</code></pre> <p>               Bases: <code>torch.utils.data.Dataset</code></p> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path | str,\n    augment: list[Augmentation] | None = None,\n    indices: list[int] | None = None,\n    bands: list[int] | None = None,\n):\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    store = zarr.storage.LocalStore(data_dir)\n    self.zroot = zarr.group(store=store)\n\n    assert \"x\" in self.zroot and \"y\" in self.zroot, (\n        f\"Dataset corrupted! {self.zroot.info=} must contain 'x' or 'y' arrays!\"\n    )\n\n    self.x = []\n    self.y = []\n    indices = indices or list(range(self.zroot[\"x\"].shape[0]))\n    for i in indices:\n        x = self.zroot[\"x\"][i, bands] if bands else self.zroot[\"x\"][i]\n        y = self.zroot[\"y\"][i]\n        self.x.append(x)\n        self.y.append(y)\n\n    self.transform = get_augmentation(augment)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = darts_segmentation.training.augmentations.get_augmentation(\n    darts_segmentation.training.data.DartsDatasetInMemory(\n        augment\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x = []\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y = []\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.zroot","title":"zroot  <code>instance-attribute</code>","text":"<pre><code>zroot = zarr.group(store=store)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    x = self.x[idx]\n    y = self.y[idx]\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.x)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr","title":"DartsDatasetZarr","text":"<pre><code>DartsDatasetZarr(\n    data_dir: pathlib.Path | str,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    indices: list[int] | None = None,\n    bands: list[int] | None = None,\n)\n</code></pre> <p>               Bases: <code>torch.utils.data.Dataset</code></p> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path | str,\n    augment: list[Augmentation] | None = None,\n    indices: list[int] | None = None,\n    bands: list[int] | None = None,\n):\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    store = zarr.storage.LocalStore(data_dir)\n    self.zroot = zarr.group(store=store)\n\n    assert \"x\" in self.zroot and \"y\" in self.zroot, (\n        f\"Dataset corrupted! {self.zroot.info=} must contain 'x' or 'y' arrays!\"\n    )\n\n    self.indices = indices if indices is not None else list(range(self.zroot[\"x\"].shape[0]))\n    self.bands = bands\n\n    self.transform = get_augmentation(augment)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands = darts_segmentation.training.data.DartsDatasetZarr(\n    bands\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.indices","title":"indices  <code>instance-attribute</code>","text":"<pre><code>indices = (\n    darts_segmentation.training.data.DartsDatasetZarr(\n        indices\n    )\n    if darts_segmentation.training.data.DartsDatasetZarr(\n        indices\n    )\n    is not None\n    else list(\n        range(\n            darts_segmentation.training.data.DartsDatasetZarr(\n                self\n            )\n            .zroot[\"x\"]\n            .shape[0]\n        )\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = darts_segmentation.training.augmentations.get_augmentation(\n    darts_segmentation.training.data.DartsDatasetZarr(\n        augment\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.zroot","title":"zroot  <code>instance-attribute</code>","text":"<pre><code>zroot = zarr.group(store=store)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    i = self.indices[idx]\n\n    x = self.zroot[\"x\"][i, self.bands] if self.bands else self.zroot[\"x\"][i]\n    y = self.zroot[\"y\"][i]\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.indices)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data._get_fold","title":"_get_fold","text":"<pre><code>_get_fold(\n    metadata: geopandas.GeoDataFrame,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n        \"none\",\n    ]\n    | None,\n    n_folds: int,\n    fold: int,\n) -&gt; tuple[list[int], list[int]]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def _get_fold(\n    metadata: gpd.GeoDataFrame,\n    fold_method: Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\", \"none\"] | None,\n    n_folds: int,\n    fold: int,\n) -&gt; tuple[list[int], list[int]]:\n    fold_method = fold_method or \"none\"\n    logger.debug(f\"Using fold method {fold_method} for fold {fold} of {n_folds} total folds.\")\n    if fold_method == \"none\":\n        return metadata.index.tolist(), metadata.index.tolist()\n\n    match fold_method:\n        case \"kfold\":\n            foldgen = KFold(n_folds).split(metadata)\n        case \"shuffle\":\n            foldgen = StratifiedShuffleSplit(n_splits=n_folds, random_state=42).split(metadata, ~metadata[\"empty\"])\n        case \"stratified\":\n            foldgen = StratifiedKFold(n_folds, random_state=42, shuffle=True).split(metadata, ~metadata[\"empty\"])\n        case \"region\":\n            foldgen = GroupShuffleSplit(n_folds).split(metadata, groups=metadata[\"region\"])\n        case \"region-stratified\":\n            foldgen = StratifiedGroupKFold(n_folds, random_state=42, shuffle=True).split(\n                metadata, ~metadata[\"empty\"], groups=metadata[\"region\"]\n            )\n        case _:\n            raise ValueError(f\"Unknown fold method: {fold_method}\")\n\n    for i, (train_index, val_index) in enumerate(foldgen):\n        if i != fold:\n            continue\n        # Turn index into metadata index\n        train_index = metadata.index[train_index].tolist()\n        val_index = metadata.index[val_index].tolist()\n        return train_index, val_index\n\n    raise ValueError(f\"Fold {fold} not found\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data._log_stats","title":"_log_stats","text":"<pre><code>_log_stats(\n    metadata: geopandas.GeoDataFrame,\n    mode: str,\n    level: int = logging.DEBUG,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def _log_stats(metadata: gpd.GeoDataFrame, mode: str, level: int = logging.DEBUG):\n    n_pos = (~metadata[\"empty\"]).sum()\n    n_neg = metadata[\"empty\"].sum()\n    logger.log(\n        level,\n        f\"{mode} dataset: {n_pos} positive, {n_neg} negative ({len(metadata)} total)\"\n        f\" with {metadata['region'].nunique()} unique regions and {metadata['sample_id'].nunique()} unique sample ids\",\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data._split_metadata","title":"_split_metadata","text":"<pre><code>_split_metadata(\n    metadata: geopandas.GeoDataFrame,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\", \"none\"\n    ]\n    | None,\n    data_split_by: list[str | float] | None,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def _split_metadata(\n    metadata: gpd.GeoDataFrame,\n    data_split_method: Literal[\"random\", \"region\", \"sample\", \"none\"] | None,\n    data_split_by: list[str | float] | None,\n):\n    # Match statement doesn't like None\n    data_split_method = data_split_method or \"none\"\n\n    match data_split_method:\n        case \"none\":\n            return metadata, metadata\n        case \"random\":\n            assert isinstance(data_split_by, list) and len(data_split_by) == 1\n            data_split_by = data_split_by[0]\n            assert isinstance(data_split_by, float)\n            for seed in range(100):\n                train_metadata = metadata.sample(frac=data_split_by, random_state=seed)\n                test_metadata = metadata.drop(train_metadata.index)\n                if (~test_metadata[\"empty\"]).sum() == 0:\n                    logger.warning(\"Test set is empty, retrying with another random seed...\")\n                    continue\n                return train_metadata, test_metadata\n            else:\n                raise ValueError(\"Could not split data randomly, please check your data.\")\n        case \"region\":\n            assert isinstance(data_split_by, list) and len(data_split_by) &gt; 0\n            train_metadata = metadata[~metadata[\"region\"].isin(data_split_by)]\n            test_metadata = metadata[metadata[\"region\"].isin(data_split_by)]\n            return train_metadata, test_metadata\n        case \"sample\":\n            assert isinstance(data_split_by, list) and len(data_split_by) &gt; 0\n            train_metadata = metadata[~metadata[\"sample_id\"].isin(data_split_by)]\n            test_metadata = metadata[metadata[\"sample_id\"].isin(data_split_by)]\n            return train_metadata, test_metadata\n        case _:\n            raise ValueError(f\"Invalid data split method: {data_split_method}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.get_augmentation","title":"get_augmentation","text":"<pre><code>get_augmentation(\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None,\n    always_apply: bool = False,\n) -&gt; albumentations.Compose | None\n</code></pre> <p>Get augmentations for segmentation tasks.</p> <p>Parameters:</p> <ul> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply. If None or emtpy, no augmentations are applied. If not empty, augmentations are applied in the order they are listed. Available augmentations:     - D4 (Combination of HorizontalFlip, VerticalFlip, and RandomRotate90)     - Blur     - RandomBrightnessContrast     - MultiplicativeNoise     - Posterize (quantization to reduce number of bits per channel)</p> </li> <li> <code>always_apply</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, augmentations are always applied. This is useful for visualization/testing augmentations. Default is False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown augmentation is provided.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>albumentations.Compose | None</code>           \u2013            <p>A.Compose | None: A Compose object containing the augmentations. If no augmentations are provided, returns None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/augmentations.py</code> <pre><code>def get_augmentation(augment: list[Augmentation] | None, always_apply: bool = False) -&gt; \"A.Compose | None\":  # noqa: C901\n    \"\"\"Get augmentations for segmentation tasks.\n\n    Args:\n        augment (list[Augmentation] | None): List of augmentations to apply.\n            If None or emtpy, no augmentations are applied.\n            If not empty, augmentations are applied in the order they are listed.\n            Available augmentations:\n                - D4 (Combination of HorizontalFlip, VerticalFlip, and RandomRotate90)\n                - Blur\n                - RandomBrightnessContrast\n                - MultiplicativeNoise\n                - Posterize (quantization to reduce number of bits per channel)\n        always_apply (bool): If True, augmentations are always applied.\n            This is useful for visualization/testing augmentations.\n            Default is False.\n\n    Raises:\n        ValueError: If an unknown augmentation is provided.\n\n    Returns:\n        A.Compose | None: A Compose object containing the augmentations.\n            If no augmentations are provided, returns None.\n\n    \"\"\"\n    import albumentations as A  # noqa: N812\n\n    if not isinstance(augment, list) or len(augment) == 0:\n        return None\n\n    # Replace HorizontalFlip, VerticalFlip, RandomRotate90 with D4\n    if \"HorizontalFlip\" in augment and \"VerticalFlip\" in augment and \"RandomRotate90\" in augment:\n        augment = [aug for aug in augment if aug not in (\"HorizontalFlip\", \"VerticalFlip\", \"RandomRotate90\")]\n        augment.insert(0, \"D4\")\n\n    transforms = []\n    for aug in augment:\n        match aug:\n            case \"D4\":\n                transforms.append(A.D4())\n            case \"HorizontalFlip\":\n                transforms.append(A.HorizontalFlip(p=1.0 if always_apply else 0.5))\n            case \"VerticalFlip\":\n                transforms.append(A.VerticalFlip(p=1.0 if always_apply else 0.5))\n            case \"RandomRotate90\":\n                transforms.append(A.RandomRotate90())\n            case \"Blur\":\n                transforms.append(A.Blur(p=1.0 if always_apply else 0.5))\n            case \"RandomBrightnessContrast\":\n                transforms.append(A.RandomBrightnessContrast(p=1.0 if always_apply else 0.5))\n            case \"MultiplicativeNoise\":\n                transforms.append(\n                    A.MultiplicativeNoise(per_channel=True, elementwise=True, p=1.0 if always_apply else 0.5)\n                )\n            case \"Posterize\":\n                # First convert to uint8, then apply posterization, then convert back to float32\n                # * Note: This does only work for float32 images.\n                transforms += [\n                    A.FromFloat(dtype=\"uint8\"),\n                    A.Posterize(num_bits=6, p=1.0),\n                    A.ToFloat(),\n                ]\n            case _:\n                raise ValueError(f\"Unknown augmentation: {aug}\")\n    return A.Compose(transforms)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.plot_training_data_distribution","title":"plot_training_data_distribution","text":"<pre><code>plot_training_data_distribution(\n    train_metadata: geopandas.GeoDataFrame,\n    val_metadata: geopandas.GeoDataFrame | None,\n    test_metadata: geopandas.GeoDataFrame | None,\n    name: str,\n) -&gt; tuple[\n    matplotlib.pyplot.Figure,\n    dict[str, matplotlib.pyplot.Axes],\n]\n</code></pre> <p>Plot the distribution of training data by region on a polar projection.</p> <p>Parameters:</p> <ul> <li> <code>train_metadata</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>GeoDataFrame containing training metadata.</p> </li> <li> <code>val_metadata</code>               (<code>geopandas.GeoDataFrame | None</code>)           \u2013            <p>GeoDataFrame containing validation metadata.</p> </li> <li> <code>test_metadata</code>               (<code>geopandas.GeoDataFrame | None</code>)           \u2013            <p>GeoDataFrame containing test metadata.</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the dataset or experiment for the plot title.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[matplotlib.pyplot.Figure, dict[str, matplotlib.pyplot.Axes]]</code>           \u2013            <p>tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/viz.py</code> <pre><code>def plot_training_data_distribution(\n    train_metadata: gpd.GeoDataFrame,\n    val_metadata: gpd.GeoDataFrame | None,\n    test_metadata: gpd.GeoDataFrame | None,\n    name: str,\n) -&gt; tuple[plt.Figure, dict[str, plt.Axes]]:\n    \"\"\"Plot the distribution of training data by region on a polar projection.\n\n    Args:\n        train_metadata (gpd.GeoDataFrame): GeoDataFrame containing training metadata.\n        val_metadata (gpd.GeoDataFrame | None): GeoDataFrame containing validation metadata.\n        test_metadata (gpd.GeoDataFrame | None): GeoDataFrame containing test metadata.\n        name (str): Name of the dataset or experiment for the plot title.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.\n\n    \"\"\"\n    # Aggregate by sample_id to get counts of not-empty tiles for train and test\n    # Get centroids of the aggregated geometries\n    train_metadata[\"not-empty\"] = ~train_metadata[\"empty\"]\n    train_sample_data = train_metadata[[\"sample_id\", \"not-empty\", \"geometry\"]].dissolve(by=\"sample_id\", aggfunc=\"sum\")\n    train_centroids = train_sample_data.geometry.centroid\n    if val_metadata is not None:\n        val_metadata[\"not-empty\"] = ~val_metadata[\"empty\"]\n        val_sample_data = val_metadata[[\"sample_id\", \"not-empty\", \"geometry\"]].dissolve(by=\"sample_id\", aggfunc=\"sum\")\n        val_centroids = val_sample_data.geometry.centroid\n    if test_metadata is not None:\n        test_metadata[\"not-empty\"] = ~test_metadata[\"empty\"]\n        test_sample_data = test_metadata[[\"sample_id\", \"not-empty\", \"geometry\"]].dissolve(by=\"sample_id\", aggfunc=\"sum\")\n        test_centroids = test_sample_data.geometry.centroid\n\n    # Create figure with NorthPolarStereo projection\n    fig, axs = plt.subplot_mosaic(\n        [[\"map\", \"map\", \"map\", \"train-dist\"], [\"map\", \"map\", \"map\", \"val-dist\"], [\"map\", \"map\", \"map\", \"test-dist\"]],\n        layout=\"constrained\",\n        figsize=(12, 8),\n        per_subplot_kw={\"map\": {\"projection\": ccrs.NorthPolarStereo()}},\n    )\n\n    # Set the extent to limit to 55\u00b0N latitude (circular boundary)\n    axs[\"map\"].set_extent([-180, 180, 55, 90], ccrs.PlateCarree())\n\n    # Add map features\n    axs[\"map\"].add_feature(cfeature.LAND, facecolor=\"lightgray\", alpha=0.3)\n    axs[\"map\"].add_feature(cfeature.OCEAN, facecolor=\"lightblue\", alpha=0.3)\n    axs[\"map\"].add_feature(cfeature.COASTLINE, linewidth=0.5)\n    axs[\"map\"].add_feature(cfeature.BORDERS, linewidth=0.3, linestyle=\":\")\n\n    # Add gridlines\n    axs[\"map\"].gridlines(draw_labels=True, linewidth=0.5, alpha=0.5, linestyle=\"--\")\n\n    # Determine common vmax for consistent color scaling\n    vmax = max(\n        train_sample_data[\"not-empty\"].max(),\n        (val_sample_data[\"not-empty\"].max() if val_metadata is not None else 0),\n        (test_sample_data[\"not-empty\"].max() if test_metadata is not None else 0),\n    )\n\n    # Plot the training regions with circles\n    train_scatter = axs[\"map\"].scatter(\n        train_centroids.x,\n        train_centroids.y,\n        c=train_sample_data[\"not-empty\"],\n        cmap=\"YlGnBu\",\n        s=120,\n        alpha=0.7,\n        transform=ccrs.PlateCarree(),\n        edgecolors=\"black\",\n        linewidths=0.5,\n        vmin=0,\n        vmax=vmax,\n        marker=\"o\",  # Circle for training data\n        label=\"Training\",\n    )\n\n    # Plot the training regions with circles\n    if val_metadata is not None:\n        axs[\"map\"].scatter(\n            val_centroids.x,\n            val_centroids.y,\n            c=val_sample_data[\"not-empty\"],\n            cmap=\"YlGnBu\",\n            s=80,\n            alpha=0.7,\n            transform=ccrs.PlateCarree(),\n            edgecolors=\"black\",\n            linewidths=0.5,\n            vmin=0,\n            vmax=vmax,\n            marker=\"*\",  # Circle for training data\n            label=\"Validation\",\n        )\n\n    # Plot the test regions with triangles\n    if test_metadata is not None:\n        axs[\"map\"].scatter(\n            test_centroids.x,\n            test_centroids.y,\n            c=test_sample_data[\"not-empty\"],\n            cmap=\"YlGnBu\",\n            s=80,\n            alpha=0.7,\n            transform=ccrs.PlateCarree(),\n            edgecolors=\"black\",\n            linewidths=0.5,\n            vmin=0,\n            vmax=vmax,\n            marker=\"^\",  # Triangle for test data\n            label=\"Test\",\n        )\n\n    # Add colorbar\n    cbar = plt.colorbar(train_scatter, ax=axs[\"map\"], shrink=0.6, pad=0.05)\n    cbar.set_label(\"Number of Patches with Data\", rotation=270, labelpad=20, fontsize=12)\n\n    # Add legend for train/test split\n    legend = axs[\"map\"].legend(\n        loc=\"lower left\", frameon=True, fancybox=True, shadow=True, fontsize=11, title=\"Data Split\"\n    )\n    legend.get_frame().set_alpha(0.9)\n\n    # Create circular boundary at 55\u00b0N\n    theta = np.linspace(0, 2 * np.pi, 100)\n    verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n    circle = mpath.Path(verts * 0.5 + 0.5)\n    axs[\"map\"].set_boundary(circle, transform=axs[\"map\"].transAxes)\n\n    axs[\"map\"].set_title(f\"Training Data Distribution by Region ({name})\", fontsize=14, fontweight=\"bold\", pad=20)\n\n    sns.histplot(\n        train_metadata,\n        y=\"region\",\n        hue=\"not-empty\",\n        multiple=\"stack\",\n        ax=axs[\"train-dist\"],\n        palette=[\"#7f8c8d\", \"#27ae60\"],  # Gray for w/o RTS, Green for w/ RTS\n    )\n    axs[\"train-dist\"].set_title(\"Training Set Distribution by Region\")\n    axs[\"train-dist\"].legend(labels=[\"w RTS\", \"w/o RTS\"])\n    axs[\"train-dist\"].set_ylabel(\"\")\n    axs[\"train-dist\"].set_xlabel(\"Number of Patches\")\n\n    if val_metadata is not None:\n        sns.histplot(\n            val_metadata,\n            y=\"region\",\n            hue=\"not-empty\",\n            multiple=\"stack\",\n            ax=axs[\"val-dist\"],\n            palette=[\"#7f8c8d\", \"#27ae60\"],  # Gray for w/o RTS, Green for w/ RTS\n        )\n        axs[\"val-dist\"].set_title(\"Validation Set Distribution by Region\")\n        axs[\"val-dist\"].legend(labels=[\"w/ RTS\", \"w/o RTS\"])\n        axs[\"val-dist\"].set_ylabel(\"\")\n        axs[\"val-dist\"].set_xlabel(\"Number of Patches\")\n\n    if test_metadata is not None:\n        sns.histplot(\n            test_metadata,\n            y=\"region\",\n            hue=\"not-empty\",\n            multiple=\"stack\",\n            ax=axs[\"test-dist\"],\n            palette=[\"#7f8c8d\", \"#27ae60\"],  # Gray for w/o RTS, Green for w/ RTS\n        )\n        axs[\"test-dist\"].set_title(\"Test Set Distribution by Region\")\n        axs[\"test-dist\"].legend(labels=[\"w/ RTS\", \"w/o RTS\"])\n        axs[\"test-dist\"].set_ylabel(\"\")\n        axs[\"test-dist\"].set_xlabel(\"Number of Patches\")\n\n    # fig.tight_layout()\n    return fig, axs\n</code></pre>"},{"location":"reference/darts_segmentation/training/data_validate/","title":"data_validate","text":""},{"location":"reference/darts_segmentation/training/data_validate/#darts_segmentation.training.data_validate","title":"darts_segmentation.training.data_validate","text":"<p>CLI-ready function for validating a training dataset config based on a training dataset.</p>"},{"location":"reference/darts_segmentation/training/data_validate/#darts_segmentation.training.data_validate.validate_dataset","title":"validate_dataset","text":"<pre><code>validate_dataset(\n    train_data_dir: str | pathlib.Path,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n        \"none\",\n    ] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Validate a training dataset config based on a training dataset.</p> <p>Please see the DartsDataModule for more information.</p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified', 'none']</code>, default:                   <code>'kfold'</code> )           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> <li> <code>bands</code>               (<code>Bands | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of bands to use. Expects the data_dir to contain a config.toml with a \"darts.bands\" key, with which the indices of the bands will be mapped to. Defaults to None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/data_validate.py</code> <pre><code>def validate_dataset(\n    train_data_dir: str | Path,\n    # data_split is for the test split\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str | float] | None = None,\n    # fold is for cross-validation split (train/val)\n    fold_method: Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\", \"none\"] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n    bands: list[str] | None = None,\n):\n    \"\"\"Validate a training dataset config based on a training dataset.\n\n    Please see the DartsDataModule for more information.\n\n    Args:\n        train_data_dir (Path): The path to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the test size can be specified\n            by providing a list with a single a float between 0 and 1 to data_split_by\n            This will be the fraction of the data to be used for testing.\n            E.g. [0.2] will use 20% of the data for testing.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str | float] | None, optional): Select by which regions/samples to split or\n            the size of test set. Defaults to None.\n        fold_method (Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\", \"none\"], optional):\n            Method for cross-validation split. Defaults to \"kfold\".\n        total_folds (int, optional): Total number of folds in cross-validation. Defaults to 5.\n        subsample (int | None, optional): If set, will subsample the dataset to this number of samples.\n            This is useful for debugging and testing. Defaults to None.\n        bands (Bands | list[str] | None, optional): List of bands to use.\n            Expects the data_dir to contain a config.toml with a \"darts.bands\" key,\n            with which the indices of the bands will be mapped to.\n            Defaults to None.\n\n    \"\"\"\n    import geopandas as gpd\n    import toml\n\n    from darts_segmentation.training.data import _get_fold, _log_stats, _split_metadata\n\n    data_dir = Path(train_data_dir)\n\n    config_file = data_dir / \"config.toml\"\n    assert config_file.exists(), f\"Config file {config_file} not found!\"\n    config = toml.load(config_file)\n    assert \"darts\" in config and \"bands\" in config[\"darts\"], f\"Config file {config_file} is missing 'darts.bands' key!\"\n\n    if bands:\n        # Check if bands are in config\n        data_bands = config[\"darts\"][\"bands\"]\n        for b in bands:\n            assert b in data_bands, f\"Band {b} not found in config file {config_file}!\"\n\n    metadata_file = data_dir / \"metadata.parquet\"\n    assert metadata_file.exists(), f\"Metadata file {metadata_file} not found!\"\n    metadata = gpd.read_parquet(data_dir / \"metadata.parquet\")\n\n    if subsample is not None:\n        metadata = metadata.sample(n=subsample, random_state=42)\n    train_metadata, test_metadata = _split_metadata(metadata, data_split_method, data_split_by)\n\n    _log_stats(train_metadata, \"Non-Test (train-split)\", level=logging.INFO)\n    _log_stats(test_metadata, \"Test (test-split)\", level=logging.INFO)\n\n    for fold in range(total_folds):\n        train_index, val_index = _get_fold(train_metadata, fold_method, total_folds, fold)\n        _log_stats(metadata.loc[train_index], f\"Training fold {fold} (train-fold)\", level=logging.INFO)\n        _log_stats(metadata.loc[val_index], f\"Validation fold {fold} (val-fold)\", level=logging.INFO)\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/","title":"hparams","text":""},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams","title":"darts_segmentation.training.hparams","text":"<p>Hyperparameters for training.</p>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Augmentation","title":"Augmentation  <code>module-attribute</code>","text":"<pre><code>Augmentation = typing.Literal[\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"D4\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\",\n    \"Posterize\",\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.HP_NAMES","title":"HP_NAMES  <code>module-attribute</code>","text":"<pre><code>HP_NAMES = [\n    (field.name)\n    for field in (\n        darts_segmentation.training.hparams.Hyperparameters.__dataclass_fields__.values()\n    )\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters","title":"Hyperparameters  <code>dataclass</code>","text":"<pre><code>Hyperparameters(\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Hyperparameters for Cyclopts CLI.</p> <p>Attributes:</p> <ul> <li> <code>model_arch</code>               (<code>str</code>)           \u2013            <p>Architecture of the model to use.</p> </li> <li> <code>model_encoder</code>               (<code>str</code>)           \u2013            <p>Encoder type for the model.</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>)           \u2013            <p>Weights for the encoder, if any.</p> </li> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>)           \u2013            <p>Learning rate for training.</p> </li> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Decay factor for learning rate.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float | None</code>)           \u2013            <p>Alpha parameter for focal loss, if using.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>)           \u2013            <p>Gamma parameter for focal loss.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>)           \u2013            <p>List of bands to use. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.augment","title":"augment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>augment: (\n    list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.bands","title":"bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bands: list[str] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.focal_loss_alpha","title":"focal_loss_alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_alpha: float | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.focal_loss_gamma","title":"focal_loss_gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_gamma: float = 2.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.9\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.001\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.model_arch","title":"model_arch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_arch: str = 'Unet'\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.model_encoder","title":"model_encoder  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder: str = 'dpn107'\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.model_encoder_weights","title":"model_encoder_weights  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder_weights: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.parse_hyperparameters","title":"parse_hyperparameters","text":"<pre><code>parse_hyperparameters(\n    hpconfig_file: pathlib.Path,\n) -&gt; dict[\n    str,\n    list\n    | scipy.stats.rv_discrete\n    | scipy.stats.rv_continuous,\n]\n</code></pre> <p>Parse hyperparameter configuration file to a valid dictionary for sklearn parameter search.</p> <p>Can be YAML or TOML. Must contain a key called \"hyperparameters\" containing a list of hyperparameters distributions. These distributions can either be explicit defined by another dictionary containing a \"distribution\" key, or they can be implicit defined by a single value, a list or a dictionary containing a \"low\" and \"high\" key.</p> The following distributions are supported <ul> <li>\"uniform\": Uniform distribution - must have a \"low\" and \"high\" value</li> <li>\"loguniform\": Log-uniform distribution - must have a \"low\" and \"high\" value</li> <li>\"intuniform\": Integer uniform distribution - must have a \"low\" and \"high\" value (both are inclusive)</li> <li>\"choice\": Choice distribution - must have a list of \"choices\" for explicit case, else just pass a list</li> <li>\"value\": Fixed value distribution - must have a \"value\" key for explicit case, else just pass a value</li> </ul> <p>Examples:</p> <p>Explicit Toml:</p> <pre><code>[hyperparameters]\nlearning_rate = {distribution = \"loguniform\", low = 1.0e-5, high = 1.0e-2}\nbatch_size = {distribution = \"choice\", choices = [32, 64, 128]}\ngamma = {distribution = \"uniform\", low = 0.9, high = 2.5}\ndropout = {distribution = \"uniform\", low = 0.0, high = 0.5}\nlayers = {distribution = \"intuniform\", low = 1, high = 10}\narchitecture = {distribution = \"constant\", value = \"resnet\"}\n</code></pre> <p>Explicit YAML:</p> <pre><code>hyperparameters:\n    learning_rate:\n        distribution: loguniform\n        low: 1.0e-5\n        high: 1.0e-2\n    batch_size:\n        distribution: choice\n        choices: [32, 64, 128]\n    gamma:\n        distribution: uniform\n        low: 0.9\n        high: 2.5\n    dropout:\n        distribution: uniform\n        low: 0.0\n        high: 0.5\n    layers:\n        distribution: intuniform\n        low: 1\n        high: 10\n    architecture:\n        distribution: constant\n        value: \"resnet\"\n</code></pre> <p>Implicit YAML:</p> <pre><code>hyperparameters:\n    learning_rate:\n        distribution: loguniform\n        low: 1.0e-5\n        high: 1.0e-2\n    batch_size: [32, 64, 128]\n    gamma:\n        low: 0.9\n        high: 2.5\n    dropout:\n        low: 0.0\n        high: 0.5\n    layers:\n        low: 1\n        high: 10\n    architecture: \"resnet\"\n</code></pre> <p>Will all result in the following dictionary:</p> <pre><code>{\n    \"learning_rate\": scipy.stats.loguniform(1.0e-5, 1.0e-2),\n    \"batch_size\": [32, 64, 128],\n    \"gamma\": scipy.stats.uniform(0.9, 1.6),\n    \"dropout\": scipy.stats.uniform(0.0, 0.5),\n    \"layers\": scipy.stats.randint(1, 11),\n    \"architecture\": [\"resnet\"]\n}\n</code></pre> <p>Parameters:</p> <ul> <li> <code>hpconfig_file</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the hyperparameter configuration file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[str, list | scipy.stats.rv_discrete | scipy.stats.rv_continuous]</code> )          \u2013            <p>Dictionary of hyperparameters to tune and their distributions.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the hyperparameter configuration file is not a valid YAML or TOML file.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/hparams.py</code> <pre><code>def parse_hyperparameters(  # noqa: C901\n    hpconfig_file: Path,\n) -&gt; dict[str, \"list | scipy.stats.rv_discrete | scipy.stats.rv_continuous\"]:\n    \"\"\"Parse hyperparameter configuration file to a valid dictionary for sklearn parameter search.\n\n    Can be YAML or TOML.\n    Must contain a key called \"hyperparameters\" containing a list of hyperparameters distributions.\n    These distributions can either be explicit defined by another dictionary containing a \"distribution\" key,\n    or they can be implicit defined by a single value, a list or a dictionary containing a \"low\" and \"high\" key.\n\n    The following distributions are supported:\n        - \"uniform\": Uniform distribution - must have a \"low\" and \"high\" value\n        - \"loguniform\": Log-uniform distribution - must have a \"low\" and \"high\" value\n        - \"intuniform\": Integer uniform distribution - must have a \"low\" and \"high\" value (both are inclusive)\n        - \"choice\": Choice distribution - must have a list of \"choices\" for explicit case, else just pass a list\n        - \"value\": Fixed value distribution - must have a \"value\" key for explicit case, else just pass a value\n\n    Examples:\n        Explicit Toml:\n\n        ```toml\n        [hyperparameters]\n        learning_rate = {distribution = \"loguniform\", low = 1.0e-5, high = 1.0e-2}\n        batch_size = {distribution = \"choice\", choices = [32, 64, 128]}\n        gamma = {distribution = \"uniform\", low = 0.9, high = 2.5}\n        dropout = {distribution = \"uniform\", low = 0.0, high = 0.5}\n        layers = {distribution = \"intuniform\", low = 1, high = 10}\n        architecture = {distribution = \"constant\", value = \"resnet\"}\n        ```\n\n        Explicit YAML:\n\n        ```yaml\n        hyperparameters:\n            learning_rate:\n                distribution: loguniform\n                low: 1.0e-5\n                high: 1.0e-2\n            batch_size:\n                distribution: choice\n                choices: [32, 64, 128]\n            gamma:\n                distribution: uniform\n                low: 0.9\n                high: 2.5\n            dropout:\n                distribution: uniform\n                low: 0.0\n                high: 0.5\n            layers:\n                distribution: intuniform\n                low: 1\n                high: 10\n            architecture:\n                distribution: constant\n                value: \"resnet\"\n        ```\n\n        Implicit YAML:\n\n        ```yaml\n        hyperparameters:\n            learning_rate:\n                distribution: loguniform\n                low: 1.0e-5\n                high: 1.0e-2\n            batch_size: [32, 64, 128]\n            gamma:\n                low: 0.9\n                high: 2.5\n            dropout:\n                low: 0.0\n                high: 0.5\n            layers:\n                low: 1\n                high: 10\n            architecture: \"resnet\"\n        ```\n\n        Will all result in the following dictionary:\n\n        ```py\n        {\n            \"learning_rate\": scipy.stats.loguniform(1.0e-5, 1.0e-2),\n            \"batch_size\": [32, 64, 128],\n            \"gamma\": scipy.stats.uniform(0.9, 1.6),\n            \"dropout\": scipy.stats.uniform(0.0, 0.5),\n            \"layers\": scipy.stats.randint(1, 11),\n            \"architecture\": [\"resnet\"]\n        }\n        ```\n\n    Args:\n        hpconfig_file (Path): Path to the hyperparameter configuration file.\n\n    Returns:\n        dict: Dictionary of hyperparameters to tune and their distributions.\n\n    Raises:\n        ValueError: If the hyperparameter configuration file is not a valid YAML or TOML file.\n\n    \"\"\"\n    import scipy.stats\n\n    # Read yaml\n    if hpconfig_file.suffix == \".yaml\" or hpconfig_file.suffix == \".yml\":\n        with hpconfig_file.open() as f:\n            hpconfig = yaml.safe_load(f)[\"hyperparameters\"]\n    # Read toml\n    elif hpconfig_file.suffix == \".toml\":\n        with hpconfig_file.open() as f:\n            hpconfig = toml.load(f)[\"hyperparameters\"]\n    else:\n        raise ValueError(f\"Invalid hyperparameter configuration file format: {hpconfig.suffix}\")\n\n    hpdistributions = {}\n    for hparam, config in hpconfig.items():\n        if \"-\" in hparam:\n            logger.debug(f\"Hyphen in hyperparameter name {hparam} is not supported. Replacing with underscore.\")\n            hparam = hparam.replace(\"-\", \"_\")\n        # Assume implicit case\n        if isinstance(config, list):\n            # Choice\n            hpdistributions[hparam] = config\n            continue\n        elif not isinstance(config, dict):\n            # Constant\n            hpdistributions[hparam] = [config]\n            continue\n        else:\n            if \"low\" in config.keys() and \"high\" in config.keys():\n                if isinstance(config[\"low\"], int) and isinstance(config[\"high\"], int):\n                    # Randint\n                    hpdistributions[hparam] = scipy.stats.randint(config[\"low\"], config[\"high\"] + 1)\n                    continue\n                elif isinstance(config[\"low\"], float) and isinstance(config[\"high\"], float):\n                    # Randfloat\n                    hpdistributions[hparam] = scipy.stats.uniform(config[\"low\"], config[\"high\"] - config[\"low\"])\n                    continue\n                else:\n                    raise ValueError(\n                        f\"Invalid hyperparameter configuration for {hparam}: low and high must be of the same type.\"\n                        f\" Got {type(config['low'])=} and {type(config['high'])=}.\"\n                    )\n\n        # Now Explicit\n        assert isinstance(config, dict), f\"Invalid hyperparameter configuration for {hparam}\"\n        assert \"distribution\" in config, (\n            f\"Could not implicitly define distribution for {hparam}.\"\n            \" Please provide a distribution type via a 'distribution' key.\"\n        )\n\n        match config[\"distribution\"]:\n            case \"uniform\":\n                assert \"low\" in config, f\"Missing 'low' key in hyperparameter configuration for uniform {hparam}\"\n                assert \"high\" in config, f\"Missing 'high' key in hyperparameter configuration for uniform {hparam}\"\n                assert isinstance(config[\"low\"], (int, float)), (\n                    f\"Invalid 'low' value in hyperparameter configuration for uniform {hparam}:\"\n                    f\" got {config['low']} of type {type(config['low'])}\"\n                )\n                assert isinstance(config[\"high\"], (int, float)), (\n                    f\"Invalid 'high' value in hyperparameter configuration for uniform {hparam}:\"\n                    f\" got {config['high']} of type {type(config['high'])}\"\n                )\n                assert config[\"low\"] &lt; config[\"high\"], (\n                    f\"'low' value must be less than 'high' value in hyperparameter configuration for uniform {hparam}:\"\n                    f\" got low={config['low']}, high={config['high']}\"\n                )\n                hpdistributions[hparam] = scipy.stats.uniform(config[\"low\"], config[\"high\"] - config[\"low\"])\n            case \"loguniform\":\n                assert \"low\" in config, f\"Missing 'low' key in hyperparameter configuration for loguniform {hparam}\"\n                assert \"high\" in config, f\"Missing 'high' key in hyperparameter configuration for loguniform {hparam}\"\n                assert isinstance(config[\"low\"], (int, float)), (\n                    f\"Invalid 'low' value in hyperparameter configuration for loguniform {hparam}:\"\n                    f\" got {config['low']} of type {type(config['low'])}\"\n                )\n                assert isinstance(config[\"high\"], (int, float)), (\n                    f\"Invalid 'high' value in hyperparameter configuration for loguniform {hparam}:\"\n                    f\" got {config['high']} of type {type(config['high'])}\"\n                )\n                assert config[\"low\"] &lt; config[\"high\"], (\n                    f\"'low' must be less than 'high' in hyperparameter configuration for loguniform {hparam}:\"\n                    f\" got low={config['low']}, high={config['high']}\"\n                )\n                hpdistributions[hparam] = scipy.stats.loguniform(config[\"low\"], config[\"high\"])\n            case \"intuniform\":\n                assert \"low\" in config, f\"Missing 'low' key in hyperparameter configuration for int_uniform {hparam}\"\n                assert \"high\" in config, f\"Missing 'high' key in hyperparameter configuration for int_uniform {hparam}\"\n                assert isinstance(config[\"low\"], int), (\n                    f\"'low' must be an integer in hyperparameter configuration for intuniform {hparam}:\"\n                    f\" got {config['low']} of type {type(config['low'])}\"\n                )\n                assert isinstance(config[\"high\"], int), (\n                    f\"'high' must be an integer in hyperparameter configuration for intuniform {hparam}:\"\n                    f\" got {config['high']} of type {type(config['high'])}\"\n                )\n                assert config[\"low\"] &lt; config[\"high\"], (\n                    f\"'low' must be less than 'high' in hyperparameter configuration for intuniform {hparam}:\"\n                    f\" got low={config['low']}, high={config['high']}\"\n                )\n                hpdistributions[hparam] = scipy.stats.randint(config[\"low\"], config[\"high\"] + 1)\n            case \"choice\":\n                assert \"choices\" in config, f\"Missing 'choices' key in hyperparameter configuration for choice {hparam}\"\n                hpdistributions[hparam] = config[\"choices\"]\n            case \"constant\":\n                assert \"value\" in config, f\"Missing 'value' key in hyperparameter configuration for constant {hparam}\"\n                hpdistributions[hparam] = [config[\"value\"]]\n            case _:\n                raise ValueError(f\"Invalid hyperparameter type: {config['distribution']}\")\n\n    return hpdistributions\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.sample_hyperparameters","title":"sample_hyperparameters","text":"<pre><code>sample_hyperparameters(\n    param_grid: dict[\n        str,\n        list\n        | scipy.stats.rv_discrete\n        | scipy.stats.rv_continuous,\n    ],\n    n_trials: int | typing.Literal[\"grid\"] = 100,\n) -&gt; list[\n    darts_segmentation.training.hparams.Hyperparameters\n]\n</code></pre> <p>Sample hyperparameters from a parameter grid.</p> <p>This function samples a list of hyperparameter combinations from a parameter grid. It supports both random sampling and grid search.</p> <p>Parameters:</p> <ul> <li> <code>param_grid</code>               (<code>dict</code>)           \u2013            <p>Dictionary of hyperparameters to tune and their distributions. Values can be lists of values or scipy.stats distribution objects.</p> </li> <li> <code>n_trials</code>               (<code>int | typing.Literal['grid']</code>, default:                   <code>100</code> )           \u2013            <p>Number of hyperparameter combinations to sample. If set to \"grid\", will perform a grid search over all possible combinations. Defaults to 100.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list[darts_segmentation.training.hparams.Hyperparameters]</code> )          \u2013            <p>List of dictionaries, where each dictionary represents a hyperparameter combination.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If n_trials is not an integer (saying a random search) or 'grid'.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/hparams.py</code> <pre><code>def sample_hyperparameters(\n    param_grid: dict[str, \"list | scipy.stats.rv_discrete | scipy.stats.rv_continuous\"],\n    n_trials: int | Literal[\"grid\"] = 100,\n) -&gt; list[Hyperparameters]:\n    \"\"\"Sample hyperparameters from a parameter grid.\n\n    This function samples a list of hyperparameter combinations from a parameter grid.\n    It supports both random sampling and grid search.\n\n    Args:\n        param_grid (dict): Dictionary of hyperparameters to tune and their distributions.\n            Values can be lists of values or scipy.stats distribution objects.\n        n_trials (int | Literal[\"grid\"], optional): Number of hyperparameter combinations to sample.\n            If set to \"grid\", will perform a grid search over all possible combinations.\n            Defaults to 100.\n\n    Returns:\n        list: List of dictionaries, where each dictionary represents a hyperparameter combination.\n\n    Raises:\n        ValueError: If n_trials is not an integer (saying a random search) or 'grid'.\n\n    \"\"\"\n    from sklearn.model_selection import ParameterSampler\n\n    # Check if the parameter of the grid are valid (part of Hyperparameters cclass)\n    for hparam in param_grid.keys():\n        assert hparam in HP_NAMES, f\"Invalid hyperparameter: {hparam} in config but not part of valid {HP_NAMES=}\"\n\n    # Random search\n    if isinstance(n_trials, int):\n        param_list = list(ParameterSampler(param_grid, n_iter=n_trials, random_state=42))\n    elif n_trials == \"grid\":\n        n_combinations = 1\n        for hparam, choices in param_grid.items():\n            assert isinstance(choices, list), (\n                f\"In a grid search, each parameter must be a list of choices. Got {type(choices)} for {hparam}.\"\n            )\n            n_combinations *= len(choices)\n        param_list = list(ParameterSampler(param_grid, n_iter=n_combinations, random_state=42))\n    else:\n        raise ValueError(\n            f\"Invalid value for n_trials: {n_trials}. Must be an integer to perform a random search or 'grid'.\"\n        )\n\n    # Convert to Hyperparameters objects\n    param_list = [Hyperparameters(**params) for params in param_list]\n\n    return param_list\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/","title":"module","text":""},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module","title":"darts_segmentation.training.module","text":"<p>Training script for DARTS segmentation.</p>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP","title":"LitSMP","text":"<pre><code>LitSMP(\n    config: darts_segmentation.segment.SMPSegmenterConfig,\n    learning_rate: float = 1e-05,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    **kwargs: dict[str, typing.Any],\n)\n</code></pre> <p>               Bases: <code>lightning.LightningModule</code></p> <p>Lightning module for training a segmentation model using the segmentation_models_pytorch library.</p> <p>Initialize the LitSMP.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>darts_segmentation.segment.SMPSegmenterConfig</code>)           \u2013            <p>Configuration for the segmentation model.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>1e-05</code> )           \u2013            <p>Initial learning rate. Defaults to 1e-5.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>kwargs</code>               (<code>dict[str, typing.Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments which should be saved to the hyperparameter file.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def __init__(\n    self,\n    config: SMPSegmenterConfig,\n    learning_rate: float = 1e-5,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Initialize the LitSMP.\n\n    Args:\n        config (SMPSegmenterConfig): Configuration for the segmentation model.\n        learning_rate (float, optional): Initial learning rate. Defaults to 1e-5.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        kwargs (dict[str, Any]): Additional keyword arguments which should be saved to the hyperparameter file.\n\n    \"\"\"\n    super().__init__()\n\n    # This saves config, learning_rate and gamma under self.hparams\n    self.save_hyperparameters()\n    self.model = smp.create_model(**config[\"model\"], activation=\"sigmoid\")\n\n    # Assumes that the training preparation was done with setting invalid pixels in the mask to 2\n    self.loss_fn = smp.losses.FocalLoss(\n        mode=\"binary\", alpha=focal_loss_alpha, gamma=focal_loss_gamma, ignore_index=2\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.loss_fn","title":"loss_fn  <code>instance-attribute</code>","text":"<pre><code>loss_fn = segmentation_models_pytorch.losses.FocalLoss(\n    mode=\"binary\",\n    alpha=darts_segmentation.training.module.LitSMP(\n        focal_loss_alpha\n    ),\n    gamma=darts_segmentation.training.module.LitSMP(\n        focal_loss_gamma\n    ),\n    ignore_index=2,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = segmentation_models_pytorch.create_model(\n    **(\n        darts_segmentation.training.module.LitSMP(config)[\n            \"model\"\n        ]\n    ),\n    activation=\"sigmoid\",\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def __repr__(self):  # noqa: D105\n    return f\"LitSMP({self.hparams['config']['model']})\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def configure_optimizers(self):  # noqa: D102\n    optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.hparams.gamma)\n    return [optimizer], [scheduler]\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def on_train_epoch_end(self):  # noqa: D102\n    self.log(\"learning_rate\", self.lr_schedulers().get_last_lr()[0])\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def test_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def training_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def validation_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.SMPSegmenterConfig","title":"SMPSegmenterConfig","text":"<p>               Bases: <code>typing.TypedDict</code></p> <p>Configuration for the segmentor.</p>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.SMPSegmenterConfig.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands: list[str]\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.SMPSegmenterConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: dict[str, typing.Any]\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.SMPSegmenterConfig.from_ckpt","title":"from_ckpt  <code>classmethod</code>","text":"<pre><code>from_ckpt(\n    ckpt: dict[str, typing.Any],\n) -&gt; darts_segmentation.segment.SMPSegmenterConfig\n</code></pre> <p>Load and validate the config from a checkpoint for the segmentor.</p> <p>Parameters:</p> <ul> <li> <code>ckpt</code>               (<code>dict[str, typing.Any]</code>)           \u2013            <p>The checkpoint to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>darts_segmentation.segment.SMPSegmenterConfig</code>           \u2013            <p>The configuration.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@classmethod\ndef from_ckpt(cls, ckpt: dict[str, Any]) -&gt; \"SMPSegmenterConfig\":\n    \"\"\"Load and validate the config from a checkpoint for the segmentor.\n\n    Args:\n        ckpt: The checkpoint to load.\n\n    Returns:\n        The configuration.\n\n    \"\"\"\n    # Legacy version: config and directly in ckpt\n    if \"config\" in ckpt:\n        config = ckpt[\"config\"]\n        # Handling legacy case that the config contains the old keys\n        if \"input_combination\" in config and \"norm_factors\" in config:\n            # Check if all input_combination features are in norm_factors\n            config[\"bands\"] = config[\"input_combination\"]\n            config.pop(\"norm_factors\")\n            config.pop(\"input_combination\")\n        # Another legacy case uses a deprecated \"Bands\" class, which is pickled into the config as dict\n        if isinstance(config[\"bands\"], dict):\n            config[\"bands\"] = config[\"bands\"][\"bands\"]\n    # New version: load directly from lightning checkpoint\n    else:\n        config = ckpt[\"hyper_parameters\"][\"config\"]\n\n    assert \"model\" in config, \"Model config is missing!\"\n    assert \"bands\" in config, \"Bands config is missing!\"\n    return config\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/","title":"prepare_training","text":""},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training","title":"darts_segmentation.training.prepare_training","text":"<p>Functions to prepare the training data for the segmentation model training.</p>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords","title":"PatchCoords  <code>dataclass</code>","text":"<pre><code>PatchCoords(\n    i: int,\n    patch_idx_y: int,\n    patch_idx_x: int,\n    y: slice,\n    x: slice,\n)\n</code></pre> <p>Wrapper which stores the coordinate information of a patch in the original image.</p>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.i","title":"i  <code>instance-attribute</code>","text":"<pre><code>i: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.patch_idx_x","title":"patch_idx_x  <code>instance-attribute</code>","text":"<pre><code>patch_idx_x: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.patch_idx_y","title":"patch_idx_y  <code>instance-attribute</code>","text":"<pre><code>patch_idx_y: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: slice\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: slice\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.from_tensor","title":"from_tensor  <code>classmethod</code>","text":"<pre><code>from_tensor(\n    coords: torch.Tensor, patch_size: int\n) -&gt; (\n    darts_segmentation.training.prepare_training.PatchCoords\n)\n</code></pre> <p>Create a PatchCoords object from the returned coord tensor of <code>create_patches</code>.</p> <p>Parameters:</p> <ul> <li> <code>coords</code>               (<code>torch.Tensor</code>)           \u2013            <p>The coordinates of the patch in the original image, from <code>create_patches</code>.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PatchCoords</code> (              <code>darts_segmentation.training.prepare_training.PatchCoords</code> )          \u2013            <p>The coordinates of the patch in the original image.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>@classmethod\ndef from_tensor(cls, coords: torch.Tensor, patch_size: int) -&gt; \"PatchCoords\":\n    \"\"\"Create a PatchCoords object from the returned coord tensor of `create_patches`.\n\n    Args:\n        coords (torch.Tensor): The coordinates of the patch in the original image, from `create_patches`.\n        patch_size (int): The size of the patch.\n\n    Returns:\n        PatchCoords: The coordinates of the patch in the original image.\n\n    \"\"\"\n    i, y, x, h, w = coords.int().numpy()\n    return cls(\n        i=i,\n        patch_idx_y=h.item(),\n        patch_idx_x=w.item(),\n        y=slice(y.item(), y.item() + patch_size),\n        x=slice(x.item(), x.item() + patch_size),\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder","title":"TrainDatasetBuilder  <code>dataclass</code>","text":"<pre><code>TrainDatasetBuilder(\n    train_data_dir: pathlib.Path,\n    patch_size: int,\n    overlap: int,\n    bands: list[str],\n    exclude_nopositive: bool,\n    exclude_nan: bool,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n    append: bool = False,\n)\n</code></pre> <p>Helper class to create all necessary files for a DARTS training dataset.</p> <p>This class manages the creation of a training dataset stored in Zarr format with associated metadata. It handles patch creation, quality filtering, and metadata tracking.</p> The dataset structure <ul> <li>data.zarr/x: Input patches (N, C, H, W) as float32</li> <li>data.zarr/y: Label patches (N, H, W) as uint8 with values 0/1/2</li> <li>metadata.parquet: Patch metadata including coordinates and geometry</li> <li>config.toml: Dataset configuration and parameters</li> </ul> <p>Attributes:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Directory where the dataset will be saved.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Size of each patch in pixels.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>Overlap between adjacent patches in pixels.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>List of band names to include in the dataset.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>)           \u2013            <p>Exclude patches without positive labels.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>)           \u2013            <p>Exclude patches with any NaN values.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>Device for patch creation operations.</p> </li> <li> <code>append</code>               (<code>bool</code>)           \u2013            <p>If True, append to existing dataset. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.append","title":"append  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>append: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands: list[str]\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: typing.Literal['cuda', 'cpu'] | int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.exclude_nan","title":"exclude_nan  <code>instance-attribute</code>","text":"<pre><code>exclude_nan: bool\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.exclude_nopositive","title":"exclude_nopositive  <code>instance-attribute</code>","text":"<pre><code>exclude_nopositive: bool\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.overlap","title":"overlap  <code>instance-attribute</code>","text":"<pre><code>overlap: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.train_data_dir","title":"train_data_dir  <code>instance-attribute</code>","text":"<pre><code>train_data_dir: pathlib.Path\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def __len__(self):  # noqa: D105\n    return len(self._metadata)\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize the TrainDatasetBuilder class based on provided dataclass params.</p> <p>This will setup everything needed to add patches to the dataset:</p> <ul> <li>Create the train_data_dir if it does not exist</li> <li>Create an emtpy zarr store</li> <li>Initialize the metadata list</li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the TrainDatasetBuilder class based on provided dataclass params.\n\n    This will setup everything needed to add patches to the dataset:\n\n    - Create the train_data_dir if it does not exist\n    - Create an emtpy zarr store\n    - Initialize the metadata list\n    \"\"\"\n    lovely_tensors.monkey_patch()\n    lovely_tensors.set_config(color=False)\n    self._metadata = []\n    if self.append and (self.train_data_dir / \"metadata.parquet\").exists():\n        self._metadata = gpd.read_parquet(self.train_data_dir / \"metadata.parquet\").to_dict(orient=\"records\")\n\n    self.train_data_dir.mkdir(exist_ok=True, parents=True)\n\n    self._zroot = zarr.group(store=LocalStore(self.train_data_dir / \"data.zarr\"), overwrite=not self.append)\n    # We need do declare the number of patches to 0, because we can't know the final number of patches\n\n    if not self.append:\n        self._zroot.create(\n            name=\"x\",\n            shape=(0, len(self.bands), self.patch_size, self.patch_size),\n            # shards=(100, len(bands), patch_size, patch_size),\n            chunks=(1, 1, self.patch_size, self.patch_size),\n            dtype=\"float32\",\n            compressors=BloscCodec(cname=\"lz4\", clevel=9),\n        )\n        self._zroot.create(\n            name=\"y\",\n            shape=(0, self.patch_size, self.patch_size),\n            # shards=(100, patch_size, patch_size),\n            chunks=(1, self.patch_size, self.patch_size),\n            dtype=\"uint8\",\n            compressors=BloscCodec(cname=\"lz4\", clevel=9),\n        )\n    else:\n        assert \"x\" in self._zroot and \"y\" in self._zroot, (\n            \"When appending to an existing dataset, the 'x' and 'y' arrays must already exist.\"\n            \"Did you set append=True by accident?\"\n        )\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.add_tile","title":"add_tile","text":"<pre><code>add_tile(\n    tile: xarray.Dataset,\n    labels: geopandas.GeoDataFrame,\n    region: str,\n    sample_id: str,\n    extent: geopandas.GeoDataFrame | None = None,\n    metadata: dict[str, str] | None = None,\n)\n</code></pre> <p>Add a tile to the dataset by creating and appending patches.</p> <p>This method processes a single tile by creating training patches and appending them to the Zarr arrays. Patch metadata including coordinates, geometry, and custom fields are tracked for later use.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data. Must contain the specified bands and a 'quality_data_mask' variable.</p> </li> <li> <code>labels</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The labels to be used for training. Geometries will be rasterized as positive samples (class 1).</p> </li> <li> <code>region</code>               (<code>str</code>)           \u2013            <p>The region identifier for this tile (e.g., \"Alaska\", \"Canada\"). Stored in metadata for tracking and filtering.</p> </li> <li> <code>sample_id</code>               (<code>str</code>)           \u2013            <p>A unique identifier for this tile/sample. Stored in metadata for tracking and filtering.</p> </li> <li> <code>extent</code>               (<code>geopandas.GeoDataFrame | None</code>, default:                   <code>None</code> )           \u2013            <p>The extent of the valid training area. Pixels outside this extent will be marked as invalid (class 2) in labels. If None, no extent masking is applied.</p> </li> <li> <code>metadata</code>               (<code>dict[str, str]</code>, default:                   <code>None</code> )           \u2013            <p>Additional metadata to be added to the metadata file. Will not be used for training, but can be used for debugging or reproducibility. Path values will be automatically converted to strings.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def add_tile(\n    self,\n    tile: xr.Dataset,\n    labels: gpd.GeoDataFrame,\n    region: str,\n    sample_id: str,\n    extent: gpd.GeoDataFrame | None = None,\n    metadata: dict[str, str] | None = None,\n):\n    \"\"\"Add a tile to the dataset by creating and appending patches.\n\n    This method processes a single tile by creating training patches and appending them\n    to the Zarr arrays. Patch metadata including coordinates, geometry, and custom fields\n    are tracked for later use.\n\n    Args:\n        tile (xr.Dataset): The input tile, containing preprocessed, harmonized data.\n            Must contain the specified bands and a 'quality_data_mask' variable.\n        labels (gpd.GeoDataFrame): The labels to be used for training.\n            Geometries will be rasterized as positive samples (class 1).\n        region (str): The region identifier for this tile (e.g., \"Alaska\", \"Canada\").\n            Stored in metadata for tracking and filtering.\n        sample_id (str): A unique identifier for this tile/sample.\n            Stored in metadata for tracking and filtering.\n        extent (gpd.GeoDataFrame | None, optional): The extent of the valid training area.\n            Pixels outside this extent will be marked as invalid (class 2) in labels.\n            If None, no extent masking is applied.\n        metadata (dict[str, str], optional): Additional metadata to be added to the metadata file.\n            Will not be used for training, but can be used for debugging or reproducibility.\n            Path values will be automatically converted to strings.\n\n    \"\"\"\n    metadata = metadata or {}\n    # Convert all paths of metadata to strings\n    metadata = {k: str(v) if isinstance(v, Path) else v for k, v in metadata.items()}\n\n    x, y, stacked_coords = create_training_patches(\n        tile=tile,\n        labels=labels,\n        extent=extent,\n        bands=self.bands,\n        patch_size=self.patch_size,\n        overlap=self.overlap,\n        exclude_nopositive=self.exclude_nopositive,\n        exclude_nan=self.exclude_nan,\n        device=self.device,\n    )\n\n    self._zroot[\"x\"].append(x.numpy().astype(\"float32\"))\n    self._zroot[\"y\"].append(y.numpy().astype(\"uint8\"))\n\n    for patch_id, coords in enumerate(stacked_coords):\n        geometry = tile.isel(x=coords.x, y=coords.y).odc.geobox.geographic_extent.geom\n        self._metadata.append(\n            {\n                \"z_idx\": len(self._metadata),\n                \"patch_id\": patch_id,\n                \"region\": region,\n                \"sample_id\": sample_id,\n                \"empty\": not (y[patch_id] == 1).any(),\n                \"x\": coords.x.start,\n                \"y\": coords.y.start,\n                \"patch_idx_x\": coords.patch_idx_x,\n                \"patch_idx_y\": coords.patch_idx_y,\n                \"geometry\": geometry,\n                **metadata,\n            }\n        )\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.finalize","title":"finalize","text":"<pre><code>finalize(data_config: dict[str, str] | None = None)\n</code></pre> <p>Finalize the dataset by saving the metadata and the config file.</p> <p>Parameters:</p> <ul> <li> <code>data_config</code>               (<code>dict[str, str]</code>, default:                   <code>None</code> )           \u2013            <p>The data config to be saved in the config file. This should contain all the information needed to recreate the dataset. It will be saved as a toml file, along with the configuration provided in this dataclass.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no patches were found in the dataset.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def finalize(self, data_config: dict[str, str] | None = None):\n    \"\"\"Finalize the dataset by saving the metadata and the config file.\n\n    Args:\n        data_config (dict[str, str], optional): The data config to be saved in the config file.\n            This should contain all the information needed to recreate the dataset.\n            It will be saved as a toml file, along with the configuration provided in this dataclass.\n\n    Raises:\n        ValueError: If no patches were found in the dataset.\n\n    \"\"\"\n    if len(self._metadata) == 0:\n        logger.error(\"No patches found in the dataset.\", exc_info=True)\n        raise ValueError(\"No patches found in the dataset.\")\n\n    # Save the metadata\n    metadata = gpd.GeoDataFrame(self._metadata, crs=\"EPSG:4326\")\n    metadata.to_parquet(self.train_data_dir / \"metadata.parquet\")\n\n    data_config = data_config or {}\n    # Convert the data_config paths to strings\n    data_config = {k: str(v) if isinstance(v, Path) else v for k, v in data_config.items()}\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"train_data_dir\": str(self.train_data_dir),\n            \"patch_size\": self.patch_size,\n            \"overlap\": self.overlap,\n            \"n_bands\": len(self.bands),\n            \"exclude_nopositive\": self.exclude_nopositive,\n            \"exclude_nan\": self.exclude_nan,\n            \"n_patches\": len(metadata),\n            \"device\": self.device,\n            \"bands\": self.bands,  # keys: bands, band_factors, band_offsets\n            **data_config,\n        }\n    }\n    with open(self.train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {len(metadata)} patches to {self.train_data_dir}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.create_labels","title":"create_labels","text":"<pre><code>create_labels(\n    tile: xarray.Dataset,\n    labels: geopandas.GeoDataFrame,\n    extent: geopandas.GeoDataFrame | None = None,\n) -&gt; xarray.DataArray\n</code></pre> <p>Create rasterized labels from vector labels and quality mask.</p> <p>This function rasterizes the provided labels and applies quality filtering based on the tile's quality_data_mask. Areas outside the extent or with low quality are marked as invalid (class 2).</p> Label encoding <ul> <li>0: Negative (no RTS)</li> <li>1: Positive (RTS present)</li> <li>2: Invalid/masked (outside extent, low quality, or no data)</li> </ul> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data. Must contain a 'quality_data_mask' variable where value 2 indicates best quality.</p> </li> <li> <code>labels</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The labels to be used for training. Geometries will be rasterized as positive samples (class 1).</p> </li> <li> <code>extent</code>               (<code>geopandas.GeoDataFrame | None</code>, default:                   <code>None</code> )           \u2013            <p>The extent of the valid training area. Pixels outside this extent will be marked as invalid (class 2). If None, no extent masking is applied.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: The rasterized labels with shape (y, x) and values 0, 1, or 2. Pixels with quality_data_mask != 2 are automatically marked as invalid (class 2).</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def create_labels(\n    tile: xr.Dataset,\n    labels: gpd.GeoDataFrame,\n    extent: gpd.GeoDataFrame | None = None,\n) -&gt; xr.DataArray:\n    \"\"\"Create rasterized labels from vector labels and quality mask.\n\n    This function rasterizes the provided labels and applies quality filtering based on\n    the tile's quality_data_mask. Areas outside the extent or with low quality are marked\n    as invalid (class 2).\n\n    Label encoding:\n        - 0: Negative (no RTS)\n        - 1: Positive (RTS present)\n        - 2: Invalid/masked (outside extent, low quality, or no data)\n\n    Args:\n        tile (xr.Dataset): The input tile, containing preprocessed, harmonized data.\n            Must contain a 'quality_data_mask' variable where value 2 indicates best quality.\n        labels (gpd.GeoDataFrame): The labels to be used for training.\n            Geometries will be rasterized as positive samples (class 1).\n        extent (gpd.GeoDataFrame | None): The extent of the valid training area.\n            Pixels outside this extent will be marked as invalid (class 2).\n            If None, no extent masking is applied.\n\n    Returns:\n        xr.DataArray: The rasterized labels with shape (y, x) and values 0, 1, or 2.\n            Pixels with quality_data_mask != 2 are automatically marked as invalid (class 2).\n\n    \"\"\"\n    # Rasterize the labels\n    if len(labels) &gt; 0:\n        labels[\"id\"] = labels.index\n        labels_rasterized = 1 - make_geocube(labels, measurements=[\"id\"], like=tile[\"quality_data_mask\"]).id.isnull()\n    else:\n        labels_rasterized = xr.zeros_like(tile[\"quality_data_mask\"])\n\n    # Rasterize the extent if provided\n    if extent is not None:\n        extent[\"id\"] = extent.index\n        extent_rasterized = 1 - make_geocube(extent, measurements=[\"id\"], like=tile[\"quality_data_mask\"]).id.isnull()\n        labels_rasterized = labels_rasterized.where(extent_rasterized, 2)\n\n    # Because rasterio use different floats, it can happen that the axes are not properly aligned\n    labels_rasterized[\"x\"] = tile.x\n    labels_rasterized[\"y\"] = tile.y\n\n    # Filter out low-quality and no-data values (class 2 -&gt; best quality)\n    quality_mask = tile[\"quality_data_mask\"] == 2\n    # quality_mask = erode_mask(tile[\"quality_data_mask\"] == 2, mask_erosion_size, device)\n    labels_rasterized = labels_rasterized.where(quality_mask, 2)\n\n    return labels_rasterized\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.create_patches","title":"create_patches","text":"<pre><code>create_patches(\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    return_coords: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Create patches from a tensor.</p> <p>Parameters:</p> <ul> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>return_coords</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/inference.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.create_training_patches","title":"create_training_patches","text":"<pre><code>create_training_patches(\n    tile: xarray.Dataset,\n    labels: geopandas.GeoDataFrame,\n    extent: geopandas.GeoDataFrame | None,\n    bands: list[str],\n    patch_size: int,\n    overlap: int,\n    exclude_nopositive: bool,\n    exclude_nan: bool,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; tuple[\n    torch.tensor,\n    torch.tensor,\n    list[\n        darts_segmentation.training.prepare_training.PatchCoords\n    ],\n]\n</code></pre> <p>Create training patches from a tile and labels with quality filtering.</p> <p>This function creates overlapping patches from the input tile and rasterized labels, applying several filtering criteria to ensure high-quality training data. Pixels with quality_data_mask == 0 are set to NaN in the input data.</p> Patch filtering <ul> <li>Excludes patches with &lt; 10% visible pixels (&gt; 90% invalid/masked)</li> <li>Excludes patches where all bands are NaN</li> <li>Optionally excludes patches without positive labels (if exclude_nopositive=True)</li> <li>Optionally excludes patches with any NaN values (if exclude_nan=True)</li> </ul> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data. Must contain a 'quality_data_mask' variable where 0=invalid and 2=best quality.</p> </li> <li> <code>labels</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The labels to be used for training. Geometries will be rasterized as positive samples.</p> </li> <li> <code>extent</code>               (<code>geopandas.GeoDataFrame | None</code>)           \u2013            <p>The extent of the valid training area. Pixels outside this extent will be marked as invalid in labels. If None, no extent masking is applied.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to extract and use for training. Will be normalized using the band manager.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of each patch in pixels (height and width).</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The overlap between adjacent patches in pixels.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>)           \u2013            <p>Whether to exclude patches where the labels do not contain any positive samples (class 1).</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>)           \u2013            <p>Whether to exclude patches where the input data has any NaN values.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for tensor operations. Can be \"cuda\", \"cpu\", or an integer GPU index.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[torch.tensor, torch.tensor, list[darts_segmentation.training.prepare_training.PatchCoords]]</code>           \u2013            <p>tuple[torch.tensor, torch.tensor, list[PatchCoords]]: A tuple containing: - Input patches with shape (N, C, H, W), NaN values replaced with 0 - Label patches with shape (N, H, W), values 0/1/2 - List of PatchCoords objects with coordinate information</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def create_training_patches(\n    tile: xr.Dataset,\n    labels: gpd.GeoDataFrame,\n    extent: gpd.GeoDataFrame | None,\n    bands: list[str],\n    patch_size: int,\n    overlap: int,\n    exclude_nopositive: bool,\n    exclude_nan: bool,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; tuple[torch.tensor, torch.tensor, list[PatchCoords]]:\n    \"\"\"Create training patches from a tile and labels with quality filtering.\n\n    This function creates overlapping patches from the input tile and rasterized labels,\n    applying several filtering criteria to ensure high-quality training data. Pixels with\n    quality_data_mask == 0 are set to NaN in the input data.\n\n    Patch filtering:\n        - Excludes patches with &lt; 10% visible pixels (&gt; 90% invalid/masked)\n        - Excludes patches where all bands are NaN\n        - Optionally excludes patches without positive labels (if exclude_nopositive=True)\n        - Optionally excludes patches with any NaN values (if exclude_nan=True)\n\n    Args:\n        tile (xr.Dataset): The input tile, containing preprocessed, harmonized data.\n            Must contain a 'quality_data_mask' variable where 0=invalid and 2=best quality.\n        labels (gpd.GeoDataFrame): The labels to be used for training.\n            Geometries will be rasterized as positive samples.\n        extent (gpd.GeoDataFrame | None): The extent of the valid training area.\n            Pixels outside this extent will be marked as invalid in labels.\n            If None, no extent masking is applied.\n        bands (list[str]): The bands to extract and use for training.\n            Will be normalized using the band manager.\n        patch_size (int): The size of each patch in pixels (height and width).\n        overlap (int): The overlap between adjacent patches in pixels.\n        exclude_nopositive (bool): Whether to exclude patches where the labels do not contain\n            any positive samples (class 1).\n        exclude_nan (bool): Whether to exclude patches where the input data has any NaN values.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for tensor operations.\n            Can be \"cuda\", \"cpu\", or an integer GPU index.\n\n    Returns:\n        tuple[torch.tensor, torch.tensor, list[PatchCoords]]: A tuple containing:\n            - Input patches with shape (N, C, H, W), NaN values replaced with 0\n            - Label patches with shape (N, H, W), values 0/1/2\n            - List of PatchCoords objects with coordinate information\n\n    \"\"\"\n    if len(labels) == 0 and exclude_nopositive:\n        logger.warning(\"No labels found in the labels GeoDataFrame. Skipping.\")\n        return\n\n    # Rasterize the labels\n    labels_rasterized = create_labels(tile, labels, extent)\n    tensor_labels = torch.tensor(labels_rasterized.values, device=device).float()\n\n    invalid_mask = (tile[\"quality_data_mask\"] == 0).data\n    tile = tile[bands].transpose(\"y\", \"x\")\n    tile = manager.normalize(tile)\n    tensor_tile = torch.as_tensor(tile.to_dataarray().data, device=device).float()\n    tensor_tile[:, invalid_mask] = float(\"nan\")  # Set invalid pixels to NaN\n\n    assert tensor_tile.dim() == 3, f\"Expects tensor_tile to has shape (C, H, W), got {tensor_tile.shape}\"\n    assert tensor_labels.dim() == 2, f\"Expects tensor_labels to has shape (H, W), got {tensor_labels.shape}\"\n\n    # Create patches\n    n_bands = len(bands)\n    tensor_patches = create_patches(tensor_tile.unsqueeze(0), patch_size, overlap)\n    tensor_patches = tensor_patches.reshape(-1, n_bands, patch_size, patch_size)\n    tensor_labels, tensor_coords = create_patches(\n        tensor_labels.unsqueeze(0).unsqueeze(0), patch_size, overlap, return_coords=True\n    )\n    tensor_labels = tensor_labels.reshape(-1, patch_size, patch_size)\n    tensor_coords = tensor_coords.reshape(-1, 5).to(device=device)\n\n    # Filter out patches based on settings\n    few_visible = ((tensor_labels != 2).sum(dim=(1, 2)) / tensor_labels[0].numel()) &lt; 0.1\n    logger.debug(f\"Excluding {few_visible.sum().item()} patches with less than 10% visible pixels\")\n    all_nans = torch.isnan(tensor_patches).all(dim=(2, 3)).any(dim=1)\n    logger.debug(f\"Excluding {all_nans.sum().item()} patches where everything is nan\")\n    filter_mask = few_visible | all_nans\n    if exclude_nopositive:\n        nopositives = (tensor_labels == 1).any(dim=(1, 2))\n        logger.debug(f\"Excluding {nopositives.sum().item()} patches with no positive labels\")\n        filter_mask |= ~nopositives\n    if exclude_nan:\n        has_nans = torch.isnan(tensor_patches).any(dim=(1, 2, 3))\n        logger.debug(f\"Excluding {has_nans.sum().item()} patches with nan values\")\n        filter_mask |= has_nans\n\n    n_patches = tensor_patches.shape[0]\n    logger.debug(f\"Using {n_patches - filter_mask.sum().item()} patches out of {n_patches} total patches\")\n\n    tensor_patches = tensor_patches[~filter_mask].cpu()\n    tensor_labels = tensor_labels[~filter_mask].cpu()\n    tensor_coords = tensor_coords[~filter_mask].cpu()\n    free_torch()\n    coords = [PatchCoords.from_tensor(tensor_coords[i], patch_size) for i in range(tensor_coords.shape[0])]\n    # Fill nan with 0, since we don't want to have NaNs in the patches\n    tensor_patches = tensor_patches.nan_to_num(0.0)\n    return tensor_patches, tensor_labels, coords\n</code></pre>"},{"location":"reference/darts_segmentation/training/scoring/","title":"scoring","text":""},{"location":"reference/darts_segmentation/training/scoring/#darts_segmentation.training.scoring","title":"darts_segmentation.training.scoring","text":"<p>Scoring calculation.</p>"},{"location":"reference/darts_segmentation/training/scoring/#darts_segmentation.training.scoring.check_score_is_unstable","title":"check_score_is_unstable","text":"<pre><code>check_score_is_unstable(\n    run_info: dict, scoring_metric: list[str] | str\n) -&gt; bool\n</code></pre> <p>Check the stability of the scoring metric.</p> <p>If any metric value is not finite or equal to zero, the scoring metric is considered unstable.</p> <p>Parameters:</p> <ul> <li> <code>run_info</code>               (<code>dict</code>)           \u2013            <p>The run information.</p> </li> <li> <code>scoring_metric</code>               (<code>list[str] | str</code>)           \u2013            <p>The scoring metric.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the scoring metric is unstable, False otherwise.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown scoring metric type is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/scoring.py</code> <pre><code>def check_score_is_unstable(run_info: dict, scoring_metric: list[str] | str) -&gt; bool:\n    \"\"\"Check the stability of the scoring metric.\n\n    If any metric value is not finite or equal to zero, the scoring metric is considered unstable.\n\n    Args:\n        run_info (dict): The run information.\n        scoring_metric (list[str] | str): The scoring metric.\n\n    Returns:\n        bool: True if the scoring metric is unstable, False otherwise.\n\n    Raises:\n        ValueError: If an unknown scoring metric type is provided.\n\n    \"\"\"\n    # Single score in list\n    if isinstance(scoring_metric, list) and len(scoring_metric) == 1:\n        scoring_metric = scoring_metric[0]\n\n    if isinstance(scoring_metric, str):\n        metric_value = run_info[scoring_metric]\n        is_unstable = not isfinite(metric_value) or metric_value == 0\n        return is_unstable\n    elif isinstance(scoring_metric, list):\n        metric_values = [run_info[metric] for metric in scoring_metric]\n        is_unstable = any(not isfinite(val) or val == 0 for val in metric_values)\n        return is_unstable\n    else:\n        raise ValueError(\"Invalid scoring metric type\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/scoring/#darts_segmentation.training.scoring.score_from_runs","title":"score_from_runs","text":"<pre><code>score_from_runs(\n    run_infos: list[dict[str, float]],\n    scoring_metric: list[str] | str,\n    multi_score_strategy: typing.Literal[\n        \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n    ] = \"harmonic\",\n) -&gt; float\n</code></pre> <p>Calculate a score from run metrics.</p> <p>Each metric can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each run the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>Ignores unstable runs when multi_score_strategy is \"harmonic\" or \"geometric\" If no runs are left, return 0. An unstable run is where one of the metrics is not finite or zero.</p> <p>Parameters:</p> <ul> <li> <code>run_infos</code>               (<code>list[dict[str, float]]</code>)           \u2013            <p>List of dictionaries containing run information and metrics</p> </li> <li> <code>scoring_metric</code>               (<code>list[str] | str</code>)           \u2013            <p>Metric(s) to use for scoring.</p> </li> <li> <code>multi_score_strategy</code>               (<code>typing.Literal['harmonic', 'arithmetic', 'geometric', 'min']</code>, default:                   <code>'harmonic'</code> )           \u2013            <p>Strategy for combining multiple metrics. Defaults to \"harmonic\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>The calculated score</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown multi-score strategy is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/scoring.py</code> <pre><code>def score_from_runs(  # noqa: C901\n    run_infos: list[dict[str, float]],\n    scoring_metric: list[str] | str,\n    multi_score_strategy: Literal[\"harmonic\", \"arithmetic\", \"geometric\", \"min\"] = \"harmonic\",\n) -&gt; float:\n    \"\"\"Calculate a score from run metrics.\n\n    Each metric can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each run the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    Ignores unstable runs when multi_score_strategy is \"harmonic\" or \"geometric\"\n    If no runs are left, return 0.\n    An unstable run is where one of the metrics is not finite or zero.\n\n    Args:\n        run_infos (list[dict[str, float]]): List of dictionaries containing run information and metrics\n        scoring_metric (list[str] | str): Metric(s) to use for scoring.\n        multi_score_strategy (Literal[\"harmonic\", \"arithmetic\", \"geometric\", \"min\"], optional):\n            Strategy for combining multiple metrics. Defaults to \"harmonic\".\n\n    Returns:\n        float: The calculated score\n\n    Raises:\n        ValueError: If an unknown multi-score strategy is provided.\n\n    \"\"\"\n    # Single score in list\n    if isinstance(scoring_metric, list) and len(scoring_metric) == 1:\n        scoring_metric = scoring_metric[0]\n\n    # Case single score\n    if isinstance(scoring_metric, str):\n        # In case the use set a specific direction\n        scoring_metric = scoring_metric.removesuffix(\":higher\").removesuffix(\":lower\")\n        metric_values = [run_info[scoring_metric] for run_info in run_infos]\n        score = mean(metric_values)\n    # Case multiple scores\n    elif isinstance(scoring_metric, list):\n        scores = []\n        for run_info in run_infos:\n            # Check if we can calculate a score\n            is_unstable = check_score_is_unstable(run_info, scoring_metric)\n            if is_unstable and multi_score_strategy in [\"harmonic\", \"geometric\"]:\n                continue\n            metric_values = []\n            for metric in scoring_metric:\n                higher_is_better = False if metric.endswith(\":lower\") else True\n                metric = metric.removesuffix(\":higher\").removesuffix(\":lower\")\n                val = run_info[metric]\n                metric_values.append(val if higher_is_better else 1 / val)\n\n            match multi_score_strategy:\n                case \"harmonic\":\n                    run_score = harmonic_mean(metric_values)\n                case \"arithmetic\":\n                    run_score = mean(metric_values)\n                case \"min\":\n                    run_score = min(metric_values)\n                case \"geometric\":\n                    run_score = geometric_mean(metric_values)\n                case _:\n                    raise ValueError(\"If an unknown multi-score strategy is provided.\")\n            scores.append(run_score)\n        if len(scores) == 0:\n            score = 0.0\n        elif len(scores) == 1:\n            score = scores[0]\n        else:\n            score = mean(scores)\n\n    return score\n</code></pre>"},{"location":"reference/darts_segmentation/training/scoring/#darts_segmentation.training.scoring.score_from_single_run","title":"score_from_single_run","text":"<pre><code>score_from_single_run(\n    run_info: dict[str, float],\n    scoring_metric: list[str] | str,\n    multi_score_strategy: typing.Literal[\n        \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n    ] = \"harmonic\",\n) -&gt; float\n</code></pre> <p>Calculate a score from run metrics.</p> <p>Each metric can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combining the metrics through the specified strategy. Please refer to the documentation to understand the different multi-score strategies.</p> <p>Unstable runs when multi_score_strategy is \"harmonic\" or \"geometric\" will result in a score of 0. An unstable run is where one of the metrics is not finite or zero.</p> <p>Parameters:</p> <ul> <li> <code>run_info</code>               (<code>dict[str, float]</code>)           \u2013            <p>Dictionary containing run information and metrics</p> </li> <li> <code>scoring_metric</code>               (<code>list[str] | str</code>)           \u2013            <p>Metric(s) to use for scoring.</p> </li> <li> <code>multi_score_strategy</code>               (<code>typing.Literal['harmonic', 'arithmetic', 'geometric', 'min']</code>, default:                   <code>'harmonic'</code> )           \u2013            <p>Strategy for combining multiple metrics. Defaults to \"harmonic\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>The calculated score</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown multi-score strategy is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/scoring.py</code> <pre><code>def score_from_single_run(\n    run_info: dict[str, float],\n    scoring_metric: list[str] | str,\n    multi_score_strategy: Literal[\"harmonic\", \"arithmetic\", \"geometric\", \"min\"] = \"harmonic\",\n) -&gt; float:\n    \"\"\"Calculate a score from run metrics.\n\n    Each metric can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combining the metrics through the specified strategy.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    Unstable runs when multi_score_strategy is \"harmonic\" or \"geometric\" will result in a score of 0.\n    An unstable run is where one of the metrics is not finite or zero.\n\n    Args:\n        run_info (dict[str, float]): Dictionary containing run information and metrics\n        scoring_metric (list[str] | str): Metric(s) to use for scoring.\n        multi_score_strategy (Literal[\"harmonic\", \"arithmetic\", \"geometric\", \"min\"], optional):\n            Strategy for combining multiple metrics. Defaults to \"harmonic\".\n\n    Returns:\n        float: The calculated score\n\n    Raises:\n        ValueError: If an unknown multi-score strategy is provided.\n\n    \"\"\"\n    if isinstance(scoring_metric, str):\n        return run_info[scoring_metric]\n\n    scores = [run_info[metric] for metric in scoring_metric]\n    is_unstable = check_score_is_unstable(run_info, scoring_metric)\n    if is_unstable and multi_score_strategy in [\"harmonic\", \"geometric\"]:\n        return 0.0\n    match multi_score_strategy:\n        case \"harmonic\":\n            return harmonic_mean(scores)\n        case \"arithmetic\":\n            return mean(scores)\n        case \"geometric\":\n            return geometric_mean(scores)\n        case \"min\":\n            return min(scores)\n        case _:\n            raise ValueError(f\"Unknown multi-score strategy: {multi_score_strategy}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/","title":"train","text":""},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train","title":"darts_segmentation.training.train","text":"<p>Training scripts for DARTS.</p>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig","title":"DataConfig  <code>dataclass</code>","text":"<pre><code>DataConfig(\n    train_data_dir: pathlib.Path | None = None,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n        \"none\",\n    ] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n    in_memory: bool = False,\n)\n</code></pre> <p>Data related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>)           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>)           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified', 'none']</code>)           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>)           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>)           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> <li> <code>in_memory</code>               (<code>bool</code>)           \u2013            <p>If True, the dataset will be loaded into memory.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.data_split_by","title":"data_split_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_by: list[str | float] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.data_split_method","title":"data_split_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_method: (\n    typing.Literal[\"random\", \"region\", \"sample\"] | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.fold_method","title":"fold_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold_method: typing.Literal[\n    \"kfold\",\n    \"shuffle\",\n    \"stratified\",\n    \"region\",\n    \"region-stratified\",\n    \"none\",\n] = \"kfold\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.in_memory","title":"in_memory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>in_memory: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.subsample","title":"subsample  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subsample: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.total_folds","title":"total_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_folds: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.train_data_dir","title":"train_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig","title":"DeviceConfig  <code>dataclass</code>","text":"<pre><code>DeviceConfig(\n    accelerator: typing.Literal[\n        \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n    ] = \"auto\",\n    strategy: typing.Literal[\n        \"auto\",\n        \"ddp\",\n        \"ddp_fork\",\n        \"ddp_notebook\",\n        \"fsdp\",\n        \"cv-parallel\",\n        \"tune-parallel\",\n    ] = \"auto\",\n    devices: list[int | str] = (lambda: [\"auto\"])(),\n    num_nodes: int = 1,\n)\n</code></pre> <p>Device and Distributed Strategy related parameters.</p> <p>Attributes:</p> <ul> <li> <code>accelerator</code>               (<code>typing.Literal['auto', 'cpu', 'gpu', 'mps', 'tpu']</code>)           \u2013            <p>Accelerator to use. Defaults to \"auto\".</p> </li> <li> <code>strategy</code>               (<code>typing.Literal['auto', 'ddp', 'ddp_fork', 'ddp_notebook', 'fsdp', 'cv-parallel', 'tune-parallel', 'cv-parallel', 'tune-parallel']</code>)           \u2013            <p>Distributed strategy to use. Defaults to \"auto\".</p> </li> <li> <code>devices</code>               (<code>list[int | str]</code>)           \u2013            <p>List of devices to use. Defaults to [\"auto\"].</p> </li> <li> <code>num_nodes</code>               (<code>int</code>)           \u2013            <p>Number of nodes to use for distributed training. Defaults to 1.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.accelerator","title":"accelerator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>accelerator: typing.Literal[\n    \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int | str] = dataclasses.field(\n    default_factory=lambda: [\"auto\"]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.lightning_strategy","title":"lightning_strategy  <code>property</code>","text":"<pre><code>lightning_strategy: str\n</code></pre> <p>Get the Lightning strategy for the current configuration.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The Lightning strategy to use.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.num_nodes","title":"num_nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_nodes: int = 1\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.strategy","title":"strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strategy: typing.Literal[\n    \"auto\",\n    \"ddp\",\n    \"ddp_fork\",\n    \"ddp_notebook\",\n    \"fsdp\",\n    \"cv-parallel\",\n    \"tune-parallel\",\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.in_parallel","title":"in_parallel","text":"<pre><code>in_parallel(\n    device: int | str | None = None,\n) -&gt; darts_segmentation.training.train.DeviceConfig\n</code></pre> <p>Turn the current configuration into a suitable configuration for parallel training.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to use for parallel training. If None, assumes non-multiprocessing parallel training and propagate all devices. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceConfig</code> (              <code>darts_segmentation.training.train.DeviceConfig</code> )          \u2013            <p>A new DeviceConfig instance that is suitable for parallel training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def in_parallel(self, device: int | str | None = None) -&gt; \"DeviceConfig\":\n    \"\"\"Turn the current configuration into a suitable configuration for parallel training.\n\n    Args:\n        device (int | str | None, optional): The device to use for parallel training.\n            If None, assumes non-multiprocessing parallel training and propagate all devices.\n            Defaults to None.\n\n    Returns:\n        DeviceConfig: A new DeviceConfig instance that is suitable for parallel training.\n\n    \"\"\"\n    # In case of parallel training via multiprocessing, only few strategies are allowed.\n    if self.strategy in [\"ddp\", \"ddp_fork\", \"ddp_notebook\", \"fsdp\"]:\n        logger.warning(\"Using 'ddp_fork' instead of 'ddp' for multiprocessing.\")\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=\"ddp_fork\",  # Fork is the only supported strategy for multiprocessing\n            devices=self.devices,\n            num_nodes=self.num_nodes,\n        )\n    elif device is not None:\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=self.strategy,\n            # If a device is specified, we assume that we want to run on a single device\n            devices=[device],\n            num_nodes=1,\n        )\n    else:\n        return self\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters","title":"Hyperparameters  <code>dataclass</code>","text":"<pre><code>Hyperparameters(\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Hyperparameters for Cyclopts CLI.</p> <p>Attributes:</p> <ul> <li> <code>model_arch</code>               (<code>str</code>)           \u2013            <p>Architecture of the model to use.</p> </li> <li> <code>model_encoder</code>               (<code>str</code>)           \u2013            <p>Encoder type for the model.</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>)           \u2013            <p>Weights for the encoder, if any.</p> </li> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>)           \u2013            <p>Learning rate for training.</p> </li> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Decay factor for learning rate.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float | None</code>)           \u2013            <p>Alpha parameter for focal loss, if using.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>)           \u2013            <p>Gamma parameter for focal loss.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>)           \u2013            <p>List of bands to use. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.augment","title":"augment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>augment: (\n    list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.bands","title":"bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bands: list[str] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.focal_loss_alpha","title":"focal_loss_alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_alpha: float | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.focal_loss_gamma","title":"focal_loss_gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_gamma: float = 2.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.9\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.001\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.model_arch","title":"model_arch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_arch: str = 'Unet'\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.model_encoder","title":"model_encoder  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder: str = 'dpn107'\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.model_encoder_weights","title":"model_encoder_weights  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder_weights: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig","title":"LoggingConfig  <code>dataclass</code>","text":"<pre><code>LoggingConfig(\n    artifact_dir: pathlib.Path | None = None,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Logging related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>artifact_dir</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Top-level path to the training output directory. Will contain checkpoints and metrics. If None, will use the default artifact directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>)           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>)           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>)           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.artifact_dir","title":"artifact_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>artifact_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.check_val_every_n_epoch","title":"check_val_every_n_epoch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>check_val_every_n_epoch: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.log_every_n_steps","title":"log_every_n_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_every_n_steps: int = 10\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.wandb_entity","title":"wandb_entity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_entity: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.wandb_project","title":"wandb_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_project: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.artifact_dir_at_cv","title":"artifact_dir_at_cv","text":"<pre><code>artifact_dir_at_cv(tune_name: str | None) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory for cross-validation runs.</p> <p>Similar to <code>parse_artifact_dir_for_run</code>, but meant to be used by the cross-validation script.</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning, if applicable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path for cross-validation runs.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_cv(self, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory for cross-validation runs.\n\n    Similar to `parse_artifact_dir_for_run`, but meant to be used by the cross-validation script.\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        tune_name (str | None): Name of the tuning, if applicable.\n\n    Returns:\n        Path: The nested artifact directory path for cross-validation runs.\n\n    \"\"\"\n    artifact_dir = self.artifact_dir or paths.artifacts\n    artifact_dir = artifact_dir / tune_name if tune_name else artifact_dir / \"_cross_validations\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.artifact_dir_at_run","title":"artifact_dir_at_run","text":"<pre><code>artifact_dir_at_run(\n    cv_name: str | None, tune_name: str | None\n) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory to avoid cluttering the root directory.</p> <p>For cv it is expected that the cv function already nests the artifact directory Meaning for cv the artifact_dir of this function should be either {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If tune_name is specified, but cv_name is not, which is invalid.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_run(self, cv_name: str | None, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory to avoid cluttering the root directory.\n\n    For cv it is expected that the cv function already nests the artifact directory\n    Meaning for cv the artifact_dir of this function should be either\n    {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        cv_name (str | None): Name of the cross-validation.\n        tune_name (str | None): Name of the tuning.\n\n    Raises:\n        ValueError: If tune_name is specified, but cv_name is not, which is invalid.\n\n    Returns:\n        Path: The nested artifact directory path.\n\n    \"\"\"\n    artifact_dir = self.artifact_dir or paths.artifacts\n    # Run only\n    if cv_name is None and tune_name is None:\n        artifact_dir = artifact_dir / \"_runs\"\n    # Cross-validation only\n    elif cv_name is not None and tune_name is None:\n        artifact_dir = artifact_dir / \"_cross_validations\" / cv_name\n    # Cross-validation and tuning\n    elif cv_name is not None and tune_name is not None:\n        artifact_dir = artifact_dir / tune_name / cv_name\n    # Tuning only (invalid)\n    else:\n        raise ValueError(\n            \"Cannot parse artifact directory for cross-validation and tuning. \"\n            \"Please specify either cv_name or tune_name, but not both.\"\n        )\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig","title":"TrainRunConfig  <code>dataclass</code>","text":"<pre><code>TrainRunConfig(\n    name: str | None = None,\n    cv_name: str | None = None,\n    tune_name: str | None = None,\n    fold: int = 0,\n    random_seed: int = 42,\n)\n</code></pre> <p>Run related parameters for training.</p> <p>Defines the script inputs for the training script. Must be build by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str | None</code>)           \u2013            <p>Name of the run. If None is generated automatically. Defaults to None.</p> </li> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation. Should only be specified by a cross-validation script. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>fold</code>               (<code>int</code>)           \u2013            <p>Index of the current fold. Defaults to 0.</p> </li> <li> <code>random_seed</code>               (<code>int</code>)           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.cv_name","title":"cv_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cv_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.fold","title":"fold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: int = 42\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.tune_name","title":"tune_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<pre><code>TrainingConfig(\n    weights_from_checkpoint: pathlib.Path | None = None,\n    continue_from_checkpoint: pathlib.Path | None = None,\n    max_epochs: int = 100,\n    early_stopping_patience: int = 5,\n    num_workers: int = 0,\n    save_top_k: int = 1,\n    advanced_profiler: bool = False,\n)\n</code></pre> <p>Training related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>weights_from_checkpoint</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Path to the lightning checkpoint to load the model from. If None, the model will be trained from scratch. Defaults to None.</p> </li> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Path to a checkpoint to continue training from. Differs from <code>weights_from_checkpoint</code> in that it will continue training from this training state, hence all optimizer states, learning rate schedulers, etc. will be continued. Defaults to None.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>)           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>)           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>save_top_k</code>               (<code>int</code>)           \u2013            <p>Number of best checkpoints to save. Set to 0 to disable saving checkpoints. Set to -1 to save all checkpoints. Defaults to 1.</p> </li> <li> <code>advanced_profiler</code>               (<code>bool</code>)           \u2013            <p>Whether to use the advanced profiler. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.advanced_profiler","title":"advanced_profiler  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>advanced_profiler: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.continue_from_checkpoint","title":"continue_from_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>continue_from_checkpoint: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.early_stopping_patience","title":"early_stopping_patience  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>early_stopping_patience: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.max_epochs","title":"max_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_epochs: int = 100\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.num_workers","title":"num_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_workers: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.save_top_k","title":"save_top_k  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>save_top_k: int = 1\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.weights_from_checkpoint","title":"weights_from_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>weights_from_checkpoint: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.convert_lightning_checkpoint","title":"convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.test_smp","title":"test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    artifact_dir: pathlib.Path | None = None,\n    num_workers: int = 0,\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be specified by providing a float between 0 and 1 to data_split_by. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str] | str | float | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which seed/regions/samples split. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory to save artifacts. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    artifact_dir: Path | None = None,\n    num_workers: int = 0,\n    device_config: DeviceConfig = DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): The path (top-level) to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int): Batch size for training and validation.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be\n            specified by providing a float between 0 and 1 to data_split_by.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str] | str | float | None, optional): Select by which seed/regions/samples split.\n            Defaults to None.\n        artifact_dir (Path | None, optional): Directory to save artifacts.\n            If None, will use the default training data directory based on the DARTS paths.\n            Defaults to None.\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device_config (DeviceConfig, optional): Device and distributed strategy related parameters.\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar, ThroughputMonitor\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n\n    LoggingManager._overwrite_wandb_logger()\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\", level=logging.INFO)\n\n    tick_fstart = time.perf_counter()\n\n    # Further nest the artifact directory to avoid cluttering the root directory\n    artifact_dir = artifact_dir / \"_runs\"\n\n    logger.info(\n        f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device_config}\")\n\n    lovely_tensors.set_config(color=False)\n    lovely_tensors.monkey_patch()\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    dataset_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    logger.debug(f\"Using model checkpoint at {model_ckp.resolve()}\")\n    model = LitSMP.load_from_checkpoint(model_ckp)\n\n    available_bands: list[str] = dataset_config[\"bands\"]\n    bands = model.hparams[\"config\"][\"bands\"]\n    assert all(b in available_bands for b in bands), (\n        f\"Some specified bands ({bands}) do not exist in the dataset config ({available_bands})!\"\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir,\n        batch_size=batch_size,\n        data_split_method=data_split_method,\n        data_split_by=data_split_by,\n        bands=bands,\n        num_workers=num_workers,\n    )\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=\"none\",\n            job_type=\"none\",\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            nbands=len(bands),\n            batch_size=batch_size,\n            patch_size=dataset_config[\"patch_size\"],\n        ),\n        ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0)),\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=device_config.accelerator,\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        devices=device_config.devices,\n        deterministic=True,\n    )\n\n    trainer.test(model, datamodule, ckpt_path=model_ckp)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.train_smp","title":"train_smp","text":"<pre><code>train_smp(\n    *,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    run: darts_segmentation.training.train.TrainRunConfig = darts_segmentation.training.train.TrainRunConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n)\n</code></pre> <p>Run the training of the SMP model, specifically binary segmentation.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This training function is meant for single training runs but is also used for cross-validation and hyperparameter tuning by cv.py and tune.py. This strongly affects where artifacts are stored:</p> <ul> <li>Run was created by a tune: <code>{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}</code></li> <li>Run was created by a cross-validation: <code>{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}</code></li> <li>Single runs: <code>{artifact_dir}/_runs/{run_name}-{run_id}</code></li> </ul> <p><code>run_name</code> can be specified by the user, else it is generated automatically. In case of cross-validation, the run name is generated automatically by the cross-validation. <code>run_id</code> is generated automatically by the training function. Both are saved to the final checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Data related parameters for training.</p> </li> <li> <code>run</code>               (<code>darts_segmentation.training.train.TrainRunConfig</code>, default:                   <code>darts_segmentation.training.train.TrainRunConfig()</code> )           \u2013            <p>Run related parameters for training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging related parameters for training.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Training related parameters for training.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the model.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>pl.Trainer: The trainer object used for training. Contains also metrics.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def train_smp(  # noqa: C901\n    *,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    run: TrainRunConfig = TrainRunConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n):\n    \"\"\"Run the training of the SMP model, specifically binary segmentation.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This training function is meant for single training runs but is also used for cross-validation and hyperparameter\n    tuning by cv.py and tune.py.\n    This strongly affects where artifacts are stored:\n\n    - Run was created by a tune: `{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}`\n    - Run was created by a cross-validation: `{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}`\n    - Single runs: `{artifact_dir}/_runs/{run_name}-{run_id}`\n\n    `run_name` can be specified by the user, else it is generated automatically.\n    In case of cross-validation, the run name is generated automatically by the cross-validation.\n    `run_id` is generated automatically by the training function.\n    Both are saved to the final checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        data_config (DataConfig): Data related parameters for training.\n        run (TrainRunConfig): Run related parameters for training.\n        logging_config (LoggingConfig): Logging related parameters for training.\n        device_config (DeviceConfig): Device and distributed strategy related parameters.\n        training_config (TrainingConfig): Training related parameters for training.\n        hparams (Hyperparameters): Hyperparameters for the model.\n\n    Returns:\n        pl.Trainer: The trainer object used for training. Contains also metrics.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from darts_utils.namegen import generate_counted_name, generate_id\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics, BinarySegmentationPreview\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n\n    LoggingManager._overwrite_wandb_logger()\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\", level=logging.INFO)\n\n    tick_fstart = time.perf_counter()\n\n    paths.set_defaults(default_dirs)\n\n    # Get the right nesting of the artifact directory\n    artifact_dir = logging_config.artifact_dir_at_run(run.cv_name, run.tune_name)\n    train_data_dir = (data_config.train_data_dir or paths.training).resolve()\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run.name or generate_counted_name(artifact_dir)\n    run_id = generate_id()  # Needed for wandb\n\n    logger.info(\n        f\"Starting training '{run_name}' ('{run_id}') with data from {train_data_dir}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(\n        f\"Using config:\\n\\t{run}\\n\\t{training_config}\\n\\t{data_config}\\n\\t{logging_config}\\n\\t\"\n        f\"{device_config}\\n\\t{hparams}\"\n    )\n    if training_config.continue_from_checkpoint:\n        logger.debug(f\"Continuing from checkpoint '{training_config.continue_from_checkpoint.resolve()}'\")\n\n    lovely_tensors.monkey_patch()\n    lovely_tensors.set_config(color=False)\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(run.random_seed, workers=True, verbose=False)\n\n    dataset_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n    # Confusing thing about bands: There is\n    # (1) the available bands derived from the dataset config and\n    # (2) the bands specified in the hyperparameters\n    available_bands: list[str] = dataset_config[\"bands\"]\n    if hparams.bands:\n        # Assert that all specified bands exist in the dataset_config\n        assert all(b in available_bands for b in hparams.bands), (\n            f\"Some specified bands ({hparams.bands}) do not exist in the dataset config ({available_bands})!\"\n        )\n        bands = hparams.bands\n    else:\n        bands = available_bands\n\n    config = SMPSegmenterConfig(\n        bands=bands,\n        model={\n            \"arch\": hparams.model_arch,\n            \"encoder_name\": hparams.model_encoder,\n            \"encoder_weights\": hparams.model_encoder_weights,\n            \"in_channels\": len(bands),\n            \"classes\": 1,\n        },\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir,\n        batch_size=hparams.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        fold_method=data_config.fold_method,\n        total_folds=data_config.total_folds,\n        fold=run.fold,\n        subsample=data_config.subsample,\n        bands=bands,\n        augment=hparams.augment,\n        num_workers=training_config.num_workers,\n        in_memory=data_config.in_memory,\n    )\n    if training_config.weights_from_checkpoint:\n        logger.debug(f\"Loading model weights from checkpoint '{training_config.weights_from_checkpoint.resolve()}'\")\n        model = LitSMP.load_from_checkpoint(\n            training_config.weights_from_checkpoint,\n            map_location=\"cpu\",\n        )\n    else:\n        model = LitSMP(\n            config=config,\n            learning_rate=hparams.learning_rate,\n            gamma=hparams.gamma,\n            focal_loss_alpha=hparams.focal_loss_alpha,\n            focal_loss_gamma=hparams.focal_loss_gamma,\n            # Storing the model / checkpoint version in the hparams\n            model_version=\"2\",\n            # These are only stored in the hparams and are only used as metadata\n            run_id=run_id,\n            run_name=run_name,\n            cv_name=run.cv_name or \"none\",\n            tune_name=run.tune_name or \"none\",\n            random_seed=run.random_seed,\n            datetime=datetime.now(),\n            model_framework=\"smp\",\n        )\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=None, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        tags = [train_data_dir.stem]\n        if run.cv_name:\n            tags.append(run.cv_name)\n        if run.tune_name:\n            tags.append(run.tune_name)\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent.parent if run.tune_name or run.cv_name else artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=logging_config.wandb_project,\n            entity=logging_config.wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=run.tune_name or \"none\",\n            job_type=run.cv_name or \"none\",\n            # Using tags to quickly identify the run\n            tags=tags,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{logging_config.wandb_entity}' and project '{logging_config.wandb_project}'\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks and profiler\n    callbacks = [\n        # RichProgressBar(),\n        ModelCheckpoint(\n            filename=\"epoch={epoch}-step={step}-val_iou={val/JaccardIndex:.2f}\",\n            auto_insert_metric_name=False,\n            verbose=True,\n            monitor=\"val/JaccardIndex\",\n            mode=\"max\",\n            save_last=\"link\",\n            save_top_k=training_config.save_top_k,\n        ),\n        BinarySegmentationMetrics(\n            nbands=len(bands),\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            is_crossval=bool(run.cv_name),\n            batch_size=hparams.batch_size,\n            patch_size=dataset_config[\"patch_size\"],\n        ),\n        BinarySegmentationPreview(\n            bands=bands,\n            augmentations=hparams.augment,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n        ),\n        # Something does not work well here...\n        # ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0), window_size=log_every_n_steps),\n    ]\n    # ! Using rich progress bar can lead to issues sometimes\n    if training_config.continue_from_checkpoint is None:\n        callbacks.append(RichProgressBar())\n\n    if training_config.early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {training_config.early_stopping_patience}\")\n        early_stopping = EarlyStopping(\n            monitor=\"val/JaccardIndex\", mode=\"max\", patience=training_config.early_stopping_patience\n        )\n        callbacks.append(early_stopping)\n\n    if training_config.advanced_profiler:\n        logger.error(\n            \"Using the advanced profiler is not supported yet. Please see: https://github.com/Lightning-AI/pytorch-lightning/issues/21365\"\n        )\n        # profiler_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"profiler\"\n        # profiler_dir.mkdir(parents=True, exist_ok=True)\n        # profiler = AdvancedProfiler(dirpath=profiler_dir, filename=\"perf_logs\", dump_stats=True)\n        # logger.debug(f\"Using profiler with output to {profiler.dirpath.resolve()}\")\n        profiler = None\n    else:\n        profiler = None\n\n    logger.debug(\n        f\"Creating lightning-trainer on {device_config.accelerator} with devices {device_config.devices}\"\n        f\" and strategy '{device_config.lightning_strategy}'\"\n    )\n    # Train\n    trainer = L.Trainer(\n        max_epochs=training_config.max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=logging_config.log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=logging_config.check_val_every_n_epoch or None,  # Set 0 to None\n        accelerator=device_config.accelerator,\n        devices=device_config.devices if device_config.devices[0] != \"auto\" else \"auto\",\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        deterministic=False,  # True does not work for some reason\n        profiler=profiler,\n    )\n    trainer.fit(model, datamodule, ckpt_path=training_config.continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/","title":"tune","text":""},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune","title":"darts_segmentation.training.tune","text":"<p>More advanced hyper-parameter tuning.</p>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.available_devices","title":"available_devices  <code>module-attribute</code>","text":"<pre><code>available_devices = multiprocessing.Queue()\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig","title":"CrossValidationConfig  <code>dataclass</code>","text":"<pre><code>CrossValidationConfig(\n    n_folds: int | None = None,\n    n_randoms: int = 3,\n    scoring_metric: list[str] = (\n        lambda: [\"val/JaccardIndex\", \"val/Recall\"]\n    )(),\n    multi_score_strategy: typing.Literal[\n        \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n    ] = \"harmonic\",\n)\n</code></pre> <p>Configuration for cross-validation.</p> <p>This is used to configure the cross-validation process. It is used by the <code>cross_validation_smp</code> function.</p> <p>Attributes:</p> <ul> <li> <code>n_folds</code>               (<code>int | None</code>)           \u2013            <p>Number of folds to perform in cross-validation. If None, all folds (total_folds) will be used. Defaults to None.</p> </li> <li> <code>n_randoms</code>               (<code>int</code>)           \u2013            <p>Number of random seeds to perform in cross-validation. First three seeds are always 42, 21, 69, further seeds are deterministic generated. Defaults to 3.</p> </li> <li> <code>scoring_metric</code>               (<code>list[str]</code>)           \u2013            <p>Metric(s) to use for scoring. Defaults to [\"val/JaccardIndex\", \"val/Recall\"].</p> </li> <li> <code>multi_score_strategy</code>               (<code>typing.Literal['harmonic', 'arithmetic', 'geometric', 'min']</code>)           \u2013            <p>Strategy for combining multiple metrics. Defaults to \"harmonic\".</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.multi_score_strategy","title":"multi_score_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>multi_score_strategy: typing.Literal[\n    \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n] = \"harmonic\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.n_folds","title":"n_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_folds: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.n_randoms","title":"n_randoms  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_randoms: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.rng_seeds","title":"rng_seeds  <code>property</code>","text":"<pre><code>rng_seeds: list[int]\n</code></pre> <p>Generate a list of seeds for cross-validation.</p> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>list[int]: A list of seeds for cross-validation.</p> </li> <li> <code>list[int]</code>           \u2013            <p>The first three seeds are always 42, 21, 69, further seeds are deterministically generated.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.scoring_metric","title":"scoring_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scoring_metric: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"val/JaccardIndex\",\n        \"val/Recall\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig","title":"DataConfig  <code>dataclass</code>","text":"<pre><code>DataConfig(\n    train_data_dir: pathlib.Path | None = None,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n        \"none\",\n    ] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n    in_memory: bool = False,\n)\n</code></pre> <p>Data related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script. If None, will use the default training data directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>)           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>)           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified', 'none']</code>)           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>)           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>)           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> <li> <code>in_memory</code>               (<code>bool</code>)           \u2013            <p>If True, the dataset will be loaded into memory.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.data_split_by","title":"data_split_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_by: list[str | float] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.data_split_method","title":"data_split_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_method: (\n    typing.Literal[\"random\", \"region\", \"sample\"] | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.fold_method","title":"fold_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold_method: typing.Literal[\n    \"kfold\",\n    \"shuffle\",\n    \"stratified\",\n    \"region\",\n    \"region-stratified\",\n    \"none\",\n] = \"kfold\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.in_memory","title":"in_memory  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>in_memory: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.subsample","title":"subsample  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subsample: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.total_folds","title":"total_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_folds: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.train_data_dir","title":"train_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_data_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig","title":"DeviceConfig  <code>dataclass</code>","text":"<pre><code>DeviceConfig(\n    accelerator: typing.Literal[\n        \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n    ] = \"auto\",\n    strategy: typing.Literal[\n        \"auto\",\n        \"ddp\",\n        \"ddp_fork\",\n        \"ddp_notebook\",\n        \"fsdp\",\n        \"cv-parallel\",\n        \"tune-parallel\",\n    ] = \"auto\",\n    devices: list[int | str] = (lambda: [\"auto\"])(),\n    num_nodes: int = 1,\n)\n</code></pre> <p>Device and Distributed Strategy related parameters.</p> <p>Attributes:</p> <ul> <li> <code>accelerator</code>               (<code>typing.Literal['auto', 'cpu', 'gpu', 'mps', 'tpu']</code>)           \u2013            <p>Accelerator to use. Defaults to \"auto\".</p> </li> <li> <code>strategy</code>               (<code>typing.Literal['auto', 'ddp', 'ddp_fork', 'ddp_notebook', 'fsdp', 'cv-parallel', 'tune-parallel', 'cv-parallel', 'tune-parallel']</code>)           \u2013            <p>Distributed strategy to use. Defaults to \"auto\".</p> </li> <li> <code>devices</code>               (<code>list[int | str]</code>)           \u2013            <p>List of devices to use. Defaults to [\"auto\"].</p> </li> <li> <code>num_nodes</code>               (<code>int</code>)           \u2013            <p>Number of nodes to use for distributed training. Defaults to 1.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.accelerator","title":"accelerator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>accelerator: typing.Literal[\n    \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int | str] = dataclasses.field(\n    default_factory=lambda: [\"auto\"]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.lightning_strategy","title":"lightning_strategy  <code>property</code>","text":"<pre><code>lightning_strategy: str\n</code></pre> <p>Get the Lightning strategy for the current configuration.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The Lightning strategy to use.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.num_nodes","title":"num_nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_nodes: int = 1\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.strategy","title":"strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strategy: typing.Literal[\n    \"auto\",\n    \"ddp\",\n    \"ddp_fork\",\n    \"ddp_notebook\",\n    \"fsdp\",\n    \"cv-parallel\",\n    \"tune-parallel\",\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.in_parallel","title":"in_parallel","text":"<pre><code>in_parallel(\n    device: int | str | None = None,\n) -&gt; darts_segmentation.training.train.DeviceConfig\n</code></pre> <p>Turn the current configuration into a suitable configuration for parallel training.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to use for parallel training. If None, assumes non-multiprocessing parallel training and propagate all devices. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceConfig</code> (              <code>darts_segmentation.training.train.DeviceConfig</code> )          \u2013            <p>A new DeviceConfig instance that is suitable for parallel training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def in_parallel(self, device: int | str | None = None) -&gt; \"DeviceConfig\":\n    \"\"\"Turn the current configuration into a suitable configuration for parallel training.\n\n    Args:\n        device (int | str | None, optional): The device to use for parallel training.\n            If None, assumes non-multiprocessing parallel training and propagate all devices.\n            Defaults to None.\n\n    Returns:\n        DeviceConfig: A new DeviceConfig instance that is suitable for parallel training.\n\n    \"\"\"\n    # In case of parallel training via multiprocessing, only few strategies are allowed.\n    if self.strategy in [\"ddp\", \"ddp_fork\", \"ddp_notebook\", \"fsdp\"]:\n        logger.warning(\"Using 'ddp_fork' instead of 'ddp' for multiprocessing.\")\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=\"ddp_fork\",  # Fork is the only supported strategy for multiprocessing\n            devices=self.devices,\n            num_nodes=self.num_nodes,\n        )\n    elif device is not None:\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=self.strategy,\n            # If a device is specified, we assume that we want to run on a single device\n            devices=[device],\n            num_nodes=1,\n        )\n    else:\n        return self\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters","title":"Hyperparameters  <code>dataclass</code>","text":"<pre><code>Hyperparameters(\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Hyperparameters for Cyclopts CLI.</p> <p>Attributes:</p> <ul> <li> <code>model_arch</code>               (<code>str</code>)           \u2013            <p>Architecture of the model to use.</p> </li> <li> <code>model_encoder</code>               (<code>str</code>)           \u2013            <p>Encoder type for the model.</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>)           \u2013            <p>Weights for the encoder, if any.</p> </li> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>)           \u2013            <p>Learning rate for training.</p> </li> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Decay factor for learning rate.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float | None</code>)           \u2013            <p>Alpha parameter for focal loss, if using.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>)           \u2013            <p>Gamma parameter for focal loss.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>)           \u2013            <p>List of bands to use. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.augment","title":"augment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>augment: (\n    list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.bands","title":"bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bands: list[str] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.focal_loss_alpha","title":"focal_loss_alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_alpha: float | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.focal_loss_gamma","title":"focal_loss_gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_gamma: float = 2.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.9\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.001\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.model_arch","title":"model_arch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_arch: str = 'Unet'\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.model_encoder","title":"model_encoder  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder: str = 'dpn107'\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.model_encoder_weights","title":"model_encoder_weights  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder_weights: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig","title":"LoggingConfig  <code>dataclass</code>","text":"<pre><code>LoggingConfig(\n    artifact_dir: pathlib.Path | None = None,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Logging related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>artifact_dir</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Top-level path to the training output directory. Will contain checkpoints and metrics. If None, will use the default artifact directory based on the DARTS paths. Defaults to None.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>)           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>)           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>)           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.artifact_dir","title":"artifact_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>artifact_dir: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.check_val_every_n_epoch","title":"check_val_every_n_epoch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>check_val_every_n_epoch: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.log_every_n_steps","title":"log_every_n_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_every_n_steps: int = 10\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.wandb_entity","title":"wandb_entity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_entity: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.wandb_project","title":"wandb_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_project: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.artifact_dir_at_cv","title":"artifact_dir_at_cv","text":"<pre><code>artifact_dir_at_cv(tune_name: str | None) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory for cross-validation runs.</p> <p>Similar to <code>parse_artifact_dir_for_run</code>, but meant to be used by the cross-validation script.</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning, if applicable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path for cross-validation runs.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_cv(self, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory for cross-validation runs.\n\n    Similar to `parse_artifact_dir_for_run`, but meant to be used by the cross-validation script.\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        tune_name (str | None): Name of the tuning, if applicable.\n\n    Returns:\n        Path: The nested artifact directory path for cross-validation runs.\n\n    \"\"\"\n    artifact_dir = self.artifact_dir or paths.artifacts\n    artifact_dir = artifact_dir / tune_name if tune_name else artifact_dir / \"_cross_validations\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.artifact_dir_at_run","title":"artifact_dir_at_run","text":"<pre><code>artifact_dir_at_run(\n    cv_name: str | None, tune_name: str | None\n) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory to avoid cluttering the root directory.</p> <p>For cv it is expected that the cv function already nests the artifact directory Meaning for cv the artifact_dir of this function should be either {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If tune_name is specified, but cv_name is not, which is invalid.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_run(self, cv_name: str | None, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory to avoid cluttering the root directory.\n\n    For cv it is expected that the cv function already nests the artifact directory\n    Meaning for cv the artifact_dir of this function should be either\n    {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        cv_name (str | None): Name of the cross-validation.\n        tune_name (str | None): Name of the tuning.\n\n    Raises:\n        ValueError: If tune_name is specified, but cv_name is not, which is invalid.\n\n    Returns:\n        Path: The nested artifact directory path.\n\n    \"\"\"\n    artifact_dir = self.artifact_dir or paths.artifacts\n    # Run only\n    if cv_name is None and tune_name is None:\n        artifact_dir = artifact_dir / \"_runs\"\n    # Cross-validation only\n    elif cv_name is not None and tune_name is None:\n        artifact_dir = artifact_dir / \"_cross_validations\" / cv_name\n    # Cross-validation and tuning\n    elif cv_name is not None and tune_name is not None:\n        artifact_dir = artifact_dir / tune_name / cv_name\n    # Tuning only (invalid)\n    else:\n        raise ValueError(\n            \"Cannot parse artifact directory for cross-validation and tuning. \"\n            \"Please specify either cv_name or tune_name, but not both.\"\n        )\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig","title":"TrainRunConfig  <code>dataclass</code>","text":"<pre><code>TrainRunConfig(\n    name: str | None = None,\n    cv_name: str | None = None,\n    tune_name: str | None = None,\n    fold: int = 0,\n    random_seed: int = 42,\n)\n</code></pre> <p>Run related parameters for training.</p> <p>Defines the script inputs for the training script. Must be build by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str | None</code>)           \u2013            <p>Name of the run. If None is generated automatically. Defaults to None.</p> </li> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation. Should only be specified by a cross-validation script. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>fold</code>               (<code>int</code>)           \u2013            <p>Index of the current fold. Defaults to 0.</p> </li> <li> <code>random_seed</code>               (<code>int</code>)           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.cv_name","title":"cv_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cv_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.fold","title":"fold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: int = 42\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.tune_name","title":"tune_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<pre><code>TrainingConfig(\n    weights_from_checkpoint: pathlib.Path | None = None,\n    continue_from_checkpoint: pathlib.Path | None = None,\n    max_epochs: int = 100,\n    early_stopping_patience: int = 5,\n    num_workers: int = 0,\n    save_top_k: int = 1,\n    advanced_profiler: bool = False,\n)\n</code></pre> <p>Training related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>weights_from_checkpoint</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Path to the lightning checkpoint to load the model from. If None, the model will be trained from scratch. Defaults to None.</p> </li> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Path to a checkpoint to continue training from. Differs from <code>weights_from_checkpoint</code> in that it will continue training from this training state, hence all optimizer states, learning rate schedulers, etc. will be continued. Defaults to None.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>)           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>)           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>save_top_k</code>               (<code>int</code>)           \u2013            <p>Number of best checkpoints to save. Set to 0 to disable saving checkpoints. Set to -1 to save all checkpoints. Defaults to 1.</p> </li> <li> <code>advanced_profiler</code>               (<code>bool</code>)           \u2013            <p>Whether to use the advanced profiler. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.advanced_profiler","title":"advanced_profiler  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>advanced_profiler: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.continue_from_checkpoint","title":"continue_from_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>continue_from_checkpoint: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.early_stopping_patience","title":"early_stopping_patience  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>early_stopping_patience: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.max_epochs","title":"max_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_epochs: int = 100\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.num_workers","title":"num_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_workers: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.save_top_k","title":"save_top_k  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>save_top_k: int = 1\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.weights_from_checkpoint","title":"weights_from_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>weights_from_checkpoint: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs","title":"_ProcessInputs  <code>dataclass</code>","text":"<pre><code>_ProcessInputs(\n    current: int,\n    total: int,\n    tune_name: str,\n    cv: darts_segmentation.training.cv.CrossValidationConfig,\n    training_config: darts_segmentation.training.train.TrainingConfig,\n    logging_config: darts_segmentation.training.train.LoggingConfig,\n    data_config: darts_segmentation.training.train.DataConfig,\n    device_config: darts_segmentation.training.train.DeviceConfig,\n    hparams: darts_segmentation.training.hparams.Hyperparameters,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.current","title":"current  <code>instance-attribute</code>","text":"<pre><code>current: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.cv","title":"cv  <code>instance-attribute</code>","text":"<pre><code>cv: darts_segmentation.training.cv.CrossValidationConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.data_config","title":"data_config  <code>instance-attribute</code>","text":"<pre><code>data_config: darts_segmentation.training.train.DataConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.device_config","title":"device_config  <code>instance-attribute</code>","text":"<pre><code>device_config: (\n    darts_segmentation.training.train.DeviceConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.hparams","title":"hparams  <code>instance-attribute</code>","text":"<pre><code>hparams: darts_segmentation.training.hparams.Hyperparameters\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.logging_config","title":"logging_config  <code>instance-attribute</code>","text":"<pre><code>logging_config: (\n    darts_segmentation.training.train.LoggingConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.total","title":"total  <code>instance-attribute</code>","text":"<pre><code>total: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.training_config","title":"training_config  <code>instance-attribute</code>","text":"<pre><code>training_config: (\n    darts_segmentation.training.train.TrainingConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.tune_name","title":"tune_name  <code>instance-attribute</code>","text":"<pre><code>tune_name: str\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessOutputs","title":"_ProcessOutputs  <code>dataclass</code>","text":"<pre><code>_ProcessOutputs(\n    run_infos: pandas.DataFrame,\n    score: float,\n    is_unstable: bool,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessOutputs.is_unstable","title":"is_unstable  <code>instance-attribute</code>","text":"<pre><code>is_unstable: bool\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessOutputs.run_infos","title":"run_infos  <code>instance-attribute</code>","text":"<pre><code>run_infos: pandas.DataFrame\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessOutputs.score","title":"score  <code>instance-attribute</code>","text":"<pre><code>score: float\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._run_cv","title":"_run_cv","text":"<pre><code>_run_cv(\n    inp: darts_segmentation.training.tune._ProcessInputs,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/tune.py</code> <pre><code>def _run_cv(inp: _ProcessInputs):\n    # Wrapper function for handling parallel multiprocessing training runs.\n    import pandas as pd\n\n    from darts_segmentation.training.cv import cross_validation_smp\n\n    cv_name = f\"{inp.tune_name}-cv{inp.current}\"\n\n    # Setup device configuration: If strategy is \"tune-parallel\" expect a mp scenario:\n    # Wait for a device to become available.\n    # Otherwise, expect a serial scenario, where the devices and strategy are set by the user.\n    is_parallel = inp.device_config.strategy == \"tune-parallel\"\n    if is_parallel:\n        device = available_devices.get()\n        device_config = inp.device_config.in_parallel(device)\n        logger.info(f\"Starting cv '{cv_name}' ({inp.current + 1}/{inp.total}) on device {device}.\")\n    else:\n        device = None\n        device_config = inp.device_config.in_parallel()\n        logger.info(f\"Starting cv '{cv_name}' ({inp.current + 1}/{inp.total}).\")\n\n    try:\n        score, is_unstable, cv_run_infos = cross_validation_smp(\n            name=cv_name,\n            tune_name=inp.tune_name,\n            cv=inp.cv,\n            training_config=inp.training_config,\n            data_config=inp.data_config,\n            logging_config=inp.logging_config,\n            hparams=inp.hparams,\n            device_config=device_config,\n        )\n\n        for key, value in asdict(inp.hparams).items():\n            cv_run_infos[key] = value if not isinstance(value, list) else pd.Series([value] * len(cv_run_infos))\n\n        cv_run_infos[\"cv_name\"] = cv_name\n        output = _ProcessOutputs(\n            run_infos=cv_run_infos,\n            score=score,\n            is_unstable=is_unstable,\n        )\n    finally:\n        # If we are in parallel mode, we need to return the device to the queue.\n        if is_parallel:\n            logger.debug(f\"Free device {device} for cv {cv_name}\")\n            available_devices.put(device)\n    return output\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.check_score_is_unstable","title":"check_score_is_unstable","text":"<pre><code>check_score_is_unstable(\n    run_info: dict, scoring_metric: list[str] | str\n) -&gt; bool\n</code></pre> <p>Check the stability of the scoring metric.</p> <p>If any metric value is not finite or equal to zero, the scoring metric is considered unstable.</p> <p>Parameters:</p> <ul> <li> <code>run_info</code>               (<code>dict</code>)           \u2013            <p>The run information.</p> </li> <li> <code>scoring_metric</code>               (<code>list[str] | str</code>)           \u2013            <p>The scoring metric.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the scoring metric is unstable, False otherwise.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown scoring metric type is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/scoring.py</code> <pre><code>def check_score_is_unstable(run_info: dict, scoring_metric: list[str] | str) -&gt; bool:\n    \"\"\"Check the stability of the scoring metric.\n\n    If any metric value is not finite or equal to zero, the scoring metric is considered unstable.\n\n    Args:\n        run_info (dict): The run information.\n        scoring_metric (list[str] | str): The scoring metric.\n\n    Returns:\n        bool: True if the scoring metric is unstable, False otherwise.\n\n    Raises:\n        ValueError: If an unknown scoring metric type is provided.\n\n    \"\"\"\n    # Single score in list\n    if isinstance(scoring_metric, list) and len(scoring_metric) == 1:\n        scoring_metric = scoring_metric[0]\n\n    if isinstance(scoring_metric, str):\n        metric_value = run_info[scoring_metric]\n        is_unstable = not isfinite(metric_value) or metric_value == 0\n        return is_unstable\n    elif isinstance(scoring_metric, list):\n        metric_values = [run_info[metric] for metric in scoring_metric]\n        is_unstable = any(not isfinite(val) or val == 0 for val in metric_values)\n        return is_unstable\n    else:\n        raise ValueError(\"Invalid scoring metric type\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.tune_smp","title":"tune_smp","text":"<pre><code>tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | typing.Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    default_dirs: darts_utils.paths.DefaultPaths = darts_utils.paths.DefaultPaths(),\n    cv_config: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    hpconfig: pathlib.Path | None = None,\n    config_file: pathlib.Path | None = None,\n)\n</code></pre> <p>Tune the hyper-parameters of the model using cross-validation and random states.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This tuning script is designed to sweep over hyperparameters with a cross-validation used to evaluate each hyperparameter configuration. Optionally, by setting <code>retrain_and_test</code> to True, the best hyperparameters are then selected based on the cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.</p> <p>Hyperparameters can be configured using a <code>hpconfig</code> file (YAML or Toml). Please consult the training guide or the documentation of <code>darts_segmentation.training.hparams.parse_hyperparameters</code> to learn how such a file should be structured. Per default, a random search is performed, where the number of samples can be specified by <code>n_trials</code>. If <code>n_trials</code> is set to \"grid\", a grid search is performed instead. However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.</p> <p>To specify on which metric(s) the cv score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\". In such cases, the configuration is not considered for further evaluation.</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning run. Will be generated based on the number of existing directories in the artifact directory if None. Defaults to None.</p> </li> <li> <code>n_trials</code>               (<code>int | typing.Literal['grid']</code>, default:                   <code>100</code> )           \u2013            <p>Number of trials to perform in hyperparameter tuning. If \"grid\", span a grid search over all configured hyperparameters. In a grid search, only constant or choice hyperparameters are allowed. Defaults to 100.</p> </li> <li> <code>retrain_and_test</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to retrain the model with the best hyperparameters and test it. Defaults to False.</p> </li> <li> <code>default_dirs</code>               (<code>darts_utils.paths.DefaultPaths</code>, default:                   <code>darts_utils.paths.DefaultPaths()</code> )           \u2013            <p>The default directories for DARTS. Defaults to a config filled with None.</p> </li> <li> <code>cv_config</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation. Defaults to CrossValidationConfig().</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for training. Defaults to TrainingConfig().</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for data. Defaults to DataConfig().</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for device. Defaults to DeviceConfig().</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Configuration for logging. Defaults to LoggingConfig().</p> </li> <li> <code>hpconfig</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the hyperparameter configuration file. Please see the documentation of <code>hyperparameters</code> for more information. Defaults to None.</p> </li> <li> <code>config_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the configuration file. If provided, it will be used instead of <code>hpconfig</code> if <code>hpconfig</code> is None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no hyperparameter configuration file is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/tune.py</code> <pre><code>def tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    default_dirs: DefaultPaths = DefaultPaths(),\n    cv_config: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    hpconfig: Path | None = None,\n    config_file: Annotated[Path | None, cyclopts.Parameter(parse=False)] = None,\n):\n    \"\"\"Tune the hyper-parameters of the model using cross-validation and random states.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This tuning script is designed to sweep over hyperparameters with a cross-validation\n    used to evaluate each hyperparameter configuration.\n    Optionally, by setting `retrain_and_test` to True, the best hyperparameters are then selected based on the\n    cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.\n\n    Hyperparameters can be configured using a `hpconfig` file (YAML or Toml).\n    Please consult the training guide or the documentation of\n    `darts_segmentation.training.hparams.parse_hyperparameters` to learn how such a file should be structured.\n    Per default, a random search is performed, where the number of samples can be specified by `n_trials`.\n    If `n_trials` is set to \"grid\", a grid search is performed instead.\n    However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.\n\n    To specify on which metric(s) the cv score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n    In such cases, the configuration is not considered for further evaluation.\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the tuning run.\n            Will be generated based on the number of existing directories in the artifact directory if None.\n            Defaults to None.\n        n_trials (int | Literal[\"grid\"], optional): Number of trials to perform in hyperparameter tuning.\n            If \"grid\", span a grid search over all configured hyperparameters.\n            In a grid search, only constant or choice hyperparameters are allowed.\n            Defaults to 100.\n        retrain_and_test (bool, optional): Whether to retrain the model with the best hyperparameters and test it.\n            Defaults to False.\n        default_dirs (DefaultPaths, optional): The default directories for DARTS. Defaults to a config filled with None.\n        cv_config (CrossValidationConfig, optional): Configuration for cross-validation.\n            Defaults to CrossValidationConfig().\n        training_config (TrainingConfig, optional): Configuration for training.\n            Defaults to TrainingConfig().\n        data_config (DataConfig, optional): Configuration for data.\n            Defaults to DataConfig().\n        device_config (DeviceConfig, optional): Configuration for device.\n            Defaults to DeviceConfig().\n        logging_config (LoggingConfig, optional): Configuration for logging.\n            Defaults to LoggingConfig().\n        hpconfig (Path | None, optional): Path to the hyperparameter configuration file.\n            Please see the documentation of `hyperparameters` for more information.\n            Defaults to None.\n        config_file (Path | None, optional): Path to the configuration file. If provided,\n            it will be used instead of `hpconfig` if `hpconfig` is None. Defaults to None.\n\n    Returns:\n        tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.\n\n    Raises:\n        ValueError: If no hyperparameter configuration file is provided.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.hparams import parse_hyperparameters, sample_hyperparameters\n    from darts_segmentation.training.scoring import score_from_single_run\n    from darts_segmentation.training.train import test_smp, train_smp\n\n    tick_fstart = time.perf_counter()\n\n    paths.set_defaults(default_dirs)\n\n    tune_name = name or generate_counted_name(logging_config.artifact_dir or paths.artifacts)\n    artifact_dir = (logging_config.artifact_dir or paths.artifacts) / tune_name\n    run_infos_file = artifact_dir / f\"{tune_name}.parquet\"\n\n    # Check if the artifact directory is empty\n    assert not artifact_dir.exists(), f\"{artifact_dir} already exists.\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    hpconfig = hpconfig or config_file\n    if hpconfig is None:\n        raise ValueError(\n            \"No hyperparameter configuration file provided. Please provide a valid file via the `--hpconfig` flag.\"\n        )\n    param_grid = parse_hyperparameters(hpconfig)\n    logger.debug(f\"Parsed hyperparameter grid: {param_grid}\")\n    param_list = sample_hyperparameters(param_grid, n_trials)\n\n    logger.info(\n        f\"Starting tune '{tune_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_trials*n_randoms*n_folds =\"\n        f\" {len(param_list)}*{cv_config.n_randoms}*{cv_config.n_folds} =\"\n        f\" {len(param_list) * cv_config.n_randoms * cv_config.n_folds} experiments.\"\n    )\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs = [\n        _ProcessInputs(\n            current=i,\n            total=len(param_list),\n            tune_name=tune_name,\n            cv=cv_config,\n            training_config=training_config,\n            logging_config=logging_config,\n            data_config=data_config,\n            device_config=device_config,\n            hparams=hparams,\n        )\n        for i, hparams in enumerate(param_list)\n    ]\n\n    run_infos: list[pd.DataFrame] = []\n    best_score = 0\n    best_hp = None\n\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"tune-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_cv,\n    ):\n        run_infos.append(output.run_infos)\n        if not output.is_unstable and output.score &gt; best_score:\n            best_score = output.score\n            best_hp = inp.hparams\n\n        # Save already here to prevent data loss if something goes wrong\n        pd.concat(run_infos).reset_index(drop=True).to_parquet(run_infos_file)\n        logger.debug(f\"Saved run infos to {run_infos_file}\")\n\n    if len(run_infos) == 0:\n        logger.error(\"No hyperparameters resulted in a valid score. Please check the logs for more information.\")\n        return 0, run_infos\n\n    run_infos = pd.concat(run_infos).reset_index(drop=True)\n\n    tick_fend = time.perf_counter()\n\n    if best_hp is None:\n        logger.warning(\n            f\"Tuning completed in {tick_fend - tick_fstart:.2f}s.\"\n            \" No hyperparameters resulted in a valid score. Please check the logs for more information.\"\n        )\n        return 0, run_infos\n    logger.info(\n        f\"Tuning completed in {tick_fend - tick_fstart:.2f}s. The best score was {best_score:.4f} with {best_hp}.\"\n    )\n\n    # =====================\n    # === End of tuning ===\n    # =====================\n\n    if not retrain_and_test:\n        return 0, run_infos\n\n    logger.info(\"Starting retraining with the best hyperparameters.\")\n\n    tick_fstart = time.perf_counter()\n    trainer = train_smp(\n        run=TrainRunConfig(name=f\"{tune_name}-retrain\"),\n        training_config=training_config,  # TODO: device and strategy\n        data_config=DataConfig(\n            train_data_dir=data_config.train_data_dir,\n            data_split_method=data_config.data_split_method,\n            data_split_by=data_config.data_split_by,\n            fold_method=None,  # No fold method for retraining\n            total_folds=None,  # No folds for retraining\n        ),\n        logging_config=LoggingConfig(\n            artifact_dir=artifact_dir,\n            log_every_n_steps=logging_config.log_every_n_steps,\n            check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            wandb_entity=logging_config.wandb_entity,\n            wandb_project=logging_config.wandb_project,\n        ),\n        hparams=best_hp,\n    )\n    run_id = trainer.lightning_module.hparams[\"run_id\"]\n    trainer = test_smp(\n        train_data_dir=data_config.train_data_dir,\n        run_id=run_id,\n        run_name=f\"{tune_name}-retrain\",\n        model_ckp=trainer.checkpoint_callback.best_model_path,\n        batch_size=best_hp.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        artifact_dir=artifact_dir,\n        num_workers=training_config.num_workers,\n        device_config=device_config,\n        wandb_entity=logging_config.wandb_entity,\n        wandb_project=logging_config.wandb_project,\n    )\n\n    run_info = {k: v.item() for k, v in trainer.callback_metrics.items()}\n    test_scoring_metric = (\n        cv_config.scoring_metric.replace(\"val/\", \"test/\")\n        if isinstance(cv_config.scoring_metric, str)\n        else [sm.replace(\"val/\", \"test/\") for sm in cv_config.scoring_metric]\n    )\n    score = score_from_single_run(run_info, test_scoring_metric, cv_config.multi_score_strategy)\n    is_unstable = check_score_is_unstable(run_info, cv_config.scoring_metric)\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Retraining and testing completed successfully in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, run_infos\n</code></pre>"},{"location":"reference/darts_segmentation/training/viz/","title":"viz","text":""},{"location":"reference/darts_segmentation/training/viz/#darts_segmentation.training.viz","title":"darts_segmentation.training.viz","text":"<p>Visualization utilities for the training module.</p>"},{"location":"reference/darts_segmentation/training/viz/#darts_segmentation.training.viz.Augmentation","title":"Augmentation  <code>module-attribute</code>","text":"<pre><code>Augmentation = typing.Literal[\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"D4\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\",\n    \"Posterize\",\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/viz/#darts_segmentation.training.viz.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/viz/#darts_segmentation.training.viz.get_augmentation","title":"get_augmentation","text":"<pre><code>get_augmentation(\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None,\n    always_apply: bool = False,\n) -&gt; albumentations.Compose | None\n</code></pre> <p>Get augmentations for segmentation tasks.</p> <p>Parameters:</p> <ul> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply. If None or emtpy, no augmentations are applied. If not empty, augmentations are applied in the order they are listed. Available augmentations:     - D4 (Combination of HorizontalFlip, VerticalFlip, and RandomRotate90)     - Blur     - RandomBrightnessContrast     - MultiplicativeNoise     - Posterize (quantization to reduce number of bits per channel)</p> </li> <li> <code>always_apply</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, augmentations are always applied. This is useful for visualization/testing augmentations. Default is False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown augmentation is provided.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>albumentations.Compose | None</code>           \u2013            <p>A.Compose | None: A Compose object containing the augmentations. If no augmentations are provided, returns None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/augmentations.py</code> <pre><code>def get_augmentation(augment: list[Augmentation] | None, always_apply: bool = False) -&gt; \"A.Compose | None\":  # noqa: C901\n    \"\"\"Get augmentations for segmentation tasks.\n\n    Args:\n        augment (list[Augmentation] | None): List of augmentations to apply.\n            If None or emtpy, no augmentations are applied.\n            If not empty, augmentations are applied in the order they are listed.\n            Available augmentations:\n                - D4 (Combination of HorizontalFlip, VerticalFlip, and RandomRotate90)\n                - Blur\n                - RandomBrightnessContrast\n                - MultiplicativeNoise\n                - Posterize (quantization to reduce number of bits per channel)\n        always_apply (bool): If True, augmentations are always applied.\n            This is useful for visualization/testing augmentations.\n            Default is False.\n\n    Raises:\n        ValueError: If an unknown augmentation is provided.\n\n    Returns:\n        A.Compose | None: A Compose object containing the augmentations.\n            If no augmentations are provided, returns None.\n\n    \"\"\"\n    import albumentations as A  # noqa: N812\n\n    if not isinstance(augment, list) or len(augment) == 0:\n        return None\n\n    # Replace HorizontalFlip, VerticalFlip, RandomRotate90 with D4\n    if \"HorizontalFlip\" in augment and \"VerticalFlip\" in augment and \"RandomRotate90\" in augment:\n        augment = [aug for aug in augment if aug not in (\"HorizontalFlip\", \"VerticalFlip\", \"RandomRotate90\")]\n        augment.insert(0, \"D4\")\n\n    transforms = []\n    for aug in augment:\n        match aug:\n            case \"D4\":\n                transforms.append(A.D4())\n            case \"HorizontalFlip\":\n                transforms.append(A.HorizontalFlip(p=1.0 if always_apply else 0.5))\n            case \"VerticalFlip\":\n                transforms.append(A.VerticalFlip(p=1.0 if always_apply else 0.5))\n            case \"RandomRotate90\":\n                transforms.append(A.RandomRotate90())\n            case \"Blur\":\n                transforms.append(A.Blur(p=1.0 if always_apply else 0.5))\n            case \"RandomBrightnessContrast\":\n                transforms.append(A.RandomBrightnessContrast(p=1.0 if always_apply else 0.5))\n            case \"MultiplicativeNoise\":\n                transforms.append(\n                    A.MultiplicativeNoise(per_channel=True, elementwise=True, p=1.0 if always_apply else 0.5)\n                )\n            case \"Posterize\":\n                # First convert to uint8, then apply posterization, then convert back to float32\n                # * Note: This does only work for float32 images.\n                transforms += [\n                    A.FromFloat(dtype=\"uint8\"),\n                    A.Posterize(num_bits=6, p=1.0),\n                    A.ToFloat(),\n                ]\n            case _:\n                raise ValueError(f\"Unknown augmentation: {aug}\")\n    return A.Compose(transforms)\n</code></pre>"},{"location":"reference/darts_segmentation/training/viz/#darts_segmentation.training.viz.plot_augmentations","title":"plot_augmentations","text":"<pre><code>plot_augmentations(\n    x: torch.Tensor,\n    augmentations: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ],\n    band_names: list[str],\n) -&gt; tuple[\n    ultraplot.Figure, ultraplot.gridspec.SubplotGrid\n]\n</code></pre> <p>Plot augmentations applied to a sample image.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>Input tensor [N, C, H, W] (float).</p> </li> <li> <code>augmentations</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation]</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>List of band names corresponding to the channels in x.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ultraplot.Figure</code>           \u2013            <p>matplotlib.figure.Figure: The figure object containing the plots.</p> </li> <li> <code>ultraplot.gridspec.SubplotGrid</code>           \u2013            <p>ultraplot.gridspec.SubplotGrid: The axes of the plot.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/viz.py</code> <pre><code>def plot_augmentations(\n    x: torch.Tensor, augmentations: list[Augmentation], band_names: list[str]\n) -&gt; tuple[uplt.Figure, uplt.gridspec.SubplotGrid]:\n    \"\"\"Plot augmentations applied to a sample image.\n\n    Args:\n        x (torch.Tensor): Input tensor [N, C, H, W] (float).\n        augmentations (list[Augmentation]): List of augmentations to apply.\n        band_names (list[str]): List of band names corresponding to the channels in x.\n\n    Returns:\n        matplotlib.figure.Figure: The figure object containing the plots.\n        ultraplot.gridspec.SubplotGrid: The axes of the plot.\n\n    \"\"\"\n    compose = get_augmentation(augmentations)\n    augmentations: dict[str, A.BasicTransform] = {aug: get_augmentation([aug], True) for aug in augmentations}\n\n    rgb_idx = [band_names.index(band) for band in [\"red\", \"green\", \"blue\"]]\n\n    nrows = 1 + len(augmentations) + 4\n    ncols = x.shape[0]\n    fig, axs = uplt.subplots(ncols=ncols, nrows=nrows, figsize=(ncols * 5, nrows * 5))\n    for i in range(ncols):\n        img = x[i, rgb_idx].permute(1, 2, 0).cpu().numpy()\n        axs[0, i].imshow(img, vmin=0, vmax=0.1)\n        axs[0, i].set_title(\"Original Image\")\n        for j, (aug_name, aug_fn) in enumerate(augmentations.items()):\n            augmented = aug_fn(image=img)\n            aug_img = augmented[\"image\"]\n            axs[j + 1, i].imshow(aug_img, vmin=0, vmax=0.1)\n            axs[j + 1, i].set_title(f\"Augmented: {aug_name}\")\n\n        # Apply full compose\n        for j in range(4):\n            augmented = compose(image=img)\n            aug_img = augmented[\"image\"]\n            axs[j + 1 + len(augmentations), i].imshow(aug_img, vmin=0, vmax=0.1)\n            axs[j + 1 + len(augmentations), i].set_title(f\"Compose Augmentation {j + 1}\")\n    return fig, axs\n</code></pre>"},{"location":"reference/darts_segmentation/training/viz/#darts_segmentation.training.viz.plot_sample","title":"plot_sample","text":"<pre><code>plot_sample(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    y_pred: torch.Tensor,\n    band_names: list[str],\n) -&gt; tuple[\n    matplotlib.pyplot.Figure,\n    dict[str, matplotlib.pyplot.Axes],\n]\n</code></pre> <p>Plot a single sample with the input, the ground truth and the prediction.</p> <p>This function does a few expections on the input: - The input is expected to be normalized to 0-1. - The prediction is expected to be converted from logits to prediction. - The target is expected to be a int or long tensor with values of:     0 (negative class)     1 (positive class) and     2 (invalid pixels).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor [C, H, W] (float).</p> </li> <li> <code>y</code>               (<code>torch.Tensor</code>)           \u2013            <p>The ground truth tensor [H, W] (int).</p> </li> <li> <code>y_pred</code>               (<code>torch.Tensor</code>)           \u2013            <p>The prediction tensor [H, W] (float).</p> </li> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>The combinations of the input bands.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[matplotlib.pyplot.Figure, dict[str, matplotlib.pyplot.Axes]]</code>           \u2013            <p>tuple[Figure, dict[str, Axes]]: The figure and the axes of the plot.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/viz.py</code> <pre><code>def plot_sample(\n    x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor, band_names: list[str]\n) -&gt; tuple[plt.Figure, dict[str, plt.Axes]]:\n    \"\"\"Plot a single sample with the input, the ground truth and the prediction.\n\n    This function does a few expections on the input:\n    - The input is expected to be normalized to 0-1.\n    - The prediction is expected to be converted from logits to prediction.\n    - The target is expected to be a int or long tensor with values of:\n        0 (negative class)\n        1 (positive class) and\n        2 (invalid pixels).\n\n    Args:\n        x (torch.Tensor): The input tensor [C, H, W] (float).\n        y (torch.Tensor): The ground truth tensor [H, W] (int).\n        y_pred (torch.Tensor): The prediction tensor [H, W] (float).\n        band_names (list[str]): The combinations of the input bands.\n\n    Returns:\n        tuple[Figure, dict[str, Axes]]: The figure and the axes of the plot.\n\n    \"\"\"\n    x = x.cpu()\n    y = y.cpu()\n    y_pred = y_pred.detach().cpu()\n\n    # Make y class 2 invalids (replace 2 with nan)\n    x = x.where(y != 2, torch.nan)\n    y_pred = y_pred.where(y != 2, torch.nan)\n    y = y.where(y != 2, torch.nan)\n\n    # pred == 0, y == 0 -&gt; 0 (true negative)\n    # pred == 1, y == 0 -&gt; 1 (false positive)\n    # pred == 0, y == 1 -&gt; 2 (false negative)\n    # pred == 1, y == 1 -&gt; 3 (true positive)\n    classification_labels = (y_pred &gt; 0.5).int() + y * 2\n    classification_labels = classification_labels.where(classification_labels != 0, torch.nan)\n\n    # Calculate f1 and iou\n    true_positive = (classification_labels == 3).sum()\n    false_positive = (classification_labels == 1).sum()\n    false_negative = (classification_labels == 2).sum()\n    true_negative = (classification_labels == 0).sum()\n    acc = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n    f1 = 2 * true_positive / (2 * true_positive + false_positive + false_negative)\n    iou = true_positive / (true_positive + false_positive + false_negative)\n\n    cmap = mcolors.ListedColormap([\"#cd43b2\", \"#3e0f2f\", \"#6cd875\"])\n    fig, axs = plt.subplot_mosaic(\n        # [[\"rgb\", \"rgb\", \"ndvi\", \"tcvis\", \"stats\"], [\"rgb\", \"rgb\", \"pred\", \"slope\", \"elev\"]],\n        [[\"rgb\", \"rgb\", \"pred\", \"tcvis\"], [\"rgb\", \"rgb\", \"ndvi\", \"slope\"], [\"none\", \"stats\", \"stats\", \"stats\"]],\n        # layout=\"constrained\",\n        figsize=(11, 8),\n    )\n\n    # Disable none plot\n    axs[\"none\"].axis(\"off\")\n\n    # RGB Plot\n    ax_rgb = axs[\"rgb\"]\n    # disable axis\n    ax_rgb.axis(\"off\")\n    is_rgb = \"red\" in band_names and \"green\" in band_names and \"blue\" in band_names\n    if is_rgb:\n        red_band = band_names.index(\"red\")\n        green_band = band_names.index(\"green\")\n        blue_band = band_names.index(\"blue\")\n        rgb = x[[red_band, green_band, blue_band]].transpose(0, 2).transpose(0, 1)\n        ax_rgb.imshow(rgb ** (1 / 1.4))\n        ax_rgb.set_title(f\"Acc: {acc:.1%} F1: {f1:.1%} IoU: {iou:.1%}\")\n    else:\n        # Plot empty with message that RGB is not provided\n        ax_rgb.set_title(\"No RGB values are provided!\")\n    ax_rgb.imshow(classification_labels, alpha=0.6, cmap=cmap, vmin=1, vmax=3)\n    # Add a legend\n    patches = [\n        mpatches.Patch(color=\"#6cd875\", label=\"True Positive\"),\n        mpatches.Patch(color=\"#3e0f2f\", label=\"False Negative\"),\n        mpatches.Patch(color=\"#cd43b2\", label=\"False Positive\"),\n    ]\n    ax_rgb.legend(handles=patches, loc=\"upper left\")\n\n    # NDVI Plot\n    ax_ndvi = axs[\"ndvi\"]\n    ax_ndvi.axis(\"off\")\n    is_ndvi = \"ndvi\" in band_names\n    if is_ndvi:\n        ndvi_band = band_names.index(\"ndvi\")\n        ndvi = x[ndvi_band]\n        ax_ndvi.imshow(ndvi, vmin=0, vmax=1, cmap=\"RdYlGn\")\n        ax_ndvi.set_title(\"NDVI\")\n    else:\n        # Plot empty with message that NDVI is not provided\n        ax_ndvi.set_title(\"No NDVI values are provided!\")\n\n    # TCVIS Plot\n    ax_tcv = axs[\"tcvis\"]\n    ax_tcv.axis(\"off\")\n    is_tcvis = \"tc_brightness\" in band_names and \"tc_greenness\" in band_names and \"tc_wetness\" in band_names\n    if is_tcvis:\n        tcb_band = band_names.index(\"tc_brightness\")\n        tcg_band = band_names.index(\"tc_greenness\")\n        tcw_band = band_names.index(\"tc_wetness\")\n        tcvis = x[[tcb_band, tcg_band, tcw_band]].transpose(0, 2).transpose(0, 1)\n        ax_tcv.imshow(tcvis)\n        ax_tcv.set_title(\"TCVIS\")\n    else:\n        ax_tcv.set_title(\"No TCVIS values are provided!\")\n\n    # Statistics Plot\n    ax_stat = axs[\"stats\"]\n    if (y == 1).sum() &gt; 0:\n        n_bands = x.shape[0]\n        n_pixel = x.shape[1] * x.shape[2]\n        x_flat = x.flatten().cpu()\n        y_flat = y.flatten().repeat(n_bands).cpu()\n        bands = list(itertools.chain.from_iterable([band_names[i]] * n_pixel for i in range(n_bands)))\n        plot_data = pd.DataFrame({\"x\": x_flat, \"y\": y_flat, \"band\": bands})\n        if len(plot_data) &gt; 50000:\n            plot_data = plot_data.sample(50000)\n        plot_data = plot_data.sort_values(\"band\")\n        sns.violinplot(\n            x=\"x\",\n            y=\"band\",\n            hue=\"y\",\n            data=plot_data,\n            split=True,\n            inner=\"quart\",\n            fill=False,\n            palette={1: \"g\", 0: \".35\"},\n            density_norm=\"width\",\n            ax=ax_stat,\n        )\n        ax_stat.set_title(\"Band Statistics\")\n    else:\n        ax_stat.set_title(\"No positive labels in this sample!\")\n        ax_stat.axis(\"off\")\n\n    # Prediction Plot\n    ax_mask = axs[\"pred\"]\n    ax_mask.imshow(y_pred, vmin=0, vmax=1)\n    ax_mask.axis(\"off\")\n    ax_mask.set_title(\"Model Output\")\n\n    # Slope Plot\n    ax_slope = axs[\"slope\"]\n    ax_slope.axis(\"off\")\n    is_slope = \"slope\" in band_names\n    if is_slope:\n        slope_band = band_names.index(\"slope\")\n        slope = x[slope_band]\n        ax_slope.imshow(slope, cmap=\"cividis\")\n        # Add TPI as contour lines\n        is_rel_elev = \"relative_elevation\" in band_names\n        if is_rel_elev:\n            rel_elev_band = band_names.index(\"relative_elevation\")\n            rel_elev = x[rel_elev_band]\n            cs = ax_slope.contour(rel_elev, [0], colors=\"red\", linewidths=0.3, alpha=0.6)\n            ax_slope.clabel(cs, inline=True, fontsize=5, fmt=\"%.1f\")\n\n        ax_slope.set_title(\"Slope\")\n    else:\n        # Plot empty with message that slope is not provided\n        ax_slope.set_title(\"No Slope values are provided!\")\n\n    # Relative Elevation Plot\n    # rel_elev_band = band_names.index(\"relative_elevation\")\n    # rel_elev = x[rel_elev_band]\n    # ax_rel_elev = axs[\"elev\"]\n    # ax_rel_elev.imshow(rel_elev, cmap=\"cividis\")\n    # ax_rel_elev.axis(\"off\")\n    # ax_rel_elev.set_title(\"Relative Elevation\")\n\n    return fig, axs\n</code></pre>"},{"location":"reference/darts_segmentation/training/viz/#darts_segmentation.training.viz.plot_training_data_distribution","title":"plot_training_data_distribution","text":"<pre><code>plot_training_data_distribution(\n    train_metadata: geopandas.GeoDataFrame,\n    val_metadata: geopandas.GeoDataFrame | None,\n    test_metadata: geopandas.GeoDataFrame | None,\n    name: str,\n) -&gt; tuple[\n    matplotlib.pyplot.Figure,\n    dict[str, matplotlib.pyplot.Axes],\n]\n</code></pre> <p>Plot the distribution of training data by region on a polar projection.</p> <p>Parameters:</p> <ul> <li> <code>train_metadata</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>GeoDataFrame containing training metadata.</p> </li> <li> <code>val_metadata</code>               (<code>geopandas.GeoDataFrame | None</code>)           \u2013            <p>GeoDataFrame containing validation metadata.</p> </li> <li> <code>test_metadata</code>               (<code>geopandas.GeoDataFrame | None</code>)           \u2013            <p>GeoDataFrame containing test metadata.</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Name of the dataset or experiment for the plot title.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[matplotlib.pyplot.Figure, dict[str, matplotlib.pyplot.Axes]]</code>           \u2013            <p>tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/viz.py</code> <pre><code>def plot_training_data_distribution(\n    train_metadata: gpd.GeoDataFrame,\n    val_metadata: gpd.GeoDataFrame | None,\n    test_metadata: gpd.GeoDataFrame | None,\n    name: str,\n) -&gt; tuple[plt.Figure, dict[str, plt.Axes]]:\n    \"\"\"Plot the distribution of training data by region on a polar projection.\n\n    Args:\n        train_metadata (gpd.GeoDataFrame): GeoDataFrame containing training metadata.\n        val_metadata (gpd.GeoDataFrame | None): GeoDataFrame containing validation metadata.\n        test_metadata (gpd.GeoDataFrame | None): GeoDataFrame containing test metadata.\n        name (str): Name of the dataset or experiment for the plot title.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: The figure and axes of the plot.\n\n    \"\"\"\n    # Aggregate by sample_id to get counts of not-empty tiles for train and test\n    # Get centroids of the aggregated geometries\n    train_metadata[\"not-empty\"] = ~train_metadata[\"empty\"]\n    train_sample_data = train_metadata[[\"sample_id\", \"not-empty\", \"geometry\"]].dissolve(by=\"sample_id\", aggfunc=\"sum\")\n    train_centroids = train_sample_data.geometry.centroid\n    if val_metadata is not None:\n        val_metadata[\"not-empty\"] = ~val_metadata[\"empty\"]\n        val_sample_data = val_metadata[[\"sample_id\", \"not-empty\", \"geometry\"]].dissolve(by=\"sample_id\", aggfunc=\"sum\")\n        val_centroids = val_sample_data.geometry.centroid\n    if test_metadata is not None:\n        test_metadata[\"not-empty\"] = ~test_metadata[\"empty\"]\n        test_sample_data = test_metadata[[\"sample_id\", \"not-empty\", \"geometry\"]].dissolve(by=\"sample_id\", aggfunc=\"sum\")\n        test_centroids = test_sample_data.geometry.centroid\n\n    # Create figure with NorthPolarStereo projection\n    fig, axs = plt.subplot_mosaic(\n        [[\"map\", \"map\", \"map\", \"train-dist\"], [\"map\", \"map\", \"map\", \"val-dist\"], [\"map\", \"map\", \"map\", \"test-dist\"]],\n        layout=\"constrained\",\n        figsize=(12, 8),\n        per_subplot_kw={\"map\": {\"projection\": ccrs.NorthPolarStereo()}},\n    )\n\n    # Set the extent to limit to 55\u00b0N latitude (circular boundary)\n    axs[\"map\"].set_extent([-180, 180, 55, 90], ccrs.PlateCarree())\n\n    # Add map features\n    axs[\"map\"].add_feature(cfeature.LAND, facecolor=\"lightgray\", alpha=0.3)\n    axs[\"map\"].add_feature(cfeature.OCEAN, facecolor=\"lightblue\", alpha=0.3)\n    axs[\"map\"].add_feature(cfeature.COASTLINE, linewidth=0.5)\n    axs[\"map\"].add_feature(cfeature.BORDERS, linewidth=0.3, linestyle=\":\")\n\n    # Add gridlines\n    axs[\"map\"].gridlines(draw_labels=True, linewidth=0.5, alpha=0.5, linestyle=\"--\")\n\n    # Determine common vmax for consistent color scaling\n    vmax = max(\n        train_sample_data[\"not-empty\"].max(),\n        (val_sample_data[\"not-empty\"].max() if val_metadata is not None else 0),\n        (test_sample_data[\"not-empty\"].max() if test_metadata is not None else 0),\n    )\n\n    # Plot the training regions with circles\n    train_scatter = axs[\"map\"].scatter(\n        train_centroids.x,\n        train_centroids.y,\n        c=train_sample_data[\"not-empty\"],\n        cmap=\"YlGnBu\",\n        s=120,\n        alpha=0.7,\n        transform=ccrs.PlateCarree(),\n        edgecolors=\"black\",\n        linewidths=0.5,\n        vmin=0,\n        vmax=vmax,\n        marker=\"o\",  # Circle for training data\n        label=\"Training\",\n    )\n\n    # Plot the training regions with circles\n    if val_metadata is not None:\n        axs[\"map\"].scatter(\n            val_centroids.x,\n            val_centroids.y,\n            c=val_sample_data[\"not-empty\"],\n            cmap=\"YlGnBu\",\n            s=80,\n            alpha=0.7,\n            transform=ccrs.PlateCarree(),\n            edgecolors=\"black\",\n            linewidths=0.5,\n            vmin=0,\n            vmax=vmax,\n            marker=\"*\",  # Circle for training data\n            label=\"Validation\",\n        )\n\n    # Plot the test regions with triangles\n    if test_metadata is not None:\n        axs[\"map\"].scatter(\n            test_centroids.x,\n            test_centroids.y,\n            c=test_sample_data[\"not-empty\"],\n            cmap=\"YlGnBu\",\n            s=80,\n            alpha=0.7,\n            transform=ccrs.PlateCarree(),\n            edgecolors=\"black\",\n            linewidths=0.5,\n            vmin=0,\n            vmax=vmax,\n            marker=\"^\",  # Triangle for test data\n            label=\"Test\",\n        )\n\n    # Add colorbar\n    cbar = plt.colorbar(train_scatter, ax=axs[\"map\"], shrink=0.6, pad=0.05)\n    cbar.set_label(\"Number of Patches with Data\", rotation=270, labelpad=20, fontsize=12)\n\n    # Add legend for train/test split\n    legend = axs[\"map\"].legend(\n        loc=\"lower left\", frameon=True, fancybox=True, shadow=True, fontsize=11, title=\"Data Split\"\n    )\n    legend.get_frame().set_alpha(0.9)\n\n    # Create circular boundary at 55\u00b0N\n    theta = np.linspace(0, 2 * np.pi, 100)\n    verts = np.vstack([np.sin(theta), np.cos(theta)]).T\n    circle = mpath.Path(verts * 0.5 + 0.5)\n    axs[\"map\"].set_boundary(circle, transform=axs[\"map\"].transAxes)\n\n    axs[\"map\"].set_title(f\"Training Data Distribution by Region ({name})\", fontsize=14, fontweight=\"bold\", pad=20)\n\n    sns.histplot(\n        train_metadata,\n        y=\"region\",\n        hue=\"not-empty\",\n        multiple=\"stack\",\n        ax=axs[\"train-dist\"],\n        palette=[\"#7f8c8d\", \"#27ae60\"],  # Gray for w/o RTS, Green for w/ RTS\n    )\n    axs[\"train-dist\"].set_title(\"Training Set Distribution by Region\")\n    axs[\"train-dist\"].legend(labels=[\"w RTS\", \"w/o RTS\"])\n    axs[\"train-dist\"].set_ylabel(\"\")\n    axs[\"train-dist\"].set_xlabel(\"Number of Patches\")\n\n    if val_metadata is not None:\n        sns.histplot(\n            val_metadata,\n            y=\"region\",\n            hue=\"not-empty\",\n            multiple=\"stack\",\n            ax=axs[\"val-dist\"],\n            palette=[\"#7f8c8d\", \"#27ae60\"],  # Gray for w/o RTS, Green for w/ RTS\n        )\n        axs[\"val-dist\"].set_title(\"Validation Set Distribution by Region\")\n        axs[\"val-dist\"].legend(labels=[\"w/ RTS\", \"w/o RTS\"])\n        axs[\"val-dist\"].set_ylabel(\"\")\n        axs[\"val-dist\"].set_xlabel(\"Number of Patches\")\n\n    if test_metadata is not None:\n        sns.histplot(\n            test_metadata,\n            y=\"region\",\n            hue=\"not-empty\",\n            multiple=\"stack\",\n            ax=axs[\"test-dist\"],\n            palette=[\"#7f8c8d\", \"#27ae60\"],  # Gray for w/o RTS, Green for w/ RTS\n        )\n        axs[\"test-dist\"].set_title(\"Test Set Distribution by Region\")\n        axs[\"test-dist\"].legend(labels=[\"w/ RTS\", \"w/o RTS\"])\n        axs[\"test-dist\"].set_ylabel(\"\")\n        axs[\"test-dist\"].set_xlabel(\"Number of Patches\")\n\n    # fig.tight_layout()\n    return fig, axs\n</code></pre>"},{"location":"reference/darts_utils/","title":"darts_utils","text":""},{"location":"reference/darts_utils/#darts_utils","title":"darts_utils","text":"<p>Utility functions for the DARTS dataset.</p>"},{"location":"reference/darts_utils/#darts_utils.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_utils/bands/","title":"bands","text":""},{"location":"reference/darts_utils/bands/#darts_utils.bands","title":"darts_utils.bands","text":"<p>Hardcoded band information for encoding/decoding and normalization.</p>"},{"location":"reference/darts_utils/bands/#darts_utils.bands._supported_dtypes","title":"_supported_dtypes  <code>module-attribute</code>","text":"<pre><code>_supported_dtypes = [\n    \"bool\",\n    \"int8\",\n    \"int16\",\n    \"int32\",\n    \"int64\",\n    \"uint8\",\n    \"uint16\",\n    \"uint32\",\n    \"uint64\",\n    \"float32\",\n    \"float64\",\n]\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.manager","title":"manager  <code>module-attribute</code>","text":"<pre><code>manager = darts_utils.bands.BandManager(\n    {\n        \"blue\": darts_utils.bands.BandCodec.optical(),\n        \"red\": darts_utils.bands.BandCodec.optical(),\n        \"green\": darts_utils.bands.BandCodec.optical(),\n        \"nir\": darts_utils.bands.BandCodec.optical(),\n        \"s2_scl\": darts_utils.bands.BandCodec.mask(11),\n        \"planet_udm\": darts_utils.bands.BandCodec.mask(8),\n        \"quality_data_mask\": darts_utils.bands.BandCodec.mask(\n            2\n        ),\n        \"dem\": darts_utils.bands.BandCodec(\n            disk_dtype=\"float32\",\n            memory_dtype=\"float32\",\n            valid_range=(-100, 3000),\n            scale_factor=0.1,\n            offset=-100.0,\n            fill_value=-1,\n        ),\n        \"arcticdem_data_mask\": darts_utils.bands.BandCodec(\n            disk_dtype=\"bool\",\n            memory_dtype=\"uint8\",\n            valid_range=(0, 1),\n        ),\n        \"tc_brightness\": darts_utils.bands.BandCodec.tc(),\n        \"tc_greenness\": darts_utils.bands.BandCodec.tc(),\n        \"tc_wetness\": darts_utils.bands.BandCodec.tc(),\n        \"ndvi\": darts_utils.bands.BandCodec.ndi(),\n        \"relative_elevation\": darts_utils.bands.BandCodec(\n            disk_dtype=\"int16\",\n            memory_dtype=\"float32\",\n            valid_range=(-50, 50),\n            scale_factor=100 / 30000,\n            offset=-50.0,\n            fill_value=-1,\n        ),\n        \"slope\": darts_utils.bands.BandCodec(\n            disk_dtype=\"int16\",\n            memory_dtype=\"float32\",\n            valid_range=(0, 90),\n            scale_factor=1 / 100,\n            offset=0.0,\n            fill_value=-1,\n        ),\n        \"aspect\": darts_utils.bands.BandCodec(\n            disk_dtype=\"int16\",\n            memory_dtype=\"float32\",\n            valid_range=(0, 360),\n            scale_factor=1 / 10,\n            offset=0.0,\n            fill_value=-1,\n        ),\n        \"hillshade\": darts_utils.bands.BandCodec(\n            disk_dtype=\"int16\",\n            memory_dtype=\"float32\",\n            valid_range=(0, 1),\n            scale_factor=1 / 10000,\n            offset=0.0,\n            fill_value=-1,\n        ),\n        \"curvature\": darts_utils.bands.BandCodec.ndi(),\n        \"probabilities\": darts_utils.bands.BandCodec.percentage(),\n        \"probabilities-*\": darts_utils.bands.BandCodec.percentage(),\n        \"binarized_segmentation\": darts_utils.bands.BandCodec.bool(),\n        \"binarized_segmentation-*\": darts_utils.bands.BandCodec.bool(),\n        \"extent\": darts_utils.bands.BandCodec.bool(),\n    }\n)\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec","title":"BandCodec  <code>dataclass</code>","text":"<pre><code>BandCodec(\n    disk_dtype: str,\n    memory_dtype: str,\n    valid_range: tuple[float | int, float | int],\n    scale_factor: float | None = None,\n    offset: float | None = None,\n    fill_value: float | int | None = None,\n)\n</code></pre> <p>Encoding / Decoding information for a single band (channel).</p> <p>Stores information about how to convert data between the three different representations:</p> <ul> <li>memory: the in-memory representation of the data (native)</li> <li>disk: the on-disk representation of the data (best compression)</li> <li>model: the representation used for model training &amp; inference (normalized between 0 and 1)</li> </ul> <p>In general the \"default\" representation is the memory representation and is further referred to as \"decoded\". The disk and model representations are referred to as \"encoded\", both with their own scale factors and offsets. The scale and offset of the disk representation must be chosen such that the encoded values fit into the disk dtype. The model representation is normalized to the range [0, 1] using the valid range of the memory representation. The formulas used are based on the NetCDF conventions for encoding and decoding data used by Xarray: https://docs.xarray.dev/en/stable/user-guide/io.html#scaling-and-type-conversions</p> <pre><code>decoded = scale_factor * encoded + add_offset\nencoded = (decoded - add_offset) / scale_factor\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.disk_dtype","title":"disk_dtype  <code>instance-attribute</code>","text":"<pre><code>disk_dtype: str\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.disk_range","title":"disk_range  <code>property</code>","text":"<pre><code>disk_range: tuple[float | int, float | int]\n</code></pre> <p>Range of the disk representation.</p>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.fill_value","title":"fill_value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fill_value: float | int | None = None\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.memory_dtype","title":"memory_dtype  <code>instance-attribute</code>","text":"<pre><code>memory_dtype: str\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.norm_factor","title":"norm_factor  <code>property</code>","text":"<pre><code>norm_factor: float\n</code></pre> <p>Normalization factor for the model representation.</p>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.norm_offset","title":"norm_offset  <code>property</code>","text":"<pre><code>norm_offset: float\n</code></pre> <p>Normalization offset for the model representation.</p>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.offset","title":"offset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offset: float | None = None\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.scale_factor","title":"scale_factor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scale_factor: float | None = None\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.valid_range","title":"valid_range  <code>instance-attribute</code>","text":"<pre><code>valid_range: tuple[float | int, float | int]\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.bool","title":"bool  <code>classmethod</code>","text":"<pre><code>bool() -&gt; darts_utils.bands.BandCodec\n</code></pre> <p>Create a BandCodec for boolean bands.</p> <p>Boolean bands are represented as <code>bool</code> in memory and on disk, with a valid range of (False, True). They do not have a scale factor or offset, and the fill value is always None.</p> <p>Returns:</p> <ul> <li> <code>BandCodec</code> (              <code>darts_utils.bands.BandCodec</code> )          \u2013            <p>A BandCodec instance for boolean bands.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>@classmethod\ndef bool(cls) -&gt; \"BandCodec\":\n    \"\"\"Create a BandCodec for boolean bands.\n\n    Boolean bands are represented as `bool` in memory and on disk, with a valid range of (False, True).\n    They do not have a scale factor or offset, and the fill value is always None.\n\n    Returns:\n        BandCodec: A BandCodec instance for boolean bands.\n\n    \"\"\"\n    return cls(\n        disk_dtype=\"bool\",\n        memory_dtype=\"bool\",\n        valid_range=(False, True),\n    )\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.mask","title":"mask  <code>classmethod</code>","text":"<pre><code>mask(vmax: int) -&gt; darts_utils.bands.BandCodec\n</code></pre> <p>Create a BandCodec for non-binary masks.</p> <p>Assumes the mask always start with 0</p> <p>Parameters:</p> <ul> <li> <code>vmax</code>               (<code>int</code>)           \u2013            <p>Maximum value of the mask</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BandCodec</code> (              <code>darts_utils.bands.BandCodec</code> )          \u2013            <p>A BandCodec instance for non-binary masks.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>@classmethod\ndef mask(cls, vmax: int) -&gt; \"BandCodec\":\n    \"\"\"Create a BandCodec for non-binary masks.\n\n    Assumes the mask always start with 0\n\n    Args:\n        vmax (int): Maximum value of the mask\n\n    Returns:\n        BandCodec: A BandCodec instance for non-binary masks.\n\n    \"\"\"\n    return cls(\n        disk_dtype=\"uint8\",\n        memory_dtype=\"uint8\",\n        valid_range=(0, vmax),\n    )\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.ndi","title":"ndi  <code>classmethod</code>","text":"<pre><code>ndi() -&gt; darts_utils.bands.BandCodec\n</code></pre> <p>Create a BandCodec for Normalized Difference Index (NDI) bands.</p> <p>NDI bands are represented as <code>float32</code> in memory and as <code>int16</code> on disk, with a valid range of (-1.0, 1.0) in memory and (0, 20000) on disk with -1 as NoData.</p> <p>Returns:</p> <ul> <li> <code>BandCodec</code> (              <code>darts_utils.bands.BandCodec</code> )          \u2013            <p>A BandCodec instance for NDI bands.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>@classmethod\ndef ndi(cls) -&gt; \"BandCodec\":\n    \"\"\"Create a BandCodec for Normalized Difference Index (NDI) bands.\n\n    NDI bands are represented as `float32` in memory and as `int16` on disk,\n    with a valid range of (-1.0, 1.0) in memory and (0, 20000) on disk with -1 as NoData.\n\n    Returns:\n        BandCodec: A BandCodec instance for NDI bands.\n\n    \"\"\"\n    return cls(\n        disk_dtype=\"int16\",\n        memory_dtype=\"float32\",\n        valid_range=(-1.0, 1.0),\n        scale_factor=1 / 10000,\n        offset=-1.0,\n        fill_value=-1,\n    )\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.optical","title":"optical  <code>classmethod</code>","text":"<pre><code>optical() -&gt; darts_utils.bands.BandCodec\n</code></pre> <p>Create a BandCodec for optical satellite imagery.</p> <p>Optical imagery bands are represented as <code>float32</code> in memory and as <code>uint16</code> on disk, with a valid range of (-0.1, 0.5), in memory and (0, 6000) on disk with 0 as NoData. Theoretically, the valid range of optical bands is between 0 and 1, but in practice, values above 0.5 are very rare in cloud and snowless regions. Further, the new Sentinel 2 L2A products allow for negative values due to atmospheric correction.</p> <p>Please see the documentation about bands for caveats with optical data.</p> <p>Returns:</p> <ul> <li> <code>BandCodec</code> (              <code>darts_utils.bands.BandCodec</code> )          \u2013            <p>A BandCodec instance for optical bands.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>@classmethod\ndef optical(cls) -&gt; \"BandCodec\":\n    \"\"\"Create a BandCodec for optical satellite imagery.\n\n    Optical imagery bands are represented as `float32` in memory and as `uint16` on disk,\n    with a valid range of (-0.1, 0.5), in memory and (0, 6000) on disk with 0 as NoData.\n    Theoretically, the valid range of optical bands is between 0 and 1, but in practice,\n    values above 0.5 are very rare in cloud and snowless regions.\n    Further, the new Sentinel 2 L2A products allow for negative values due to atmospheric correction.\n\n    Please see the documentation about bands for caveats with optical data.\n\n    Returns:\n        BandCodec: A BandCodec instance for optical bands.\n\n    \"\"\"\n    return cls(\n        disk_dtype=\"uint16\",\n        memory_dtype=\"float32\",\n        valid_range=(-0.1, 0.5),\n        scale_factor=1 / 10000,\n        offset=-0.1,\n        fill_value=0,\n    )\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.percentage","title":"percentage  <code>classmethod</code>","text":"<pre><code>percentage() -&gt; darts_utils.bands.BandCodec\n</code></pre> <p>Create a BandCodec for percentage bands.</p> <p>Percentage bands are represented as <code>float32</code> in memory and as <code>uint8</code> on disk, with a valid range of (0.0, 1.0) in memory and (0, 100) on disk with 255 as NoData.</p> <p>Returns:</p> <ul> <li> <code>BandCodec</code> (              <code>darts_utils.bands.BandCodec</code> )          \u2013            <p>A BandCodec instance for percentage bands.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>@classmethod\ndef percentage(cls) -&gt; \"BandCodec\":\n    \"\"\"Create a BandCodec for percentage bands.\n\n    Percentage bands are represented as `float32` in memory and as `uint8` on disk,\n    with a valid range of (0.0, 1.0) in memory and (0, 100) on disk with 255 as NoData.\n\n    Returns:\n        BandCodec: A BandCodec instance for percentage bands.\n\n    \"\"\"\n    return cls(\n        disk_dtype=\"uint8\",\n        memory_dtype=\"float32\",\n        valid_range=(0.0, 1.0),\n        scale_factor=1 / 100,\n        offset=0.0,\n        fill_value=255,\n    )\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.tc","title":"tc  <code>classmethod</code>","text":"<pre><code>tc() -&gt; darts_utils.bands.BandCodec\n</code></pre> <p>Create a BandCodec for tcvis data.</p> <p>TCVis bands are represented as <code>uint8</code> in memory and on dask, utilizing the complete 0-255 range. There are no NoData values.</p> <p>Returns:</p> <ul> <li> <code>BandCodec</code> (              <code>darts_utils.bands.BandCodec</code> )          \u2013            <p>A BandCodec instance for TCVis bands.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>@classmethod\ndef tc(cls) -&gt; \"BandCodec\":\n    \"\"\"Create a BandCodec for tcvis data.\n\n    TCVis bands are represented as `uint8` in memory and on dask, utilizing the complete 0-255 range.\n    There are no NoData values.\n\n    Returns:\n        BandCodec: A BandCodec instance for TCVis bands.\n\n    \"\"\"\n    return cls(\n        disk_dtype=\"uint8\",\n        memory_dtype=\"uint8\",\n        valid_range=(0, 255),\n    )\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandCodec.validate","title":"validate","text":"<pre><code>validate() -&gt; str | None\n</code></pre> <p>Validate the codec configuration.</p> <p>Checks if the disk representation's valid range fits within the limits of the disk dtype, and if the model representation's valid range is normalized between 0 and 1.</p> <p>Further checks the validity of the data types for scale factor, offset, and fill value.</p> <p>Returns:</p> <ul> <li> <code>str | None</code>           \u2013            <p>str | None: Reason for invalid configuration if any, otherwise None.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def validate(self) -&gt; str | None:  # noqa: C901\n    \"\"\"Validate the codec configuration.\n\n    Checks if the disk representation's valid range fits within the limits of the disk dtype,\n    and if the model representation's valid range is normalized between 0 and 1.\n\n    Further checks the validity of the data types for scale factor, offset, and fill value.\n\n    Returns:\n        str | None: Reason for invalid configuration if any, otherwise None.\n\n    \"\"\"\n    # ?: This complete function is written to be easily readable and understandable.\n    # Of course, lot of if statements could be chained / combined, but that would make it harder to read.\n\n    # Check dtype compatibility\n    if self.memory_dtype not in _supported_dtypes:\n        return f\"Unsupported memory dtype: {self.memory_dtype}\"\n\n    if self.disk_dtype not in _supported_dtypes:\n        return f\"Unsupported disk dtype: {self.disk_dtype}\"\n\n    is_bool = self.memory_dtype == \"bool\"\n    is_float = self.memory_dtype.startswith(\"float\")\n\n    # Check range validity\n    if not is_bool:\n        disk_range = (\n            (self.valid_range[0] - (self.offset or 0)) / (self.scale_factor or 1),\n            (self.valid_range[1] - (self.offset or 0)) / (self.scale_factor or 1),\n        )\n        disk_dtype_min, disk_dtype_max = _get_dtype_min_max(self.disk_dtype)\n        if disk_range[0] &lt; disk_dtype_min or disk_range[1] &gt; disk_dtype_max:\n            return (\n                f\"Disk range {disk_range} is out of bounds \"\n                f\"for dtype {self.disk_dtype} ({disk_dtype_min}, {disk_dtype_max})\"\n            )\n\n        norm_range = (\n            (self.valid_range[0] - self.norm_offset) / self.norm_factor,\n            (self.valid_range[1] - self.norm_offset) / self.norm_factor,\n        )\n        if norm_range[0] &lt; 0 or norm_range[1] &gt; 1:\n            return (\n                f\"Model range {norm_range} is out of bounds for normalized representation \"\n                \"(should be between 0 and 1)\"\n            )\n    else:\n        # For boolean bands, valid range is always (False, True)\n        if self.valid_range != (False, True):\n            return \"Boolean bands must have valid range (False, True)\"\n\n    # Check scale factor and offset validity\n    if self.scale_factor is not None or self.offset is not None:\n        if not is_float:\n            return \"Integer and Boolean bands must not have scale factor or offset\"\n        # Check if one is None\n        if self.scale_factor is None or self.offset is None:\n            return \"Float bands must have both or none of scale factor and offset defined\"\n\n    # Check fill value validity\n    if is_float:\n        if not isinstance(self.fill_value, float | int) and self.fill_value is not None:\n            return \"Float bands must have float or integer fill value if present\"\n    else:\n        if self.fill_value is not None:\n            return \"Integer and Boolean bands must not have fill value\"\n\n    return None\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager","title":"BandManager  <code>dataclass</code>","text":"<pre><code>BandManager(codecs: dict[str, darts_utils.bands.BandCodec])\n</code></pre> <p>Meta class for loading, storing and encoding xarray datasets based on band codecs.</p> <p>Supports wildcard patterns for band names, e.g. \"probabilities_*\" to match all dataset variables starting with \"probabilities_\".</p>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.codecs","title":"codecs  <code>instance-attribute</code>","text":"<pre><code>codecs: dict[str, darts_utils.bands.BandCodec]\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.__contains__","title":"__contains__","text":"<pre><code>__contains__(band: str) -&gt; bool\n</code></pre> <p>Check if a band is present in the manager.</p> <p>Parameters:</p> <ul> <li> <code>band</code>               (<code>str</code>)           \u2013            <p>The band name to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the band is present, False otherwise.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def __contains__(self, band: str) -&gt; bool:\n    \"\"\"Check if a band is present in the manager.\n\n    Args:\n        band (str): The band name to check.\n\n    Returns:\n        bool: True if the band is present, False otherwise.\n\n    \"\"\"\n    return band in self.codecs\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(\n    selector: list[str] | str,\n) -&gt; (\n    dict[str, darts_utils.bands.BandCodec]\n    | darts_utils.bands.BandCodec\n)\n</code></pre> <p>Get a subset of codecs by band names or a single codec by band name.</p> <p>Parameters:</p> <ul> <li> <code>selector</code>               (<code>list[str] | str</code>)           \u2013            <p>A list of band names or a single band name to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, darts_utils.bands.BandCodec] | darts_utils.bands.BandCodec</code>           \u2013            <p>dict[str, BandCodec]: A dictionary of selected codecs.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>             \u2013            <p>If the band name is not found in the codecs.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def __getitem__(self, selector: list[str] | str) -&gt; dict[str, BandCodec] | BandCodec:\n    \"\"\"Get a subset of codecs by band names or a single codec by band name.\n\n    Args:\n        selector (list[str] | str): A list of band names or a single band name to select.\n\n    Returns:\n        dict[str, BandCodec]: A dictionary of selected codecs.\n\n    Raises:\n        KeyError: If the band name is not found in the codecs.\n\n    \"\"\"\n    if isinstance(selector, str):\n        codec = self.codecs.get(selector)\n        if codec is None:\n            wildcard_bands = [band for band in self.codecs if \"*\" in band]\n            codec = next((self.codecs[wb] for wb in wildcard_bands if wb.replace(\"*\", \"\") in selector), None)\n            if codec is None:\n                raise KeyError(f\"Band '{selector}' not found in codecs.\")\n        return codec\n\n    return BandManager({band: self.codecs[band] for band in selector if band in self.codecs})\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; collections.abc.Generator[\n    str, typing.Any, None\n]\n</code></pre> <p>Iterate over the bands in the manager.</p> <p>Yields:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The band names in the manager.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def __iter__(self) -&gt; Generator[str, Any, None]:\n    \"\"\"Iterate over the bands in the manager.\n\n    Yields:\n        str: The band names in the manager.\n\n    \"\"\"\n    yield from self.codecs\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.crop","title":"crop","text":"<pre><code>crop(dataset: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Crop the dataset to the valid range of each band.</p> <p>Clips each band in the dataset to its valid range defined in the codec. This is useful for ensuring that the data fits into the encoding.</p> <p>Inplace operation</p> <p>This operation happens inplace - hence the data of the input dataset is changed.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The dataset to crop.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The cropped dataset.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def crop(self, dataset: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Crop the dataset to the valid range of each band.\n\n    Clips each band in the dataset to its valid range defined in the codec.\n    This is useful for ensuring that the data fits into the encoding.\n\n    !!! warning \"Inplace operation\"\n\n        This operation happens inplace - hence the data of the input dataset is changed.\n\n    Args:\n        dataset (xr.Dataset): The dataset to crop.\n\n    Returns:\n        xr.Dataset: The cropped dataset.\n\n    \"\"\"\n    for band in dataset:\n        codec = self.get(band)\n        if codec is None:\n            continue\n        min_val, max_val = codec.valid_range\n        dataset[band] = dataset[band].clip(min=min_val, max=max_val)\n    return dataset\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.get","title":"get","text":"<pre><code>get(selector: str) -&gt; darts_utils.bands.BandCodec | None\n</code></pre> <p>Get a codec by band name.</p> <p>Parameters:</p> <ul> <li> <code>selector</code>               (<code>str</code>)           \u2013            <p>The band name to select.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>darts_utils.bands.BandCodec | None</code>           \u2013            <p>BandCodec | None: The codec for the band, or None if not found.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def get(self, selector: str) -&gt; BandCodec | None:\n    \"\"\"Get a codec by band name.\n\n    Args:\n        selector (str): The band name to select.\n\n    Returns:\n        BandCodec | None: The codec for the band, or None if not found.\n\n    \"\"\"\n    try:\n        return self[selector]\n    except KeyError:\n        return None\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.normalize","title":"normalize","text":"<pre><code>normalize(dataset: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Normalize the dataset to the model representation.</p> <p>Applies the normalization formula to each band in the dataset based on the codec configuration.</p> <p>Leaves boolean bands as they are, just converting them to float32. Also fills NaN values with 0.0 after normalization. All other bands are normalized to the range [0, 1] using the valid range of the memory representation and converted to float32.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The dataset to normalize.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The normalized dataset.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the dataset has unknown bands / channels.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def normalize(self, dataset: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Normalize the dataset to the model representation.\n\n    Applies the normalization formula to each band in the dataset based on the codec configuration.\n\n    Leaves boolean bands as they are, just converting them to float32.\n    Also fills NaN values with 0.0 after normalization.\n    All other bands are normalized to the range [0, 1] using the valid range of the memory representation\n    and converted to float32.\n\n    Args:\n        dataset (xr.Dataset): The dataset to normalize.\n\n    Returns:\n        xr.Dataset: The normalized dataset.\n\n    Raises:\n        ValueError: If the dataset has unknown bands / channels.\n\n    \"\"\"\n    # ?: We do not provide a default codec, so we cannot normalize if the dataset has unknown bands.\n    # This is a design choice, because we want to ensure that we can track down what went into the model.\n    # Note: Wildcard bands are not supported here for the same reason.\n    if not set(dataset).issubset(self):\n        raise ValueError(f\"Dataset has unknown bands: {set(dataset) - set(self)}\")\n    dataset = dataset.copy(deep=True)\n    for band in dataset:\n        if dataset[band].dtype == \"bool\":\n            # Convert boolean to float to it is already 0-1\n            dataset[band] = dataset[band].astype(\"float32\")\n            continue\n        codec = self.codecs[band]  # Safe because we checked above\n        dataset[band] = (\n            ((dataset[band] - codec.norm_offset) / codec.norm_factor).astype(\"float32\").fillna(0.0).clip(0.0, 1.0)\n        )\n    return dataset\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.open","title":"open","text":"<pre><code>open(path: pathlib.Path | str) -&gt; xarray.Dataset\n</code></pre> <p>Load a dataset from a NetCDF file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The path to the NetCDF file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def open(self, path: Path | str) -&gt; xr.Dataset:\n    \"\"\"Load a dataset from a NetCDF file.\n\n    Args:\n        path (Path | str): The path to the NetCDF file.\n\n    Returns:\n        xr.Dataset: The loaded dataset.\n\n    \"\"\"\n    # ! Unknown why, but decode_coords=\"all\" sometimes fails! Falls back to manually set\n    # dataset = xr.open_dataset(path, engine=\"h5netcdf\", decode_coords=\"all\", decode_cf=True).load()\n    dataset = xr.open_dataset(path, engine=\"h5netcdf\").set_coords(\"spatial_ref\").load()\n    # Change the dtypes to the memory representation\n    for band in dataset:\n        codec = self.get(band)\n        if codec is None:\n            continue\n        if dataset[band].dtype != codec.memory_dtype:\n            dataset[band] = dataset[band].astype(codec.memory_dtype)\n\n    return dataset\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.to_netcdf","title":"to_netcdf","text":"<pre><code>to_netcdf(\n    dataset: xarray.Dataset,\n    path: pathlib.Path | str,\n    crop: bool = True,\n) -&gt; None\n</code></pre> <p>Store the dataset to a NetCDF file.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The dataset to store.</p> </li> <li> <code>path</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The path to the NetCDF file.</p> </li> <li> <code>crop</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to crop the dataset to the valid range. This happens inplace! Defaults to True.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def to_netcdf(self, dataset: xr.Dataset, path: Path | str, crop: bool = True) -&gt; None:\n    \"\"\"Store the dataset to a NetCDF file.\n\n    Args:\n        dataset (xr.Dataset): The dataset to store.\n        path (Path | str): The path to the NetCDF file.\n        crop (bool): Whether to crop the dataset to the valid range. This happens inplace! Defaults to True.\n\n    \"\"\"\n    path = Path(path)\n    encodings = self._get_encodings(dataset)\n    if crop:\n        dataset = self.crop(dataset)\n    dataset.to_netcdf(\n        path,\n        encoding=encodings,\n        engine=\"h5netcdf\",\n    )\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands.BandManager.validate","title":"validate","text":"<pre><code>validate()\n</code></pre> <p>Validate all codecs in the manager.</p> <p>Iterates through all codecs and checks if they are valid.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If any codec is invalid, with a message indicating the band and reason for</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def validate(self):\n    \"\"\"Validate all codecs in the manager.\n\n    Iterates through all codecs and checks if they are valid.\n\n    Raises:\n        ValueError: If any codec is invalid, with a message indicating the band and reason for\n\n    \"\"\"\n    for band, codec in self.codecs.items():\n        invalid_reason = codec.validate()\n        if invalid_reason:\n            raise ValueError(f\"Validation failed for {band=}: {invalid_reason}\")\n</code></pre>"},{"location":"reference/darts_utils/bands/#darts_utils.bands._get_dtype_min_max","title":"_get_dtype_min_max","text":"<pre><code>_get_dtype_min_max(\n    dtype: str,\n) -&gt; tuple[int | float, int | float]\n</code></pre> Source code in <code>darts-utils/src/darts_utils/bands.py</code> <pre><code>def _get_dtype_min_max(dtype: str) -&gt; tuple[int | float, int | float]:\n    if dtype.startswith(\"int\") or dtype.startswith(\"uint\"):\n        return np.iinfo(dtype).min, np.iinfo(dtype).max\n    elif dtype.startswith(\"float\"):\n        return np.finfo(dtype).min, np.finfo(dtype).max\n    elif dtype == \"bool\":\n        return 0, 1\n    else:\n        raise ValueError(f\"Unsupported dtype: {dtype}\")\n</code></pre>"},{"location":"reference/darts_utils/cuda/","title":"cuda","text":""},{"location":"reference/darts_utils/cuda/#darts_utils.cuda","title":"darts_utils.cuda","text":"<p>Utility functions around cuda, e.g. memory management.</p>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.DEFAULT_DEVICE","title":"DEFAULT_DEVICE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DEVICE = 'cuda'\n</code></pre>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.free_cupy","title":"free_cupy","text":"<pre><code>free_cupy()\n</code></pre> <p>Free the CUDA memory of cupy.</p> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def free_cupy():\n    \"\"\"Free the CUDA memory of cupy.\"\"\"\n    try:\n        import cupy as cp  # type: ignore\n    except ImportError:\n        cp = None\n\n    if cp is not None:\n        gc.collect()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.free_torch","title":"free_torch","text":"<pre><code>free_torch()\n</code></pre> <p>Free the CUDA memory of pytorch.</p> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def free_torch():\n    \"\"\"Free the CUDA memory of pytorch.\"\"\"\n    import torch\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.move_to_device","title":"move_to_device","text":"<pre><code>move_to_device(\n    tile: xarray.Dataset,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Context manager to ensure a dataset is on the correct device.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The xarray dataset to operate on.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for calculations (either \"cuda\", \"cpu\", or a specific GPU index).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The xarray dataset on the specified device.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def move_to_device(\n    tile: xr.Dataset,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xr.Dataset:\n    \"\"\"Context manager to ensure a dataset is on the correct device.\n\n    Args:\n        tile: The xarray dataset to operate on.\n        device: The device to use for calculations (either \"cuda\", \"cpu\", or a specific GPU index).\n\n    Returns:\n        xr.Dataset: The xarray dataset on the specified device.\n\n    \"\"\"\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not has_cuda_and_cupy():\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating optical indices on CPU.\"\n        )\n        use_gpu = False\n\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        # Persist in case of dask - since cupy-dask is not supported\n        if tile.chunks is not None:\n            logger.debug(\"Persisting dask array before moving to GPU.\")\n            tile = tile.persist()\n        # Move and calculate on specified device\n        logger.debug(f\"Moving tile to GPU:{device}.\")\n        with cp.cuda.Device(device_nr):\n            tile = tile.cupy.as_cupy()\n    return tile\n</code></pre>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.move_to_host","title":"move_to_host","text":"<pre><code>move_to_host(tile: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Move a dataset from GPU to CPU.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The xarray dataset to move.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: description</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def move_to_host(tile: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Move a dataset from GPU to CPU.\n\n    Args:\n        tile (xr.Dataset): The xarray dataset to move.\n\n    Returns:\n        xr.Dataset: _description_\n\n    \"\"\"\n    if tile.cupy.is_cupy:\n        tile = tile.cupy.as_numpy()\n        free_cupy()\n    return tile\n</code></pre>"},{"location":"reference/darts_utils/functools/","title":"functools","text":""},{"location":"reference/darts_utils/functools/#darts_utils.functools","title":"darts_utils.functools","text":"<p>Function helpers.</p>"},{"location":"reference/darts_utils/functools/#darts_utils.functools.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_utils\", \"darts.shared_utils\")\n)\n</code></pre>"},{"location":"reference/darts_utils/functools/#darts_utils.functools.write_function_args_to_config_file","title":"write_function_args_to_config_file","text":"<pre><code>write_function_args_to_config_file(\n    fpath: pathlib.Path, function: callable, locals_: dict\n)\n</code></pre> <p>Write the arguments of a function.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the config file</p> </li> <li> <code>function</code>               (<code>callable</code>)           \u2013            <p>function to get the arguments from</p> </li> <li> <code>locals_</code>               (<code>dict</code>)           \u2013            <p>locals() dictionary. Needs to be called in parent function</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/functools.py</code> <pre><code>@stopwatch.f(\"Save function arguments to config file\", printer=logger.debug, print_kwargs=[\"fpath\"])\ndef write_function_args_to_config_file(\n    fpath: Path,\n    function: callable,\n    locals_: dict,\n):\n    \"\"\"Write the arguments of a function.\n\n    Args:\n        fpath (Path): Path to the config file\n        function (callable): function to get the arguments from\n        locals_ (dict): locals() dictionary. Needs to be called in parent function\n\n    \"\"\"\n    nargs = function.__code__.co_argcount + function.__code__.co_kwonlyargcount\n    args_ = function.__code__.co_varnames[:nargs]\n    config = {k: locals_[k] for k in args_ if k in locals_}\n    # Convert everything to toml serializable\n    for key, value in config.items():\n        if isinstance(value, Path):\n            config[key] = str(value.resolve())\n        elif isinstance(value, list):\n            config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        elif is_dataclass(value):\n            config[key] = asdict(value)\n    with open(fpath, \"w\") as f:\n        toml.dump(config, f)\n</code></pre>"},{"location":"reference/darts_utils/namegen/","title":"namegen","text":""},{"location":"reference/darts_utils/namegen/#darts_utils.namegen","title":"darts_utils.namegen","text":"<p>Random name generator.</p>"},{"location":"reference/darts_utils/namegen/#darts_utils.namegen.generate_counted_name","title":"generate_counted_name","text":"<pre><code>generate_counted_name(artifact_dir: pathlib.Path) -&gt; str\n</code></pre> <p>Generate a random name with a count attached.</p> <p>The count is calculated by the number of existing directories in the specified artifact directory. The final name is in the format '{somename}-{somesecondname}-{count+1}'.</p> <p>Parameters:</p> <ul> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory of existing runs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The final name.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/namegen.py</code> <pre><code>def generate_counted_name(artifact_dir: Path) -&gt; str:\n    \"\"\"Generate a random name with a count attached.\n\n    The count is calculated by the number of existing directories in the specified artifact directory.\n    The final name is in the format '{somename}-{somesecondname}-{count+1}'.\n\n    Args:\n        artifact_dir (Path): The directory of existing runs.\n\n    Returns:\n        str: The final name.\n\n    \"\"\"\n    from names_generator import generate_name as _generate_name\n\n    run_name = _generate_name(style=\"hyphen\")\n    # Count the number of existing runs in the artifact_dir, increase the number by one and append it to the name\n    run_count = sum(1 for p in artifact_dir.glob(\"*\") if p.is_dir())\n    run_name = f\"{run_name}-{run_count + 1}\"\n    return run_name\n</code></pre>"},{"location":"reference/darts_utils/namegen/#darts_utils.namegen.generate_id","title":"generate_id","text":"<pre><code>generate_id(length: int = 8) -&gt; str\n</code></pre> <p>Generate a random base-36 string of <code>length</code> digits.</p> <p>This method is taken from the wandb SDK.</p> <p>There are ~2.8T base-36 8-digit strings. Generating 210k ids will have a ~1% chance of collision.</p> <p>Parameters:</p> <ul> <li> <code>length</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The length of the string. Defaults to 8.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A random base-36 string of <code>length</code> digits.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/namegen.py</code> <pre><code>def generate_id(length: int = 8) -&gt; str:\n    \"\"\"Generate a random base-36 string of `length` digits.\n\n    This method is taken from the wandb SDK.\n\n    There are ~2.8T base-36 8-digit strings. Generating 210k ids will have a ~1% chance of collision.\n\n    Args:\n        length (int, optional): The length of the string. Defaults to 8.\n\n    Returns:\n        str: A random base-36 string of `length` digits.\n\n    \"\"\"\n    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))\n</code></pre>"},{"location":"reference/darts_utils/namegen/#darts_utils.namegen.generate_name","title":"generate_name","text":"<pre><code>generate_name() -&gt; str\n</code></pre> <p>Generate a random name.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The final name.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/namegen.py</code> <pre><code>def generate_name() -&gt; str:\n    \"\"\"Generate a random name.\n\n    Returns:\n        str: The final name.\n\n    \"\"\"\n    from names_generator import generate_name as _generate_name\n\n    return _generate_name(style=\"hyphen\")\n</code></pre>"},{"location":"reference/darts_utils/patcher/","title":"patcher","text":""},{"location":"reference/darts_utils/patcher/#darts_utils.patcher","title":"darts_utils.patcher","text":"<p>Patch a dataset into smaller patches.</p>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch","title":"Patch  <code>dataclass</code>","text":"<pre><code>Patch(\n    i: int,\n    patch_idx_y: int,\n    patch_idx_x: int,\n    y: slice,\n    x: slice,\n    data: xarray.Dataset | xarray.DataArray,\n)\n</code></pre> <p>Class representing a patch of a dataset.</p>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: xarray.Dataset | xarray.DataArray\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.i","title":"i  <code>instance-attribute</code>","text":"<pre><code>i: int\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.patch_idx","title":"patch_idx  <code>property</code>","text":"<pre><code>patch_idx: tuple[int, int]\n</code></pre> <p>Return the patch index as a tuple.</p>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.patch_idx_x","title":"patch_idx_x  <code>instance-attribute</code>","text":"<pre><code>patch_idx_x: int\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.patch_idx_y","title":"patch_idx_y  <code>instance-attribute</code>","text":"<pre><code>patch_idx_y: int\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: slice\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: slice\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __repr__(self) -&gt; str:  # noqa: D105\n    return f\"Patch {self.i} ({self.patch_idx_y}, {self.patch_idx_x})\"\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset","title":"PatchedDataset","text":"<pre><code>PatchedDataset(\n    ds: xarray.Dataset | xarray.DataArray,\n    patch_size: int,\n    overlap: int,\n)\n</code></pre> <p>Class representing a dataset that has been patched into smaller patches.</p> Example <p>Via getter/setter:</p> <pre><code>tile: xr.Dataset\npatches = PatchedDataset(tile, patch_size, overlap)\nprint(len(patches))\ngrey = (patches[\"blue\"] + patches[\"green\"] + patches[\"red\"]) / 3 # grey is a numpy array\npatches[None] = grey # Replace the data in the patches with the gray data\nnew_tile = patches.combine_patches()\nnew_tile # This is now a DataArray containing the gray data\n</code></pre> <p>Via loop:</p> <pre><code>tile: xr.Dataset\npatches = PatchedDataset(tile, patch_size, overlap)\n# Calculate gray area for each patch\nfor patch in patches:\n    patch.data = (patch.data.blue + patch.data.green + patch.data.red) / 3\n\nnew_tile = patches.combine_patches()\nnew_tile # This is now a DataArray containing the gray data\n</code></pre> <p>Initialize the PatchedDataset.</p> <p>Parameters:</p> <ul> <li> <code>ds</code>               (<code>xarray.Dataset | xarray.DataArray</code>)           \u2013            <p>The dataset to patch.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap between patches.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __init__(self, ds: xr.Dataset | xr.DataArray, patch_size: int, overlap: int) -&gt; list[\"Patch\"]:\n    \"\"\"Initialize the PatchedDataset.\n\n    Args:\n        ds (xr.Dataset | xr.DataArray): The dataset to patch.\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap between patches.\n\n    \"\"\"\n    self.patch_size = patch_size\n    self.overlap = overlap\n    self.coords = ds.coords\n    self._patches = []\n    h, w = ds.sizes[\"y\"], ds.sizes[\"x\"]\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            ys = slice(y, y + patch_size)\n            xs = slice(x, x + patch_size)\n            self._patches.append(\n                Patch(\n                    i=len(self._patches),\n                    patch_idx_y=patch_idx_y,\n                    patch_idx_x=patch_idx_x,\n                    y=ys,\n                    x=xs,\n                    data=ds.isel(y=ys, x=xs),\n                )\n            )\n\n    # Create a soft margin for the patches (NumPy version)\n    margin_ramp = np.concatenate(\n        [\n            np.linspace(0, 1, overlap),\n            np.ones(patch_size - 2 * overlap),\n            np.linspace(1, 0, overlap),\n        ]\n    )\n    self.soft_margin = margin_ramp.reshape(1, patch_size) * margin_ramp.reshape(patch_size, 1)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.coords","title":"coords  <code>instance-attribute</code>","text":"<pre><code>coords = darts_utils.patcher.PatchedDataset(ds).coords\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.overlap","title":"overlap  <code>instance-attribute</code>","text":"<pre><code>overlap = darts_utils.patcher.PatchedDataset(overlap)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size = darts_utils.patcher.PatchedDataset(patch_size)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.soft_margin","title":"soft_margin  <code>instance-attribute</code>","text":"<pre><code>soft_margin = margin_ramp.reshape(\n    1, darts_utils.patcher.PatchedDataset(patch_size)\n) * margin_ramp.reshape(\n    darts_utils.patcher.PatchedDataset(patch_size), 1\n)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str | None) -&gt; numpy.ndarray\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __getitem__(self, key: str | None) -&gt; np.ndarray:  # noqa: D105\n    is_dataarray = all(isinstance(patch.data, xr.DataArray) for patch in self._patches)\n    is_dataset = all(isinstance(patch.data, xr.Dataset) for patch in self._patches)\n    if is_dataset:\n        assert key is not None, \"Key must be provided for Dataset\"\n        return np.array([patch.data[key].data for patch in self._patches])\n    elif is_dataarray:\n        assert key is None, \"Key must be None for DataArray\"\n        return np.array([patch.data.data for patch in self._patches])\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; collections.abc.Iterator[\n    darts_utils.patcher.Patch\n]\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Patch]:  # noqa: D105\n    return iter(self._patches)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __len__(self) -&gt; int:  # noqa: D105\n    return len(self._patches)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key: str | None, a: numpy.array)\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __setitem__(self, key: str | None, a: np.array):  # noqa: D105\n    for i, patch in enumerate(self._patches):\n        if key is None:\n            patch.data = xr.DataArray(a[i], dims=(\"y\", \"x\"))\n        else:\n            patch.data[key] = xr.DataArray(a[i], dims=(\"y\", \"x\"))\n    return self\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.combine_patches","title":"combine_patches","text":"<pre><code>combine_patches() -&gt; xarray.DataArray | xarray.Dataset\n</code></pre> <p>Combine patches into a single dataarray.</p> <p>Returns:</p> <ul> <li> <code>xarray.DataArray | xarray.Dataset</code>           \u2013            <p>xr.DataArray | xr.Dataset: The combined dataarray or dataset.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def combine_patches(self) -&gt; xr.DataArray | xr.Dataset:\n    \"\"\"Combine patches into a single dataarray.\n\n    Returns:\n        xr.DataArray | xr.Dataset: The combined dataarray or dataset.\n\n    \"\"\"\n    is_dataarray = all(isinstance(patch.data, xr.DataArray) for patch in self._patches)\n    is_dataset = all(isinstance(patch.data, xr.Dataset) for patch in self._patches)\n\n    if is_dataarray:\n        combined = xr.DataArray(0.0, dims=(\"y\", \"x\"), coords=self.coords)\n    elif is_dataset:\n        combined = xr.Dataset(coords=self.coords)\n        for var in self._patches[0].data.data_vars:\n            combined[var] = xr.DataArray(0.0, dims=(\"y\", \"x\"), coords=self.coords)\n\n    weights = xr.DataArray(0.0, dims=(\"y\", \"x\"), coords=self.coords)\n    for patch in self._patches:\n        weights[patch.y, patch.x] += self.soft_margin\n        if is_dataarray:\n            combined[patch.y, patch.x] += patch.data * self.soft_margin\n        elif is_dataset:\n            for var in patch.data.data_vars:\n                combined[var][patch.y, patch.x] += patch.data[var] * self.soft_margin\n    # Normalize the combined data by the weights\n    combined /= weights\n    return combined\n</code></pre>"},{"location":"reference/darts_utils/paths/","title":"paths","text":""},{"location":"reference/darts_utils/paths/#darts_utils.paths","title":"darts_utils.paths","text":"<p>Default Path management for all DARTS modules.</p> <p>Places where this default path management should be used:</p> <ul> <li>The root darts CLI</li> <li>The root darts pipelines</li> <li>The segmentation training functions</li> </ul> <p>This module allows for setting and getting default paths for DARTS data storage.</p> <p>Intended usage is to use the provided root_dir, fast_dir, and large_dir functions to build paths.</p> Example <pre><code>from darts_utils.paths import paths\n\nmy_data_pool = paths.large / \"my_data_pool\"\nmy_fast_cache = paths.fast / \"my_fast_cache\"\n</code></pre> <p>The default paths can be set using the set_default_paths function, this should be done at the start of the CLI:</p> <pre><code>```python\nfrom darts_utils.paths import paths\n\ndef cli(data_dir: str = \"data\"):\n    paths.set_defaults(darts_dir=data_dir)\n    ...\n```\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths--structure","title":"Structure:","text":"<p>The paths are grouped in 4 hierarchical levels: - DARTS Data Directory (DARTS_DATA_DIR): The root directory for all DARTS data. - Fast vs. Large Storage:     - Fast Storage (DARTS_FAST_DATA_DIR): For data that requires fast access, e.g., training data, models.     - Large Storage (DARTS_LARGE_DATA_DIR): For large datasets that do not require fast access. - Storage groups:     - Auxiliary Data     - Artifacts     - Training Data (per default in Fast Storage)     - Cache     - Logs     - Output Data     - Models (per default in Fast Storage)     - Input Data     - Archive Data - The respective directories</p>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.paths","title":"paths  <code>module-attribute</code>","text":"<pre><code>paths = darts_utils.paths.PathManagerSingleton()\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.DefaultPaths","title":"DefaultPaths  <code>dataclass</code>","text":"<pre><code>DefaultPaths(\n    darts_dir: pathlib.Path | str | None = None,\n    fast_dir: pathlib.Path | str | None = None,\n    large_dir: pathlib.Path | str | None = None,\n)\n</code></pre> <p>Dataclass for holding default DARTS paths.</p> <p>Attributes:</p> <ul> <li> <code>darts_dir</code>               (<code>pathlib.Path | str | None</code>)           \u2013            <p>The default DARTS data directory. If None, defaults to the current working directory. Defaults to None.</p> </li> <li> <code>fast_dir</code>               (<code>pathlib.Path | str | None</code>)           \u2013            <p>The default DARTS fast data directory. If None, defaults to the DARTS data directory. Defaults to None.</p> </li> <li> <code>large_dir</code>               (<code>pathlib.Path | str | None</code>)           \u2013            <p>The default DARTS large data directory. If None, defaults to the DARTS data directory. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.DefaultPaths.darts_dir","title":"darts_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>darts_dir: pathlib.Path | str | None = None\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.DefaultPaths.fast_dir","title":"fast_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fast_dir: pathlib.Path | str | None = None\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.DefaultPaths.large_dir","title":"large_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>large_dir: pathlib.Path | str | None = None\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton","title":"PathManagerSingleton","text":"<p>Singleton class for managing DARTS paths.</p>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.archive","title":"archive  <code>property</code>","text":"<pre><code>archive: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.artifacts","title":"artifacts  <code>property</code>","text":"<pre><code>artifacts: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.aux","title":"aux  <code>property</code>","text":"<pre><code>aux: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.cache","title":"cache  <code>property</code>","text":"<pre><code>cache: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.fast","title":"fast  <code>property</code>","text":"<pre><code>fast: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.input","title":"input  <code>property</code>","text":"<pre><code>input: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.large","title":"large  <code>property</code>","text":"<pre><code>large: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.logs","title":"logs  <code>property</code>","text":"<pre><code>logs: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.models","title":"models  <code>property</code>","text":"<pre><code>models: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.out","title":"out  <code>property</code>","text":"<pre><code>out: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.training","title":"training  <code>property</code>","text":"<pre><code>training: pathlib.Path\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create a new instance of PathsSingleton or return the existing instance.</p> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def __new__(cls):\n    \"\"\"Create a new instance of PathsSingleton or return the existing instance.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n        cls._instance._initialize_paths()\n    return cls._instance\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.admin_boundaries","title":"admin_boundaries","text":"<pre><code>admin_boundaries() -&gt; pathlib.Path\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def admin_boundaries(self) -&gt; Path:  # noqa: D102\n    d = (self.aux / \"admin_boundaries\").resolve()\n    d.mkdir(parents=True, exist_ok=True)\n    logger.debug(f\"Using administrative boundaries path: {d}\")\n    return d\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.arcticdem","title":"arcticdem","text":"<pre><code>arcticdem(res: typing.Literal[2, 10, 32]) -&gt; pathlib.Path\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def arcticdem(self, res: Literal[2, 10, 32]) -&gt; Path:  # noqa: D102\n    d = (self.aux / f\"arcticdem_{res}m.icechunk\").resolve()\n    logger.debug(f\"Using ArcticDEM path for resolution {res}m: {d}\")\n    return d\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.ensemble_models","title":"ensemble_models","text":"<pre><code>ensemble_models() -&gt; list[pathlib.Path]\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def ensemble_models(self) -&gt; list[Path]:  # noqa: D102\n    model_paths = list(self.models.glob(\"*.pt\"))\n    logger.debug(f\"Using ensemble model paths: {model_paths}\")\n    return model_paths\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.log_all_paths","title":"log_all_paths","text":"<pre><code>log_all_paths(level: int = logging.DEBUG)\n</code></pre> <p>Log all paths managed.</p> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def log_all_paths(self, level: int = logging.DEBUG):\n    \"\"\"Log all paths managed.\"\"\"\n    label_width = 47\n    logmsg = textwrap.dedent(\n        f\"\"\"\n        Logging all default DARTS paths.\n        NOTE: these paths may be overridden by the respective pipelines.\n\n        === DARTS Path-Types ===\n        {\"Fast Directory:\":&lt;{label_width}} {self.fast_dir}\n        {\"Large Directory:\":&lt;{label_width}} {self.large_dir}\n\n        === DARTS Path-Groups ===\n        {\"Aux Directory:\":&lt;{label_width}} {self.aux}\n        {\"Artifacts Directory:\":&lt;{label_width}} {self.artifacts}\n        {\"Training Directory:\":&lt;{label_width}} {self.training}\n        {\"Cache Directory:\":&lt;{label_width}} {self.cache}\n        {\"Logs Directory:\":&lt;{label_width}} {self.logs}\n        {\"Out Directory:\":&lt;{label_width}} {self.out}\n        {\"Models Directory:\":&lt;{label_width}} {self.models}\n        {\"Input Directory:\":&lt;{label_width}} {self.input}\n        {\"Archive Directory:\":&lt;{label_width}} {self.archive}\n\n        === DARTS Paths ===\n        {\"Output Data Directory ('base_pipeline'):\":&lt;{label_width}} {self.output_data(\"base_pipeline\")}\n        {\"Administrative boundaries Directory:\":&lt;{label_width}} {self.admin_boundaries()}\n        {\"ArcticDEM Directory (2m):\":&lt;{label_width}} {self.arcticdem(2)}\n        {\"ArcticDEM Directory (10m):\":&lt;{label_width}} {self.arcticdem(10)}\n        {\"ArcticDEM Directory (32m):\":&lt;{label_width}} {self.arcticdem(32)}\n        {\"TCVIS Directory:\":&lt;{label_width}} {self.tcvis()}\n        {\"Planet Orthotiles Directory:\":&lt;{label_width}} {self.planet_orthotiles()}\n        {\"Planet Scenes Directory:\":&lt;{label_width}} {self.planet_scenes()}\n        {\"Sentinel-2 Grid Directory:\":&lt;{label_width}} {self.sentinel2_grid()}\n        {\"Sentinel-2 Raw Data Directory (CDSE):\":&lt;{label_width}} {self.sentinel2_raw_data(\"cdse\")}\n        {\"Sentinel-2 Raw Data Directory (GEE):\":&lt;{label_width}} {self.sentinel2_raw_data(\"gee\")}\n        {\"Training Data Directory ('pipeline', 256x256):\":&lt;{label_width}} {self.train_data_dir(\"pipeline\", 256)}\n    \"\"\"\n    ).strip()\n    logger.log(level, logmsg)\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.output_data","title":"output_data","text":"<pre><code>output_data(pipeline_name: str) -&gt; pathlib.Path\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def output_data(self, pipeline_name: str) -&gt; Path:  # noqa: D102\n    d = (self.out / pipeline_name).resolve()\n    logger.debug(f\"Using output data path for pipeline '{pipeline_name}': {d}\")\n    return d\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.planet_orthotiles","title":"planet_orthotiles","text":"<pre><code>planet_orthotiles() -&gt; pathlib.Path\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def planet_orthotiles(self) -&gt; Path:  # noqa: D102\n    d = (self.input / \"planet\" / \"tiles\").resolve()\n    logger.debug(f\"Using Planet orthotiles path: {d}\")\n    return d\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.planet_scenes","title":"planet_scenes","text":"<pre><code>planet_scenes() -&gt; pathlib.Path\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def planet_scenes(self) -&gt; Path:  # noqa: D102\n    d = (self.input / \"planet\" / \"scenes\").resolve()\n    logger.debug(f\"Using Planet scenes path: {d}\")\n    return d\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.sentinel2_grid","title":"sentinel2_grid","text":"<pre><code>sentinel2_grid() -&gt; pathlib.Path\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def sentinel2_grid(self) -&gt; Path:  # noqa: D102\n    d = (self.input / \"sentinel2\" / \"grid\").resolve()\n    d.mkdir(parents=True, exist_ok=True)\n    logger.debug(f\"Using Sentinel-2 grid path: {d}\")\n    return d\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.sentinel2_raw_data","title":"sentinel2_raw_data","text":"<pre><code>sentinel2_raw_data(\n    source: typing.Literal[\"cdse\", \"gee\"],\n) -&gt; pathlib.Path\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def sentinel2_raw_data(self, source: Literal[\"cdse\", \"gee\"]) -&gt; Path:  # noqa: D102\n    d = (self.input / \"sentinel2\" / f\"{source}-scenes\").resolve()\n    d.mkdir(parents=True, exist_ok=True)\n    logger.debug(f\"Using Sentinel-2 raw data path for source '{source}': {d}\")\n    return d\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.set_defaults","title":"set_defaults","text":"<pre><code>set_defaults(\n    defaults: darts_utils.paths.DefaultPaths,\n) -&gt; None\n</code></pre> <p>Set the default directories for DARTS.</p> <p>The priority for setting the directories is as follows: 1. Directly set paths via this function. 2. Environment variables. 3. Current working directory.</p> <p>Where the fast_dir and large_dir default to darts_dir if not set.</p> <p>Parameters:</p> <ul> <li> <code>defaults</code>               (<code>darts_utils.paths.DefaultPaths</code>)           \u2013            <p>The default paths to set.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def set_defaults(self, defaults: DefaultPaths) -&gt; None:\n    \"\"\"Set the default directories for DARTS.\n\n    The priority for setting the directories is as follows:\n    1. Directly set paths via this function.\n    2. Environment variables.\n    3. Current working directory.\n\n    Where the fast_dir and large_dir default to darts_dir if not set.\n\n    Args:\n        defaults (DefaultPaths): The default paths to set.\n\n    \"\"\"\n    darts_dir = _parse_path(defaults.darts_dir)\n    self.fast_dir = _parse_path(defaults.fast_dir) or darts_dir or self.fast_dir\n    self.large_dir = _parse_path(defaults.large_dir) or darts_dir or self.large_dir\n    logger.debug(f\"Set DARTS default paths: fast_dir={self.fast_dir}, large_dir={self.large_dir}\")\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.tcvis","title":"tcvis","text":"<pre><code>tcvis() -&gt; pathlib.Path\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def tcvis(self) -&gt; Path:  # noqa: D102\n    d = (self.aux / \"tcvis.icechunk\").resolve()\n    logger.debug(f\"Using TCVIS path: {d}\")\n    return d\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths.PathManagerSingleton.train_data_dir","title":"train_data_dir","text":"<pre><code>train_data_dir(\n    pipeline: str, patch_size: int\n) -&gt; pathlib.Path\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def train_data_dir(self, pipeline: str, patch_size: int) -&gt; Path:  # noqa: D102\n    d = (self.training / f\"{pipeline}_{patch_size}\").resolve()\n    d.mkdir(parents=True, exist_ok=True)\n    logger.debug(f\"Using training data directory for pipeline '{pipeline}' and patch size {patch_size}: {d}\")\n    return d\n</code></pre>"},{"location":"reference/darts_utils/paths/#darts_utils.paths._parse_path","title":"_parse_path","text":"<pre><code>_parse_path(\n    p: pathlib.Path | str | None,\n) -&gt; pathlib.Path | None\n</code></pre> Source code in <code>darts-utils/src/darts_utils/paths.py</code> <pre><code>def _parse_path(p: Path | str | None) -&gt; Path | None:\n    if isinstance(p, str):\n        p = Path(p)\n    if p is not None:\n        p = p.resolve()\n    return p\n</code></pre>"},{"location":"reference/darts_utils/tilecache/","title":"tilecache","text":""},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache","title":"darts_utils.tilecache","text":"<p>Caching functionality for xarray datasets.</p>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.manager","title":"manager  <code>module-attribute</code>","text":"<pre><code>manager = darts_utils.bands.BandManager(\n    {\n        \"blue\": darts_utils.bands.BandCodec.optical(),\n        \"red\": darts_utils.bands.BandCodec.optical(),\n        \"green\": darts_utils.bands.BandCodec.optical(),\n        \"nir\": darts_utils.bands.BandCodec.optical(),\n        \"s2_scl\": darts_utils.bands.BandCodec.mask(11),\n        \"planet_udm\": darts_utils.bands.BandCodec.mask(8),\n        \"quality_data_mask\": darts_utils.bands.BandCodec.mask(\n            2\n        ),\n        \"dem\": darts_utils.bands.BandCodec(\n            disk_dtype=\"float32\",\n            memory_dtype=\"float32\",\n            valid_range=(-100, 3000),\n            scale_factor=0.1,\n            offset=-100.0,\n            fill_value=-1,\n        ),\n        \"arcticdem_data_mask\": darts_utils.bands.BandCodec(\n            disk_dtype=\"bool\",\n            memory_dtype=\"uint8\",\n            valid_range=(0, 1),\n        ),\n        \"tc_brightness\": darts_utils.bands.BandCodec.tc(),\n        \"tc_greenness\": darts_utils.bands.BandCodec.tc(),\n        \"tc_wetness\": darts_utils.bands.BandCodec.tc(),\n        \"ndvi\": darts_utils.bands.BandCodec.ndi(),\n        \"relative_elevation\": darts_utils.bands.BandCodec(\n            disk_dtype=\"int16\",\n            memory_dtype=\"float32\",\n            valid_range=(-50, 50),\n            scale_factor=100 / 30000,\n            offset=-50.0,\n            fill_value=-1,\n        ),\n        \"slope\": darts_utils.bands.BandCodec(\n            disk_dtype=\"int16\",\n            memory_dtype=\"float32\",\n            valid_range=(0, 90),\n            scale_factor=1 / 100,\n            offset=0.0,\n            fill_value=-1,\n        ),\n        \"aspect\": darts_utils.bands.BandCodec(\n            disk_dtype=\"int16\",\n            memory_dtype=\"float32\",\n            valid_range=(0, 360),\n            scale_factor=1 / 10,\n            offset=0.0,\n            fill_value=-1,\n        ),\n        \"hillshade\": darts_utils.bands.BandCodec(\n            disk_dtype=\"int16\",\n            memory_dtype=\"float32\",\n            valid_range=(0, 1),\n            scale_factor=1 / 10000,\n            offset=0.0,\n            fill_value=-1,\n        ),\n        \"curvature\": darts_utils.bands.BandCodec.ndi(),\n        \"probabilities\": darts_utils.bands.BandCodec.percentage(),\n        \"probabilities-*\": darts_utils.bands.BandCodec.percentage(),\n        \"binarized_segmentation\": darts_utils.bands.BandCodec.bool(),\n        \"binarized_segmentation-*\": darts_utils.bands.BandCodec.bool(),\n        \"extent\": darts_utils.bands.BandCodec.bool(),\n    }\n)\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager","title":"XarrayCacheManager","text":"<pre><code>XarrayCacheManager(\n    cache_dir: str | pathlib.Path | None = None,\n)\n</code></pre> <p>Manager for caching xarray datasets.</p> Example <pre><code>    def process_tile(tile_id: str):\n        # Initialize cache manager\n        preprocess_cache = Path(\"preprocess_cache\")\n        cache_manager = XarrayCacheManager(preprocess_cache)\n\n        def create_tile():\n            # Your existing tile creation logic goes here\n            return create_tile(...)  # Replace with actual implementation\n\n        # Get cached tile or create and cache it\n        tile = cache_manager.get_or_create(\n            identifier=tile_id,\n            creation_func=create_tile\n        )\n\n        return tile\n</code></pre> <p>Initialize the cache manager.</p> <p>Parameters:</p> <ul> <li> <code>cache_dir</code>               (<code>str | pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory path for caching files</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def __init__(self, cache_dir: str | Path | None = None):\n    \"\"\"Initialize the cache manager.\n\n    Args:\n        cache_dir (str | Path | None): Directory path for caching files\n\n    \"\"\"\n    self.cache_dir = Path(cache_dir) if isinstance(cache_dir, str) else cache_dir\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.cache_dir","title":"cache_dir  <code>instance-attribute</code>","text":"<pre><code>cache_dir = (\n    pathlib.Path(\n        darts_utils.tilecache.XarrayCacheManager(cache_dir)\n    )\n    if isinstance(\n        darts_utils.tilecache.XarrayCacheManager(cache_dir),\n        str,\n    )\n    else darts_utils.tilecache.XarrayCacheManager(cache_dir)\n)\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.exists","title":"exists","text":"<pre><code>exists(identifier: str) -&gt; bool\n</code></pre> <p>Check if a cached Dataset exists.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the cached file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the Dataset exists in cache, False otherwise</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def exists(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a cached Dataset exists.\n\n    Args:\n        identifier (str): Unique identifier for the cached file\n\n    Returns:\n        bool: True if the Dataset exists in cache, False otherwise\n\n    \"\"\"\n    if not self.cache_dir:\n        return False\n\n    cache_path = self.cache_dir / f\"{identifier}.nc\"\n    return cache_path.exists()\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.get_or_create","title":"get_or_create","text":"<pre><code>get_or_create(\n    identifier: str,\n    creation_func: callable,\n    force: bool,\n    use_band_manager: bool = True,\n    *args: tuple[typing.Any, ...],\n    **kwargs: dict[str, typing.Any],\n) -&gt; xarray.Dataset\n</code></pre> <p>Get cached Dataset or create and cache it if it doesn't exist.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the cached file</p> </li> <li> <code>creation_func</code>               (<code>callable</code>)           \u2013            <p>Function to create the Dataset if not cached</p> </li> <li> <code>force</code>               (<code>bool</code>)           \u2013            <p>If True, forces reprocessing even if cached</p> </li> <li> <code>use_band_manager</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, uses the band manager save and load the data. Defaults to True.</p> </li> <li> <code>*args</code>               (<code>tuple[typing.Any, ...]</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to pass to creation_func</p> </li> <li> <code>**kwargs</code>               (<code>dict[str, typing.Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to creation_func</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The Dataset (either loaded from cache or newly created)</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def get_or_create(\n    self,\n    identifier: str,\n    creation_func: callable,\n    force: bool,\n    use_band_manager: bool = True,\n    *args: tuple[Any, ...],\n    **kwargs: dict[str, Any],\n) -&gt; xr.Dataset:\n    \"\"\"Get cached Dataset or create and cache it if it doesn't exist.\n\n    Args:\n        identifier (str): Unique identifier for the cached file\n        creation_func (callable): Function to create the Dataset if not cached\n        force (bool): If True, forces reprocessing even if cached\n        use_band_manager (bool): If True, uses the band manager save and load the data. Defaults to True.\n        *args: Arguments to pass to creation_func\n        **kwargs: Keyword arguments to pass to creation_func\n\n    Returns:\n        xr.Dataset: The Dataset (either loaded from cache or newly created)\n\n    \"\"\"\n    cached_dataset = None if force else self.load_from_cache(identifier, use_band_manager)\n    if not force and self.cache_dir:\n        logger.debug(f\"Cache hit for '{identifier}': {cached_dataset is not None}\")\n\n    if cached_dataset is not None:\n        return cached_dataset\n\n    dataset = creation_func(*args, **kwargs)\n    if cached_dataset is None:\n        self.save_to_cache(dataset, identifier, use_band_manager)\n    return dataset\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.load_from_cache","title":"load_from_cache","text":"<pre><code>load_from_cache(\n    identifier: str, use_band_manager: bool = True\n) -&gt; xarray.Dataset | None\n</code></pre> <p>Load a Dataset from cache if it exists.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the cached file</p> </li> <li> <code>use_band_manager</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, uses the band manager to load the data. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset | None</code>           \u2013            <p>xr.Dataset | None: Dataset if found in cache, otherwise None</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def load_from_cache(self, identifier: str, use_band_manager: bool = True) -&gt; xr.Dataset | None:\n    \"\"\"Load a Dataset from cache if it exists.\n\n    Args:\n        identifier (str): Unique identifier for the cached file\n        use_band_manager (bool): If True, uses the band manager to load the data. Defaults to True.\n\n    Returns:\n        xr.Dataset | None: Dataset if found in cache, otherwise None\n\n    \"\"\"\n    if not self.cache_dir:\n        return None\n\n    cache_path = self.cache_dir / f\"{identifier}.nc\"\n    if not cache_path.exists():\n        return None\n    if use_band_manager:\n        dataset = manager.open(cache_path)\n    else:\n        # ! Unknown why, but decode_coords=\"all\" sometimes fails! Falls back to manually set\n        # dataset = xr.open_dataset(path, engine=\"h5netcdf\", decode_coords=\"all\", decode_cf=True).load()\n        dataset = xr.open_dataset(cache_path, engine=\"h5netcdf\").set_coords(\"spatial_ref\").load()\n    return dataset\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.save_to_cache","title":"save_to_cache","text":"<pre><code>save_to_cache(\n    dataset: xarray.Dataset,\n    identifier: str,\n    use_band_manager: bool = True,\n) -&gt; bool\n</code></pre> <p>Save a Dataset to cache.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to cache</p> </li> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the cached file</p> </li> <li> <code>use_band_manager</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, uses the band manager to save the data. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>Success of operation</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def save_to_cache(self, dataset: xr.Dataset, identifier: str, use_band_manager: bool = True) -&gt; bool:\n    \"\"\"Save a Dataset to cache.\n\n    Args:\n        dataset (xr.Dataset): Dataset to cache\n        identifier (str): Unique identifier for the cached file\n        use_band_manager (bool): If True, uses the band manager to save the data. Defaults to True.\n\n    Returns:\n        bool: Success of operation\n\n    \"\"\"\n    if not self.cache_dir:\n        return False\n\n    self.cache_dir.mkdir(exist_ok=True, parents=True)\n    cache_path = self.cache_dir / f\"{identifier}.nc\"\n    logger.debug(f\"Caching {identifier=} to {cache_path}\")\n    if use_band_manager:\n        manager.to_netcdf(dataset, cache_path)\n    else:\n        dataset.to_netcdf(cache_path, engine=\"h5netcdf\")\n    return True\n</code></pre>"}]}