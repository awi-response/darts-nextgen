{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "# Project is specified by <entity/project-name>\n",
    "runs = api.runs(\"ingmarnitze_team/darts\")\n",
    "cache = Path(\"../data/analysis/sweep-cv-large-planet.csv\")\n",
    "cache.parent.mkdir(parents=True, exist_ok=True)\n",
    "if cache.exists():\n",
    "    runs_df = pd.read_parquet(cache)\n",
    "    print(\"Loaded from cache\")\n",
    "else:\n",
    "    summary_list, config_list, name_list = [], [], []\n",
    "    for run in runs:\n",
    "        # .summary contains the output keys/values for metrics like accuracy.\n",
    "        #  We call ._json_dict to omit large files\n",
    "        summary_list.append(run.summary._json_dict)\n",
    "\n",
    "        # .config contains the hyperparameters.\n",
    "        #  We remove special values that start with _.\n",
    "        config_list.append({k: v for k, v in run.config.items() if not k.startswith(\"_\")})\n",
    "\n",
    "        # .name is the human-readable name of the run.\n",
    "        name_list.append(run.name)\n",
    "\n",
    "    runs_df = pd.DataFrame({\"summary\": summary_list, \"config\": config_list, \"name\": name_list})\n",
    "    runs_df.to_csv(cache)\n",
    "runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_keys = set()\n",
    "for i, row in runs_df.iterrows():\n",
    "    summary_list = row[\"config\"]\n",
    "    if not summary_list:\n",
    "        summary_keys = set(summary_list.keys())\n",
    "\n",
    "    diff = summary_keys - set(summary_list.keys())\n",
    "    if diff:\n",
    "        print(row[\"name\"], diff)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.json_normalize(runs_df[\"summary\"])\n",
    "config_df = pd.json_normalize(runs_df[\"config\"])\n",
    "metrics_df = pd.concat([summary_df, config_df], axis=1)\n",
    "# Filter out row which are not group of the sweep\n",
    "metrics_df = metrics_df[~metrics_df[\"trial_name\"].isna() & metrics_df[\"epoch\"] > 0]\n",
    "# Drop columns which are all nan\n",
    "metrics_df = metrics_df.dropna(axis=1, how=\"all\")\n",
    "# Kick out all visualization columns\n",
    "for i in range(3):\n",
    "    viz_columns = [c for c in metrics_df.columns if c.startswith(f\"val{i}-samples/\")]\n",
    "    metrics_df = metrics_df.drop(columns=viz_columns)\n",
    "# Kick out all prc, cmx and roc columns\n",
    "prc_columns = [c for c in metrics_df.columns if c.startswith(\"val/prc\")]\n",
    "cmx_columns = [c for c in metrics_df.columns if c.startswith(\"val/cmx\")]\n",
    "roc_columns = [c for c in metrics_df.columns if c.startswith(\"val/roc\")]\n",
    "# Kick out learning_rate because it is doubled for some reason\n",
    "metrics_df = metrics_df.drop(columns=[\"learning_rate\"])\n",
    "metrics_df = metrics_df.drop(columns=prc_columns + cmx_columns + roc_columns)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some inverses for log plotting\n",
    "metrics_df[\"focal_loss_alpha_inv\"] = 1 - metrics_df[\"focal_loss_alpha\"]\n",
    "metrics_df[\"val/JaccardIndex_inv\"] = 1 - metrics_df[\"val/JaccardIndex\"]\n",
    "metrics_df[\"val/AveragePrecision_inv\"] = 1 - metrics_df[\"val/AveragePrecision\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_combi = metrics_df[\n",
    "    (metrics_df[\"config.model.arch\"] == \"UPerNet\")\n",
    "    & (metrics_df[\"config.model.encoder_name\"] == \"tu-maxvit_tiny_rw_224\")\n",
    "]\n",
    "metrics_df_combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_combi.hvplot.scatter(x=\"val/AveragePrecision\", y=\"focal_loss_gamma\", c=\"val/JaccardIndex\", groupby=\"fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_combi.hvplot.scatter(c=\"val/AveragePrecision\", y=\"focal_loss_gamma\", x=\"val/JaccardIndex\", groupby=\"fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_combi.hvplot.scatter(y=\"val/AveragePrecision\", c=\"focal_loss_gamma\", x=\"val/JaccardIndex\", groupby=\"fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[metrics_df[\"config.model.arch\"].isin([\"UPerNet\", \"Unet\"])].hvplot.violin(\n",
    "    by=\"config.model.arch\", y=\"val/JaccardIndex\", groupby=\"fold\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
