{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DARTS nextgen","text":"<p>Panarctic Database of Active Layer Detachment Slides and Retrogressive Thaw Slumps from Deep Learning on High Resolution Satellite Imagery. This is te successor of the thaw-slump-segmentation (pipeline), with which the first version of the DARTS dataset was created.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li> <p>Download source code from the GitHub repository:</p> <pre><code>git clone git@github.com:awi-response/darts-nextgen.git\ncd darts-nextgen\n</code></pre> </li> <li> <p>Install the required dependencies:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> </li> <li> <p>Run the Sentinel 2 based pipeline on an area of interest:</p> <pre><code>uv run darts run-native-sentinel2-pipeline-from-aoi \\\n  --aoi-shapefile path/to/your/aoi.geojson \\\n  --model-file path/to/your/model/checkpoint \\\n  --start-date 2024-07 \\\n  --end-date 2024-09\n</code></pre> </li> </ol> <p>Continue reading with an Overview for more detailed information or the Install Guide for detailed information about the installation.</p>"},{"location":"#contribute","title":"Contribute","text":"<p>Before contributing please contact one of the authors and make sure to read the Contribution Guidelines.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>This page is also meant for internal documentation.</p>"},{"location":"contribute/#editor-setup","title":"Editor setup","text":"<p>There is only setup files provided for VSCode and no other editor (yet). A list of extensions and some settings can be found in the <code>.vscode</code>. At the first start, VSCode should ask you if you want to install the recommended extension. The settings should be automaticly used by VSCode. Both should provide the developers with a better experience and enforce code-style.</p>"},{"location":"contribute/#environment-setup","title":"Environment setup","text":"<p>Please read and follow the installation guide to setup the environment.</p>"},{"location":"contribute/#writing-docs","title":"Writing docs","text":"<p>The documentation is managed with Material for MkDocs. The documentation related dependencies are separated from the main dependencies and can be installed with:</p> <pre><code>uv sync --group docs\n</code></pre> <p>Note</p> <p>You should combine the <code>--group docs</code> with the extras you previously used, e.g. <code>uv sync --extra training --extra cuda126 --group docs</code>.</p> <p>To start the documentation server for live-update, run:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>In general all mkdocs commands can be run with <code>uv run mkdocs ...</code>.</p>"},{"location":"contribute/#recommended-notebook-header","title":"Recommended Notebook header","text":"<p>The following code snipped can be put in the very first cell of a notebook to already to add logging and initialize earth engine.</p> <pre><code>import logging\n\nfrom rich.logging import RichHandler\nfrom rich import traceback\n\nfrom darts.utils.earthengine import init_ee\nfrom darts.utils.logging import LoggingManager\n\nLoggingManager.setup_logging()\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\ntraceback.install(show_locals=True)  # Change to False if you encounter too large tracebacks\ninit_ee(\"ee-project\")  # Replace with your project\n</code></pre>"},{"location":"overview/","title":"Overview","text":"<p>This is a guide to help you, as a user / data engineer, get started with the project.</p>"},{"location":"overview/#installation","title":"Installation","text":"<p>To setup the environment for the project, you need to install uv and run the following command, assuming CUDA 12.6 is installed:</p> <pre><code>uv sync --extra cuda126\n</code></pre> <p>For other CUDA versions, see the installation guide.</p> <p>Training specific dependencies are optional and therefore not installed by default. To install them, add <code>--extra training</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> <p>To see if the installation was successful, you can run the following command:</p> <pre><code>uv run darts env-info\n</code></pre>"},{"location":"overview/#running-stuff-via-the-cli","title":"Running stuff via the CLI","text":"<p>The project provides a CLI to run different pipelines, training and other utility functions. Because the environment is setup with <code>uv</code>, you can run the CLI commands with <code>uv run darts ...</code>. If you manually active the environment with <code>source .venv/bin/activate</code>, you can run the CLI commands just via <code>darts ...</code> without <code>uv run</code>.</p> <p>To see a list of all available commands, run:</p> <pre><code>uv run darts --help\n</code></pre> <p>To get help for a specific command, run:</p> <pre><code>uv run darts the-specific-command --help\n</code></pre>"},{"location":"overview/#config-files","title":"Config files","text":"<p>The CLI supports config files in TOML format to reduce the amount of parameters you need to pass or to safe different configurations. By default the CLI tries to load a <code>config.toml</code> file from the current directory. However, you can specify a different file with the <code>--config-file</code> parameter.</p> <p>As of right now, the CLI tries to match all parameters under the <code>darts</code> key of the config file, skipping not needed ones. For more information about the config file,  see the config guide..</p>"},{"location":"overview/#logging","title":"Logging","text":"<p>By default the CLI sets up a logging handler at <code>DEBUG</code> level for the <code>darts</code> specific packages found in this workspace. Running any command will output a logging file at the logging directory, which can be specified via the <code>--log-dir</code> parameter. The logging file will be named after the command and the current timestamp. If you want to change the logging behavior in python code, you can check out the logging guide.</p>"},{"location":"overview/#running-a-pipeline-based-on-sentinel-2-data","title":"Running a pipeline based on Sentinel 2 data","text":"<p>The <code>run-native-sentinel2-pipeline-from-aoi</code> automatically downloads and processes Sentinel 2 data based on an Area of Interest (AOI) in GeoJSON format. Before running you need access to a trained model. Note, that only special checkpoints can be used, as described in the architecture guide. In future versions, downloading of the model via huggingface will be supported, but for now you need to ask the developers for a valid model checkpoint.</p> <p>To run the pipeline run:</p> <pre><code>uv run darts run-native-sentinel2-pipeline-from-aoi --aoi-shapefile path/to/your/aoi.geojson --model-file path/to/your/model/checkpoint --start-date 2024-07 --end-date 2024-09\n</code></pre> <p>Run <code>uv run darts run-native-sentinel2-pipeline-from-aoi --help</code> for more configuration options.</p>"},{"location":"overview/#running-a-pipeline-based-on-planet-data","title":"Running a pipeline based on PLANET data","text":"<p>PLANET data cannot be downloaded automatically. Hence, you need to download the data manually and place it a directory of you choice.</p> <p>Example directory structure of a PLANET Orthotile:</p> <pre><code>    data/input/planet/PSOrthoTile/\n    \u251c\u2500\u2500 4372514/\n    \u2502  \u2514\u2500\u2500 5790392_4372514_2022-07-16_2459/\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_Analytic_metadata.xml\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_DN_udm.tif\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_SR.tif\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_metadata.json\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_udm2.tif\n    \u2502      \u2514\u2500\u2500 Thumbs.db\n    \u2514\u2500\u2500 4974017/\n        \u2514\u2500\u2500 5854937_4974017_2022-08-14_2475/\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_Analytic_metadata.xml\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_DN_udm.tif\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_SR.tif\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_metadata.json\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_udm2.tif\n            \u2514\u2500\u2500 Thumbs.db\n</code></pre> <p>Example directory structure of a PLANET Scene:</p> <pre><code>    data/input/planet/PSScene/\n    \u251c\u2500\u2500 20230703_194241_43_2427/\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427.json\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_metadata.xml\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_SR.tif\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_udm2.tif\n    \u2502  \u2514\u2500\u2500 20230703_194241_43_2427_metadata.json\n    \u2514\u2500\u2500 20230703_194243_54_2427/\n       \u251c\u2500\u2500 20230703_194243_54_2427.json\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_AnalyticMS_metadata.xml\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_AnalyticMS_SR.tif\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_udm2.tif\n       \u2514\u2500\u2500 20230703_194243_54_2427_metadata.json\n</code></pre> <p>Backcompatability of Sentinel 2 data</p> <p>For historical reasons, it is possible to run similar pipelines with Sentinel 2 data. For this, the Sentinel 2 data is expected to be in the same directory structure as the PLANET data. Hence, data from Google EarthEngine or from the Copernicus Cloud needs to be adjusted and scaled by the factor of <code>0.0001</code>.</p> <pre><code>data/input/sentinel2/\n\u251c\u2500\u2500 20210818T223529_20210818T223531_T03WXP/\n\u2502  \u251c\u2500\u2500 20210818T223529_20210818T223531_T03WXP_SCL_clip.tif\n\u2502  \u2514\u2500\u2500 20210818T223529_20210818T223531_T03WXP_SR_clip.tif\n\u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ/\n\u251c\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SCL_clip.tif\n\u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SR_clip.tif\n</code></pre>"},{"location":"overview/#create-a-config-file","title":"Create a config file","text":"<p>Because the minimal amount of parameters to pass for the PLANET pipeline, it is recommended to use a config file.</p> <p>An example config file can be found in the root of this repository called <code>config.toml.example</code>. You can copy this file to either <code>configs/</code> or copy and rename it to <code>config.toml</code>, so that you personal config will be ignored by git.</p> <p>Please change  <code>orthotiles-dir</code> and <code>scenes-dir</code> according to your PLANET download directory.</p> <p>You also need to specify the paths the model checkpoints (<code>model-dir</code>, <code>tcvis-model-name</code> and <code>notcvis-model-name</code>) you want to use. Note, that only special checkpoints can be used, as described in the architecture guide By setting <code>notcvis-model-name</code> to <code>None</code>, the pipeline will only use the TCVIS model.</p> <p>Auxiliary data (TCVIS and ArcticDEM) will be downloaded on demand into a datacube, which paths needs to be specified as well (<code>arcticdem-dir</code> and <code>tcvis-dir</code>).</p> <p>Finally, specify an output directory (<code>output-dir</code>), where you want to save the results of the pipeline.</p> <p>Of course you can tweak all other options aswell, also via the CLI. A list of all options can be found in the config guide or by running a command with the <code>--help</code> parameter.</p>"},{"location":"overview/#run-a-the-pipeline","title":"Run a the pipeline","text":"<p>Finally run the pipeline with the following command. Additional parameters can be passed via the CLI, which will overwrite the config file.</p> <pre><code>rye run darts run-native-planet-pipeline-fast --config-file path/to/your/config.toml\n</code></pre>"},{"location":"overview/#creating-your-own-pipeline","title":"Creating your own pipeline","text":"<p>The project was build with the idea in mind, that it is easy to create a new pipeline, with e.g. different parallelisation techniques. The architecture guide provides an overview of the project structure and the key components. A good starting point to understand the components is the intro to components. The build-in pipelines are a good example how the components can be used and put together to create a new pipeline.</p>"},{"location":"ref/","title":"Combined Reference","text":"<p>All references on one page</p> Table of Contents<ul> <li>Combined Reference<ul> <li>\u00a0darts<ul> <li>\u00a0__version__</li> </ul> </li> <li>\u00a0darts_acquisition<ul> <li>\u00a0__version__</li> <li>\u00a0download_admin_files</li> <li>\u00a0load_arcticdem</li> <li>\u00a0load_arcticdem_from_vrt</li> <li>\u00a0load_planet_masks</li> <li>\u00a0load_planet_scene</li> <li>\u00a0load_s2_masks</li> <li>\u00a0load_s2_scene</li> <li>\u00a0load_tcvis</li> </ul> </li> <li>\u00a0darts_ensemble<ul> <li>\u00a0__version__</li> </ul> </li> <li>\u00a0darts_export<ul> <li>\u00a0__version__</li> <li>\u00a0export_arcticdem_datamask</li> <li>\u00a0export_binarized</li> <li>\u00a0export_datamask</li> <li>\u00a0export_dem</li> <li>\u00a0export_extent</li> <li>\u00a0export_optical</li> <li>\u00a0export_polygonized</li> <li>\u00a0export_probabilities</li> <li>\u00a0export_tcvis</li> <li>\u00a0export_thumbnail</li> </ul> </li> <li>\u00a0darts_postprocessing<ul> <li>\u00a0__version__</li> </ul> </li> <li>\u00a0darts_preprocessing<ul> <li>\u00a0__version__</li> <li>\u00a0preprocess_legacy</li> <li>\u00a0preprocess_legacy_fast</li> </ul> </li> <li>\u00a0darts_segmentation<ul> <li>\u00a0__version__</li> <li>\u00a0SMPSegmenter<ul> <li>\u00a0config</li> <li>\u00a0device</li> <li>\u00a0model</li> <li>\u00a0__call__</li> <li>\u00a0__init__</li> <li>\u00a0segment_tile</li> <li>\u00a0segment_tile_batched</li> <li>\u00a0tile2tensor</li> <li>\u00a0tile2tensor_batched</li> </ul> </li> <li>\u00a0SMPSegmenterConfig<ul> <li>\u00a0input_combination</li> <li>\u00a0model</li> <li>\u00a0norm_factors</li> </ul> </li> <li>\u00a0create_patches</li> <li>\u00a0patch_coords</li> <li>\u00a0predict_in_patches</li> </ul> </li> <li>\u00a0darts_superresolution<ul> <li>\u00a0__version__</li> </ul> </li> <li>\u00a0darts_utils<ul> <li>\u00a0__version__</li> </ul> </li> </ul> </li> </ul>"},{"location":"ref/#darts","title":"<code>darts</code>","text":"<p>DARTS processing pipeline.</p>"},{"location":"ref/#darts.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/#darts_acquisition","title":"<code>darts_acquisition</code>","text":"<p>Acquisition of data from various sources for the DARTS dataset.</p>"},{"location":"ref/#darts_acquisition.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/#darts_acquisition.download_admin_files","title":"<code>download_admin_files(admin_dir)</code>","text":"<p>Download the admin files for the regions.</p> <p>Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.</p> <p>Parameters:</p> Name Type Description Default <code>admin_dir</code> <code>Path</code> <p>The path to the admin files.</p> required Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>def download_admin_files(admin_dir: Path):\n    \"\"\"Download the admin files for the regions.\n\n    Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.\n\n    Args:\n        admin_dir (Path): The path to the admin files.\n\n    \"\"\"\n    tick_fstart = time.perf_counter()\n\n    # Download the admin files\n    admin_1_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.zip\"\n    admin_2_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.zip\"\n\n    admin_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.debug(f\"Downloading {admin_1_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_1_url, admin_dir)\n\n    logger.debug(f\"Downloading {admin_2_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_2_url, admin_dir)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Downloaded admin files in {tick_fend - tick_fstart:.2f} seconds\")\n</code></pre>"},{"location":"ref/#darts_acquisition.load_arcticdem","title":"<code>load_arcticdem(geobox, data_dir, resolution, buffer=0, persist=True)</code>","text":"<p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>geobox</code> <code>GeoBox</code> <p>The geobox for which the tile should be loaded.</p> required <code>data_dir</code> <code>Path | str</code> <p>The directory where the ArcticDEM data is stored.</p> required <code>resolution</code> <code>Literal[2, 10, 32]</code> <p>The resolution of the ArcticDEM data in m.</p> required <code>buffer</code> <code>int</code> <p>The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.</p> <code>0</code> <code>persist</code> <code>bool</code> <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The ArcticDEM tile, with a buffer applied. Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different. Final dataset is NOT matched to the reference CRS and resolution.</p> Warning <p>Geobox must be in a meter based CRS.</p> Usage <p>Since the API of the <code>load_arcticdem</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_arcticdem\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\narcticdem = load_arcticdem(\n    optical.odc.geobox,\n    \"/path/to/arcticdem-parent-directory\",\n    resolution=2,\n    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n)\n\n# Now we can for example match the resolution and extent of the optical data:\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> <p>The <code>buffer</code> parameter is used to extend the region of interest by a certain amount of pixels. This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.</p> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem/datacube.py</code> <pre><code>def load_arcticdem(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    resolution: RESOLUTIONS,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded.\n        data_dir (Path | str): The directory where the ArcticDEM data is stored.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in m.\n        buffer (int, optional): The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The ArcticDEM tile, with a buffer applied.\n            Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different.\n            Final dataset is NOT matched to the reference CRS and resolution.\n\n    Warning:\n        Geobox must be in a meter based CRS.\n\n    Usage:\n        Since the API of the `load_arcticdem` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_arcticdem\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        arcticdem = load_arcticdem(\n            optical.odc.geobox,\n            \"/path/to/arcticdem-parent-directory\",\n            resolution=2,\n            buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n        The `buffer` parameter is used to extend the region of interest by a certain amount of pixels.\n        This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.\n\n    \"\"\"  # noqa: E501\n    tick_fstart = time.perf_counter()\n\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    datacube_fpath = data_dir / f\"datacube_{resolution}m_v4.1.zarr\"\n    storage = zarr.storage.FSStore(datacube_fpath)\n    logger.debug(f\"Getting ArcticDEM tile from {datacube_fpath.resolve()}\")\n\n    # ! The geobox must be in a meter based CRS\n    logger.debug(f\"Found a reference resolution of {geobox.resolution.x}m\")\n\n    # Check if the zarr data already exists\n    if not datacube_fpath.exists():\n        logger.debug(f\"Creating a new zarr datacube at {datacube_fpath.resolve()} with {storage=}\")\n        create_empty_datacube(\n            \"ArcticDEM Data Cube\",\n            storage,\n            DATA_EXTENT[resolution],\n            CHUNK_SIZE,\n            DATA_VARS,\n            DATA_VARS_META,\n            DATA_VARS_ENCODING,\n        )\n\n    # Get the adjacent arcticdem tiles\n    # Note: We could also use pystac here, but this would result in a slight performance decrease\n    # because of the network overhead, hence we use the extent file\n    # Download the extent, download if the file does not exist\n    extent_fpath = data_dir / f\"ArcticDEM_Mosaic_Index_v4_1_{resolution}m.parquet\"\n    with download_lock:\n        if not extent_fpath.exists():\n            download_arcticdem_extent(data_dir)\n    extent = gpd.read_parquet(extent_fpath)\n\n    # Add a buffer around the geobox to get the adjacent tiles\n    reference_geobox = geobox.to_crs(\"epsg:3413\", resolution=resolution).pad(buffer)\n    adjacent_tiles = extent[extent.intersects(reference_geobox.extent.geom)]\n\n    # Download the adjacent tiles (if necessary)\n    with download_lock:\n        procedural_download_datacube(storage, adjacent_tiles)\n\n    # Load the datacube and set the spatial_ref since it is set as a coordinate within the zarr format\n    chunks = None if persist else \"auto\"\n    arcticdem_datacube = xr.open_zarr(storage, mask_and_scale=False, chunks=chunks).set_coords(\"spatial_ref\")\n\n    # Get an AOI slice of the datacube\n    arcticdem_aoi = arcticdem_datacube.odc.crop(reference_geobox.extent, apply_mask=False)\n\n    # The following code would load the lazy zarr data from disk into memory\n    if persist:\n        tick_sload = time.perf_counter()\n        arcticdem_aoi = arcticdem_aoi.load()\n        tick_eload = time.perf_counter()\n        logger.debug(f\"ArcticDEM AOI loaded from disk in {tick_eload - tick_sload:.2f} seconds\")\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem_aoi[\"datamask\"] = arcticdem_aoi.datamask.astype(\"uint8\")\n\n    logger.info(\n        f\"ArcticDEM tile {'loaded' if persist else 'lazy-opened'} in {time.perf_counter() - tick_fstart:.2f} seconds\"\n    )\n    return arcticdem_aoi\n</code></pre>"},{"location":"ref/#darts_acquisition.load_arcticdem_from_vrt","title":"<code>load_arcticdem_from_vrt(slope_vrt, elevation_vrt, reference_dataset)</code>","text":"<p>Load ArcticDEM data and reproject it to match the reference dataset.</p> <p>Parameters:</p> Name Type Description Default <code>slope_vrt</code> <code>Path</code> <p>Path to the ArcticDEM slope VRT file.</p> required <code>elevation_vrt</code> <code>Path</code> <p>Path to the ArcticDEM elevation VRT file.</p> required <code>reference_dataset</code> <code>Dataset</code> <p>The reference dataset to reproject, resampled and cropped the ArcticDEM data to.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The ArcticDEM data reprojected, resampled and cropped to match the reference dataset.</p> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem/vrt.py</code> <pre><code>def load_arcticdem_from_vrt(slope_vrt: Path, elevation_vrt: Path, reference_dataset: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Load ArcticDEM data and reproject it to match the reference dataset.\n\n    Args:\n        slope_vrt (Path): Path to the ArcticDEM slope VRT file.\n        elevation_vrt (Path): Path to the ArcticDEM elevation VRT file.\n        reference_dataset (xr.Dataset): The reference dataset to reproject, resampled and cropped the ArcticDEM data to.\n\n    Returns:\n        xr.Dataset: The ArcticDEM data reprojected, resampled and cropped to match the reference dataset.\n\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(f\"Loading ArcticDEM slope from {slope_vrt.resolve()} and elevation from {elevation_vrt.resolve()}\")\n\n    slope = load_vrt(slope_vrt, reference_dataset)\n    slope: xr.Dataset = (\n        slope.assign_attrs({\"data_source\": \"arcticdem\", \"long_name\": \"Slope\"})\n        .rio.write_nodata(float(\"nan\"))\n        .astype(\"float32\")\n        .to_dataset(name=\"slope\")\n    )\n\n    relative_elevation = load_vrt(elevation_vrt, reference_dataset)\n    relative_elevation: xr.Dataset = (\n        relative_elevation.assign_attrs({\"data_source\": \"arcticdem\", \"long_name\": \"Relative Elevation\", \"units\": \"m\"})\n        .fillna(0)\n        .rio.write_nodata(0)\n        .astype(\"int16\")\n        .to_dataset(name=\"relative_elevation\")\n    )\n\n    articdem_ds = xr.merge([relative_elevation, slope])\n    logger.debug(f\"Loaded ArcticDEM data in {time.time() - start_time} seconds.\")\n    return articdem_ds\n</code></pre>"},{"location":"ref/#darts_acquisition.load_planet_masks","title":"<code>load_planet_masks(fpath)</code>","text":"<p>Load the valid and quality data masks from a Planet scene.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str | Path</code> <p>The file path to the Planet scene from which to derive the masks.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no matching UDM-2 TIFF file is found in the specified path.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def load_planet_masks(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Planet scene.\n\n    Args:\n        fpath (str | Path): The file path to the Planet scene from which to derive the masks.\n\n    Raises:\n        FileNotFoundError: If no matching UDM-2 TIFF file is found in the specified path.\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    start_time = time.time()\n\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # Get imagepath\n    udm_path = next(fpath.glob(\"*_udm2.tif\"), None)\n    if not udm_path:\n        udm_path = next(fpath.glob(\"*_udm2_clip.tif\"), None)\n    if not udm_path:\n        raise FileNotFoundError(f\"No matching UDM-2 TIFF files found in {fpath.resolve()} (.glob('*_udm2.tif'))\")\n\n    # See udm classes here: https://developers.planet.com/docs/data/udm-2/\n    da_udm = xr.open_dataarray(udm_path)\n\n    invalids = da_udm.sel(band=8).fillna(0) != 0\n    low_quality = da_udm.sel(band=[2, 3, 4, 5, 6]).max(axis=0) == 1\n    high_quality = ~low_quality &amp; ~invalids\n    qa_ds = xr.Dataset(coords={c: da_udm.coords[c] for c in da_udm.coords})\n    qa_ds[\"quality_data_mask\"] = (\n        xr.zeros_like(da_udm.sel(band=8)).where(invalids, 0).where(low_quality, 1).where(high_quality, 2)\n    )\n    qa_ds[\"quality_data_mask\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Quality data mask\",\n        \"description\": \"0 = Invalid, 1 = Low Quality, 2 = High Quality\",\n    }\n    logger.debug(f\"Loaded data masks in {time.time() - start_time} seconds.\")\n    return qa_ds\n</code></pre>"},{"location":"ref/#darts_acquisition.load_planet_scene","title":"<code>load_planet_scene(fpath)</code>","text":"<p>Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str | Path</code> <p>The path to the directory containing the TIFF files or a specific path to the TIFF file.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The loaded dataset</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no matching TIFF file is found in the specified path.</p> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def load_planet_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files or a specific path to the TIFF file.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    start_time = time.time()\n\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    # Check if the directory contains a PSOrthoTile or PSScene\n    planet_type = parse_planet_type(fpath)\n    logger.debug(f\"Loading Planet PS {planet_type.capitalize()} from {fpath.resolve()}\")\n\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    # Define band names and corresponding indices\n    planet_da = xr.open_dataarray(ps_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_planet = (\n        planet_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n    )\n    for var in ds_planet.variables:\n        ds_planet[var].assign_attrs(\n            {\n                \"long_name\": f\"PLANET {var.capitalize()}\",\n                \"data_source\": \"planet\",\n                \"planet_type\": planet_type,\n                \"units\": \"Reflectance\",\n            }\n        )\n    ds_planet.attrs = {\"tile_id\": fpath.parent.stem if planet_type == \"orthotile\" else fpath.stem}\n    logger.debug(f\"Loaded Planet scene in {time.time() - start_time} seconds.\")\n    return ds_planet\n</code></pre>"},{"location":"ref/#darts_acquisition.load_s2_masks","title":"<code>load_s2_masks(fpath, reference_geobox)</code>","text":"<p>Load the valid and quality data masks from a Sentinel 2 scene.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str | Path</code> <p>The path to the directory containing the TIFF files.</p> required <code>reference_geobox</code> <code>GeoBox</code> <p>The reference geobox to reproject, resample and crop the masks data to.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>def load_s2_masks(fpath: str | Path, reference_geobox: GeoBox) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Sentinel 2 scene.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n        reference_geobox (GeoBox): The reference geobox to reproject, resample and crop the masks data to.\n\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    start_time = time.time()\n\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # TODO: SCL band in SR file\n    try:\n        scl_path = next(fpath.glob(\"*_SCL*.tif\"))\n    except StopIteration:\n        logger.warning(\"Found no data quality mask (SCL). No masking will occur.\")\n        valid_data_mask = (odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\") + 1).to_dataset(name=\"valid_data_mask\")\n        valid_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Valid Data Mask\"}\n        quality_data_mask = odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\").to_dataset(name=\"quality_data_mask\")\n        quality_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Quality Data Mask\"}\n        qa_ds = xr.merge([valid_data_mask, quality_data_mask])\n        return qa_ds\n\n    # See scene classes here: https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/scene-classification/\n    da_scl = xr.open_dataarray(scl_path)\n\n    da_scl = da_scl.odc.reproject(reference_geobox, sampling=\"nearest\")\n\n    # Match crs\n    da_scl = da_scl.rio.write_crs(reference_geobox.crs)\n\n    da_scl = xr.Dataset({\"scl\": da_scl.sel(band=1).fillna(0).drop_vars(\"band\").astype(\"uint8\")})\n    da_scl = convert_masks(da_scl)\n\n    logger.debug(f\"Loaded data masks in {time.time() - start_time} seconds.\")\n    return da_scl\n</code></pre>"},{"location":"ref/#darts_acquisition.load_s2_scene","title":"<code>load_s2_scene(fpath)</code>","text":"<p>Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str | Path</code> <p>The path to the directory containing the TIFF files.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The loaded dataset</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no matching TIFF file is found in the specified path.</p> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>def load_s2_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    start_time = time.time()\n\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading Sentinel 2 scene from {fpath.resolve()}\")\n\n    # Get imagepath\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n\n    # Define band names and corresponding indices\n    s2_da = xr.open_dataarray(s2_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_s2 = s2_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].assign_attrs(\n            {\"data_source\": \"s2\", \"long_name\": f\"Sentinel 2 {var.capitalize()}\", \"units\": \"Reflectance\"}\n        )\n\n    planet_crop_id, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n    ds_s2.attrs[\"planet_crop_id\"] = planet_crop_id\n    ds_s2.attrs[\"s2_tile_id\"] = s2_tile_id\n    ds_s2.attrs[\"tile_id\"] = tile_id\n    logger.debug(f\"Loaded Sentinel 2 scene in {time.time() - start_time} seconds.\")\n    return ds_s2\n</code></pre>"},{"location":"ref/#darts_acquisition.load_tcvis","title":"<code>load_tcvis(geobox, data_dir, buffer=0, persist=True)</code>","text":"<p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>geobox</code> <code>GeoBox</code> <p>The geobox to load the data for.</p> required <code>data_dir</code> <code>Path | str</code> <p>The directory to store the downloaded data for faster access for consecutive calls.</p> required <code>buffer</code> <code>int</code> <p>The buffer around the geobox in pixels. Defaults to 0.</p> <code>0</code> <code>persist</code> <code>bool</code> <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The TCVIS dataset.</p> Usage <p>Since the API of the <code>load_tcvis</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_tcvis\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\ntcvis = load_tcvis(\n    optical.odc.geobox,\n    \"/path/to/tcvis-parent-directory\",\n)\n\n# Now we can for example match the resolution and extent of the optical data:\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>def load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the TCVIS for the given geobox, fetch new data from GEE if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox to load the data for.\n        data_dir (Path | str): The directory to store the downloaded data for faster access for consecutive calls.\n        buffer (int, optional): The buffer around the geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset.\n\n    Usage:\n        Since the API of the `load_tcvis` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_tcvis\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        tcvis = load_tcvis(\n            optical.odc.geobox,\n            \"/path/to/tcvis-parent-directory\",\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"  # noqa: E501\n    tick_fstart = time.perf_counter()\n\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    datacube_fpath = data_dir / \"tcvis_2000-2019.zarr\"\n    storage = zarr.storage.FSStore(datacube_fpath)\n    logger.debug(f\"Loading TCVis from {datacube_fpath.resolve()}\")\n\n    if not datacube_fpath.exists():\n        logger.debug(f\"Creating a new zarr datacube at {datacube_fpath.resolve()} with {storage=}\")\n        create_empty_datacube(\n            title=\"Landsat Trends TCVIS 2000-2019\",\n            storage=storage,\n            geobox=DATA_EXTENT,\n            chunk_size=CHUNK_SIZE,\n            data_vars=DATA_VARS,\n            meta=DATA_VARS_META,\n            var_encoding=DATA_VARS_ENCODING,\n        )\n\n    # Download the adjacent tiles (if necessary)\n    reference_geobox = geobox.to_crs(\"epsg:4326\", resolution=DATA_EXTENT.resolution.x).pad(buffer)\n    with download_lock:\n        procedural_download_datacube(storage, reference_geobox)\n\n    # Load the datacube and set the spatial_ref since it is set as a coordinate within the zarr format\n    chunks = None if persist else \"auto\"\n    tcvis_datacube = xr.open_zarr(storage, mask_and_scale=False, chunks=chunks).set_coords(\"spatial_ref\")\n\n    # Get an AOI slice of the datacube\n    tcvis_aoi = tcvis_datacube.odc.crop(reference_geobox.extent, apply_mask=False)\n\n    # The following code would load the lazy zarr data from disk into memory\n    if persist:\n        tick_sload = time.perf_counter()\n        tcvis_aoi = tcvis_aoi.load()\n        tick_eload = time.perf_counter()\n        logger.debug(f\"TCVIS AOI loaded from disk in {tick_eload - tick_sload:.2f} seconds\")\n\n    logger.info(\n        f\"TCVIS tile {'loaded' if persist else 'lazy-opened'} in {time.perf_counter() - tick_fstart:.2f} seconds\"\n    )\n    return tcvis_aoi\n</code></pre>"},{"location":"ref/#darts_ensemble","title":"<code>darts_ensemble</code>","text":"<p>Inference and model ensembling for the DARTS dataset.</p>"},{"location":"ref/#darts_ensemble.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/#darts_export","title":"<code>darts_export</code>","text":"<p>Dataset export for the DARTS dataset.</p>"},{"location":"ref/#darts_export.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/#darts_export.export_arcticdem_datamask","title":"<code>export_arcticdem_datamask(tile, out_dir)</code>","text":"<p>Export the arcticdem data mask as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_arcticdem_datamask(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the arcticdem data mask as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"arcticdem_data_mask.tif\"\n    with stopuhr(f\"Exporting arcticdem data mask to {fpath}\", logger.debug):\n        tile[\"arcticdem_data_mask\"].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/#darts_export.export_binarized","title":"<code>export_binarized(tile, out_dir, export_ensemble_inputs=False, tags={})</code>","text":"<p>Export the binarized segmentation layer to a file.</p> <p>If <code>export_ensemble_inputs</code> is set to True and the ensemble used at least two models for inference, the binarized segmentation of the models will be written as individual files as well.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required <code>export_ensemble_inputs</code> <code>bool</code> <p>Also save the model outputs, not only the ensemble result. Only applies if the inference result is an ensemble result and has at least two inputs. Defaults to False.</p> <code>False</code> <code>tags</code> <code>dict</code> <p>optional GeoTIFF metadata to be written. Defaults to no additional metadata.</p> <code>{}</code> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_binarized(tile: xr.Dataset, out_dir: Path, export_ensemble_inputs: bool = False, tags: dict = {}):\n    \"\"\"Export the binarized segmentation layer to a file.\n\n    If `export_ensemble_inputs` is set to True and the ensemble used at least two models for inference,\n    the binarized segmentation of the models will be written as individual files as well.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n        export_ensemble_inputs (bool, optional): Also save the model outputs, not only the ensemble result.\n            Only applies if the inference result is an ensemble result and has at least two inputs.\n            Defaults to False.\n        tags (dict, optional): optional GeoTIFF metadata to be written. Defaults to no additional metadata.\n\n    \"\"\"\n    subset_names = _get_subset_names(tile)\n    if export_ensemble_inputs and len(subset_names) &gt; 1:\n        for subset in _get_subset_names(tile):\n            tick_estart = time.perf_counter()\n            layer_name = f\"binarized_segmentation-{subset}\"\n            fpath = out_dir / f\"{layer_name}.tif\"\n            tile[layer_name].rio.to_raster(fpath, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n            tick_eend = time.perf_counter()\n            logger.debug(f\"Exported binarized segmentation for {subset} to {fpath} in {tick_eend - tick_estart:.2f}s\")\n\n    fpath = out_dir / \"binarized.tif\"\n    with stopuhr(f\"Exporting binarized segmentation to {fpath}\", logger.debug):\n        tile[\"binarized_segmentation\"].rio.to_raster(fpath, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n</code></pre>"},{"location":"ref/#darts_export.export_datamask","title":"<code>export_datamask(tile, out_dir)</code>","text":"<p>Export the data mask as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_datamask(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the data mask as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"data_mask.tif\"\n    with stopuhr(f\"Exporting data mask to {fpath}\", logger.debug):\n        tile[\"quality_data_mask\"].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/#darts_export.export_dem","title":"<code>export_dem(tile, out_dir)</code>","text":"<p>Export the DEM data as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_dem(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the DEM data as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"dem.tif\"\n    with stopuhr(f\"Exporting DEM data to {fpath}\", logger.debug):\n        tile[[\"slope\", \"relative_elevation\"]].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/#darts_export.export_extent","title":"<code>export_extent(tile, out_dir)</code>","text":"<p>Export the extent of the prediction as a vector dataset in GeoPackage and GeoParquet format.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_extent(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the extent of the prediction as a vector dataset in GeoPackage and GeoParquet format.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath_gpkg = out_dir / \"prediction_extent.gpkg\"\n    fpath_parquet = out_dir / \"prediction_extent.parquet\"\n    with stopuhr(f\"Exporting extent to {fpath_gpkg} and {fpath_parquet}\", logger.debug):\n        polygon_gdf = vectorization.vectorize(tile, \"quality_data_mask\", minimum_mapping_unit=0)\n        polygon_gdf.to_file(fpath_gpkg, layer=\"prediction_extent\")\n        polygon_gdf.to_parquet(fpath_parquet)\n</code></pre>"},{"location":"ref/#darts_export.export_optical","title":"<code>export_optical(tile, out_dir)</code>","text":"<p>Export the optical data as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_optical(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the optical data as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"optical.tif\"\n    with stopuhr(f\"Exporting optical data to {fpath}\", logger.debug):\n        tile[[\"red\", \"green\", \"blue\", \"nir\"]].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/#darts_export.export_polygonized","title":"<code>export_polygonized(tile, out_dir, export_ensemble_inputs=False, minimum_mapping_unit=32)</code>","text":"<p>Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.</p> <p>If <code>export_ensemble_inputs</code> is set to True and the ensemble used at least two models for inference, the vectorized binarized segmentation of the models will be written as individual files as well.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required <code>export_ensemble_inputs</code> <code>bool</code> <p>Also save the model outputs, not only the ensemble result. Only applies if the inference result is an ensemble result and has at least two inputs. Defaults to False.</p> <code>False</code> <code>minimum_mapping_unit</code> <code>int</code> <p>segments covering less pixel are removed. Defaults to 32.</p> <code>32</code> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_polygonized(\n    tile: xr.Dataset, out_dir: Path, export_ensemble_inputs: bool = False, minimum_mapping_unit: int = 32\n):\n    \"\"\"Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.\n\n    If `export_ensemble_inputs` is set to True and the ensemble used at least two models for inference,\n    the vectorized binarized segmentation of the models will be written as individual files as well.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n        export_ensemble_inputs (bool, optional): Also save the model outputs, not only the ensemble result.\n            Only applies if the inference result is an ensemble result and has at least two inputs.\n            Defaults to False.\n        minimum_mapping_unit (int, optional): segments covering less pixel are removed. Defaults to 32.\n\n    \"\"\"\n    subset_names = _get_subset_names(tile)\n    if export_ensemble_inputs and len(subset_names) &gt; 1:\n        for subset in _get_subset_names(tile):\n            tick_estart = time.perf_counter()\n            layer_name = f\"binarized_segmentation-{subset}\"\n            fpath_gpkg = out_dir / f\"prediction_segments-{subset}.gpkg\"\n            fpath_parquet = out_dir / f\"prediction_segments-{subset}.parquet\"\n            polygon_gdf = vectorization.vectorize(tile, layer_name, minimum_mapping_unit=minimum_mapping_unit)\n            polygon_gdf.to_file(fpath_gpkg, layer=f\"prediction_segments-{subset}\")\n            polygon_gdf.to_parquet(fpath_parquet)\n            tick_eend = time.perf_counter()\n            logger.debug(\n                f\"Exported binarized segmentation for {subset} to {fpath_gpkg} and {fpath_parquet}\"\n                f\" in {tick_eend - tick_estart:.2f}s\"\n            )\n\n    fpath_gpkg = out_dir / \"prediction_segments.gpkg\"\n    fpath_parquet = out_dir / \"prediction_segments.parquet\"\n    with stopuhr(f\"Exporting binarized segmentation to {fpath_gpkg} and {fpath_parquet}\", logger.debug):\n        polygon_gdf = vectorization.vectorize(tile, \"binarized_segmentation\", minimum_mapping_unit=minimum_mapping_unit)\n        polygon_gdf.to_file(fpath_gpkg, layer=\"prediction_segments\")\n        polygon_gdf.to_parquet(fpath_parquet)\n</code></pre>"},{"location":"ref/#darts_export.export_probabilities","title":"<code>export_probabilities(tile, out_dir, export_ensemble_inputs=False, tags={})</code>","text":"<p>Export the probabilities layer to a file.</p> <p>If <code>export_ensemble_inputs</code> is set to True and the ensemble used at least two models for inference, the probabilities of the models will be written as individual files as well.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required <code>export_ensemble_inputs</code> <code>bool</code> <p>Also save the model outputs, not only the ensemble result. Only applies if the inference result is an ensemble result and has at least two inputs. Defaults to False.</p> <code>False</code> <code>tags</code> <code>dict</code> <p>optional GeoTIFF metadata to be written. Defaults to no additional metadata.</p> <code>{}</code> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_probabilities(tile: xr.Dataset, out_dir: Path, export_ensemble_inputs: bool = False, tags: dict = {}):\n    \"\"\"Export the probabilities layer to a file.\n\n    If `export_ensemble_inputs` is set to True and the ensemble used at least two models for inference,\n    the probabilities of the models will be written as individual files as well.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n        export_ensemble_inputs (bool, optional): Also save the model outputs, not only the ensemble result.\n            Only applies if the inference result is an ensemble result and has at least two inputs.\n            Defaults to False.\n        tags (dict, optional): optional GeoTIFF metadata to be written. Defaults to no additional metadata.\n\n    \"\"\"\n    subset_names = _get_subset_names(tile)\n    if export_ensemble_inputs and len(subset_names) &gt; 1:\n        for subset in _get_subset_names(tile):\n            tick_estart = time.perf_counter()\n            layer_name = f\"probabilities-{subset}\"\n            fpath = out_dir / f\"{layer_name}.tif\"\n            tile[layer_name].rio.to_raster(fpath, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n            tick_eend = time.perf_counter()\n            logger.debug(f\"Exported probabilities for {subset} to {fpath} in {tick_eend - tick_estart:.2f}s\")\n\n    fpath = out_dir / \"probabilities.tif\"\n    with stopuhr(f\"Exporting probabilities to {fpath}\", logger.debug):\n        tile[\"probabilities\"].rio.to_raster(fpath, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n</code></pre>"},{"location":"ref/#darts_export.export_tcvis","title":"<code>export_tcvis(tile, out_dir)</code>","text":"<p>Export the TCVIS data as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_tcvis(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the TCVIS data as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"tcvis.tif\"\n    with stopuhr(f\"Exporting TCVIS data to {fpath}\", logger.debug):\n        tile[[\"tc_brightness\", \"tc_greenness\", \"tc_wetness\"]].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/#darts_export.export_thumbnail","title":"<code>export_thumbnail(tile, out_dir)</code>","text":"<p>Export a thumbnail of the optical data.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_thumbnail(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export a thumbnail of the optical data.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"thumbnail.jpg\"\n    with stopuhr(f\"Exporting thumbnail to {fpath}\", logger.debug):\n        fig = thumbnail(tile)\n        fig.savefig(fpath)\n        fig.clear()\n</code></pre>"},{"location":"ref/#darts_postprocessing","title":"<code>darts_postprocessing</code>","text":"<p>Postprocessing steps for the DARTS dataset.</p>"},{"location":"ref/#darts_postprocessing.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/#darts_preprocessing","title":"<code>darts_preprocessing</code>","text":"<p>Data preprocessing and feature engineering for the DARTS dataset.</p>"},{"location":"ref/#darts_preprocessing.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/#darts_preprocessing.preprocess_legacy","title":"<code>preprocess_legacy(ds_optical, ds_arcticdem, ds_tcvis)</code>","text":"<p>Preprocess optical data with legacy (DARTS v1) preprocessing steps.</p> <p>The processing steps are: - Calculate NDVI - Merge everything into a single ds.</p> <p>Parameters:</p> Name Type Description Default <code>ds_optical</code> <code>Dataset</code> <p>The Planet scene optical data or Sentinel 2 scene optical data.</p> required <code>ds_arcticdem</code> <code>Dataset</code> <p>The ArcticDEM data.</p> required <code>ds_tcvis</code> <code>Dataset</code> <p>The TCVIS data.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>def preprocess_legacy(\n    ds_optical: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Merge everything into a single ds.\n\n    Args:\n        ds_optical (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical data.\n        ds_arcticdem (xr.Dataset): The ArcticDEM data.\n        ds_tcvis (xr.Dataset): The TCVIS data.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Calculate NDVI\n    ds_ndvi = calculate_ndvi(ds_optical)\n\n    # Reproject TCVIS to optical data\n    ds_tcvis = ds_tcvis.odc.reproject(ds_optical.odc.geobox, resampling=\"cubic\")\n\n    # Since this function expects the arcticdem to be loaded from a VRT, which already contains slope and tpi,\n    # we dont need to calculate them here\n\n    # merge to final dataset\n    ds_merged = xr.merge([ds_optical, ds_ndvi, ds_arcticdem, ds_tcvis])\n\n    return ds_merged\n</code></pre>"},{"location":"ref/#darts_preprocessing.preprocess_legacy_fast","title":"<code>preprocess_legacy_fast(ds_merged, ds_arcticdem, ds_tcvis, tpi_outer_radius=100, tpi_inner_radius=0, device=DEFAULT_DEVICE)</code>","text":"<p>Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>The main difference to preprocess_legacy is the new data concept of the arcticdem. Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw arcticdem data and calculates slope and relative elevation on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>ds_merged</code> <code>Dataset</code> <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> required <code>ds_arcticdem</code> <code>Dataset</code> <p>The ArcticDEM dataset.</p> required <code>ds_tcvis</code> <code>Dataset</code> <p>The TCVIS dataset.</p> required <code>tpi_outer_radius</code> <code>int</code> <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> <code>100</code> <code>tpi_inner_radius</code> <code>int</code> <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> <code>0</code> <code>device</code> <code>Literal['cuda', 'cpu'] | int</code> <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> <code>DEFAULT_DEVICE</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>def preprocess_legacy_fast(\n    ds_merged: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    The main difference to preprocess_legacy is the new data concept of the arcticdem.\n    Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw\n    arcticdem data and calculates slope and relative elevation on the fly.\n\n    Args:\n        ds_merged (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    tick_fstart = time.perf_counter()\n    logger.debug(\"Starting fast v1 preprocessing.\")\n\n    # Calculate NDVI\n    ds_merged[\"ndvi\"] = calculate_ndvi(ds_merged).ndvi\n\n    # Reproject TCVIS to optical data\n    tick_sproj = time.perf_counter()\n    ds_tcvis = ds_tcvis.odc.reproject(ds_merged.odc.geobox, resampling=\"cubic\")\n    tick_eproj = time.perf_counter()\n    logger.debug(f\"Reprojection of TCVIS done in {tick_eproj - tick_sproj:.2f} seconds.\")\n\n    ds_merged[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_merged[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_merged[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    tick_sproj = time.perf_counter()\n    ds_arcticdem = ds_arcticdem.odc.reproject(ds_merged.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n    tick_eproj = time.perf_counter()\n    logger.debug(f\"Reprojection of ArcticDEM done in {tick_eproj - tick_sproj:.2f} seconds.\")\n\n    ds_arcticdem = preprocess_legacy_arcticdem_fast(ds_arcticdem, tpi_outer_radius, tpi_inner_radius, device)\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_merged.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_merged)\n\n    ds_merged[\"dem\"] = ds_arcticdem.dem\n    ds_merged[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_merged[\"slope\"] = ds_arcticdem.slope\n    ds_merged[\"arcticdem_data_mask\"] = ds_arcticdem.datamask\n\n    # Update datamask with arcticdem mask\n    # with xr.set_options(keep_attrs=True):\n    #     ds_merged[\"quality_data_mask\"] = ds_merged.quality_data_mask * ds_arcticdem.datamask\n    # ds_merged.quality_data_mask.attrs[\"data_source\"] += \" + ArcticDEM\"\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Preprocessing done in {tick_fend - tick_fstart:.2f} seconds.\")\n    return ds_merged\n</code></pre>"},{"location":"ref/#darts_segmentation","title":"<code>darts_segmentation</code>","text":"<p>Image segmentation of thaw-slumps for the DARTS dataset.</p>"},{"location":"ref/#darts_segmentation.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenter","title":"<code>SMPSegmenter</code>","text":"<p>An actor that keeps a model as its state and segments tiles.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>class SMPSegmenter:\n    \"\"\"An actor that keeps a model as its state and segments tiles.\"\"\"\n\n    config: SMPSegmenterConfig\n    model: nn.Module\n    device: torch.device\n\n    def __init__(self, model_checkpoint: Path | str, device: torch.device = DEFAULT_DEVICE):\n        \"\"\"Initialize the segmenter.\n\n        Args:\n            model_checkpoint (Path): The path to the model checkpoint.\n            device (torch.device): The device to run the model on.\n                Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n        \"\"\"\n        model_checkpoint = model_checkpoint if isinstance(model_checkpoint, Path) else Path(model_checkpoint)\n        self.device = device\n        ckpt = torch.load(model_checkpoint, map_location=self.device)\n        self.config = validate_config(ckpt[\"config\"])\n        # Overwrite the encoder weights with None, because we load our own\n        self.config[\"model\"] |= {\"encoder_weights\": None}\n        self.model = smp.create_model(**self.config[\"model\"])\n        self.model.to(self.device)\n        self.model.load_state_dict(ckpt[\"statedict\"])\n        self.model.eval()\n\n        logger.debug(\n            f\"Successfully loaded model from {model_checkpoint.resolve()} with inputs: \"\n            f\"{self.config['input_combination']}\"\n        )\n\n    def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n        \"\"\"Take a tile and convert it to a pytorch tensor.\n\n        Respects the input combination from the config.\n\n        Returns:\n            A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n        \"\"\"\n        bands = []\n        # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n        # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n        for feature_name in self.config[\"input_combination\"]:\n            norm = self.config[\"norm_factors\"][feature_name]\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n        return torch.stack(bands, dim=0)\n\n    def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n        \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n        Respects the the input combination from the config.\n\n        Returns:\n            A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n        \"\"\"\n        bands = []\n        for feature_name in self.config[\"input_combination\"]:\n            norm = self.config[\"norm_factors\"][feature_name]\n            for tile in tiles:\n                band_data = tile[feature_name]\n                # Normalize the band data\n                band_data = band_data * norm\n                bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n        # TODO: Test this\n        return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n\n    def segment_tile(\n        self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n    ) -&gt; xr.Dataset:\n        \"\"\"Run inference on a tile.\n\n        Args:\n            tile: The input tile, containing preprocessed, harmonized data.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n                Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n        \"\"\"\n        # Convert the tile to a tensor\n        tensor_tile = self.tile2tensor(tile)\n\n        # Create a batch dimension, because predict expects it\n        tensor_tile = tensor_tile.unsqueeze(0)\n\n        probabilities = predict_in_patches(\n            self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n        ).squeeze(0)\n\n        # Highly sophisticated DL-based predictor\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\n            \"long_name\": \"Probabilities\",\n        }\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n        # Cleanup cuda memory\n        del tensor_tile, probabilities\n        free_torch()\n\n        return tile\n\n    def segment_tile_batched(\n        self,\n        tiles: list[xr.Dataset],\n        patch_size: int = 1024,\n        overlap: int = 16,\n        batch_size: int = 8,\n        reflection: int = 0,\n    ) -&gt; list[xr.Dataset]:\n        \"\"\"Run inference on a list of tiles.\n\n        Args:\n            tiles: The input tiles, containing preprocessed, harmonized data.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n                Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n        \"\"\"\n        # Convert the tiles to tensors\n        # TODO: maybe create a batched tile2tensor function?\n        # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n        tensor_tiles = self.tile2tensor_batched(tiles)\n\n        # Create a batch dimension, because predict expects it\n        tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n        probabilities = predict_in_patches(\n            self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n        )\n\n        # Highly sophisticated DL-based predictor\n        for tile, probs in zip(tiles, probabilities):\n            # TODO: is there a better way to pass metadata?\n            tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n            tile[\"probabilities\"].attrs = {\n                \"long_name\": \"Probabilities\",\n            }\n            tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n        # Cleanup cuda memory\n        del tensor_tiles, probabilities\n        free_torch()\n\n        return tiles\n\n    def __call__(\n        self,\n        input: xr.Dataset | list[xr.Dataset],\n        patch_size: int = 1024,\n        overlap: int = 16,\n        batch_size: int = 8,\n        reflection: int = 0,\n    ) -&gt; xr.Dataset | list[xr.Dataset]:\n        \"\"\"Run inference on a single tile or a list of tiles.\n\n        Args:\n            input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n                Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n            Each `probability` has type float32 and range [0, 1].\n\n        Raises:\n            ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n        \"\"\"\n        if isinstance(input, xr.Dataset):\n            return self.segment_tile(\n                input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n            )\n        elif isinstance(input, list):\n            return self.segment_tile_batched(\n                input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n            )\n        else:\n            raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.config","title":"<code>config = validate_config(ckpt['config'])</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenter.device","title":"<code>device = device</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenter.model","title":"<code>model = smp.create_model(**self.config['model'])</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenter.__call__","title":"<code>__call__(input, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a single tile or a list of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dataset | list[Dataset]</code> <p>A single tile or a list of tiles.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dataset | list[Dataset]</code> <p>A single tile or a list of tiles augmented by a predicted <code>probabilities</code> layer, depending on the input.</p> <code>Dataset | list[Dataset]</code> <p>Each <code>probability</code> has type float32 and range [0, 1].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xr.Dataset | list[xr.Dataset]:\n    \"\"\"Run inference on a single tile or a list of tiles.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n        Each `probability` has type float32 and range [0, 1].\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    else:\n        raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.__init__","title":"<code>__init__(model_checkpoint, device=DEFAULT_DEVICE)</code>","text":"<p>Initialize the segmenter.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>Path</code> <p>The path to the model checkpoint.</p> required <code>device</code> <code>device</code> <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> <code>DEFAULT_DEVICE</code> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __init__(self, model_checkpoint: Path | str, device: torch.device = DEFAULT_DEVICE):\n    \"\"\"Initialize the segmenter.\n\n    Args:\n        model_checkpoint (Path): The path to the model checkpoint.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_checkpoint = model_checkpoint if isinstance(model_checkpoint, Path) else Path(model_checkpoint)\n    self.device = device\n    ckpt = torch.load(model_checkpoint, map_location=self.device)\n    self.config = validate_config(ckpt[\"config\"])\n    # Overwrite the encoder weights with None, because we load our own\n    self.config[\"model\"] |= {\"encoder_weights\": None}\n    self.model = smp.create_model(**self.config[\"model\"])\n    self.model.to(self.device)\n    self.model.load_state_dict(ckpt[\"statedict\"])\n    self.model.eval()\n\n    logger.debug(\n        f\"Successfully loaded model from {model_checkpoint.resolve()} with inputs: \"\n        f\"{self.config['input_combination']}\"\n    )\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.segment_tile","title":"<code>segment_tile(tile, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The input tile, containing preprocessed, harmonized data.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def segment_tile(\n    self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tile to a tensor\n    tensor_tile = self.tile2tensor(tile)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tile = tensor_tile.unsqueeze(0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n    ).squeeze(0)\n\n    # Highly sophisticated DL-based predictor\n    # TODO: is there a better way to pass metadata?\n    tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n    tile[\"probabilities\"].attrs = {\n        \"long_name\": \"Probabilities\",\n    }\n    tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tile, probabilities\n    free_torch()\n\n    return tile\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.segment_tile_batched","title":"<code>segment_tile_batched(tiles, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a list of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list[Dataset]</code> <p>The input tiles, containing preprocessed, harmonized data.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[Dataset]</code> <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tiles to tensors\n    # TODO: maybe create a batched tile2tensor function?\n    # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n    tensor_tiles = self.tile2tensor_batched(tiles)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n    )\n\n    # Highly sophisticated DL-based predictor\n    for tile, probs in zip(tiles, probabilities):\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\n            \"long_name\": \"Probabilities\",\n        }\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tiles, probabilities\n    free_torch()\n\n    return tiles\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.tile2tensor","title":"<code>tile2tensor(tile)</code>","text":"<p>Take a tile and convert it to a pytorch tensor.</p> <p>Respects the input combination from the config.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n    \"\"\"Take a tile and convert it to a pytorch tensor.\n\n    Respects the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n    # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        band_data = tile[feature_name]\n        # Normalize the band data\n        band_data = band_data * norm\n        bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n    return torch.stack(bands, dim=0)\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.tile2tensor_batched","title":"<code>tile2tensor_batched(tiles)</code>","text":"<p>Take a list of tiles and convert them to a pytorch tensor.</p> <p>Respects the the input combination from the config.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n    \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n    Respects the the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        for tile in tiles:\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n    # TODO: Test this\n    return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenterConfig","title":"<code>SMPSegmenterConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for the segmentor.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>class SMPSegmenterConfig(TypedDict):\n    \"\"\"Configuration for the segmentor.\"\"\"\n\n    input_combination: list[str]\n    model: dict[str, Any]\n    norm_factors: dict[str, float]\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenterConfig.input_combination","title":"<code>input_combination</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenterConfig.model","title":"<code>model</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenterConfig.norm_factors","title":"<code>norm_factors</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.create_patches","title":"<code>create_patches(tensor_tiles, patch_size, overlap, return_coords=False)</code>","text":"<p>Create patches from a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_tiles</code> <code>Tensor</code> <p>The input tensor. Shape: (BS, C, H, W).</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches.</p> required <code>overlap</code> <code>int</code> <p>The size of the overlap.</p> required <code>return_coords</code> <code>bool</code> <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    logger.debug(f\"Creating {nh * nw} patches took {time.time() - start_time:.2f}s\")\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"ref/#darts_segmentation.patch_coords","title":"<code>patch_coords(h, w, patch_size, overlap)</code>","text":"<p>Yield patch coordinates based on height, width, patch size and margin size.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>int</code> <p>Height of the image.</p> required <code>w</code> <code>int</code> <p>Width of the image.</p> required <code>patch_size</code> <code>int</code> <p>Patch size.</p> required <code>overlap</code> <code>int</code> <p>Margin size.</p> required <p>Yields:</p> Type Description <code>tuple[int, int, int, int]</code> <p>tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def patch_coords(h: int, w: int, patch_size: int, overlap: int) -&gt; Generator[tuple[int, int, int, int], None, None]:\n    \"\"\"Yield patch coordinates based on height, width, patch size and margin size.\n\n    Args:\n        h (int): Height of the image.\n        w (int): Width of the image.\n        patch_size (int): Patch size.\n        overlap (int): Margin size.\n\n    Yields:\n        tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.\n\n    \"\"\"\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            yield y, x, patch_idx_y, patch_idx_x\n</code></pre>"},{"location":"ref/#darts_segmentation.predict_in_patches","title":"<code>predict_in_patches(model, tensor_tiles, patch_size, overlap, batch_size, reflection, device=torch.device, return_weights=False)</code>","text":"<p>Predict on a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to use for prediction.</p> required <code>tensor_tiles</code> <code>Tensor</code> <p>The input tensor. Shape: (BS, C, H, W).</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches.</p> required <code>overlap</code> <code>int</code> <p>The size of the overlap.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> required <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor.</p> required <code>device</code> <code>device</code> <p>The device to use for the prediction.</p> <code>device</code> <code>return_weights</code> <code>bool</code> <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The predicted tensor.</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    n_skipped = 0\n    for i, batch in enumerate(patches):\n        # If batch contains only nans, skip it\n        # TODO: This doesn't work as expected -&gt; check if torch.isnan(batch).all() is correct\n        if torch.isnan(batch).all(axis=0).any():\n            patched_probabilities[i * batch_size : (i + 1) * batch_size] = 0\n            n_skipped += 1\n            continue\n        # If batch contains some nans, replace them with zeros\n        batch[torch.isnan(batch)] = 0\n\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    if n_skipped &gt; 0:\n        logger.debug(f\"Skipped {n_skipped} batches because they only contained NaNs\")\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n    logger.info(f\"Predicting {nh * nw} patches took {time.time() - start_time:.2f}s\")\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"ref/#darts_superresolution","title":"<code>darts_superresolution</code>","text":"<p>Image superresolution of Sentinel 2 imagery for the DARTS dataset.</p>"},{"location":"ref/#darts_superresolution.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/#darts_utils","title":"<code>darts_utils</code>","text":"<p>Utility functions for the DARTS dataset.</p>"},{"location":"ref/#darts_utils.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"dev/arch/","title":"Architecture describtion","text":"<p>Old documentation</p> <p>This document is not up-to-date. E.g. Rye is not used anymore.</p> <p>This repository is a workspace repository, managed by Rye. Read more about workspaces at the Rye docs. Each workspace-member starts with <code>darts-*</code> and can be seen as an own package or module, except the <code>darts</code> directory which is the top-level package. Each package has it's own internal functions and it's public facing API. The public facing API of each package MUST follow the following section API paradigms.</p> Table of Contents<ul> <li>Architecture describtion<ul> <li>Package overview<ul> <li>Conceptual migration from thaw-slump-segmentation</li> <li>Create a new package</li> </ul> </li> <li>APIs between pipeline steps<ul> <li>Preprocessing Output</li> <li>Segmentation / Ensemble Output</li> <li>Postprocessing Output</li> <li>PyTorch Model checkpoints</li> </ul> </li> <li>API paradigms<ul> <li>Examples</li> <li>About the Xarray overhead with Ray</li> </ul> </li> </ul> </li> </ul>"},{"location":"dev/arch/#package-overview","title":"Package overview","text":"Package Name Type Description (Major) Dependencies - all need Xarray <code>darts-acquisition</code> Data Fetches data from the data sources GEE, rasterio, ODC-Geo <code>darts-preprocessing</code> Data Loads data and combines the features to a Xarray Dataset Cupy, Xarray-Spatial <code>darts-superresolution</code> Train Trains a supper resolution model to scale Sentinel 2 images from 10m to 3m resolution PyTorch <code>darts-segmentation</code> Train Trains an segmentation model PyTorch, segmentation_models_pytorch <code>darts-ensemble</code> Ensemble Ensembles the different models and run the multi-stage inference pipeline. PyTorch <code>darts-postprocessing</code> Data Further refines the output from an ensemble or segmentaion and binarizes the probs Scipy, Cucim <code>darts-export</code> Data Saves the results from inference and combines the result to the final DARTS dataset GeoPandas <code>darts-utils</code> Data Shared utilities for data processing <p>The following modules are planned or potential ideas for future expansion of the project:</p> Package Name Type Description (Major) Dependencies - all need Xarray <code>darts-detection</code> Train Trains an object detection model PyTorch <code>darts-?</code> Train Trains a ? model for more complex multi-stage ensembles ? <code>darts-evaluation</code> Test Evaluates the end-to-end process on a test dataset and external dataset GeoPandas <code>darts-train-utils</code> Train Shared utilities for training PyTorch <p>The packages should follow this architecture: </p> <p>The <code>darts-nextgen</code> is planned to utilize Ray to automaticly parallize the different computations. However, each package should be designed so that one could build their own pipeline without Ray. Hence, all Ray-related functions / transformations etc. should be defined in the toplevel <code>darts</code> sub-directory.</p> <p>The packages can decide to wrap their public functions into a CLI with typer.</p> <p>The <code>Train</code> packages should also hold the code for training specific data preparation, model training and model evaluation. These packages should get their data from (already processed) data from the <code>darts-preprocessing</code> package. They should expose a statefull Model class with an <code>inference</code> function, which can be used by the <code>darts-ensemble</code> package.</p>"},{"location":"dev/arch/#conceptual-migration-from-thaw-slump-segmentation","title":"Conceptual migration from thaw-slump-segmentation","text":"<ul> <li>The <code>darts-ensemble</code> and <code>darts-postprocessing</code> packages is the successor of the <code>process-02-inference</code> and <code>process-03-ensemble</code> scripts.</li> <li>The <code>darts-preprocessing</code> and <code>darts-acquisition</code> packages are the successors of the <code>setup-raw-data</code> script and manual work of obtaining data.</li> <li>The <code>darts-export</code> package is splitted from the  <code>inference</code> script, should include the previous manual works of combining everything into the final dataset.</li> <li>The <code>darts-superresolution</code> package is the successor of the <code>superresolution</code> repository.</li> <li>The <code>darts-segmentation</code> package is the successor of the <code>train</code> and <code>prepare_data</code> script.</li> <li>The <code>darts-evaluation</code> package is the successor of the different manual evaluations.</li> </ul>"},{"location":"dev/arch/#create-a-new-package","title":"Create a new package","text":"<p>A new package can easily created with:</p> <pre><code>rye init darts-packagename\n</code></pre> <p>Rye creates a minimal project structure for us.</p> <p>The following things needs to be updates:</p> <ol> <li>The <code>pyproject.toml</code> file inside the new package.</li> </ol> <p>Add to the <code>pyproject.toml</code> file inside the new package is the following to enable Ruff:</p> <pre><code>```toml\n[tool.ruff]\n# Extend the `pyproject.toml` file in the parent directory...\nextend = \"../pyproject.toml\"\n```\n\nPlease also provide a description and a list of authors to the file.\n</code></pre> <ol> <li> <p>The <code>.github/workflows/update_version.yml</code> file, to include the package in the workflow.</p> <p>Under <code>package</code> and under step <code>Update version in pyproject.toml</code>.</p> </li> <li> <p>The docs by creating a <code>ref/name.md</code> file and add them to the nav inside the <code>mkdocs.yml</code>.</p> <p>To enable code detection, also add the package directory under <code>plugins</code> in the <code>mkdocs.yml</code>. Please also add the refs to the top-level <code>ref.md</code>.</p> </li> <li> <p>The Readme of the package</p> </li> </ol>"},{"location":"dev/arch/#apis-between-pipeline-steps","title":"APIs between pipeline steps","text":"<p>The following diagram visualizes the steps of the major <code>packages</code> of the pipeline: </p> <p>Each Tile should be represented as a single <code>xr.Dataset</code> with each feature / band as <code>DataVariable</code>. Each DataVariable should have their <code>data_source</code> documented in the <code>attrs</code>, aswell as <code>long_name</code> and <code>units</code> if any for plotting. A <code>_FillValue</code> should also be set for no-data with <code>.rio.write_nodata(\"no-data-value\")</code></p>"},{"location":"dev/arch/#preprocessing-output","title":"Preprocessing Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs note <code>blue</code> (x, y) uint16 0 data_source, long_name, units <code>green</code> (x, y) uint16 0 data_source, long_name, units <code>red</code> (x, y) uint16 0 data_source, long_name, units <code>nir</code> (x, y) uint16 0 data_source, long_name, units <code>ndvi</code> (x, y) uint16 0 data_source, long_name Values between 0-20.000 (+1, *1e4) <code>relative_elevation</code> (x, y) int16 0 data_source, long_name, units <code>slope</code> (x, y) float32 nan data_source, long_name <code>tc_brightness</code> (x, y) uint8 - data_source, long_name <code>tc_greenness</code> (x, y) uint8 - data_source, long_name <code>tc_wetness</code> (x, y) uint8 - data_source, long_name <code>valid_data_mask</code> (x, y) bool - data_source, long_name <code>quality_data_mask</code> (x, y) bool - data_source, long_name"},{"location":"dev/arch/#segmentation-ensemble-output","title":"Segmentation / Ensemble Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs [Output from Preprocessing] <code>probabilities</code> (x, y) float32 nan long_name <code>probabilities-model-X*</code> (x, y) float32 nan long_name <p>*: optional intermedia probabilities in an ensemble</p>"},{"location":"dev/arch/#postprocessing-output","title":"Postprocessing Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs note [Output from Preprocessing] <code>probabilities_percent</code> (x, y) uint8 255 long_name, units Values between 0-100 <code>binarized_segmentation</code> (x, y) uint8 - long_name"},{"location":"dev/arch/#pytorch-model-checkpoints","title":"PyTorch Model checkpoints","text":"<p>Each checkpoint is stored as a torch <code>.pt</code> tensor file. The checkpoint MUST have the following structure:</p> <pre><code>{\n    \"config\": {\n        \"model_framework\": \"smp\", # Identifier which framework or model was used\n        \"model\": { ... }, # Model specific hyperparameter which are needed to create the model\n        \"input_combination\": [ ... ], # List of strings of the names with which the model was trained, order is important\n        \"patch_size\": 1024, # Patch size on which the model was trained\n        ... # More model-framework specific parameter, e.g. normalization method and factors\n    },\n    \"statedict\": model.module.state_dict(),\n}\n</code></pre>"},{"location":"dev/arch/#api-paradigms","title":"API paradigms","text":"<p>The packages should pass the data as Xarray Datasets between each other. Datasets can hold coordinate information aswell as other metadata (like CRS) in a single self-describing object. Since different <code>tiles</code> do not share the same coordinates or metadata, each <code>tile</code> should be represented by a single Xarray <code>Dataset</code>.</p> <ul> <li>Each public facing API function which in some way transforms data should accept a Xarray Dataset as input and return an Xarray Dataset.</li> <li>Data can also be accepted as a list of Xarray Dataset as input and returned as a list of Xarray Datasets for batched processing.     In this case, concattenation should happend internally and on <code>numpy</code> or <code>pytorch</code> level, NOT on <code>xarray</code> abstraction level.     The reason behind this it that the tiles don't share their coordinates, resulting in a lot of empty spaces between the tiles and high memory usage.     The name of the function should then be <code>function_batched</code>.</li> <li>Each public facing API function which loads data should return a single Xarray Dataset for each <code>tile</code>.</li> <li>Data should NOT be saved to file internally, with <code>darts-export</code> as the only exception. Instead, data should returned in-memory as a Xarray Dataset, so the user / pipeline can decide what to save and when.</li> <li>Function names should be verbs, e.g. <code>process</code>, <code>ensemble</code>, <code>do_inference</code>.</li> <li>If a function is stateless it should NOT be part of a class or wrapper</li> <li>If a function is stateful it should be part of a class or wrapper, this is important for Ray</li> </ul>"},{"location":"dev/arch/#examples","title":"Examples","text":"<p>Here are some examples, how these API paradigms should look like.</p> <ol> <li> <p>Single transformation</p> <pre><code>import darts-package\nimport xarray as xr\n\n# User loads / creates the dataset (a single tile) by themself\nds = xr.open_dataset(\"...\")\n\n# User calls the function to transform the dataset\nds = darts-package.transform(ds, **kwargs)\n\n# User can decide by themself what to do next, e.g. save\nds.to_netcdf(\"...\")\n</code></pre> </li> <li> <p>Batched transformation</p> <pre><code>import darts_package\nimport xarray as xr\n\n# User loads / creates multiple datasets (hence, multiple tiles) by themself\ndata = [xr.open_dataset(\"...\"), xr.open_dataset(\"...\"), ...]\n\n# User calls the function to transform the dataset\ndata = darts_package.transform_batched(data, **kwargs)\n\n# User can decide by themself what to do next\ndata[0].whatever()\n</code></pre> </li> <li> <p>Load &amp; preprocess some data</p> <pre><code>import darts_package\n\n# User calls the function to transform the dataset\nds = darts_package.load(\"path/to/data\", **kwargs)\n\n# User can decide by themself what to do next\nds.whatever()\n</code></pre> </li> <li> <p>Custom pipeline example</p> <pre><code>from pathlib import Path\nimport darts_preprocess\nimport darts_inference\n\nDATA_DIR = Path(\"./data/\")\nMODEL_DIR = Path(\"./models/\")\nOUT_DIR = Path(\"./out/\")\n\n# Inference is a stateful transformation, because it needs to load the model\n# Hence, the \nensemble = darts_inference.Ensemble.load(MODEL_DIR)\n\n# The data directory contains subfolders which then hold the input data\nfor dir in DATA_DIR:\n    name = dir.name\n\n    # Load the files from the processing directory\n    ds = darts_preprocess.load_and_preprocess(dir)\n\n    # Do the inferencce\n    ds = ensemble.inference(ds)\n\n    # Save the results\n    ds.to_netcdf(OUT_DIR / f\"{name}-result.nc\")\n</code></pre> </li> <li> <p>Pipeline with Ray</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\nimport ray\nimport darts_preprocess\nimport darts_inference\nimport darts_export\n\nDATA_DIR = Path(\"./data/\")\nMODEL_DIR = Path(\"./models/\")\nOUT_DIR = Path(\"./out/\")\n\nray.init()\n\n# We need to wrap the Xarray dataset in a class, so that Ray can serialize it\n@dataclass\nclass Tile:\n    ds: xr.Dataset\n\n# Wrapper for ray\ndef open_dataset_ray(row: dict[str, Any]) -&gt; dict[str, Any]:\n    data = xr.open_dataset(row[\"path\"])\n    tile = Tile(data)\n    return {\n        \"input\": tile,\n    }\n\n# Wrapper for the preprocessing -&gt; Stateless\ndef preprocess_tile_ray(row: dict[str, Tile]) -&gt; dict[str, Tile]:\n    ds = darts_preprocess.preprocess(row[\"input\"].ds)\n    return {\n        \"preprocessed\": Tile(ds),\n        \"input\": row[\"input\"]\n    }\n\n# Wrapper for the inference -&gt; Statefull\nclass EnsembleRay:\n    def __init__(self):\n        self.ensemble = darts_inference.Ensemble.load(MODEL_DIR)\n\n    def __call__(self, row: dict[str, Tile]) -&gt; dict[str, Tile]:\n        ds = self.ensemble.inference(row[\"preprocessed\"].ds)\n        return {\n            \"output\": Tile(ds),\n            \"preprocessed\": row[\"preprocessed\"],\n            \"input\": row[\"input\"],\n        }\n\n# We need to add 'local:///' to tell ray that we want to use the local filesystem\nfiles = data.glob(\"*.nc\")\nfile_list = [f\"local:////{file.resolve().absolute()}\" for file in files]\n\nds = ray.data.read_binary_files(file_list, include_paths=True)\nds = ds.map(open_dataset_ray) # Lazy open\nds = ds.map(preprocess_tile_ray) # Lazy preprocess\nds = ds.map(EnsembleRay) # Lazy inference\n\n# Save the results\nfor row in ds.iter_rows():\n    darts_export.save(row[\"output\"].ds, OUT_DIR / f\"{row['input'].ds.name}-result.nc\")\n</code></pre> </li> </ol>"},{"location":"dev/arch/#about-the-xarray-overhead-with-ray","title":"About the Xarray overhead with Ray","text":"<p>Ray expects batched data to be in either numpy or pandas format and can't work with Xarray datasets directly. Hence, a wrapper with custom stacking functions is needed. This tradeoff is not small, however, the benefits in terms of maintainability and readability are worth it.</p> <p></p>"},{"location":"dev/auxiliary/","title":"Auxiliary Data and Datacubes","text":"<p>DARTS uses several auxiliary data - data which does not change between different scenes and / or time steps. Raster auxiliary data is stored in Zarr Datacubes.</p> <p>Currently, the following auxiliary data is used:</p> <ul> <li>ArcticDEM</li> <li>Tasseled Cap indices (Brightness, Greenness, Wetness)</li> </ul> <p>with more to come.</p>"},{"location":"dev/auxiliary/#arcticdem","title":"ArcticDEM","text":"<p>The ArcticDEM is downloaded via their STAC server using these extend files.</p> <p>The user can specify the download directory, where the ArcticDEM will be procedurally stored in a Zarr Datacube. The user can also specify the resolution of the ArcticDEM, which is either 2m, 10m or 32m. Each resolution is stored in their own Zarr Datacube.</p>"},{"location":"dev/auxiliary/#darts_acquisition.load_arcticdem","title":"<code>darts_acquisition.load_arcticdem(geobox, data_dir, resolution, buffer=0, persist=True)</code>","text":"<p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>geobox</code> <code>GeoBox</code> <p>The geobox for which the tile should be loaded.</p> required <code>data_dir</code> <code>Path | str</code> <p>The directory where the ArcticDEM data is stored.</p> required <code>resolution</code> <code>Literal[2, 10, 32]</code> <p>The resolution of the ArcticDEM data in m.</p> required <code>buffer</code> <code>int</code> <p>The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.</p> <code>0</code> <code>persist</code> <code>bool</code> <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The ArcticDEM tile, with a buffer applied. Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different. Final dataset is NOT matched to the reference CRS and resolution.</p> Warning <p>Geobox must be in a meter based CRS.</p> Usage <p>Since the API of the <code>load_arcticdem</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_arcticdem\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\narcticdem = load_arcticdem(\n    optical.odc.geobox,\n    \"/path/to/arcticdem-parent-directory\",\n    resolution=2,\n    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n)\n\n# Now we can for example match the resolution and extent of the optical data:\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> <p>The <code>buffer</code> parameter is used to extend the region of interest by a certain amount of pixels. This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.</p> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem/datacube.py</code> <pre><code>def load_arcticdem(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    resolution: RESOLUTIONS,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded.\n        data_dir (Path | str): The directory where the ArcticDEM data is stored.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in m.\n        buffer (int, optional): The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The ArcticDEM tile, with a buffer applied.\n            Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different.\n            Final dataset is NOT matched to the reference CRS and resolution.\n\n    Warning:\n        Geobox must be in a meter based CRS.\n\n    Usage:\n        Since the API of the `load_arcticdem` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_arcticdem\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        arcticdem = load_arcticdem(\n            optical.odc.geobox,\n            \"/path/to/arcticdem-parent-directory\",\n            resolution=2,\n            buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n        The `buffer` parameter is used to extend the region of interest by a certain amount of pixels.\n        This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.\n\n    \"\"\"  # noqa: E501\n    tick_fstart = time.perf_counter()\n\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    datacube_fpath = data_dir / f\"datacube_{resolution}m_v4.1.zarr\"\n    storage = zarr.storage.FSStore(datacube_fpath)\n    logger.debug(f\"Getting ArcticDEM tile from {datacube_fpath.resolve()}\")\n\n    # ! The geobox must be in a meter based CRS\n    logger.debug(f\"Found a reference resolution of {geobox.resolution.x}m\")\n\n    # Check if the zarr data already exists\n    if not datacube_fpath.exists():\n        logger.debug(f\"Creating a new zarr datacube at {datacube_fpath.resolve()} with {storage=}\")\n        create_empty_datacube(\n            \"ArcticDEM Data Cube\",\n            storage,\n            DATA_EXTENT[resolution],\n            CHUNK_SIZE,\n            DATA_VARS,\n            DATA_VARS_META,\n            DATA_VARS_ENCODING,\n        )\n\n    # Get the adjacent arcticdem tiles\n    # Note: We could also use pystac here, but this would result in a slight performance decrease\n    # because of the network overhead, hence we use the extent file\n    # Download the extent, download if the file does not exist\n    extent_fpath = data_dir / f\"ArcticDEM_Mosaic_Index_v4_1_{resolution}m.parquet\"\n    with download_lock:\n        if not extent_fpath.exists():\n            download_arcticdem_extent(data_dir)\n    extent = gpd.read_parquet(extent_fpath)\n\n    # Add a buffer around the geobox to get the adjacent tiles\n    reference_geobox = geobox.to_crs(\"epsg:3413\", resolution=resolution).pad(buffer)\n    adjacent_tiles = extent[extent.intersects(reference_geobox.extent.geom)]\n\n    # Download the adjacent tiles (if necessary)\n    with download_lock:\n        procedural_download_datacube(storage, adjacent_tiles)\n\n    # Load the datacube and set the spatial_ref since it is set as a coordinate within the zarr format\n    chunks = None if persist else \"auto\"\n    arcticdem_datacube = xr.open_zarr(storage, mask_and_scale=False, chunks=chunks).set_coords(\"spatial_ref\")\n\n    # Get an AOI slice of the datacube\n    arcticdem_aoi = arcticdem_datacube.odc.crop(reference_geobox.extent, apply_mask=False)\n\n    # The following code would load the lazy zarr data from disk into memory\n    if persist:\n        tick_sload = time.perf_counter()\n        arcticdem_aoi = arcticdem_aoi.load()\n        tick_eload = time.perf_counter()\n        logger.debug(f\"ArcticDEM AOI loaded from disk in {tick_eload - tick_sload:.2f} seconds\")\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem_aoi[\"datamask\"] = arcticdem_aoi.datamask.astype(\"uint8\")\n\n    logger.info(\n        f\"ArcticDEM tile {'loaded' if persist else 'lazy-opened'} in {time.perf_counter() - tick_fstart:.2f} seconds\"\n    )\n    return arcticdem_aoi\n</code></pre>"},{"location":"dev/auxiliary/#tasseled-cap-indices-tcvis","title":"Tasseled Cap indices (TCVIS)","text":"<p>The TCVIS data is downloaded from Google Earth-Engine (GEE) using the TCVIS collection from Ingmar Nitze: <code>\"users/ingmarnitze/TCTrend_SR_2000-2019_TCVIS\"</code>.</p>"},{"location":"dev/auxiliary/#darts_acquisition.load_tcvis","title":"<code>darts_acquisition.load_tcvis(geobox, data_dir, buffer=0, persist=True)</code>","text":"<p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>geobox</code> <code>GeoBox</code> <p>The geobox to load the data for.</p> required <code>data_dir</code> <code>Path | str</code> <p>The directory to store the downloaded data for faster access for consecutive calls.</p> required <code>buffer</code> <code>int</code> <p>The buffer around the geobox in pixels. Defaults to 0.</p> <code>0</code> <code>persist</code> <code>bool</code> <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The TCVIS dataset.</p> Usage <p>Since the API of the <code>load_tcvis</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_tcvis\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\ntcvis = load_tcvis(\n    optical.odc.geobox,\n    \"/path/to/tcvis-parent-directory\",\n)\n\n# Now we can for example match the resolution and extent of the optical data:\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>def load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the TCVIS for the given geobox, fetch new data from GEE if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox to load the data for.\n        data_dir (Path | str): The directory to store the downloaded data for faster access for consecutive calls.\n        buffer (int, optional): The buffer around the geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset.\n\n    Usage:\n        Since the API of the `load_tcvis` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_tcvis\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        tcvis = load_tcvis(\n            optical.odc.geobox,\n            \"/path/to/tcvis-parent-directory\",\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"  # noqa: E501\n    tick_fstart = time.perf_counter()\n\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    datacube_fpath = data_dir / \"tcvis_2000-2019.zarr\"\n    storage = zarr.storage.FSStore(datacube_fpath)\n    logger.debug(f\"Loading TCVis from {datacube_fpath.resolve()}\")\n\n    if not datacube_fpath.exists():\n        logger.debug(f\"Creating a new zarr datacube at {datacube_fpath.resolve()} with {storage=}\")\n        create_empty_datacube(\n            title=\"Landsat Trends TCVIS 2000-2019\",\n            storage=storage,\n            geobox=DATA_EXTENT,\n            chunk_size=CHUNK_SIZE,\n            data_vars=DATA_VARS,\n            meta=DATA_VARS_META,\n            var_encoding=DATA_VARS_ENCODING,\n        )\n\n    # Download the adjacent tiles (if necessary)\n    reference_geobox = geobox.to_crs(\"epsg:4326\", resolution=DATA_EXTENT.resolution.x).pad(buffer)\n    with download_lock:\n        procedural_download_datacube(storage, reference_geobox)\n\n    # Load the datacube and set the spatial_ref since it is set as a coordinate within the zarr format\n    chunks = None if persist else \"auto\"\n    tcvis_datacube = xr.open_zarr(storage, mask_and_scale=False, chunks=chunks).set_coords(\"spatial_ref\")\n\n    # Get an AOI slice of the datacube\n    tcvis_aoi = tcvis_datacube.odc.crop(reference_geobox.extent, apply_mask=False)\n\n    # The following code would load the lazy zarr data from disk into memory\n    if persist:\n        tick_sload = time.perf_counter()\n        tcvis_aoi = tcvis_aoi.load()\n        tick_eload = time.perf_counter()\n        logger.debug(f\"TCVIS AOI loaded from disk in {tick_eload - tick_sload:.2f} seconds\")\n\n    logger.info(\n        f\"TCVIS tile {'loaded' if persist else 'lazy-opened'} in {time.perf_counter() - tick_fstart:.2f} seconds\"\n    )\n    return tcvis_aoi\n</code></pre>"},{"location":"dev/auxiliary/#why-zarr-datacubes","title":"Why Zarr Datacubes?","text":"<p>Zarr is a file format for storing chunked, compressed, N-dimensional arrays. It is designed to store large arrays of data, and to facilitate fast and efficient IO. Zarr works well integrated with Dask and Xarray.</p> <p>By storing the auxiliary data in Zarr Datacubes, it is much easier and faster to access the data of interest. If we would use GeoTiffs, we would have to first create a Cloud-Optimized GeoTiff (COG), which is basically an ensemble (mosaic) of multiple GeoTiffs. Then we would have to read from the COG, which behind the scenes would open multiple GeoTiffs and crops them to fit the region of interest. E.g. Opening a specific region of interest 10km x 10km from a 2m resolution COG would take up to 2 minutes, if the COGs extend is panarctic. Opening the same region from a Zarr Datacube takes less than 1 second.</p> <p>Inspiration</p> <p>This implementation and concept is heavily inspired by EarthMovers implementation of serverless datacube generation.</p>"},{"location":"dev/auxiliary/#procedural-download","title":"Procedural download","text":"<p>Info</p> <p>The currently used auxiliary data is downloaded on demand, only data actually used is downloaded and stored on your local machine. Hence, the stored datacubes can be thought of as a cache, which is filled with data as needed.</p> <p>There are currently two implementations of the procedural download used: a cloud based STAC download and a download via Google Earth-Engine.</p> <p>Because the single tiles of the STAC mosaic can be overlapping and intersect with multiple Zarr chunks, the STAC download is slightly more complicated. Since Google Earth-Engine allows for exact geoboxes, download of the exact chunks is possible. This reduces the complexity of the download.</p> STAC GEE 1. ROI 2. ROI <p>The above graphics shows the difference between loading data from STAC (left) and Google Earth-Engine (right). With the STAC download, the data is downloaded from a mosaic of tiles, which can be overlapping with each other and cover multiple Zarr chunks. It may occur that a chunk is not fully covered by the STAC mosaic, which results in only partial loaded chunks. In such cases, the missing data in these chunks will be updated if the other intersecting tile is downloaded, which may occur to a later time if a connected ROI is requested. The download process is much easier for GEE, since one can request the exact geoboxes of the Zarr chunks and GEE will handle the rest. Hence, chunks will always be fully covered by the downloaded data.</p> <p>Regarding the open ROI process, both implementations follow the same principle:</p> <ol> <li>Check which Tiles / Chunks intersect with the region of interest</li> <li>Dowload all new Tiles / Chunks</li> <li>Store the new Tiles / Chunks in their specific Zarr chunks</li> <li>Return the region of interest of the Zarr Datacube</li> </ol>"},{"location":"dev/auxiliary/#stac-download","title":"STAC download","text":""},{"location":"dev/auxiliary/#google-earth-engine-download","title":"Google Earth-Engine download","text":""},{"location":"dev/config/","title":"Config Files","text":"<p>The <code>darts</code> CLI support passing parameters via a config file in TOML format. This can be useful to reduce the amount of parameters you need to pass or to safe different configurations. In general, the CLI tries to match all parameters under the <code>darts</code> key of the config file, skipping not needed ones.</p>"},{"location":"dev/config/#example-usage","title":"Example usage","text":"<p>Let's take a closer look with the example command <code>darts hello</code>. This command has the following function signature:</p> <pre><code>def hello(name: str, n: int = 1):\n    \"\"\"Say hello to someone.\n\n    Args:\n        name (str): The name of the person to say hello to\n        n (int, optional): The number of times to say hello. Defaults to 1.\n\n    Raises:\n        ValueError: If n is 3.\n\n    \"\"\"\n    for i in range(n):\n        logger.debug(f\"Currently at {i=}\")\n        if n == 3:\n            raise ValueError(\"I don't like 3\")\n        logger.info(f\"Hello {name}\")\n</code></pre> <p>Let's run the command without making a config file:</p> <pre><code>$ uv run darts hello Alice\nDEBUG Currently at i=0\nINFO Hello Alice\n</code></pre> <p>Now specify a config file <code>config.toml</code>:</p> <pre><code>[darts]\nname = \"Not Alice\"\nn = 2\n</code></pre> <p>And run the same command:</p> <pre><code>$ uv run darts hello Alice\nDEBUG Currently at i=0\nINFO Hello Alice\nDEBUG Currently at i=1\nINFO Hello Alice\n</code></pre> <p>The <code>name</code> parameter is still taken from the CLI, while the <code>n</code> parameter is taken from the config file.</p> <p>Because the CLI utilized a custom TOML parser to parse the config file and pass it to the CLI tool cyclopts, only parameters under the <code>darts</code> key are considered. Subheading keys are not considered, but can be used to structure the config file:</p> <pre><code>[darts]\nname = \"Not Alice\"\n\n[darts.numbers]\nn = 2\n</code></pre> <p>The <code>numbers</code> key is ignored by the CLI, hence <code>n</code> will be add to the command as before.</p> <p>Warning</p> <p>The only parameters not passed from the config file are the <code>--config-file</code> and <code>--log-dir</code> parameters. The <code>--log-dir</code> parameter is evaluated before the config file is parsed, hence it is not possible to specify the logging directory via the config file.</p>"},{"location":"dev/config/#real-world-example-with-sentinel-2-processing","title":"Real world example with Sentinel 2 processing","text":"<p>Sentinel 2 processing via. Area of Interest file:</p> <pre><code>[darts]\nee-project = \"your-ee-project\"\ndask-worker = 4\n\n[darts.paths]\ninput-cache = \"./data/cache/s2gee\"\noutput-data-dir = \"./data/out\"\narcticdem-dir = \"./data/datacubes/arcticdem\"\ntcvis-dir = \"./data/datacubes/tcvis\"\nmodel-file = \"./models/s2-tcvis-final-large_2025-02-12.ckpt\"\n</code></pre> <p>Running the command:</p> <pre><code>uv run darts run-native-sentinel2-pipeline-from-aoi --aoi-shapefile path/to/your/aoi.geojson --start-date 2024-07 --end-date 2024-09\n</code></pre>"},{"location":"dev/logging/","title":"Logging","text":"<p>We want to use the python logging module as much as possible to traceback errors and document the pipeline processes. Furthermore, we want to configure each logger with the <code>RichHandler</code>, which prettyfies the output with rich.</p>"},{"location":"dev/logging/#setup-guide","title":"Setup Guide","text":"<p>Currently, all setup related to logging is found in the <code>darts.utils.logging.py</code> file. It contains two functions:</p> <ol> <li>A setup function which sets the log-level for all <code>darts.*</code> logger and add default options to xarray and pytorch to supress arrays. See how to supress arrays.</li> <li>A function which adds a file and a rich log handler.</li> </ol> <p>Both functions are used in the CLI setup but can also be called from e.g. a notebook. The recommended approach for handling logging within a notebook is the following:</p> <pre><code>import logging\nfrom rich.logging import RichHandler\nfrom darts.utils.logging import LoggingManager\n\nLoggingManager.setup_logging()\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\n</code></pre> <p>This way the notebook won't spam logfiles everywhere and we still have control over our rich handler.</p>"},{"location":"dev/logging/#usage-guide","title":"Usage Guide","text":"<p>For logging inside a darts-package should be done without any further configuration:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__.replace(\"darts_\", \"darts.\")) # don't replace __name__\n</code></pre> <p>Logging at the top-level <code>darts</code> package can just use a <code>__name__</code> logger:</p> <pre><code>import loggin\n\nlogger = logging.getLogger(__name__) # don't replace __name__\n</code></pre>"},{"location":"dev/logging/#supressing-arrays","title":"Supressing Arrays","text":"<p>When printing or logging large numpy arrays a lot of numbers get truncated, however the array still takes a lot of space. Using <code>lovely_numpy</code> and <code>lovely_tensor</code> can help here:</p> <pre><code>import numyp as np\nimport torch\nimport xarray as xr\nfrom lovely_numpy import lo\nfrom lovely_tensors import monkey_patch\n\nmonkey_patch()\nxr.set_options(display_expand_data=False)\n\na = np.zeros((8, 1024, 1024))\nla = lo(a)\nda = xr.DataArray(a)\nt = torch.tensor(a)\n\nlogger.warning(la)\nlogger.warning(da)\nlogger.warning(t)\n</code></pre>"},{"location":"dev/logging/#when-to-use-which-level","title":"When to use which level","text":"<p>The following is only a recommendation and should help writing helpful and not cluttered logs.</p> <ol> <li> <p><code>Debug</code> should be used ot tell what will happen next, <code>Info</code> for conclusive statements.</p> <p>Example:</p> <pre><code>import logging\nimport time\n\nlogger = logging.getLogger(__name__.replace(\"darts_\", \"darts.\")) # don't replace __name__\n\ndef my_func(param):\n    tick_fstart = time.perf_counter()  # pattern used a lot in the code is: fstart = function_start\n    logger.debug(f\"Doing x with {param=}\")\n    ...  # Doing x\n    logger.info(f\"Done x in {time.perf_counter() - tick_fstart:.2f}s\")\n</code></pre> </li> <li> <p>Unimportant or very often called functions should only log on <code>debug</code> level, independent of the above statement types.</p> </li> </ol>"},{"location":"dev/training/","title":"Training","text":"<p>Old documentation</p> <p>This page is still under construction.</p>"},{"location":"dev/training/#simple-smp-train-and-test","title":"Simple SMP train and test","text":"<p>To train a simple SMP (Segmentation Model Pytorch) model, you first need to preprocess your S2 data:</p> <pre><code>[rye run] darts preprocess-s2-train-data --your-args-here ... \n</code></pre> <p>This will create three data splits:</p> <ul> <li><code>cross-val</code>, used for train and validation</li> <li><code>val-test</code> 5% random leave-out for testing the randomness distribution shift of the data</li> <li><code>test</code> leave-out region for testing the spatial distribution shift of the data</li> </ul> <p>Now you can train a model:</p> <pre><code>[rye run] darts train-smp --your-args-here ...\n</code></pre> <p>Change defaults</p> <p>Even though the defaults from the CLI are somewhat useful, it is recommended to create a config file and change the behavior of the training there.</p> <p>This will train a model with the <code>cross-val</code> data and save the model to disk. The training relies on PyTorch Lightning, which is a high-level interface for PyTorch. You can now test the model on the other two splits:</p> <pre><code>[rye run] darts test-smp --your-args-here ...\n</code></pre> <p>The checkpoint stored is not usable for the pipeline yet, since it is stored in a different format. To convert the model to a format, you need to convert is first:</p> <pre><code>[rye run] darts convert-lightning-checkpoint --your-args-here ...\n</code></pre>"},{"location":"guides/components/","title":"Introduction to the darts components","text":"<p>Danger</p> <p>This page is work in progress.</p>"},{"location":"guides/installation/","title":"Advanced Installation","text":"<p>Prereq:</p> <ul> <li>uv: <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code></li> <li>postgresql (optional for training-only)</li> <li>cuda (optional for GPU support)</li> </ul> <p>This project uses <code>uv</code> to manage the python environment. If you are not familiar with <code>uv</code> yet, please read their documentation first. Please don't use <code>pip</code> or <code>conda</code> to install the dependencies, as this often leads to problems. We have spend a lot of time making sure that the install process is easy and quick, but this is only possible with <code>uv</code>. So please use it.</p> <p>In general the environment can be installed with <code>uv sync</code>. However, this project depends on some libraries (torch and torchvision) which don't get installed per default. Therefore you need to specify an extra flag to install the correct dependencies, e.g.</p> <pre><code>uv sync --extra cuda126\n</code></pre> <p>This will install the environment with the correct dependencies for CUDA 12.6. The following sections will explain the different extra flags and groups which can be used to install the environment for different purposes and systems.</p>"},{"location":"guides/installation/#cuda-and-cpu-only-installations","title":"CUDA and CPU-only installations","text":"<p>Several CUDA versions can be used, but it may happen that some problems occur on different systems. Currently CUDA 11.8, 12.1, 12.4, and 12.6 are supported, but sometimes other versions work as well. We use python extra dependencies, so it is possible to specify the CUDA version via an <code>--extra</code> flag in the <code>uv sync</code> command.</p> <p>You can check the currently installed CUDA version via:</p> <pre><code>nvidia-smi\n# Look at the top right corner for the CUDA version\n</code></pre> <p>Warning</p> <p>If the <code>nvidia-smi</code> command is not found, you might need to install the nvidia drivers. Be very cautious with the installation of the driver, rather read the documentation with care.</p> <p>To install the python environment for a specific CUDA version use one of the following commands respectively:</p> <pre><code>uv sync --extra cuda118\nuv sync --extra cuda121\nuv sync --extra cuda124\nuv sync --extra cuda126\n</code></pre> <p>CUDA version missmatch</p> <p>Sometimes it is possible to use a different CUDA version for the python packages than the one installed. E.g. we tested our code on a system with CUDA 12.2 installed, but used the python packages for CUDA 12.1. This is not recommended, but sometimes it works.</p> <p>Install the python environment for CPU-only use:</p> <pre><code>uv sync --extra cpu\n</code></pre>"},{"location":"guides/installation/#training-specific-dependencies","title":"Training specific dependencies","text":"<p>Training specific dependencies are optional and therefore not installed by default. To install them, add <code>--extra training</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> <p>psycopg2</p> <p>The training dependencies depend on psycopg2, which requires postgresql installed on your system.</p>"},{"location":"guides/installation/#packages-for-the-documentation","title":"Packages for the documentation","text":"<p>Packages which are used to create this documentation are not installed by default and are not available via as an extra. Instead they are installed as part of an optional <code>dependency-group</code>, or <code>group</code> for short. To install the documentation dependencies, add <code>--group docs</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training --group docs\n</code></pre>"},{"location":"ref/acquisition/","title":"Acquisition Reference","text":""},{"location":"ref/acquisition/#darts_acquisition","title":"<code>darts_acquisition</code>","text":"<p>Acquisition of data from various sources for the DARTS dataset.</p>"},{"location":"ref/acquisition/#darts_acquisition.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/acquisition/#darts_acquisition.download_admin_files","title":"<code>download_admin_files(admin_dir)</code>","text":"<p>Download the admin files for the regions.</p> <p>Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.</p> <p>Parameters:</p> Name Type Description Default <code>admin_dir</code> <code>Path</code> <p>The path to the admin files.</p> required Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>def download_admin_files(admin_dir: Path):\n    \"\"\"Download the admin files for the regions.\n\n    Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.\n\n    Args:\n        admin_dir (Path): The path to the admin files.\n\n    \"\"\"\n    tick_fstart = time.perf_counter()\n\n    # Download the admin files\n    admin_1_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.zip\"\n    admin_2_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.zip\"\n\n    admin_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.debug(f\"Downloading {admin_1_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_1_url, admin_dir)\n\n    logger.debug(f\"Downloading {admin_2_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_2_url, admin_dir)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Downloaded admin files in {tick_fend - tick_fstart:.2f} seconds\")\n</code></pre>"},{"location":"ref/acquisition/#darts_acquisition.load_arcticdem","title":"<code>load_arcticdem(geobox, data_dir, resolution, buffer=0, persist=True)</code>","text":"<p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>geobox</code> <code>GeoBox</code> <p>The geobox for which the tile should be loaded.</p> required <code>data_dir</code> <code>Path | str</code> <p>The directory where the ArcticDEM data is stored.</p> required <code>resolution</code> <code>Literal[2, 10, 32]</code> <p>The resolution of the ArcticDEM data in m.</p> required <code>buffer</code> <code>int</code> <p>The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.</p> <code>0</code> <code>persist</code> <code>bool</code> <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The ArcticDEM tile, with a buffer applied. Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different. Final dataset is NOT matched to the reference CRS and resolution.</p> Warning <p>Geobox must be in a meter based CRS.</p> Usage <p>Since the API of the <code>load_arcticdem</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_arcticdem\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\narcticdem = load_arcticdem(\n    optical.odc.geobox,\n    \"/path/to/arcticdem-parent-directory\",\n    resolution=2,\n    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n)\n\n# Now we can for example match the resolution and extent of the optical data:\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> <p>The <code>buffer</code> parameter is used to extend the region of interest by a certain amount of pixels. This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.</p> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem/datacube.py</code> <pre><code>def load_arcticdem(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    resolution: RESOLUTIONS,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded.\n        data_dir (Path | str): The directory where the ArcticDEM data is stored.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in m.\n        buffer (int, optional): The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The ArcticDEM tile, with a buffer applied.\n            Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different.\n            Final dataset is NOT matched to the reference CRS and resolution.\n\n    Warning:\n        Geobox must be in a meter based CRS.\n\n    Usage:\n        Since the API of the `load_arcticdem` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_arcticdem\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        arcticdem = load_arcticdem(\n            optical.odc.geobox,\n            \"/path/to/arcticdem-parent-directory\",\n            resolution=2,\n            buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n        The `buffer` parameter is used to extend the region of interest by a certain amount of pixels.\n        This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.\n\n    \"\"\"  # noqa: E501\n    tick_fstart = time.perf_counter()\n\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    datacube_fpath = data_dir / f\"datacube_{resolution}m_v4.1.zarr\"\n    storage = zarr.storage.FSStore(datacube_fpath)\n    logger.debug(f\"Getting ArcticDEM tile from {datacube_fpath.resolve()}\")\n\n    # ! The geobox must be in a meter based CRS\n    logger.debug(f\"Found a reference resolution of {geobox.resolution.x}m\")\n\n    # Check if the zarr data already exists\n    if not datacube_fpath.exists():\n        logger.debug(f\"Creating a new zarr datacube at {datacube_fpath.resolve()} with {storage=}\")\n        create_empty_datacube(\n            \"ArcticDEM Data Cube\",\n            storage,\n            DATA_EXTENT[resolution],\n            CHUNK_SIZE,\n            DATA_VARS,\n            DATA_VARS_META,\n            DATA_VARS_ENCODING,\n        )\n\n    # Get the adjacent arcticdem tiles\n    # Note: We could also use pystac here, but this would result in a slight performance decrease\n    # because of the network overhead, hence we use the extent file\n    # Download the extent, download if the file does not exist\n    extent_fpath = data_dir / f\"ArcticDEM_Mosaic_Index_v4_1_{resolution}m.parquet\"\n    with download_lock:\n        if not extent_fpath.exists():\n            download_arcticdem_extent(data_dir)\n    extent = gpd.read_parquet(extent_fpath)\n\n    # Add a buffer around the geobox to get the adjacent tiles\n    reference_geobox = geobox.to_crs(\"epsg:3413\", resolution=resolution).pad(buffer)\n    adjacent_tiles = extent[extent.intersects(reference_geobox.extent.geom)]\n\n    # Download the adjacent tiles (if necessary)\n    with download_lock:\n        procedural_download_datacube(storage, adjacent_tiles)\n\n    # Load the datacube and set the spatial_ref since it is set as a coordinate within the zarr format\n    chunks = None if persist else \"auto\"\n    arcticdem_datacube = xr.open_zarr(storage, mask_and_scale=False, chunks=chunks).set_coords(\"spatial_ref\")\n\n    # Get an AOI slice of the datacube\n    arcticdem_aoi = arcticdem_datacube.odc.crop(reference_geobox.extent, apply_mask=False)\n\n    # The following code would load the lazy zarr data from disk into memory\n    if persist:\n        tick_sload = time.perf_counter()\n        arcticdem_aoi = arcticdem_aoi.load()\n        tick_eload = time.perf_counter()\n        logger.debug(f\"ArcticDEM AOI loaded from disk in {tick_eload - tick_sload:.2f} seconds\")\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem_aoi[\"datamask\"] = arcticdem_aoi.datamask.astype(\"uint8\")\n\n    logger.info(\n        f\"ArcticDEM tile {'loaded' if persist else 'lazy-opened'} in {time.perf_counter() - tick_fstart:.2f} seconds\"\n    )\n    return arcticdem_aoi\n</code></pre>"},{"location":"ref/acquisition/#darts_acquisition.load_arcticdem_from_vrt","title":"<code>load_arcticdem_from_vrt(slope_vrt, elevation_vrt, reference_dataset)</code>","text":"<p>Load ArcticDEM data and reproject it to match the reference dataset.</p> <p>Parameters:</p> Name Type Description Default <code>slope_vrt</code> <code>Path</code> <p>Path to the ArcticDEM slope VRT file.</p> required <code>elevation_vrt</code> <code>Path</code> <p>Path to the ArcticDEM elevation VRT file.</p> required <code>reference_dataset</code> <code>Dataset</code> <p>The reference dataset to reproject, resampled and cropped the ArcticDEM data to.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The ArcticDEM data reprojected, resampled and cropped to match the reference dataset.</p> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem/vrt.py</code> <pre><code>def load_arcticdem_from_vrt(slope_vrt: Path, elevation_vrt: Path, reference_dataset: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Load ArcticDEM data and reproject it to match the reference dataset.\n\n    Args:\n        slope_vrt (Path): Path to the ArcticDEM slope VRT file.\n        elevation_vrt (Path): Path to the ArcticDEM elevation VRT file.\n        reference_dataset (xr.Dataset): The reference dataset to reproject, resampled and cropped the ArcticDEM data to.\n\n    Returns:\n        xr.Dataset: The ArcticDEM data reprojected, resampled and cropped to match the reference dataset.\n\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(f\"Loading ArcticDEM slope from {slope_vrt.resolve()} and elevation from {elevation_vrt.resolve()}\")\n\n    slope = load_vrt(slope_vrt, reference_dataset)\n    slope: xr.Dataset = (\n        slope.assign_attrs({\"data_source\": \"arcticdem\", \"long_name\": \"Slope\"})\n        .rio.write_nodata(float(\"nan\"))\n        .astype(\"float32\")\n        .to_dataset(name=\"slope\")\n    )\n\n    relative_elevation = load_vrt(elevation_vrt, reference_dataset)\n    relative_elevation: xr.Dataset = (\n        relative_elevation.assign_attrs({\"data_source\": \"arcticdem\", \"long_name\": \"Relative Elevation\", \"units\": \"m\"})\n        .fillna(0)\n        .rio.write_nodata(0)\n        .astype(\"int16\")\n        .to_dataset(name=\"relative_elevation\")\n    )\n\n    articdem_ds = xr.merge([relative_elevation, slope])\n    logger.debug(f\"Loaded ArcticDEM data in {time.time() - start_time} seconds.\")\n    return articdem_ds\n</code></pre>"},{"location":"ref/acquisition/#darts_acquisition.load_planet_masks","title":"<code>load_planet_masks(fpath)</code>","text":"<p>Load the valid and quality data masks from a Planet scene.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str | Path</code> <p>The file path to the Planet scene from which to derive the masks.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no matching UDM-2 TIFF file is found in the specified path.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def load_planet_masks(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Planet scene.\n\n    Args:\n        fpath (str | Path): The file path to the Planet scene from which to derive the masks.\n\n    Raises:\n        FileNotFoundError: If no matching UDM-2 TIFF file is found in the specified path.\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    start_time = time.time()\n\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # Get imagepath\n    udm_path = next(fpath.glob(\"*_udm2.tif\"), None)\n    if not udm_path:\n        udm_path = next(fpath.glob(\"*_udm2_clip.tif\"), None)\n    if not udm_path:\n        raise FileNotFoundError(f\"No matching UDM-2 TIFF files found in {fpath.resolve()} (.glob('*_udm2.tif'))\")\n\n    # See udm classes here: https://developers.planet.com/docs/data/udm-2/\n    da_udm = xr.open_dataarray(udm_path)\n\n    invalids = da_udm.sel(band=8).fillna(0) != 0\n    low_quality = da_udm.sel(band=[2, 3, 4, 5, 6]).max(axis=0) == 1\n    high_quality = ~low_quality &amp; ~invalids\n    qa_ds = xr.Dataset(coords={c: da_udm.coords[c] for c in da_udm.coords})\n    qa_ds[\"quality_data_mask\"] = (\n        xr.zeros_like(da_udm.sel(band=8)).where(invalids, 0).where(low_quality, 1).where(high_quality, 2)\n    )\n    qa_ds[\"quality_data_mask\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Quality data mask\",\n        \"description\": \"0 = Invalid, 1 = Low Quality, 2 = High Quality\",\n    }\n    logger.debug(f\"Loaded data masks in {time.time() - start_time} seconds.\")\n    return qa_ds\n</code></pre>"},{"location":"ref/acquisition/#darts_acquisition.load_planet_scene","title":"<code>load_planet_scene(fpath)</code>","text":"<p>Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str | Path</code> <p>The path to the directory containing the TIFF files or a specific path to the TIFF file.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The loaded dataset</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no matching TIFF file is found in the specified path.</p> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def load_planet_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files or a specific path to the TIFF file.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    start_time = time.time()\n\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    # Check if the directory contains a PSOrthoTile or PSScene\n    planet_type = parse_planet_type(fpath)\n    logger.debug(f\"Loading Planet PS {planet_type.capitalize()} from {fpath.resolve()}\")\n\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    # Define band names and corresponding indices\n    planet_da = xr.open_dataarray(ps_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_planet = (\n        planet_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n    )\n    for var in ds_planet.variables:\n        ds_planet[var].assign_attrs(\n            {\n                \"long_name\": f\"PLANET {var.capitalize()}\",\n                \"data_source\": \"planet\",\n                \"planet_type\": planet_type,\n                \"units\": \"Reflectance\",\n            }\n        )\n    ds_planet.attrs = {\"tile_id\": fpath.parent.stem if planet_type == \"orthotile\" else fpath.stem}\n    logger.debug(f\"Loaded Planet scene in {time.time() - start_time} seconds.\")\n    return ds_planet\n</code></pre>"},{"location":"ref/acquisition/#darts_acquisition.load_s2_masks","title":"<code>load_s2_masks(fpath, reference_geobox)</code>","text":"<p>Load the valid and quality data masks from a Sentinel 2 scene.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str | Path</code> <p>The path to the directory containing the TIFF files.</p> required <code>reference_geobox</code> <code>GeoBox</code> <p>The reference geobox to reproject, resample and crop the masks data to.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>def load_s2_masks(fpath: str | Path, reference_geobox: GeoBox) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Sentinel 2 scene.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n        reference_geobox (GeoBox): The reference geobox to reproject, resample and crop the masks data to.\n\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    start_time = time.time()\n\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # TODO: SCL band in SR file\n    try:\n        scl_path = next(fpath.glob(\"*_SCL*.tif\"))\n    except StopIteration:\n        logger.warning(\"Found no data quality mask (SCL). No masking will occur.\")\n        valid_data_mask = (odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\") + 1).to_dataset(name=\"valid_data_mask\")\n        valid_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Valid Data Mask\"}\n        quality_data_mask = odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\").to_dataset(name=\"quality_data_mask\")\n        quality_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Quality Data Mask\"}\n        qa_ds = xr.merge([valid_data_mask, quality_data_mask])\n        return qa_ds\n\n    # See scene classes here: https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/scene-classification/\n    da_scl = xr.open_dataarray(scl_path)\n\n    da_scl = da_scl.odc.reproject(reference_geobox, sampling=\"nearest\")\n\n    # Match crs\n    da_scl = da_scl.rio.write_crs(reference_geobox.crs)\n\n    da_scl = xr.Dataset({\"scl\": da_scl.sel(band=1).fillna(0).drop_vars(\"band\").astype(\"uint8\")})\n    da_scl = convert_masks(da_scl)\n\n    logger.debug(f\"Loaded data masks in {time.time() - start_time} seconds.\")\n    return da_scl\n</code></pre>"},{"location":"ref/acquisition/#darts_acquisition.load_s2_scene","title":"<code>load_s2_scene(fpath)</code>","text":"<p>Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str | Path</code> <p>The path to the directory containing the TIFF files.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The loaded dataset</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If no matching TIFF file is found in the specified path.</p> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>def load_s2_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    start_time = time.time()\n\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading Sentinel 2 scene from {fpath.resolve()}\")\n\n    # Get imagepath\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n\n    # Define band names and corresponding indices\n    s2_da = xr.open_dataarray(s2_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_s2 = s2_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].assign_attrs(\n            {\"data_source\": \"s2\", \"long_name\": f\"Sentinel 2 {var.capitalize()}\", \"units\": \"Reflectance\"}\n        )\n\n    planet_crop_id, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n    ds_s2.attrs[\"planet_crop_id\"] = planet_crop_id\n    ds_s2.attrs[\"s2_tile_id\"] = s2_tile_id\n    ds_s2.attrs[\"tile_id\"] = tile_id\n    logger.debug(f\"Loaded Sentinel 2 scene in {time.time() - start_time} seconds.\")\n    return ds_s2\n</code></pre>"},{"location":"ref/acquisition/#darts_acquisition.load_tcvis","title":"<code>load_tcvis(geobox, data_dir, buffer=0, persist=True)</code>","text":"<p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>geobox</code> <code>GeoBox</code> <p>The geobox to load the data for.</p> required <code>data_dir</code> <code>Path | str</code> <p>The directory to store the downloaded data for faster access for consecutive calls.</p> required <code>buffer</code> <code>int</code> <p>The buffer around the geobox in pixels. Defaults to 0.</p> <code>0</code> <code>persist</code> <code>bool</code> <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The TCVIS dataset.</p> Usage <p>Since the API of the <code>load_tcvis</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_tcvis\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\ntcvis = load_tcvis(\n    optical.odc.geobox,\n    \"/path/to/tcvis-parent-directory\",\n)\n\n# Now we can for example match the resolution and extent of the optical data:\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>def load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the TCVIS for the given geobox, fetch new data from GEE if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox to load the data for.\n        data_dir (Path | str): The directory to store the downloaded data for faster access for consecutive calls.\n        buffer (int, optional): The buffer around the geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset.\n\n    Usage:\n        Since the API of the `load_tcvis` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_tcvis\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        tcvis = load_tcvis(\n            optical.odc.geobox,\n            \"/path/to/tcvis-parent-directory\",\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"  # noqa: E501\n    tick_fstart = time.perf_counter()\n\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    datacube_fpath = data_dir / \"tcvis_2000-2019.zarr\"\n    storage = zarr.storage.FSStore(datacube_fpath)\n    logger.debug(f\"Loading TCVis from {datacube_fpath.resolve()}\")\n\n    if not datacube_fpath.exists():\n        logger.debug(f\"Creating a new zarr datacube at {datacube_fpath.resolve()} with {storage=}\")\n        create_empty_datacube(\n            title=\"Landsat Trends TCVIS 2000-2019\",\n            storage=storage,\n            geobox=DATA_EXTENT,\n            chunk_size=CHUNK_SIZE,\n            data_vars=DATA_VARS,\n            meta=DATA_VARS_META,\n            var_encoding=DATA_VARS_ENCODING,\n        )\n\n    # Download the adjacent tiles (if necessary)\n    reference_geobox = geobox.to_crs(\"epsg:4326\", resolution=DATA_EXTENT.resolution.x).pad(buffer)\n    with download_lock:\n        procedural_download_datacube(storage, reference_geobox)\n\n    # Load the datacube and set the spatial_ref since it is set as a coordinate within the zarr format\n    chunks = None if persist else \"auto\"\n    tcvis_datacube = xr.open_zarr(storage, mask_and_scale=False, chunks=chunks).set_coords(\"spatial_ref\")\n\n    # Get an AOI slice of the datacube\n    tcvis_aoi = tcvis_datacube.odc.crop(reference_geobox.extent, apply_mask=False)\n\n    # The following code would load the lazy zarr data from disk into memory\n    if persist:\n        tick_sload = time.perf_counter()\n        tcvis_aoi = tcvis_aoi.load()\n        tick_eload = time.perf_counter()\n        logger.debug(f\"TCVIS AOI loaded from disk in {tick_eload - tick_sload:.2f} seconds\")\n\n    logger.info(\n        f\"TCVIS tile {'loaded' if persist else 'lazy-opened'} in {time.perf_counter() - tick_fstart:.2f} seconds\"\n    )\n    return tcvis_aoi\n</code></pre>"},{"location":"ref/darts/","title":"DARTS Reference","text":""},{"location":"ref/darts/#darts","title":"<code>darts</code>","text":"<p>DARTS processing pipeline.</p>"},{"location":"ref/darts/#darts.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/ensemble/","title":"Ensemble Reference","text":""},{"location":"ref/ensemble/#darts_ensemble","title":"<code>darts_ensemble</code>","text":"<p>Inference and model ensembling for the DARTS dataset.</p>"},{"location":"ref/ensemble/#darts_ensemble.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/export/","title":"Export Reference","text":""},{"location":"ref/export/#darts_export","title":"<code>darts_export</code>","text":"<p>Dataset export for the DARTS dataset.</p>"},{"location":"ref/export/#darts_export.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/export/#darts_export.export_arcticdem_datamask","title":"<code>export_arcticdem_datamask(tile, out_dir)</code>","text":"<p>Export the arcticdem data mask as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_arcticdem_datamask(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the arcticdem data mask as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"arcticdem_data_mask.tif\"\n    with stopuhr(f\"Exporting arcticdem data mask to {fpath}\", logger.debug):\n        tile[\"arcticdem_data_mask\"].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/export/#darts_export.export_binarized","title":"<code>export_binarized(tile, out_dir, export_ensemble_inputs=False, tags={})</code>","text":"<p>Export the binarized segmentation layer to a file.</p> <p>If <code>export_ensemble_inputs</code> is set to True and the ensemble used at least two models for inference, the binarized segmentation of the models will be written as individual files as well.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required <code>export_ensemble_inputs</code> <code>bool</code> <p>Also save the model outputs, not only the ensemble result. Only applies if the inference result is an ensemble result and has at least two inputs. Defaults to False.</p> <code>False</code> <code>tags</code> <code>dict</code> <p>optional GeoTIFF metadata to be written. Defaults to no additional metadata.</p> <code>{}</code> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_binarized(tile: xr.Dataset, out_dir: Path, export_ensemble_inputs: bool = False, tags: dict = {}):\n    \"\"\"Export the binarized segmentation layer to a file.\n\n    If `export_ensemble_inputs` is set to True and the ensemble used at least two models for inference,\n    the binarized segmentation of the models will be written as individual files as well.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n        export_ensemble_inputs (bool, optional): Also save the model outputs, not only the ensemble result.\n            Only applies if the inference result is an ensemble result and has at least two inputs.\n            Defaults to False.\n        tags (dict, optional): optional GeoTIFF metadata to be written. Defaults to no additional metadata.\n\n    \"\"\"\n    subset_names = _get_subset_names(tile)\n    if export_ensemble_inputs and len(subset_names) &gt; 1:\n        for subset in _get_subset_names(tile):\n            tick_estart = time.perf_counter()\n            layer_name = f\"binarized_segmentation-{subset}\"\n            fpath = out_dir / f\"{layer_name}.tif\"\n            tile[layer_name].rio.to_raster(fpath, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n            tick_eend = time.perf_counter()\n            logger.debug(f\"Exported binarized segmentation for {subset} to {fpath} in {tick_eend - tick_estart:.2f}s\")\n\n    fpath = out_dir / \"binarized.tif\"\n    with stopuhr(f\"Exporting binarized segmentation to {fpath}\", logger.debug):\n        tile[\"binarized_segmentation\"].rio.to_raster(fpath, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n</code></pre>"},{"location":"ref/export/#darts_export.export_datamask","title":"<code>export_datamask(tile, out_dir)</code>","text":"<p>Export the data mask as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_datamask(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the data mask as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"data_mask.tif\"\n    with stopuhr(f\"Exporting data mask to {fpath}\", logger.debug):\n        tile[\"quality_data_mask\"].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/export/#darts_export.export_dem","title":"<code>export_dem(tile, out_dir)</code>","text":"<p>Export the DEM data as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_dem(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the DEM data as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"dem.tif\"\n    with stopuhr(f\"Exporting DEM data to {fpath}\", logger.debug):\n        tile[[\"slope\", \"relative_elevation\"]].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/export/#darts_export.export_extent","title":"<code>export_extent(tile, out_dir)</code>","text":"<p>Export the extent of the prediction as a vector dataset in GeoPackage and GeoParquet format.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_extent(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the extent of the prediction as a vector dataset in GeoPackage and GeoParquet format.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath_gpkg = out_dir / \"prediction_extent.gpkg\"\n    fpath_parquet = out_dir / \"prediction_extent.parquet\"\n    with stopuhr(f\"Exporting extent to {fpath_gpkg} and {fpath_parquet}\", logger.debug):\n        polygon_gdf = vectorization.vectorize(tile, \"quality_data_mask\", minimum_mapping_unit=0)\n        polygon_gdf.to_file(fpath_gpkg, layer=\"prediction_extent\")\n        polygon_gdf.to_parquet(fpath_parquet)\n</code></pre>"},{"location":"ref/export/#darts_export.export_optical","title":"<code>export_optical(tile, out_dir)</code>","text":"<p>Export the optical data as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_optical(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the optical data as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"optical.tif\"\n    with stopuhr(f\"Exporting optical data to {fpath}\", logger.debug):\n        tile[[\"red\", \"green\", \"blue\", \"nir\"]].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/export/#darts_export.export_polygonized","title":"<code>export_polygonized(tile, out_dir, export_ensemble_inputs=False, minimum_mapping_unit=32)</code>","text":"<p>Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.</p> <p>If <code>export_ensemble_inputs</code> is set to True and the ensemble used at least two models for inference, the vectorized binarized segmentation of the models will be written as individual files as well.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required <code>export_ensemble_inputs</code> <code>bool</code> <p>Also save the model outputs, not only the ensemble result. Only applies if the inference result is an ensemble result and has at least two inputs. Defaults to False.</p> <code>False</code> <code>minimum_mapping_unit</code> <code>int</code> <p>segments covering less pixel are removed. Defaults to 32.</p> <code>32</code> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_polygonized(\n    tile: xr.Dataset, out_dir: Path, export_ensemble_inputs: bool = False, minimum_mapping_unit: int = 32\n):\n    \"\"\"Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.\n\n    If `export_ensemble_inputs` is set to True and the ensemble used at least two models for inference,\n    the vectorized binarized segmentation of the models will be written as individual files as well.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n        export_ensemble_inputs (bool, optional): Also save the model outputs, not only the ensemble result.\n            Only applies if the inference result is an ensemble result and has at least two inputs.\n            Defaults to False.\n        minimum_mapping_unit (int, optional): segments covering less pixel are removed. Defaults to 32.\n\n    \"\"\"\n    subset_names = _get_subset_names(tile)\n    if export_ensemble_inputs and len(subset_names) &gt; 1:\n        for subset in _get_subset_names(tile):\n            tick_estart = time.perf_counter()\n            layer_name = f\"binarized_segmentation-{subset}\"\n            fpath_gpkg = out_dir / f\"prediction_segments-{subset}.gpkg\"\n            fpath_parquet = out_dir / f\"prediction_segments-{subset}.parquet\"\n            polygon_gdf = vectorization.vectorize(tile, layer_name, minimum_mapping_unit=minimum_mapping_unit)\n            polygon_gdf.to_file(fpath_gpkg, layer=f\"prediction_segments-{subset}\")\n            polygon_gdf.to_parquet(fpath_parquet)\n            tick_eend = time.perf_counter()\n            logger.debug(\n                f\"Exported binarized segmentation for {subset} to {fpath_gpkg} and {fpath_parquet}\"\n                f\" in {tick_eend - tick_estart:.2f}s\"\n            )\n\n    fpath_gpkg = out_dir / \"prediction_segments.gpkg\"\n    fpath_parquet = out_dir / \"prediction_segments.parquet\"\n    with stopuhr(f\"Exporting binarized segmentation to {fpath_gpkg} and {fpath_parquet}\", logger.debug):\n        polygon_gdf = vectorization.vectorize(tile, \"binarized_segmentation\", minimum_mapping_unit=minimum_mapping_unit)\n        polygon_gdf.to_file(fpath_gpkg, layer=\"prediction_segments\")\n        polygon_gdf.to_parquet(fpath_parquet)\n</code></pre>"},{"location":"ref/export/#darts_export.export_probabilities","title":"<code>export_probabilities(tile, out_dir, export_ensemble_inputs=False, tags={})</code>","text":"<p>Export the probabilities layer to a file.</p> <p>If <code>export_ensemble_inputs</code> is set to True and the ensemble used at least two models for inference, the probabilities of the models will be written as individual files as well.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required <code>export_ensemble_inputs</code> <code>bool</code> <p>Also save the model outputs, not only the ensemble result. Only applies if the inference result is an ensemble result and has at least two inputs. Defaults to False.</p> <code>False</code> <code>tags</code> <code>dict</code> <p>optional GeoTIFF metadata to be written. Defaults to no additional metadata.</p> <code>{}</code> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_probabilities(tile: xr.Dataset, out_dir: Path, export_ensemble_inputs: bool = False, tags: dict = {}):\n    \"\"\"Export the probabilities layer to a file.\n\n    If `export_ensemble_inputs` is set to True and the ensemble used at least two models for inference,\n    the probabilities of the models will be written as individual files as well.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n        export_ensemble_inputs (bool, optional): Also save the model outputs, not only the ensemble result.\n            Only applies if the inference result is an ensemble result and has at least two inputs.\n            Defaults to False.\n        tags (dict, optional): optional GeoTIFF metadata to be written. Defaults to no additional metadata.\n\n    \"\"\"\n    subset_names = _get_subset_names(tile)\n    if export_ensemble_inputs and len(subset_names) &gt; 1:\n        for subset in _get_subset_names(tile):\n            tick_estart = time.perf_counter()\n            layer_name = f\"probabilities-{subset}\"\n            fpath = out_dir / f\"{layer_name}.tif\"\n            tile[layer_name].rio.to_raster(fpath, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n            tick_eend = time.perf_counter()\n            logger.debug(f\"Exported probabilities for {subset} to {fpath} in {tick_eend - tick_estart:.2f}s\")\n\n    fpath = out_dir / \"probabilities.tif\"\n    with stopuhr(f\"Exporting probabilities to {fpath}\", logger.debug):\n        tile[\"probabilities\"].rio.to_raster(fpath, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n</code></pre>"},{"location":"ref/export/#darts_export.export_tcvis","title":"<code>export_tcvis(tile, out_dir)</code>","text":"<p>Export the TCVIS data as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_tcvis(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export the TCVIS data as a GeoTIFF file.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"tcvis.tif\"\n    with stopuhr(f\"Exporting TCVIS data to {fpath}\", logger.debug):\n        tile[[\"tc_brightness\", \"tc_greenness\", \"tc_wetness\"]].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"ref/export/#darts_export.export_thumbnail","title":"<code>export_thumbnail(tile, out_dir)</code>","text":"<p>Export a thumbnail of the optical data.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The inference result.</p> required <code>out_dir</code> <code>Path</code> <p>The path where to export to.</p> required Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_thumbnail(tile: xr.Dataset, out_dir: Path):\n    \"\"\"Export a thumbnail of the optical data.\n\n    Args:\n        tile (xr.Dataset): The inference result.\n        out_dir (Path): The path where to export to.\n\n    \"\"\"\n    fpath = out_dir / \"thumbnail.jpg\"\n    with stopuhr(f\"Exporting thumbnail to {fpath}\", logger.debug):\n        fig = thumbnail(tile)\n        fig.savefig(fpath)\n        fig.clear()\n</code></pre>"},{"location":"ref/preprocessing/","title":"Preprocessing Reference","text":""},{"location":"ref/preprocessing/#darts_preprocessing","title":"<code>darts_preprocessing</code>","text":"<p>Data preprocessing and feature engineering for the DARTS dataset.</p>"},{"location":"ref/preprocessing/#darts_preprocessing.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/preprocessing/#darts_preprocessing.preprocess_legacy","title":"<code>preprocess_legacy(ds_optical, ds_arcticdem, ds_tcvis)</code>","text":"<p>Preprocess optical data with legacy (DARTS v1) preprocessing steps.</p> <p>The processing steps are: - Calculate NDVI - Merge everything into a single ds.</p> <p>Parameters:</p> Name Type Description Default <code>ds_optical</code> <code>Dataset</code> <p>The Planet scene optical data or Sentinel 2 scene optical data.</p> required <code>ds_arcticdem</code> <code>Dataset</code> <p>The ArcticDEM data.</p> required <code>ds_tcvis</code> <code>Dataset</code> <p>The TCVIS data.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>def preprocess_legacy(\n    ds_optical: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Merge everything into a single ds.\n\n    Args:\n        ds_optical (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical data.\n        ds_arcticdem (xr.Dataset): The ArcticDEM data.\n        ds_tcvis (xr.Dataset): The TCVIS data.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Calculate NDVI\n    ds_ndvi = calculate_ndvi(ds_optical)\n\n    # Reproject TCVIS to optical data\n    ds_tcvis = ds_tcvis.odc.reproject(ds_optical.odc.geobox, resampling=\"cubic\")\n\n    # Since this function expects the arcticdem to be loaded from a VRT, which already contains slope and tpi,\n    # we dont need to calculate them here\n\n    # merge to final dataset\n    ds_merged = xr.merge([ds_optical, ds_ndvi, ds_arcticdem, ds_tcvis])\n\n    return ds_merged\n</code></pre>"},{"location":"ref/preprocessing/#darts_preprocessing.preprocess_legacy_fast","title":"<code>preprocess_legacy_fast(ds_merged, ds_arcticdem, ds_tcvis, tpi_outer_radius=100, tpi_inner_radius=0, device=DEFAULT_DEVICE)</code>","text":"<p>Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>The main difference to preprocess_legacy is the new data concept of the arcticdem. Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw arcticdem data and calculates slope and relative elevation on the fly.</p> <p>Parameters:</p> Name Type Description Default <code>ds_merged</code> <code>Dataset</code> <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> required <code>ds_arcticdem</code> <code>Dataset</code> <p>The ArcticDEM dataset.</p> required <code>ds_tcvis</code> <code>Dataset</code> <p>The TCVIS dataset.</p> required <code>tpi_outer_radius</code> <code>int</code> <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> <code>100</code> <code>tpi_inner_radius</code> <code>int</code> <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> <code>0</code> <code>device</code> <code>Literal['cuda', 'cpu'] | int</code> <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> <code>DEFAULT_DEVICE</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>def preprocess_legacy_fast(\n    ds_merged: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    The main difference to preprocess_legacy is the new data concept of the arcticdem.\n    Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw\n    arcticdem data and calculates slope and relative elevation on the fly.\n\n    Args:\n        ds_merged (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    tick_fstart = time.perf_counter()\n    logger.debug(\"Starting fast v1 preprocessing.\")\n\n    # Calculate NDVI\n    ds_merged[\"ndvi\"] = calculate_ndvi(ds_merged).ndvi\n\n    # Reproject TCVIS to optical data\n    tick_sproj = time.perf_counter()\n    ds_tcvis = ds_tcvis.odc.reproject(ds_merged.odc.geobox, resampling=\"cubic\")\n    tick_eproj = time.perf_counter()\n    logger.debug(f\"Reprojection of TCVIS done in {tick_eproj - tick_sproj:.2f} seconds.\")\n\n    ds_merged[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_merged[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_merged[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    tick_sproj = time.perf_counter()\n    ds_arcticdem = ds_arcticdem.odc.reproject(ds_merged.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n    tick_eproj = time.perf_counter()\n    logger.debug(f\"Reprojection of ArcticDEM done in {tick_eproj - tick_sproj:.2f} seconds.\")\n\n    ds_arcticdem = preprocess_legacy_arcticdem_fast(ds_arcticdem, tpi_outer_radius, tpi_inner_radius, device)\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_merged.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_merged)\n\n    ds_merged[\"dem\"] = ds_arcticdem.dem\n    ds_merged[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_merged[\"slope\"] = ds_arcticdem.slope\n    ds_merged[\"arcticdem_data_mask\"] = ds_arcticdem.datamask\n\n    # Update datamask with arcticdem mask\n    # with xr.set_options(keep_attrs=True):\n    #     ds_merged[\"quality_data_mask\"] = ds_merged.quality_data_mask * ds_arcticdem.datamask\n    # ds_merged.quality_data_mask.attrs[\"data_source\"] += \" + ArcticDEM\"\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Preprocessing done in {tick_fend - tick_fstart:.2f} seconds.\")\n    return ds_merged\n</code></pre>"},{"location":"ref/segmentation/","title":"Export Reference","text":""},{"location":"ref/segmentation/#darts_segmentation","title":"<code>darts_segmentation</code>","text":"<p>Image segmentation of thaw-slumps for the DARTS dataset.</p>"},{"location":"ref/segmentation/#darts_segmentation.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter","title":"<code>SMPSegmenter</code>","text":"<p>An actor that keeps a model as its state and segments tiles.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>class SMPSegmenter:\n    \"\"\"An actor that keeps a model as its state and segments tiles.\"\"\"\n\n    config: SMPSegmenterConfig\n    model: nn.Module\n    device: torch.device\n\n    def __init__(self, model_checkpoint: Path | str, device: torch.device = DEFAULT_DEVICE):\n        \"\"\"Initialize the segmenter.\n\n        Args:\n            model_checkpoint (Path): The path to the model checkpoint.\n            device (torch.device): The device to run the model on.\n                Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n        \"\"\"\n        model_checkpoint = model_checkpoint if isinstance(model_checkpoint, Path) else Path(model_checkpoint)\n        self.device = device\n        ckpt = torch.load(model_checkpoint, map_location=self.device)\n        self.config = validate_config(ckpt[\"config\"])\n        # Overwrite the encoder weights with None, because we load our own\n        self.config[\"model\"] |= {\"encoder_weights\": None}\n        self.model = smp.create_model(**self.config[\"model\"])\n        self.model.to(self.device)\n        self.model.load_state_dict(ckpt[\"statedict\"])\n        self.model.eval()\n\n        logger.debug(\n            f\"Successfully loaded model from {model_checkpoint.resolve()} with inputs: \"\n            f\"{self.config['input_combination']}\"\n        )\n\n    def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n        \"\"\"Take a tile and convert it to a pytorch tensor.\n\n        Respects the input combination from the config.\n\n        Returns:\n            A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n        \"\"\"\n        bands = []\n        # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n        # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n        for feature_name in self.config[\"input_combination\"]:\n            norm = self.config[\"norm_factors\"][feature_name]\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n        return torch.stack(bands, dim=0)\n\n    def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n        \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n        Respects the the input combination from the config.\n\n        Returns:\n            A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n        \"\"\"\n        bands = []\n        for feature_name in self.config[\"input_combination\"]:\n            norm = self.config[\"norm_factors\"][feature_name]\n            for tile in tiles:\n                band_data = tile[feature_name]\n                # Normalize the band data\n                band_data = band_data * norm\n                bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n        # TODO: Test this\n        return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n\n    def segment_tile(\n        self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n    ) -&gt; xr.Dataset:\n        \"\"\"Run inference on a tile.\n\n        Args:\n            tile: The input tile, containing preprocessed, harmonized data.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n                Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n        \"\"\"\n        # Convert the tile to a tensor\n        tensor_tile = self.tile2tensor(tile)\n\n        # Create a batch dimension, because predict expects it\n        tensor_tile = tensor_tile.unsqueeze(0)\n\n        probabilities = predict_in_patches(\n            self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n        ).squeeze(0)\n\n        # Highly sophisticated DL-based predictor\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\n            \"long_name\": \"Probabilities\",\n        }\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n        # Cleanup cuda memory\n        del tensor_tile, probabilities\n        free_torch()\n\n        return tile\n\n    def segment_tile_batched(\n        self,\n        tiles: list[xr.Dataset],\n        patch_size: int = 1024,\n        overlap: int = 16,\n        batch_size: int = 8,\n        reflection: int = 0,\n    ) -&gt; list[xr.Dataset]:\n        \"\"\"Run inference on a list of tiles.\n\n        Args:\n            tiles: The input tiles, containing preprocessed, harmonized data.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n                Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n        \"\"\"\n        # Convert the tiles to tensors\n        # TODO: maybe create a batched tile2tensor function?\n        # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n        tensor_tiles = self.tile2tensor_batched(tiles)\n\n        # Create a batch dimension, because predict expects it\n        tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n        probabilities = predict_in_patches(\n            self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n        )\n\n        # Highly sophisticated DL-based predictor\n        for tile, probs in zip(tiles, probabilities):\n            # TODO: is there a better way to pass metadata?\n            tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n            tile[\"probabilities\"].attrs = {\n                \"long_name\": \"Probabilities\",\n            }\n            tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n        # Cleanup cuda memory\n        del tensor_tiles, probabilities\n        free_torch()\n\n        return tiles\n\n    def __call__(\n        self,\n        input: xr.Dataset | list[xr.Dataset],\n        patch_size: int = 1024,\n        overlap: int = 16,\n        batch_size: int = 8,\n        reflection: int = 0,\n    ) -&gt; xr.Dataset | list[xr.Dataset]:\n        \"\"\"Run inference on a single tile or a list of tiles.\n\n        Args:\n            input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n                Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n            Each `probability` has type float32 and range [0, 1].\n\n        Raises:\n            ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n        \"\"\"\n        if isinstance(input, xr.Dataset):\n            return self.segment_tile(\n                input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n            )\n        elif isinstance(input, list):\n            return self.segment_tile_batched(\n                input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n            )\n        else:\n            raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.config","title":"<code>config = validate_config(ckpt['config'])</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.device","title":"<code>device = device</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.model","title":"<code>model = smp.create_model(**self.config['model'])</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.__call__","title":"<code>__call__(input, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a single tile or a list of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dataset | list[Dataset]</code> <p>A single tile or a list of tiles.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dataset | list[Dataset]</code> <p>A single tile or a list of tiles augmented by a predicted <code>probabilities</code> layer, depending on the input.</p> <code>Dataset | list[Dataset]</code> <p>Each <code>probability</code> has type float32 and range [0, 1].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xr.Dataset | list[xr.Dataset]:\n    \"\"\"Run inference on a single tile or a list of tiles.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n        Each `probability` has type float32 and range [0, 1].\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    else:\n        raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.__init__","title":"<code>__init__(model_checkpoint, device=DEFAULT_DEVICE)</code>","text":"<p>Initialize the segmenter.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>Path</code> <p>The path to the model checkpoint.</p> required <code>device</code> <code>device</code> <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> <code>DEFAULT_DEVICE</code> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __init__(self, model_checkpoint: Path | str, device: torch.device = DEFAULT_DEVICE):\n    \"\"\"Initialize the segmenter.\n\n    Args:\n        model_checkpoint (Path): The path to the model checkpoint.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_checkpoint = model_checkpoint if isinstance(model_checkpoint, Path) else Path(model_checkpoint)\n    self.device = device\n    ckpt = torch.load(model_checkpoint, map_location=self.device)\n    self.config = validate_config(ckpt[\"config\"])\n    # Overwrite the encoder weights with None, because we load our own\n    self.config[\"model\"] |= {\"encoder_weights\": None}\n    self.model = smp.create_model(**self.config[\"model\"])\n    self.model.to(self.device)\n    self.model.load_state_dict(ckpt[\"statedict\"])\n    self.model.eval()\n\n    logger.debug(\n        f\"Successfully loaded model from {model_checkpoint.resolve()} with inputs: \"\n        f\"{self.config['input_combination']}\"\n    )\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.segment_tile","title":"<code>segment_tile(tile, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The input tile, containing preprocessed, harmonized data.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def segment_tile(\n    self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tile to a tensor\n    tensor_tile = self.tile2tensor(tile)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tile = tensor_tile.unsqueeze(0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n    ).squeeze(0)\n\n    # Highly sophisticated DL-based predictor\n    # TODO: is there a better way to pass metadata?\n    tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n    tile[\"probabilities\"].attrs = {\n        \"long_name\": \"Probabilities\",\n    }\n    tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tile, probabilities\n    free_torch()\n\n    return tile\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.segment_tile_batched","title":"<code>segment_tile_batched(tiles, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a list of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list[Dataset]</code> <p>The input tiles, containing preprocessed, harmonized data.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[Dataset]</code> <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tiles to tensors\n    # TODO: maybe create a batched tile2tensor function?\n    # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n    tensor_tiles = self.tile2tensor_batched(tiles)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n    )\n\n    # Highly sophisticated DL-based predictor\n    for tile, probs in zip(tiles, probabilities):\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\n            \"long_name\": \"Probabilities\",\n        }\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tiles, probabilities\n    free_torch()\n\n    return tiles\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.tile2tensor","title":"<code>tile2tensor(tile)</code>","text":"<p>Take a tile and convert it to a pytorch tensor.</p> <p>Respects the input combination from the config.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n    \"\"\"Take a tile and convert it to a pytorch tensor.\n\n    Respects the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n    # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        band_data = tile[feature_name]\n        # Normalize the band data\n        band_data = band_data * norm\n        bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n    return torch.stack(bands, dim=0)\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.tile2tensor_batched","title":"<code>tile2tensor_batched(tiles)</code>","text":"<p>Take a list of tiles and convert them to a pytorch tensor.</p> <p>Respects the the input combination from the config.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n    \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n    Respects the the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        for tile in tiles:\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n    # TODO: Test this\n    return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenterConfig","title":"<code>SMPSegmenterConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for the segmentor.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>class SMPSegmenterConfig(TypedDict):\n    \"\"\"Configuration for the segmentor.\"\"\"\n\n    input_combination: list[str]\n    model: dict[str, Any]\n    norm_factors: dict[str, float]\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenterConfig.input_combination","title":"<code>input_combination</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenterConfig.model","title":"<code>model</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenterConfig.norm_factors","title":"<code>norm_factors</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.create_patches","title":"<code>create_patches(tensor_tiles, patch_size, overlap, return_coords=False)</code>","text":"<p>Create patches from a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_tiles</code> <code>Tensor</code> <p>The input tensor. Shape: (BS, C, H, W).</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches.</p> required <code>overlap</code> <code>int</code> <p>The size of the overlap.</p> required <code>return_coords</code> <code>bool</code> <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    logger.debug(f\"Creating {nh * nw} patches took {time.time() - start_time:.2f}s\")\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.patch_coords","title":"<code>patch_coords(h, w, patch_size, overlap)</code>","text":"<p>Yield patch coordinates based on height, width, patch size and margin size.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>int</code> <p>Height of the image.</p> required <code>w</code> <code>int</code> <p>Width of the image.</p> required <code>patch_size</code> <code>int</code> <p>Patch size.</p> required <code>overlap</code> <code>int</code> <p>Margin size.</p> required <p>Yields:</p> Type Description <code>tuple[int, int, int, int]</code> <p>tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def patch_coords(h: int, w: int, patch_size: int, overlap: int) -&gt; Generator[tuple[int, int, int, int], None, None]:\n    \"\"\"Yield patch coordinates based on height, width, patch size and margin size.\n\n    Args:\n        h (int): Height of the image.\n        w (int): Width of the image.\n        patch_size (int): Patch size.\n        overlap (int): Margin size.\n\n    Yields:\n        tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.\n\n    \"\"\"\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            yield y, x, patch_idx_y, patch_idx_x\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.predict_in_patches","title":"<code>predict_in_patches(model, tensor_tiles, patch_size, overlap, batch_size, reflection, device=torch.device, return_weights=False)</code>","text":"<p>Predict on a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to use for prediction.</p> required <code>tensor_tiles</code> <code>Tensor</code> <p>The input tensor. Shape: (BS, C, H, W).</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches.</p> required <code>overlap</code> <code>int</code> <p>The size of the overlap.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> required <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor.</p> required <code>device</code> <code>device</code> <p>The device to use for the prediction.</p> <code>device</code> <code>return_weights</code> <code>bool</code> <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The predicted tensor.</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    n_skipped = 0\n    for i, batch in enumerate(patches):\n        # If batch contains only nans, skip it\n        # TODO: This doesn't work as expected -&gt; check if torch.isnan(batch).all() is correct\n        if torch.isnan(batch).all(axis=0).any():\n            patched_probabilities[i * batch_size : (i + 1) * batch_size] = 0\n            n_skipped += 1\n            continue\n        # If batch contains some nans, replace them with zeros\n        batch[torch.isnan(batch)] = 0\n\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    if n_skipped &gt; 0:\n        logger.debug(f\"Skipped {n_skipped} batches because they only contained NaNs\")\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n    logger.info(f\"Predicting {nh * nw} patches took {time.time() - start_time:.2f}s\")\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"ref/superresolution/","title":"Superresolution Reference","text":""},{"location":"ref/superresolution/#darts_superresolution","title":"<code>darts_superresolution</code>","text":"<p>Image superresolution of Sentinel 2 imagery for the DARTS dataset.</p>"},{"location":"ref/superresolution/#darts_superresolution.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/utils/","title":"Utility Reference","text":""},{"location":"ref/utils/#darts_utils","title":"<code>darts_utils</code>","text":"<p>Utility functions for the DARTS dataset.</p>"},{"location":"ref/utils/#darts_utils.__version__","title":"<code>__version__ = importlib.metadata.version('darts-nextgen')</code>  <code>module-attribute</code>","text":""}]}