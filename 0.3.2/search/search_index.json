{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DARTS nextgen","text":"<p>Panarctic Database of Active Layer Detachment Slides and Retrogressive Thaw Slumps from Deep Learning on High Resolution Satellite Imagery. This is te successor of the thaw-slump-segmentation (pipeline), with which the first version of the DARTS dataset was created.</p>"},{"location":"#contribute","title":"Contribute","text":"<p>Before contributing please contact one of the authors and make sure to read the Contribution Guidelines.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>This page is also meant for internal documentation.</p>"},{"location":"contribute/#editor-setup","title":"Editor setup","text":"<p>There is only setup files provided for VSCode and no other editor (yet). A list of extensions and some settings can be found in the <code>.vscode</code>. At the first start, VSCode should ask you if you want to install the recommended extension. The settings should be automaticly used by VSCode. Both should provide the developers with a better experience and enforce code-style.</p>"},{"location":"contribute/#environment-setup","title":"Environment setup","text":"<p>Prereq:</p> <ul> <li>Rye: <code>curl -sSf https://rye.astral.sh/get | bash</code></li> <li>GDAL: <code>sudo apt update &amp;&amp; sudo apt install libpq-dev gdal-bin libgdal-dev</code> or for HPC <code>conda install conda-forge::gdal</code></li> <li>Clang: <code>sudo apt update &amp;&amp; sudo apt install clang</code> or for HPC <code>conda install conda-forge::clang_linux-64</code></li> </ul> <p>If you install GDAL via apt for linux you can view the supported versions here: https://pkgs.org/search/?q=libgdal-dev. For a finer controll over the versions please use conda.</p> <p>Now first check your gdal-version:</p> <pre><code>$ gdal-config --version\n3.9.2\n</code></pre> <p>And your CUDA version (if you want to use CUDA):</p> <pre><code>$ nvidia-smi\n# Now look on the top right of the table\n</code></pre> <p>The GDAL version is relevant, since the version of the python bindings needs to match the installed GDAL version</p> <p>Now, to sync with a specific <code>gdal</code> version, add <code>gdalXX</code> to the <code>--features</code> flag. To sync with a specific <code>cuda</code> version, add <code>cuda1X</code> or without cuda <code>cpu</code>. E.g.:</p> <pre><code>rye sync -f --features gdal39,cuda12 # For CUDA 12 and GDAL 3.9.2\n</code></pre> <p>As of right now, the supported <code>gdal</code> versions are: 3.9.2 (<code>gdal39</code>), 3.8.5 (<code>gdal38</code>), 3.8.4 (<code>gdal384</code>), 3.7.3 (<code>gdal37</code>) and 3.6.4 (<code>gdal36</code>). If your GDAL version is not supported (yet) please sync without GDAL and then install GDAL to an new optional group. For example, if your GDAL version is 3.8.4:</p> <pre><code>rye sync -f\nrye add --optional=gdal384 \"gdal==3.8.4\"\n</code></pre> <p>IMPORTANT! If you installed any of clang or gdal with conda, please ensure that while installing dependencies and working on the project to have the conda environment activated in which you installed clang and or gdal.</p> <p>Another option is to install the windows GDAL binary wheels compiled by cgoehlke from https://github.com/cgohlke/geospatial-wheels:</p> <pre><code>rye sync -f --features gdal384_win64\n</code></pre>"},{"location":"contribute/#troubleshoot-rye-cant-find-the-right-versions","title":"Troubleshoot: Rye can't find the right versions","text":"<p>Because the <code>pyproject.toml</code> specifies additional sources, e.g. <code>https://download.pytorch.org/whl/cpu</code>, it can happen that the a package with an older version is found in these package-indexes. If such a version is found, <code>uv</code> (the installer behind <code>Rye</code>) currently stops searching other sources for the right version and stops with an <code>Version not found</code> error. This can look something like this:</p> <pre><code>No solution found when resolving dependencies:\n  \u2570\u2500\u25b6 Because only torchmetrics==1.0.3 is available and you require torchmetrics&gt;=1.4.1, we can conclude that your requirements are unsatisfiable.\n</code></pre> <p>To fix this you can set an environment variable to tell <code>uv</code> to search all package-indicies:</p> <pre><code>UV_INDEX_STRATEGY=\"unsafe-best-match\" rye sync ...\n</code></pre> <p>I recommend adding the following to your <code>.zshrc</code> or <code>.bashrc</code>:</p> <pre><code># Change the behaviour of uv package resolution to enable additional sources without breaking existing version-requirements\nexport UV_INDEX_STRATEGY=\"unsafe-best-match\"\n</code></pre> <p>Please see these issues:</p> <ul> <li>Rye: Can't specify per-dependency package index / can't specify uv behavior in config file</li> <li>UV: Add support for pinning a package to a specific index</li> </ul>"},{"location":"contribute/#recommended-notebook-header","title":"Recommended Notebook header","text":"<p>The following code snipped can be put in the very first cell of a notebook to already to add logging and initialize earth engine.</p> <pre><code>import logging\n\nfrom rich.logging import RichHandler\nfrom rich.traceback import install\n\nfrom darts.utils.earthengine import init_ee\nfrom darts.utils.logging import setup_logging\n\nsetup_logging()\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\ninstall(show_locals=True)  # Change to False if you encounter too large tracebacks\ninit_ee(\"ee-project\")  # Replace with your project\n</code></pre>"},{"location":"ref/","title":"Combined Reference","text":"<p>All references on one page</p> Table of Contents<ul> <li>Combined Reference<ul> <li>\u00a0darts<ul> <li>\u00a0__version__</li> <li>\u00a0run_native_planet_pipeline</li> <li>\u00a0run_native_sentinel2_pipeline</li> </ul> </li> <li>\u00a0darts_acquisition<ul> <li>\u00a0hello</li> </ul> </li> <li>\u00a0darts_ensemble<ul> <li>\u00a0hello</li> </ul> </li> <li>\u00a0darts_export<ul> <li>\u00a0InferenceResultWriter<ul> <li>\u00a0ds</li> <li>\u00a0__init__</li> <li>\u00a0export_binarized</li> <li>\u00a0export_polygonized</li> <li>\u00a0export_probabilities</li> </ul> </li> </ul> </li> <li>\u00a0darts_postprocessing</li> <li>\u00a0darts_preprocessing<ul> <li>\u00a0load_and_preprocess_planet_scene<ul> <li>PS Orthotile</li> <li>PS Scene</li> </ul> </li> <li>\u00a0load_and_preprocess_sentinel2_scene</li> </ul> </li> <li>\u00a0darts_segmentation<ul> <li>\u00a0SMPSegmenter<ul> <li>\u00a0config</li> <li>\u00a0device</li> <li>\u00a0model</li> <li>\u00a0__call__</li> <li>\u00a0__init__</li> <li>\u00a0segment_tile</li> <li>\u00a0segment_tile_batched</li> <li>\u00a0tile2tensor</li> <li>\u00a0tile2tensor_batched</li> </ul> </li> <li>\u00a0SMPSegmenterConfig<ul> <li>\u00a0input_combination</li> <li>\u00a0model</li> <li>\u00a0norm_factors</li> </ul> </li> <li>\u00a0create_patches</li> <li>\u00a0patch_coords</li> <li>\u00a0predict_in_patches</li> </ul> </li> <li>\u00a0darts_superresolution<ul> <li>\u00a0hello</li> </ul> </li> </ul> </li> </ul>"},{"location":"ref/#darts","title":"<code>darts</code>","text":"<p>DARTS processing pipeline.</p>"},{"location":"ref/#darts.__version__","title":"<code>__version__ = version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/#darts.run_native_planet_pipeline","title":"<code>run_native_planet_pipeline(orthotiles_dir, scenes_dir, output_data_dir, arcticdem_slope_vrt, arcticdem_elevation_vrt, model_dir, tcvis_model_name='RTS_v6_tcvis.pt', notcvis_model_name='RTS_v6_notcvis.pt', cache_dir=None, ee_project=None, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Search for all PlanetScope scenes in the given directory and runs the segmentation pipeline on them.</p> <p>Parameters:</p> Name Type Description Default <code>orthotiles_dir</code> <code>Path</code> <p>The directory containing the PlanetScope orthotiles.</p> required <code>scenes_dir</code> <code>Path</code> <p>The directory containing the PlanetScope scenes.</p> required <code>output_data_dir</code> <code>Path</code> <p>The \"output\" directory.</p> required <code>arcticdem_slope_vrt</code> <code>Path</code> <p>The path to the ArcticDEM slope VRT file.</p> required <code>arcticdem_elevation_vrt</code> <code>Path</code> <p>The path to the ArcticDEM elevation VRT file.</p> required <code>model_dir</code> <code>Path</code> <p>The path to the models to use for segmentation.</p> required <code>tcvis_model_name</code> <code>str</code> <p>The name of the model to use for TCVis. Defaults to \"RTS_v6_tcvis.pt\".</p> <code>'RTS_v6_tcvis.pt'</code> <code>notcvis_model_name</code> <code>str</code> <p>The name of the model to use for not TCVis. Defaults to \"RTS_v6_notcvis.pt\".</p> <code>'RTS_v6_notcvis.pt'</code> <code>cache_dir</code> <code>Path | None</code> <p>The cache directory. If None, no caching will be used. Defaults to None.</p> <code>None</code> <code>ee_project</code> <code>str</code> <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> <code>None</code> <code>patch_size</code> <code>int</code> <p>The patch size to use for inference. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The overlap to use for inference. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for inference. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>The reflection padding to use for inference. Defaults to 0.</p> <code>0</code> Todo <p>Document the structure of the input data dir.</p> Source code in <code>darts/src/darts/native.py</code> <pre><code>def run_native_planet_pipeline(\n    orthotiles_dir: Path,\n    scenes_dir: Path,\n    output_data_dir: Path,\n    arcticdem_slope_vrt: Path,\n    arcticdem_elevation_vrt: Path,\n    model_dir: Path,\n    tcvis_model_name: str = \"RTS_v6_tcvis.pt\",\n    notcvis_model_name: str = \"RTS_v6_notcvis.pt\",\n    cache_dir: Path | None = None,\n    ee_project: str | None = None,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n):\n    \"\"\"Search for all PlanetScope scenes in the given directory and runs the segmentation pipeline on them.\n\n    Args:\n        orthotiles_dir (Path): The directory containing the PlanetScope orthotiles.\n        scenes_dir (Path): The directory containing the PlanetScope scenes.\n        output_data_dir (Path): The \"output\" directory.\n        arcticdem_slope_vrt (Path): The path to the ArcticDEM slope VRT file.\n        arcticdem_elevation_vrt (Path): The path to the ArcticDEM elevation VRT file.\n        model_dir (Path): The path to the models to use for segmentation.\n        tcvis_model_name (str, optional): The name of the model to use for TCVis. Defaults to \"RTS_v6_tcvis.pt\".\n        notcvis_model_name (str, optional): The name of the model to use for not TCVis. Defaults to \"RTS_v6_notcvis.pt\".\n        cache_dir (Path | None, optional): The cache directory. If None, no caching will be used. Defaults to None.\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        batch_size (int, optional): The batch size to use for inference. Defaults to 8.\n        reflection (int, optional): The reflection padding to use for inference. Defaults to 0.\n\n    Todo:\n        Document the structure of the input data dir.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    from darts_ensemble.ensemble_v1 import EnsembleV1\n    from darts_export.inference import InferenceResultWriter\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import load_and_preprocess_planet_scene\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(ee_project)\n\n    # Find all PlanetScope orthotiles\n    for fpath, outpath in planet_file_generator(orthotiles_dir, scenes_dir, output_data_dir):\n        tile = load_and_preprocess_planet_scene(fpath, arcticdem_slope_vrt, arcticdem_elevation_vrt, cache_dir)\n\n        ensemble = EnsembleV1(model_dir / tcvis_model_name, model_dir / notcvis_model_name)\n        tile = ensemble.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n        tile = prepare_export(tile)\n\n        outpath.mkdir(parents=True, exist_ok=True)\n        writer = InferenceResultWriter(tile)\n        writer.export_probabilities(outpath)\n        writer.export_binarized(outpath)\n        writer.export_polygonized(outpath)\n</code></pre>"},{"location":"ref/#darts.run_native_sentinel2_pipeline","title":"<code>run_native_sentinel2_pipeline(sentinel2_dir, output_data_dir, arcticdem_slope_vrt, arcticdem_elevation_vrt, model_dir, tcvis_model_name='RTS_v6_tcvis.pt', notcvis_model_name='RTS_v6_notcvis.pt', cache_dir=None, ee_project=None, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Search for all PlanetScope scenes in the given directory and runs the segmentation pipeline on them.</p> <p>Parameters:</p> Name Type Description Default <code>sentinel2_dir</code> <code>Path</code> <p>The directory containing the Sentinel 2 scenes.</p> required <code>output_data_dir</code> <code>Path</code> <p>The \"output\" directory.</p> required <code>arcticdem_slope_vrt</code> <code>Path</code> <p>The path to the ArcticDEM slope VRT file.</p> required <code>arcticdem_elevation_vrt</code> <code>Path</code> <p>The path to the ArcticDEM elevation VRT file.</p> required <code>model_dir</code> <code>Path</code> <p>The path to the models to use for segmentation.</p> required <code>tcvis_model_name</code> <code>str</code> <p>The name of the model to use for TCVis. Defaults to \"RTS_v6_tcvis.pt\".</p> <code>'RTS_v6_tcvis.pt'</code> <code>notcvis_model_name</code> <code>str</code> <p>The name of the model to use for not TCVis. Defaults to \"RTS_v6_notcvis.pt\".</p> <code>'RTS_v6_notcvis.pt'</code> <code>cache_dir</code> <code>Path | None</code> <p>The cache directory. If None, no caching will be used. Defaults to None.</p> <code>None</code> <code>ee_project</code> <code>str</code> <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> <code>None</code> <code>patch_size</code> <code>int</code> <p>The patch size to use for inference. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The overlap to use for inference. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for inference. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>The reflection padding to use for inference. Defaults to 0.</p> <code>0</code> Todo <p>Document the structure of the input data dir.</p> Source code in <code>darts/src/darts/native.py</code> <pre><code>def run_native_sentinel2_pipeline(\n    sentinel2_dir: Path,\n    output_data_dir: Path,\n    arcticdem_slope_vrt: Path,\n    arcticdem_elevation_vrt: Path,\n    model_dir: Path,\n    tcvis_model_name: str = \"RTS_v6_tcvis.pt\",\n    notcvis_model_name: str = \"RTS_v6_notcvis.pt\",\n    cache_dir: Path | None = None,\n    ee_project: str | None = None,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n):\n    \"\"\"Search for all PlanetScope scenes in the given directory and runs the segmentation pipeline on them.\n\n    Args:\n        sentinel2_dir (Path): The directory containing the Sentinel 2 scenes.\n        output_data_dir (Path): The \"output\" directory.\n        arcticdem_slope_vrt (Path): The path to the ArcticDEM slope VRT file.\n        arcticdem_elevation_vrt (Path): The path to the ArcticDEM elevation VRT file.\n        model_dir (Path): The path to the models to use for segmentation.\n        tcvis_model_name (str, optional): The name of the model to use for TCVis. Defaults to \"RTS_v6_tcvis.pt\".\n        notcvis_model_name (str, optional): The name of the model to use for not TCVis. Defaults to \"RTS_v6_notcvis.pt\".\n        cache_dir (Path | None, optional): The cache directory. If None, no caching will be used. Defaults to None.\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        batch_size (int, optional): The batch size to use for inference. Defaults to 8.\n        reflection (int, optional): The reflection padding to use for inference. Defaults to 0.\n\n    Todo:\n        Document the structure of the input data dir.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    from darts_ensemble.ensemble_v1 import EnsembleV1\n    from darts_export.inference import InferenceResultWriter\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import load_and_preprocess_sentinel2_scene\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(ee_project)\n\n    # Find all Sentinel 2 scenes\n    for fpath in sentinel2_dir.glob(\"*/\"):\n        scene_id = fpath.name\n        outpath = output_data_dir / scene_id\n        tile = load_and_preprocess_sentinel2_scene(fpath, arcticdem_slope_vrt, arcticdem_elevation_vrt, cache_dir)\n\n        ensemble = EnsembleV1(model_dir / tcvis_model_name, model_dir / notcvis_model_name)\n        tile = ensemble.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n        tile = prepare_export(tile)\n\n        outpath.mkdir(parents=True, exist_ok=True)\n        writer = InferenceResultWriter(tile)\n        writer.export_probabilities(outpath)\n        writer.export_binarized(outpath)\n        writer.export_polygonized(outpath)\n</code></pre>"},{"location":"ref/#darts_acquisition","title":"<code>darts_acquisition</code>","text":"<p>Acquisition of data from various sources for the DARTS dataset.</p>"},{"location":"ref/#darts_acquisition.hello","title":"<code>hello(name)</code>","text":"<p>Say hello to the user.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the user.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-acquisition/src/darts_acquisition/__init__.py</code> <pre><code>def hello(name: str) -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Args:\n        name (str): Name of the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return f\"Hello, {name}, from darts-acquisition!\"\n</code></pre>"},{"location":"ref/#darts_ensemble","title":"<code>darts_ensemble</code>","text":"<p>Inference and model ensembling for the DARTS dataset.</p>"},{"location":"ref/#darts_ensemble.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-ensemble/src/darts_ensemble/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-ensemble!\"\n</code></pre>"},{"location":"ref/#darts_export","title":"<code>darts_export</code>","text":"<p>Dataset export for the DARTS dataset.</p>"},{"location":"ref/#darts_export.InferenceResultWriter","title":"<code>InferenceResultWriter</code>","text":"<p>Writer class to export inference result datasets.</p> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>class InferenceResultWriter:\n    \"\"\"Writer class to export inference result datasets.\"\"\"\n\n    def __init__(self, ds) -&gt; None:\n        \"\"\"Initialize the dataset.\"\"\"\n        self.ds: xarray.Dataset = ds\n\n    def export_probabilities(self, path: Path, filename=\"pred_probabilities.tif\", tags={}):\n        \"\"\"Export the probabilities layer to a file.\n\n        Args:\n            path (Path): The path where to export to.\n            filename (str, optional): the filename. Defaults to \"pred_probabilities.tif\".\n            tags (dict, optional): optional GeoTIFF metadate to be written. Defaults to no additional metadata.\n\n        Returns:\n            the Path of the written file\n\n        \"\"\"\n        # write the probability layer from the raster to a GeoTiff\n        file_path = path / filename\n        self.ds.probabilities.rio.to_raster(file_path, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n        return file_path\n\n    def export_binarized(self, path: Path, filename=\"pred_binarized.tif\", tags={}):\n        \"\"\"Export the binarized segmentation result of the inference Result.\n\n        Args:\n            path (Path): The path where to export to.\n            filename (str, optional): the filename. Defaults to \"pred_binarized.tif\".\n            tags (dict, optional): optional GeoTIFF metadate to be written. Defaults to no additional metadata.\n\n        Returns:\n            the Path of the written file\n\n        \"\"\"\n        file_path = path / filename\n        self.ds.binarized_segmentation.rio.to_raster(file_path, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n        return file_path\n\n    def export_polygonized(self, path: Path, filename_prefix=\"pred_segments\", minimum_mapping_unit=32):\n        \"\"\"Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.\n\n        Args:\n            path (Path): The path where to export the files\n            filename_prefix (str, optional): the file prefix of the exported files. Defaults to \"pred_segments\".\n            minimum_mapping_unit (int, optional): segments covering less pixel are removed. Defaults to 32.\n\n        \"\"\"\n        polygon_gdf = vectorization.vectorize(self.ds, minimum_mapping_unit=minimum_mapping_unit)\n\n        path_gpkg = path / f\"{filename_prefix}.gpkg\"\n        path_parquet = path / f\"{filename_prefix}.parquet\"\n\n        polygon_gdf.to_file(path_gpkg, layer=filename_prefix)\n        polygon_gdf.to_parquet(path_parquet)\n</code></pre>"},{"location":"ref/#darts_export.InferenceResultWriter.ds","title":"<code>ds: xarray.Dataset = ds</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_export.InferenceResultWriter.__init__","title":"<code>__init__(ds)</code>","text":"<p>Initialize the dataset.</p> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def __init__(self, ds) -&gt; None:\n    \"\"\"Initialize the dataset.\"\"\"\n    self.ds: xarray.Dataset = ds\n</code></pre>"},{"location":"ref/#darts_export.InferenceResultWriter.export_binarized","title":"<code>export_binarized(path, filename='pred_binarized.tif', tags={})</code>","text":"<p>Export the binarized segmentation result of the inference Result.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path where to export to.</p> required <code>filename</code> <code>str</code> <p>the filename. Defaults to \"pred_binarized.tif\".</p> <code>'pred_binarized.tif'</code> <code>tags</code> <code>dict</code> <p>optional GeoTIFF metadate to be written. Defaults to no additional metadata.</p> <code>{}</code> <p>Returns:</p> Type Description <p>the Path of the written file</p> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_binarized(self, path: Path, filename=\"pred_binarized.tif\", tags={}):\n    \"\"\"Export the binarized segmentation result of the inference Result.\n\n    Args:\n        path (Path): The path where to export to.\n        filename (str, optional): the filename. Defaults to \"pred_binarized.tif\".\n        tags (dict, optional): optional GeoTIFF metadate to be written. Defaults to no additional metadata.\n\n    Returns:\n        the Path of the written file\n\n    \"\"\"\n    file_path = path / filename\n    self.ds.binarized_segmentation.rio.to_raster(file_path, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n    return file_path\n</code></pre>"},{"location":"ref/#darts_export.InferenceResultWriter.export_polygonized","title":"<code>export_polygonized(path, filename_prefix='pred_segments', minimum_mapping_unit=32)</code>","text":"<p>Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path where to export the files</p> required <code>filename_prefix</code> <code>str</code> <p>the file prefix of the exported files. Defaults to \"pred_segments\".</p> <code>'pred_segments'</code> <code>minimum_mapping_unit</code> <code>int</code> <p>segments covering less pixel are removed. Defaults to 32.</p> <code>32</code> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_polygonized(self, path: Path, filename_prefix=\"pred_segments\", minimum_mapping_unit=32):\n    \"\"\"Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.\n\n    Args:\n        path (Path): The path where to export the files\n        filename_prefix (str, optional): the file prefix of the exported files. Defaults to \"pred_segments\".\n        minimum_mapping_unit (int, optional): segments covering less pixel are removed. Defaults to 32.\n\n    \"\"\"\n    polygon_gdf = vectorization.vectorize(self.ds, minimum_mapping_unit=minimum_mapping_unit)\n\n    path_gpkg = path / f\"{filename_prefix}.gpkg\"\n    path_parquet = path / f\"{filename_prefix}.parquet\"\n\n    polygon_gdf.to_file(path_gpkg, layer=filename_prefix)\n    polygon_gdf.to_parquet(path_parquet)\n</code></pre>"},{"location":"ref/#darts_export.InferenceResultWriter.export_probabilities","title":"<code>export_probabilities(path, filename='pred_probabilities.tif', tags={})</code>","text":"<p>Export the probabilities layer to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path where to export to.</p> required <code>filename</code> <code>str</code> <p>the filename. Defaults to \"pred_probabilities.tif\".</p> <code>'pred_probabilities.tif'</code> <code>tags</code> <code>dict</code> <p>optional GeoTIFF metadate to be written. Defaults to no additional metadata.</p> <code>{}</code> <p>Returns:</p> Type Description <p>the Path of the written file</p> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_probabilities(self, path: Path, filename=\"pred_probabilities.tif\", tags={}):\n    \"\"\"Export the probabilities layer to a file.\n\n    Args:\n        path (Path): The path where to export to.\n        filename (str, optional): the filename. Defaults to \"pred_probabilities.tif\".\n        tags (dict, optional): optional GeoTIFF metadate to be written. Defaults to no additional metadata.\n\n    Returns:\n        the Path of the written file\n\n    \"\"\"\n    # write the probability layer from the raster to a GeoTiff\n    file_path = path / filename\n    self.ds.probabilities.rio.to_raster(file_path, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n    return file_path\n</code></pre>"},{"location":"ref/#darts_postprocessing","title":"<code>darts_postprocessing</code>","text":"<p>Postprocessing steps for the DARTS dataset.</p>"},{"location":"ref/#darts_preprocessing","title":"<code>darts_preprocessing</code>","text":"<p>Data preprocessing and feature engineering for the DARTS dataset.</p>"},{"location":"ref/#darts_preprocessing.load_and_preprocess_planet_scene","title":"<code>load_and_preprocess_planet_scene(planet_scene_path, slope_vrt, elevation_vrt, cache_dir=None)</code>","text":"<p>Load and preprocess a Planet Scene (PSOrthoTile or PSScene) into an xr.Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>planet_scene_path</code> <code>Path</code> <p>path to the Planet Scene</p> required <code>slope_vrt</code> <code>Path</code> <p>path to the ArcticDEM slope VRT file</p> required <code>elevation_vrt</code> <code>Path</code> <p>path to the ArcticDEM elevation VRT file</p> required <code>cache_dir</code> <code>Path | None</code> <p>The cache directory. If None, no caching will be used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: preprocessed Planet Scene</p> <p>Examples:</p>"},{"location":"ref/#darts_preprocessing.load_and_preprocess_planet_scene--ps-orthotile","title":"PS Orthotile","text":"<p>Data directory structure:</p> <pre><code>    data/input\n    \u251c\u2500\u2500 ArcticDEM\n    \u2502   \u251c\u2500\u2500 elevation.vrt\n    \u2502   \u251c\u2500\u2500 slope.vrt\n    \u2502   \u251c\u2500\u2500 relative_elevation\n    \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n    \u2502   \u2514\u2500\u2500 slope\n    \u2502       \u2514\u2500\u2500 4372514_slope.tif\n    \u2514\u2500\u2500 planet\n        \u2514\u2500\u2500 PSOrthoTile\n            \u2514\u2500\u2500 4372514/5790392_4372514_2022-07-16_2459\n                \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_Analytic_metadata.xml\n                \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_DN_udm.tif\n                \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_SR.tif\n                \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_metadata.json\n                \u2514\u2500\u2500 5790392_4372514_2022-07-16_2459_udm2.tif\n</code></pre> <p>Load and preprocess a Planet Scene:</p> <pre><code>    from pathlib import Path\n    from darts_preprocessing.preprocess import load_and_preprocess_planet_scene\n\n    fpath = Path(\"data/input/planet/PSOrthoTile/4372514/5790392_4372514_2022-07-16_2459\")\n    arcticdem_dir = input_data_dir / \"ArcticDEM\"\n    tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n</code></pre>"},{"location":"ref/#darts_preprocessing.load_and_preprocess_planet_scene--ps-scene","title":"PS Scene","text":"<p>Data directory structure:</p> <pre><code>    data/input\n    \u251c\u2500\u2500 ArcticDEM\n    \u2502   \u251c\u2500\u2500 elevation.vrt\n    \u2502   \u251c\u2500\u2500 slope.vrt\n    \u2502   \u251c\u2500\u2500 relative_elevation\n    \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n    \u2502   \u2514\u2500\u2500 slope\n    \u2502       \u2514\u2500\u2500 4372514_slope.tif\n    \u2514\u2500\u2500 planet\n        \u2514\u2500\u2500 PSScene\n            \u2514\u2500\u2500 20230703_194241_43_2427\n                \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_metadata.xml\n                \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_SR.tif\n                \u251c\u2500\u2500 20230703_194241_43_2427_3B_udm2.tif\n                \u251c\u2500\u2500 20230703_194241_43_2427_metadata.json\n                \u2514\u2500\u2500 20230703_194241_43_2427.json\n</code></pre> <p>Load and preprocess a Planet Scene:</p> <pre><code>    from pathlib import Path\n    from darts_preprocessing.preprocess import load_and_preprocess_planet_scene\n\n    fpath = Path(\"data/input/planet/PSOrthoTile/20230703_194241_43_2427\")\n    arcticdem_dir = input_data_dir / \"ArcticDEM\"\n    tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>def load_and_preprocess_planet_scene(\n    planet_scene_path: Path, slope_vrt: Path, elevation_vrt: Path, cache_dir: Path | None = None\n) -&gt; xr.Dataset:\n    \"\"\"Load and preprocess a Planet Scene (PSOrthoTile or PSScene) into an xr.Dataset.\n\n    Args:\n        planet_scene_path (Path): path to the Planet Scene\n        slope_vrt (Path): path to the ArcticDEM slope VRT file\n        elevation_vrt (Path): path to the ArcticDEM elevation VRT file\n        cache_dir (Path | None): The cache directory. If None, no caching will be used. Defaults to None.\n\n    Returns:\n        xr.Dataset: preprocessed Planet Scene\n\n    Examples:\n        ### PS Orthotile\n\n        Data directory structure:\n\n        ```sh\n            data/input\n            \u251c\u2500\u2500 ArcticDEM\n            \u2502   \u251c\u2500\u2500 elevation.vrt\n            \u2502   \u251c\u2500\u2500 slope.vrt\n            \u2502   \u251c\u2500\u2500 relative_elevation\n            \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n            \u2502   \u2514\u2500\u2500 slope\n            \u2502       \u2514\u2500\u2500 4372514_slope.tif\n            \u2514\u2500\u2500 planet\n                \u2514\u2500\u2500 PSOrthoTile\n                    \u2514\u2500\u2500 4372514/5790392_4372514_2022-07-16_2459\n                        \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_Analytic_metadata.xml\n                        \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_DN_udm.tif\n                        \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_SR.tif\n                        \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_metadata.json\n                        \u2514\u2500\u2500 5790392_4372514_2022-07-16_2459_udm2.tif\n        ```\n\n        Load and preprocess a Planet Scene:\n\n        ```python\n            from pathlib import Path\n            from darts_preprocessing.preprocess import load_and_preprocess_planet_scene\n\n            fpath = Path(\"data/input/planet/PSOrthoTile/4372514/5790392_4372514_2022-07-16_2459\")\n            arcticdem_dir = input_data_dir / \"ArcticDEM\"\n            tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n        ```\n\n\n        ### PS Scene\n\n        Data directory structure:\n\n        ```sh\n            data/input\n            \u251c\u2500\u2500 ArcticDEM\n            \u2502   \u251c\u2500\u2500 elevation.vrt\n            \u2502   \u251c\u2500\u2500 slope.vrt\n            \u2502   \u251c\u2500\u2500 relative_elevation\n            \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n            \u2502   \u2514\u2500\u2500 slope\n            \u2502       \u2514\u2500\u2500 4372514_slope.tif\n            \u2514\u2500\u2500 planet\n                \u2514\u2500\u2500 PSScene\n                    \u2514\u2500\u2500 20230703_194241_43_2427\n                        \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_metadata.xml\n                        \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_SR.tif\n                        \u251c\u2500\u2500 20230703_194241_43_2427_3B_udm2.tif\n                        \u251c\u2500\u2500 20230703_194241_43_2427_metadata.json\n                        \u2514\u2500\u2500 20230703_194241_43_2427.json\n        ```\n\n        Load and preprocess a Planet Scene:\n\n        ```python\n            from pathlib import Path\n            from darts_preprocessing.preprocess import load_and_preprocess_planet_scene\n\n            fpath = Path(\"data/input/planet/PSOrthoTile/20230703_194241_43_2427\")\n            arcticdem_dir = input_data_dir / \"ArcticDEM\"\n            tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n        ```\n\n    \"\"\"\n    # load planet scene\n    ds_planet = load_planet_scene(planet_scene_path)\n\n    # calculate xr.dataset ndvi\n    ds_ndvi = calculate_ndvi(ds_planet)\n\n    ds_articdem = load_arcticdem(slope_vrt, elevation_vrt, ds_planet)\n\n    ds_tcvis = load_tcvis(ds_planet, cache_dir)\n\n    # load udm2\n    ds_data_masks = load_planet_masks(planet_scene_path)\n\n    # merge to final dataset\n    ds_merged = xr.merge([ds_planet, ds_ndvi, ds_articdem, ds_tcvis, ds_data_masks])\n\n    return ds_merged\n</code></pre>"},{"location":"ref/#darts_preprocessing.load_and_preprocess_sentinel2_scene","title":"<code>load_and_preprocess_sentinel2_scene(s2_scene_path, slope_vrt, elevation_vrt, cache_dir=None)</code>","text":"<p>Load and preprocess a Sentinel 2 scene into an xr.Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>s2_scene_path</code> <code>Path</code> <p>path to the Sentinel 2 Scene</p> required <code>slope_vrt</code> <code>Path</code> <p>path to the ArcticDEM slope VRT file</p> required <code>elevation_vrt</code> <code>Path</code> <p>path to the ArcticDEM elevation VRT file</p> required <code>cache_dir</code> <code>Path | None</code> <p>The cache directory. If None, no caching will be used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: preprocessed Sentinel Scene</p> <p>Examples:</p> <p>Data directory structure:</p> <pre><code>    data/input\n    \u251c\u2500\u2500 ArcticDEM\n    \u2502   \u251c\u2500\u2500 elevation.vrt\n    \u2502   \u251c\u2500\u2500 slope.vrt\n    \u2502   \u251c\u2500\u2500 relative_elevation\n    \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n    \u2502   \u2514\u2500\u2500 slope\n    \u2502       \u2514\u2500\u2500 4372514_slope.tif\n    \u2514\u2500\u2500 sentinel2\n        \u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ/\n            \u251c\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SCL_clip.tif\n            \u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SR_clip.tif\n</code></pre> <p>Load and preprocess a Sentinel Scene:</p> <pre><code>    from pathlib import Path\n    from darts_preprocessing.preprocess import load_and_preprocess_sentinel2_scene\n\n    fpath = Path(\"data/input/sentinel2/20220826T200911_20220826T200905_T17XMJ\")\n    arcticdem_dir = input_data_dir / \"ArcticDEM\"\n    tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>def load_and_preprocess_sentinel2_scene(\n    s2_scene_path: Path, slope_vrt: Path, elevation_vrt: Path, cache_dir: Path | None = None\n) -&gt; xr.Dataset:\n    \"\"\"Load and preprocess a Sentinel 2 scene into an xr.Dataset.\n\n    Args:\n        s2_scene_path (Path): path to the Sentinel 2 Scene\n        slope_vrt (Path): path to the ArcticDEM slope VRT file\n        elevation_vrt (Path): path to the ArcticDEM elevation VRT file\n        cache_dir (Path | None): The cache directory. If None, no caching will be used. Defaults to None.\n\n    Returns:\n        xr.Dataset: preprocessed Sentinel Scene\n\n    Examples:\n        Data directory structure:\n\n        ```sh\n            data/input\n            \u251c\u2500\u2500 ArcticDEM\n            \u2502   \u251c\u2500\u2500 elevation.vrt\n            \u2502   \u251c\u2500\u2500 slope.vrt\n            \u2502   \u251c\u2500\u2500 relative_elevation\n            \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n            \u2502   \u2514\u2500\u2500 slope\n            \u2502       \u2514\u2500\u2500 4372514_slope.tif\n            \u2514\u2500\u2500 sentinel2\n                \u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ/\n                    \u251c\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SCL_clip.tif\n                    \u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SR_clip.tif\n        ```\n\n        Load and preprocess a Sentinel Scene:\n\n        ```python\n            from pathlib import Path\n            from darts_preprocessing.preprocess import load_and_preprocess_sentinel2_scene\n\n            fpath = Path(\"data/input/sentinel2/20220826T200911_20220826T200905_T17XMJ\")\n            arcticdem_dir = input_data_dir / \"ArcticDEM\"\n            tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n        ```\n\n    \"\"\"\n    # load planet scene\n    ds_s2 = load_s2_scene(s2_scene_path)\n\n    # calculate xr.dataset ndvi\n    ds_ndvi = calculate_ndvi(ds_s2)\n\n    ds_articdem = load_arcticdem(slope_vrt, elevation_vrt, ds_s2)\n\n    ds_tcvis = load_tcvis(ds_s2, cache_dir)\n\n    # load scl\n    ds_data_masks = load_s2_masks(s2_scene_path)\n\n    # merge to final dataset\n    ds_merged = xr.merge([ds_s2, ds_ndvi, ds_articdem, ds_tcvis, ds_data_masks])\n\n    return ds_merged\n</code></pre>"},{"location":"ref/#darts_segmentation","title":"<code>darts_segmentation</code>","text":"<p>Image segmentation of thaw-slumps for the DARTS dataset.</p>"},{"location":"ref/#darts_segmentation.SMPSegmenter","title":"<code>SMPSegmenter</code>","text":"<p>An actor that keeps a model as its state and segments tiles.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>class SMPSegmenter:\n    \"\"\"An actor that keeps a model as its state and segments tiles.\"\"\"\n\n    config: SMPSegmenterConfig\n    model: nn.Module\n    device: torch.device\n\n    def __init__(self, model_checkpoint: Path | str):\n        \"\"\"Initialize the segmenter.\n\n        Args:\n            model_checkpoint (Path): The path to the model checkpoint.\n\n        \"\"\"\n        self.device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n        ckpt = torch.load(model_checkpoint, map_location=self.device)\n        self.config = validate_config(ckpt[\"config\"])\n        self.model = smp.create_model(**self.config[\"model\"], encoder_weights=None)\n        self.model.to(self.device)\n        self.model.load_state_dict(ckpt[\"statedict\"])\n        self.model.eval()\n\n    def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n        \"\"\"Take a tile and convert it to a pytorch tensor.\n\n        Respects the input combination from the config.\n\n        Returns:\n            A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n        \"\"\"\n        bands = []\n        # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n        # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n        for feature_name in self.config[\"input_combination\"]:\n            norm = self.config[\"norm_factors\"][feature_name]\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n        return torch.stack(bands, dim=0)\n\n    def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n        \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n        Respects the the input combination from the config.\n\n        Returns:\n            A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n        \"\"\"\n        bands = []\n        for feature_name in self.config[\"input_combination\"]:\n            norm = self.config[\"norm_factors\"][feature_name]\n            for tile in tiles:\n                band_data = tile[feature_name]\n                # Normalize the band data\n                band_data = band_data * norm\n                bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n        # TODO: Test this\n        return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n\n    def segment_tile(\n        self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n    ) -&gt; xr.Dataset:\n        \"\"\"Run inference on a tile.\n\n        Args:\n            tile: The input tile, containing preprocessed, harmonized data.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n        \"\"\"\n        # Convert the tile to a tensor\n        tensor_tile = self.tile2tensor(tile)\n\n        # Create a batch dimension, because predict expects it\n        tensor_tile = tensor_tile.unsqueeze(0)\n\n        probabilities = predict_in_patches(\n            self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n        ).squeeze(0)\n\n        # Highly sophisticated DL-based predictor\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\n            \"long_name\": \"Probabilities\",\n        }\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n        return tile\n\n    def segment_tile_batched(\n        self,\n        tiles: list[xr.Dataset],\n        patch_size: int = 1024,\n        overlap: int = 16,\n        batch_size: int = 8,\n        reflection: int = 0,\n    ) -&gt; list[xr.Dataset]:\n        \"\"\"Run inference on a list of tiles.\n\n        Args:\n            tiles: The input tiles, containing preprocessed, harmonized data.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n        \"\"\"\n        # Convert the tiles to tensors\n        # TODO: maybe create a batched tile2tensor function?\n        # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n        tensor_tiles = self.tile2tensor_batched(tiles)\n\n        # Create a batch dimension, because predict expects it\n        tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n        probabilities = predict_in_patches(\n            self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n        )\n\n        # Highly sophisticated DL-based predictor\n        for tile, probs in zip(tiles, probabilities):\n            # TODO: is there a better way to pass metadata?\n            tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n            tile[\"probabilities\"].attrs = {\n                \"long_name\": \"Probabilities\",\n            }\n            tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n        return tiles\n\n    def __call__(\n        self,\n        input: xr.Dataset | list[xr.Dataset],\n        patch_size: int = 1024,\n        overlap: int = 16,\n        batch_size: int = 8,\n        reflection: int = 0,\n    ) -&gt; xr.Dataset | list[xr.Dataset]:\n        \"\"\"Run inference on a single tile or a list of tiles.\n\n        Args:\n            input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n            Each `probability` has type float32 and range [0, 1].\n\n        Raises:\n            ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n        \"\"\"\n        if isinstance(input, xr.Dataset):\n            return self.segment_tile(\n                input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n            )\n        elif isinstance(input, list):\n            return self.segment_tile_batched(\n                input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n            )\n        else:\n            raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.config","title":"<code>config: SMPSegmenterConfig = validate_config(ckpt['config'])</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenter.device","title":"<code>device: torch.device = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenter.model","title":"<code>model: nn.Module = smp.create_model(**self.config['model'], encoder_weights=None)</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenter.__call__","title":"<code>__call__(input, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a single tile or a list of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dataset | list[Dataset]</code> <p>A single tile or a list of tiles.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dataset | list[Dataset]</code> <p>A single tile or a list of tiles augmented by a predicted <code>probabilities</code> layer, depending on the input.</p> <code>Dataset | list[Dataset]</code> <p>Each <code>probability</code> has type float32 and range [0, 1].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xr.Dataset | list[xr.Dataset]:\n    \"\"\"Run inference on a single tile or a list of tiles.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n        Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n        Each `probability` has type float32 and range [0, 1].\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    else:\n        raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.__init__","title":"<code>__init__(model_checkpoint)</code>","text":"<p>Initialize the segmenter.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>Path</code> <p>The path to the model checkpoint.</p> required Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __init__(self, model_checkpoint: Path | str):\n    \"\"\"Initialize the segmenter.\n\n    Args:\n        model_checkpoint (Path): The path to the model checkpoint.\n\n    \"\"\"\n    self.device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n    ckpt = torch.load(model_checkpoint, map_location=self.device)\n    self.config = validate_config(ckpt[\"config\"])\n    self.model = smp.create_model(**self.config[\"model\"], encoder_weights=None)\n    self.model.to(self.device)\n    self.model.load_state_dict(ckpt[\"statedict\"])\n    self.model.eval()\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.segment_tile","title":"<code>segment_tile(tile, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The input tile, containing preprocessed, harmonized data.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def segment_tile(\n    self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n        Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tile to a tensor\n    tensor_tile = self.tile2tensor(tile)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tile = tensor_tile.unsqueeze(0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n    ).squeeze(0)\n\n    # Highly sophisticated DL-based predictor\n    # TODO: is there a better way to pass metadata?\n    tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n    tile[\"probabilities\"].attrs = {\n        \"long_name\": \"Probabilities\",\n    }\n    tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n    return tile\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.segment_tile_batched","title":"<code>segment_tile_batched(tiles, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a list of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list[Dataset]</code> <p>The input tiles, containing preprocessed, harmonized data.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[Dataset]</code> <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n        Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tiles to tensors\n    # TODO: maybe create a batched tile2tensor function?\n    # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n    tensor_tiles = self.tile2tensor_batched(tiles)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n    )\n\n    # Highly sophisticated DL-based predictor\n    for tile, probs in zip(tiles, probabilities):\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\n            \"long_name\": \"Probabilities\",\n        }\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n    return tiles\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.tile2tensor","title":"<code>tile2tensor(tile)</code>","text":"<p>Take a tile and convert it to a pytorch tensor.</p> <p>Respects the input combination from the config.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n    \"\"\"Take a tile and convert it to a pytorch tensor.\n\n    Respects the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n    # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        band_data = tile[feature_name]\n        # Normalize the band data\n        band_data = band_data * norm\n        bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n    return torch.stack(bands, dim=0)\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenter.tile2tensor_batched","title":"<code>tile2tensor_batched(tiles)</code>","text":"<p>Take a list of tiles and convert them to a pytorch tensor.</p> <p>Respects the the input combination from the config.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n    \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n    Respects the the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        for tile in tiles:\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n    # TODO: Test this\n    return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenterConfig","title":"<code>SMPSegmenterConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for the segmentor.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>class SMPSegmenterConfig(TypedDict):\n    \"\"\"Configuration for the segmentor.\"\"\"\n\n    input_combination: list[str]\n    model: dict[str, Any]\n    norm_factors: dict[str, float]\n</code></pre>"},{"location":"ref/#darts_segmentation.SMPSegmenterConfig.input_combination","title":"<code>input_combination: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenterConfig.model","title":"<code>model: dict[str, Any]</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.SMPSegmenterConfig.norm_factors","title":"<code>norm_factors: dict[str, float]</code>  <code>instance-attribute</code>","text":""},{"location":"ref/#darts_segmentation.create_patches","title":"<code>create_patches(tensor_tiles, patch_size, overlap, return_coords=False)</code>","text":"<p>Create patches from a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_tiles</code> <code>Tensor</code> <p>The input tensor. Shape: (BS, C, H, W).</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches.</p> required <code>overlap</code> <code>int</code> <p>The size of the overlap.</p> required <code>return_coords</code> <code>bool</code> <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    logger.debug(f\"Creating {nh * nw} patches took {time.time() - start_time:.2f}s\")\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"ref/#darts_segmentation.patch_coords","title":"<code>patch_coords(h, w, patch_size, overlap)</code>","text":"<p>Yield patch coordinates based on height, width, patch size and margin size.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>int</code> <p>Height of the image.</p> required <code>w</code> <code>int</code> <p>Width of the image.</p> required <code>patch_size</code> <code>int</code> <p>Patch size.</p> required <code>overlap</code> <code>int</code> <p>Margin size.</p> required <p>Yields:</p> Type Description <code>tuple[int, int, int, int]</code> <p>tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def patch_coords(h: int, w: int, patch_size: int, overlap: int) -&gt; Generator[tuple[int, int, int, int], None, None]:\n    \"\"\"Yield patch coordinates based on height, width, patch size and margin size.\n\n    Args:\n        h (int): Height of the image.\n        w (int): Width of the image.\n        patch_size (int): Patch size.\n        overlap (int): Margin size.\n\n    Yields:\n        tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.\n\n    \"\"\"\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            yield y, x, patch_idx_y, patch_idx_x\n</code></pre>"},{"location":"ref/#darts_segmentation.predict_in_patches","title":"<code>predict_in_patches(model, tensor_tiles, patch_size, overlap, batch_size, reflection, device=torch.device, return_weights=False)</code>","text":"<p>Predict on a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to use for prediction.</p> required <code>tensor_tiles</code> <code>Tensor</code> <p>The input tensor. Shape: (BS, C, H, W).</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches.</p> required <code>overlap</code> <code>int</code> <p>The size of the overlap.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> required <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor.</p> required <code>device</code> <code>device</code> <p>The device to use for the prediction.</p> <code>device</code> <code>return_weights</code> <code>bool</code> <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The predicted tensor.</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    for i, batch in track(enumerate(patches), total=len(patches), description=\"Predicting on patches\"):\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n    logger.debug(f\"Predicting took {time.time() - start_time:.2f}s\")\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"ref/#darts_superresolution","title":"<code>darts_superresolution</code>","text":"<p>Image superresolution of Sentinel 2 imagery for the DARTS dataset.</p>"},{"location":"ref/#darts_superresolution.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-superresolution/src/darts_superresolution/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-superresolution!\"\n</code></pre>"},{"location":"dev/arch/","title":"Architecture describtion","text":"<p>This repository is a workspace repository, managed by Rye. Read more about workspaces at the Rye docs. Each workspace-member starts with <code>darts-*</code> and can be seen as an own package or module, except the <code>darts-nextgen</code> directory which is the top-level package. Each package has it's own internal functions and it's public facing API. The public facing API of each package MUST follow the following section API paradigms.</p> Table of Contents<ul> <li>Architecture describtion<ul> <li>Package overview<ul> <li>Conceptual migration from thaw-slump-segmentation</li> <li>Create a new package</li> </ul> </li> <li>APIs between pipeline steps<ul> <li>Preprocessing Output</li> <li>Segmentation / Ensemble Output</li> <li>Postprocessing Output</li> <li>PyTorch Model checkpoints</li> </ul> </li> <li>API paradigms<ul> <li>Examples</li> <li>About the Xarray overhead with Ray</li> </ul> </li> </ul> </li> </ul>"},{"location":"dev/arch/#package-overview","title":"Package overview","text":"Package Name Type Description (Major) Dependencies - all need Xarray <code>darts-preprocessing</code> Data Loads data and combines the features to a Xarray Dataset GDAL <code>darts-superresolution</code> Train Trains a supper resolution model to scale Sentinel 2 images from 10m to 3m resolution PyTorch <code>darts-segmentation</code> Train Trains an segmentation model PyTorch, segmentation_models_pytorch <code>darts-ensemble</code> Ensemble Ensembles the different models and run the multi-stage inference pipeline. PyTorch <code>darts-postprocessing</code> Data Further refines the output from an ensemble or segmentaion and binarizes the probs PyTorch <code>darts-export</code> Data Saves the results from inference and combines the result to the final DARTS dataset GeoPandas, Scipy, Cucim <p>The following modules are planned or potential ideas for future expansion of the project:</p> Package Name Type Description (Major) Dependencies - all need Xarray <code>darts-acquisition</code> Data Fetches data from the data sources GEE, rasterio, ? <code>darts-detection</code> Train Trains an object detection model PyTorch <code>darts-?</code> Train Trains a ? model for more complex multi-stage ensembles ? <code>darts-evaluation</code> Test Evaluates the end-to-end process on a test dataset and external dataset GeoPandas <code>darts-utils</code> Data Shared utilities for data processing Scipy, Cucim, GeoPandas <code>darts-train-utils</code> Train Shared utilities for training PyTorch <p>The packages should follow this architecture: </p> <p>The <code>darts-nextgen</code> utilizes Ray to automaticly parallize the different computations. However, each package should be designed so that one could build their own pipeline without Ray. Hence, all Ray-related functions / transformations etc. should be defined in the <code>darts-nextgen</code> sub-directory.</p> <p>The packages can decide to wrap their public functions into a CLI with typer.</p> <p>The <code>Train</code> packages should also hold the code for training specific data preparation, model training and model evaluation. These packages should get their data from (already processed) data from the <code>darts-preprocessing</code> package. They should expose a statefull Model class with an <code>inference</code> function, which can be used by the <code>darts-ensemble</code> package.</p>"},{"location":"dev/arch/#conceptual-migration-from-thaw-slump-segmentation","title":"Conceptual migration from thaw-slump-segmentation","text":"<ul> <li>The <code>darts-ensemble</code> and <code>darts-postprocessing</code> packages is the successor of the <code>process-02-inference</code> and <code>process-03-ensemble</code> scripts.</li> <li>The <code>darts-preprocessing</code> and <code>darts-acquisition</code> packages are the successors of the <code>setup-raw-data</code> script and manual work of obtaining data.</li> <li>The <code>darts-export</code> package is splitted from the  <code>inference</code> script, should include the previous manual works of combining everything into the final dataset.</li> <li>The <code>darts-superresolution</code> package is the successor of the <code>superresolution</code> repository.</li> <li>The <code>darts-segmentation</code> package is the successor of the <code>train</code> and <code>prepare_data</code> script.</li> <li>The <code>darts-evaluation</code> package is the successor of the different manual evaluations.</li> </ul>"},{"location":"dev/arch/#create-a-new-package","title":"Create a new package","text":"<p>A new package can easily created with:</p> <pre><code>rye init darts-packagename\n</code></pre> <p>Rye creates a minimal project structure for us.</p> <p>The following things needs to be updates:</p> <ol> <li>The <code>pyproject.toml</code> file inside the new package.</li> </ol> <p>Add to the <code>pyproject.toml</code> file inside the new package is the following to enable Ruff:</p> <pre><code>```toml\n[tool.ruff]\n# Extend the `pyproject.toml` file in the parent directory...\nextend = \"../pyproject.toml\"\n```\n\nPlease also provide a description and a list of authors to the file.\n</code></pre> <ol> <li> <p>The <code>.github/workflows/update_version.yml</code> file, to include the package in the workflow.</p> <p>Under <code>package</code> and under step <code>Update version in pyproject.toml</code>.</p> </li> <li> <p>The docs by creating a <code>ref/name.md</code> file and add them to the nav inside the <code>mkdocs.yml</code>.</p> <p>To enable code detection, also add the package directory under <code>plugins</code> in the <code>mkdocs.yml</code>. Please also add the refs to the top-level <code>ref.md</code>.</p> </li> <li> <p>The Readme of the package</p> </li> </ol>"},{"location":"dev/arch/#apis-between-pipeline-steps","title":"APIs between pipeline steps","text":"<p>The following diagram visualizes the steps of the major <code>packages</code> of the pipeline: </p> <p>Each Tile should be represented as a single <code>xr.Dataset</code> with each feature / band as <code>DataVariable</code>. Each DataVariable should have their <code>data_source</code> documented in the <code>attrs</code>, aswell as <code>long_name</code> and <code>units</code> if any for plotting. A <code>_FillValue</code> should also be set for no-data with <code>.rio.write_nodata(\"no-data-value\")</code></p>"},{"location":"dev/arch/#preprocessing-output","title":"Preprocessing Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs note <code>blue</code> (x, y) uint16 0 data_source, long_name, units <code>green</code> (x, y) uint16 0 data_source, long_name, units <code>red</code> (x, y) uint16 0 data_source, long_name, units <code>nir</code> (x, y) uint16 0 data_source, long_name, units <code>ndvi</code> (x, y) uint16 0 data_source, long_name Values between 0-20.000 (+1, *1e4) <code>relative_elevation</code> (x, y) int16 0 data_source, long_name, units <code>slope</code> (x, y) float32 nan data_source, long_name <code>tc_brightness</code> (x, y) uint8 - data_source, long_name <code>tc_greenness</code> (x, y) uint8 - data_source, long_name <code>tc_wetness</code> (x, y) uint8 - data_source, long_name <code>valid_data_mask</code> (x, y) bool - data_source, long_name <code>quality_data_mask</code> (x, y) bool - data_source, long_name"},{"location":"dev/arch/#segmentation-ensemble-output","title":"Segmentation / Ensemble Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs [Output from Preprocessing] <code>probabilities</code> (x, y) float32 nan long_name <code>probabilities-model-X*</code> (x, y) float32 nan long_name <p>*: optional intermedia probabilities in an ensemble</p>"},{"location":"dev/arch/#postprocessing-output","title":"Postprocessing Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs note [Output from Preprocessing] <code>probabilities_percent</code> (x, y) uint8 255 long_name, units Values between 0-100 <code>binarized_segmentation</code> (x, y) uint8 - long_name"},{"location":"dev/arch/#pytorch-model-checkpoints","title":"PyTorch Model checkpoints","text":"<p>Each checkpoint is stored as a torch <code>.pt</code> tensor file. The checkpoint MUST have the following structure:</p> <pre><code>{\n    \"config\": {\n        \"model_framework\": \"smp\", # Identifier which framework or model was used\n        \"model\": { ... }, # Model specific hyperparameter which are needed to create the model\n        \"input_combination\": [ ... ], # List of strings of the names with which the model was trained, order is important\n        \"patch_size\": 1024, # Patch size on which the model was trained\n        ... # More model-framework specific parameter, e.g. normalization method and factors\n    },\n    \"statedict\": model.module.state_dict(),\n}\n</code></pre>"},{"location":"dev/arch/#api-paradigms","title":"API paradigms","text":"<p>The packages should pass the data as Xarray Datasets between each other. Datasets can hold coordinate information aswell as other metadata (like CRS) in a single self-describing object. Since different <code>tiles</code> do not share the same coordinates or metadata, each <code>tile</code> should be represented by a single Xarray <code>Dataset</code>.</p> <ul> <li>Each public facing API function which in some way transforms data should accept a Xarray Dataset as input and return an Xarray Dataset.</li> <li>Data can also be accepted as a list of Xarray Dataset as input and returned as a list of Xarray Datasets for batched processing.     In this case, concattenation should happend internally and on <code>numpy</code> or <code>pytorch</code> level, NOT on <code>xarray</code> abstraction level.     The reason behind this it that the tiles don't share their coordinates, resulting in a lot of empty spaces between the tiles and high memory usage.     The name of the function should then be <code>function_batched</code>.</li> <li>Each public facing API function which loads data should return a single Xarray Dataset for each <code>tile</code>.</li> <li>Data should NOT be saved to file internally, with <code>darts-export</code> as the only exception. Instead, data should returned in-memory as a Xarray Dataset, so the user / pipeline can decide what to save and when.</li> <li>Function names should be verbs, e.g. <code>process</code>, <code>ensemble</code>, <code>do_inference</code>.</li> <li>If a function is stateless it should NOT be part of a class or wrapper</li> <li>If a function is stateful it should be part of a class or wrapper, this is important for Ray</li> </ul>"},{"location":"dev/arch/#examples","title":"Examples","text":"<p>Here are some examples, how these API paradigms should look like.</p> <ol> <li> <p>Single transformation</p> <pre><code>import darts-package\nimport xarray as xr\n\n# User loads / creates the dataset (a single tile) by themself\nds = xr.open_dataset(\"...\")\n\n# User calls the function to transform the dataset\nds = darts-package.transform(ds, **kwargs)\n\n# User can decide by themself what to do next, e.g. save\nds.to_netcdf(\"...\")\n</code></pre> </li> <li> <p>Batched transformation</p> <pre><code>import darts_package\nimport xarray as xr\n\n# User loads / creates multiple datasets (hence, multiple tiles) by themself\ndata = [xr.open_dataset(\"...\"), xr.open_dataset(\"...\"), ...]\n\n# User calls the function to transform the dataset\ndata = darts_package.transform_batched(data, **kwargs)\n\n# User can decide by themself what to do next\ndata[0].whatever()\n</code></pre> </li> <li> <p>Load &amp; preprocess some data</p> <pre><code>import darts_package\n\n# User calls the function to transform the dataset\nds = darts_package.load(\"path/to/data\", **kwargs)\n\n# User can decide by themself what to do next\nds.whatever()\n</code></pre> </li> <li> <p>Custom pipeline example</p> <pre><code>from pathlib import Path\nimport darts_preprocess\nimport darts_inference\n\nDATA_DIR = Path(\"./data/\")\nMODEL_DIR = Path(\"./models/\")\nOUT_DIR = Path(\"./out/\")\n\n# Inference is a stateful transformation, because it needs to load the model\n# Hence, the \nensemble = darts_inference.Ensemble.load(MODEL_DIR)\n\n# The data directory contains subfolders which then hold the input data\nfor dir in DATA_DIR:\n    name = dir.name\n\n    # Load the files from the processing directory\n    ds = darts_preprocess.load_and_preprocess(dir)\n\n    # Do the inferencce\n    ds = ensemble.inference(ds)\n\n    # Save the results\n    ds.to_netcdf(OUT_DIR / f\"{name}-result.nc\")\n</code></pre> </li> <li> <p>Pipeline with Ray</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\nimport ray\nimport darts_preprocess\nimport darts_inference\nimport darts_export\n\nDATA_DIR = Path(\"./data/\")\nMODEL_DIR = Path(\"./models/\")\nOUT_DIR = Path(\"./out/\")\n\nray.init()\n\n# We need to wrap the Xarray dataset in a class, so that Ray can serialize it\n@dataclass\nclass Tile:\n    ds: xr.Dataset\n\n# Wrapper for ray\ndef open_dataset_ray(row: dict[str, Any]) -&gt; dict[str, Any]:\n    data = xr.open_dataset(row[\"path\"])\n    tile = Tile(data)\n    return {\n        \"input\": tile,\n    }\n\n# Wrapper for the preprocessing -&gt; Stateless\ndef preprocess_tile_ray(row: dict[str, Tile]) -&gt; dict[str, Tile]:\n    ds = darts_preprocess.preprocess(row[\"input\"].ds)\n    return {\n        \"preprocessed\": Tile(ds),\n        \"input\": row[\"input\"]\n    }\n\n# Wrapper for the inference -&gt; Statefull\nclass EnsembleRay:\n    def __init__(self):\n        self.ensemble = darts_inference.Ensemble.load(MODEL_DIR)\n\n    def __call__(self, row: dict[str, Tile]) -&gt; dict[str, Tile]:\n        ds = self.ensemble.inference(row[\"preprocessed\"].ds)\n        return {\n            \"output\": Tile(ds),\n            \"preprocessed\": row[\"preprocessed\"],\n            \"input\": row[\"input\"],\n        }\n\n# We need to add 'local:///' to tell ray that we want to use the local filesystem\nfiles = data.glob(\"*.nc\")\nfile_list = [f\"local:////{file.resolve().absolute()}\" for file in files]\n\nds = ray.data.read_binary_files(file_list, include_paths=True)\nds = ds.map(open_dataset_ray) # Lazy open\nds = ds.map(preprocess_tile_ray) # Lazy preprocess\nds = ds.map(EnsembleRay) # Lazy inference\n\n# Save the results\nfor row in ds.iter_rows():\n    darts_export.save(row[\"output\"].ds, OUT_DIR / f\"{row['input'].ds.name}-result.nc\")\n</code></pre> </li> </ol>"},{"location":"dev/arch/#about-the-xarray-overhead-with-ray","title":"About the Xarray overhead with Ray","text":"<p>Ray expects batched data to be in either numpy or pandas format and can't work with Xarray datasets directly. Hence, a wrapper with custom stacking functions is needed. This tradeoff is not small, however, the benefits in terms of maintainability and readability are worth it.</p> <p></p>"},{"location":"dev/logging/","title":"Logging","text":"<p>We want to use the python logging module as much as possible to traceback errors and document the pipeline processes. Furthermore, we want to configure each logger with the <code>RichHandler</code>, which prettyfies the output with rich.</p>"},{"location":"dev/logging/#setup-guide","title":"Setup Guide","text":"<p>Currently, all setup related to logging is found in the <code>darts.utils.logging.py</code> file. It contains two functions:</p> <ol> <li>A setup function which sets the log-level for all <code>darts.*</code> logger and add default options to xarray and pytorch to supress arrays. See how to supress arrays.</li> <li>A function which adds a file and a rich log handler.</li> </ol> <p>Both functions are used in the CLI setup but can also be called from e.g. a notebook. The recommended approach for handling logging within a notebook is the following:</p> <pre><code>import logging\nfrom rich.logging import RichHandler\nfrom darts.utils.logging import setup_logging\n\nsetup_logging()\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\n</code></pre> <p>This way the notebook won't spam logfiles everywhere and we still have control over our rich handler.</p>"},{"location":"dev/logging/#usage-guide","title":"Usage Guide","text":"<p>For logging inside a darts-package should be done without any further configuration:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__.replace(\"darts_\", \"darts.\")) # don't replace __name__\n</code></pre> <p>Logging at the top-level <code>darts</code> package can just use a <code>__name__</code> logger:</p> <pre><code>import loggin\n\nlogger = logging.getLogger(__name__) # don't replace __name__\n</code></pre>"},{"location":"dev/logging/#supressing-arrays","title":"Supressing Arrays","text":"<p>When printing or logging large numpy arrays a lot of numbers get truncated, however the array still takes a lot of space. Using <code>lovely_numpy</code> and <code>lovely_tensor</code> can help here:</p> <pre><code>import numyp as np\nimport torch\nimport xarray as xr\nfrom lovely_numpy import lo\nfrom lovely_tensors import monkey_patch\n\nmonkey_patch()\nxr.set_options(display_expand_data=False)\n\na = np.zeros((8, 1024, 1024))\nla = lo(a)\nda = xr.DataArray(a)\nt = torch.tensor(a)\n\nlogger.warning(la)\nlogger.warning(da)\nlogger.warning(t)\n</code></pre>"},{"location":"ref/acquisition/","title":"Acquisition Reference","text":""},{"location":"ref/acquisition/#darts_acquisition","title":"<code>darts_acquisition</code>","text":"<p>Acquisition of data from various sources for the DARTS dataset.</p>"},{"location":"ref/acquisition/#darts_acquisition.hello","title":"<code>hello(name)</code>","text":"<p>Say hello to the user.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the user.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-acquisition/src/darts_acquisition/__init__.py</code> <pre><code>def hello(name: str) -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Args:\n        name (str): Name of the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return f\"Hello, {name}, from darts-acquisition!\"\n</code></pre>"},{"location":"ref/darts/","title":"DARTS Reference","text":""},{"location":"ref/darts/#darts","title":"<code>darts</code>","text":"<p>DARTS processing pipeline.</p>"},{"location":"ref/darts/#darts.__version__","title":"<code>__version__ = version('darts-nextgen')</code>  <code>module-attribute</code>","text":""},{"location":"ref/darts/#darts.run_native_planet_pipeline","title":"<code>run_native_planet_pipeline(orthotiles_dir, scenes_dir, output_data_dir, arcticdem_slope_vrt, arcticdem_elevation_vrt, model_dir, tcvis_model_name='RTS_v6_tcvis.pt', notcvis_model_name='RTS_v6_notcvis.pt', cache_dir=None, ee_project=None, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Search for all PlanetScope scenes in the given directory and runs the segmentation pipeline on them.</p> <p>Parameters:</p> Name Type Description Default <code>orthotiles_dir</code> <code>Path</code> <p>The directory containing the PlanetScope orthotiles.</p> required <code>scenes_dir</code> <code>Path</code> <p>The directory containing the PlanetScope scenes.</p> required <code>output_data_dir</code> <code>Path</code> <p>The \"output\" directory.</p> required <code>arcticdem_slope_vrt</code> <code>Path</code> <p>The path to the ArcticDEM slope VRT file.</p> required <code>arcticdem_elevation_vrt</code> <code>Path</code> <p>The path to the ArcticDEM elevation VRT file.</p> required <code>model_dir</code> <code>Path</code> <p>The path to the models to use for segmentation.</p> required <code>tcvis_model_name</code> <code>str</code> <p>The name of the model to use for TCVis. Defaults to \"RTS_v6_tcvis.pt\".</p> <code>'RTS_v6_tcvis.pt'</code> <code>notcvis_model_name</code> <code>str</code> <p>The name of the model to use for not TCVis. Defaults to \"RTS_v6_notcvis.pt\".</p> <code>'RTS_v6_notcvis.pt'</code> <code>cache_dir</code> <code>Path | None</code> <p>The cache directory. If None, no caching will be used. Defaults to None.</p> <code>None</code> <code>ee_project</code> <code>str</code> <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> <code>None</code> <code>patch_size</code> <code>int</code> <p>The patch size to use for inference. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The overlap to use for inference. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for inference. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>The reflection padding to use for inference. Defaults to 0.</p> <code>0</code> Todo <p>Document the structure of the input data dir.</p> Source code in <code>darts/src/darts/native.py</code> <pre><code>def run_native_planet_pipeline(\n    orthotiles_dir: Path,\n    scenes_dir: Path,\n    output_data_dir: Path,\n    arcticdem_slope_vrt: Path,\n    arcticdem_elevation_vrt: Path,\n    model_dir: Path,\n    tcvis_model_name: str = \"RTS_v6_tcvis.pt\",\n    notcvis_model_name: str = \"RTS_v6_notcvis.pt\",\n    cache_dir: Path | None = None,\n    ee_project: str | None = None,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n):\n    \"\"\"Search for all PlanetScope scenes in the given directory and runs the segmentation pipeline on them.\n\n    Args:\n        orthotiles_dir (Path): The directory containing the PlanetScope orthotiles.\n        scenes_dir (Path): The directory containing the PlanetScope scenes.\n        output_data_dir (Path): The \"output\" directory.\n        arcticdem_slope_vrt (Path): The path to the ArcticDEM slope VRT file.\n        arcticdem_elevation_vrt (Path): The path to the ArcticDEM elevation VRT file.\n        model_dir (Path): The path to the models to use for segmentation.\n        tcvis_model_name (str, optional): The name of the model to use for TCVis. Defaults to \"RTS_v6_tcvis.pt\".\n        notcvis_model_name (str, optional): The name of the model to use for not TCVis. Defaults to \"RTS_v6_notcvis.pt\".\n        cache_dir (Path | None, optional): The cache directory. If None, no caching will be used. Defaults to None.\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        batch_size (int, optional): The batch size to use for inference. Defaults to 8.\n        reflection (int, optional): The reflection padding to use for inference. Defaults to 0.\n\n    Todo:\n        Document the structure of the input data dir.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    from darts_ensemble.ensemble_v1 import EnsembleV1\n    from darts_export.inference import InferenceResultWriter\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import load_and_preprocess_planet_scene\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(ee_project)\n\n    # Find all PlanetScope orthotiles\n    for fpath, outpath in planet_file_generator(orthotiles_dir, scenes_dir, output_data_dir):\n        tile = load_and_preprocess_planet_scene(fpath, arcticdem_slope_vrt, arcticdem_elevation_vrt, cache_dir)\n\n        ensemble = EnsembleV1(model_dir / tcvis_model_name, model_dir / notcvis_model_name)\n        tile = ensemble.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n        tile = prepare_export(tile)\n\n        outpath.mkdir(parents=True, exist_ok=True)\n        writer = InferenceResultWriter(tile)\n        writer.export_probabilities(outpath)\n        writer.export_binarized(outpath)\n        writer.export_polygonized(outpath)\n</code></pre>"},{"location":"ref/darts/#darts.run_native_sentinel2_pipeline","title":"<code>run_native_sentinel2_pipeline(sentinel2_dir, output_data_dir, arcticdem_slope_vrt, arcticdem_elevation_vrt, model_dir, tcvis_model_name='RTS_v6_tcvis.pt', notcvis_model_name='RTS_v6_notcvis.pt', cache_dir=None, ee_project=None, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Search for all PlanetScope scenes in the given directory and runs the segmentation pipeline on them.</p> <p>Parameters:</p> Name Type Description Default <code>sentinel2_dir</code> <code>Path</code> <p>The directory containing the Sentinel 2 scenes.</p> required <code>output_data_dir</code> <code>Path</code> <p>The \"output\" directory.</p> required <code>arcticdem_slope_vrt</code> <code>Path</code> <p>The path to the ArcticDEM slope VRT file.</p> required <code>arcticdem_elevation_vrt</code> <code>Path</code> <p>The path to the ArcticDEM elevation VRT file.</p> required <code>model_dir</code> <code>Path</code> <p>The path to the models to use for segmentation.</p> required <code>tcvis_model_name</code> <code>str</code> <p>The name of the model to use for TCVis. Defaults to \"RTS_v6_tcvis.pt\".</p> <code>'RTS_v6_tcvis.pt'</code> <code>notcvis_model_name</code> <code>str</code> <p>The name of the model to use for not TCVis. Defaults to \"RTS_v6_notcvis.pt\".</p> <code>'RTS_v6_notcvis.pt'</code> <code>cache_dir</code> <code>Path | None</code> <p>The cache directory. If None, no caching will be used. Defaults to None.</p> <code>None</code> <code>ee_project</code> <code>str</code> <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> <code>None</code> <code>patch_size</code> <code>int</code> <p>The patch size to use for inference. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The overlap to use for inference. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for inference. Defaults to 8.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>The reflection padding to use for inference. Defaults to 0.</p> <code>0</code> Todo <p>Document the structure of the input data dir.</p> Source code in <code>darts/src/darts/native.py</code> <pre><code>def run_native_sentinel2_pipeline(\n    sentinel2_dir: Path,\n    output_data_dir: Path,\n    arcticdem_slope_vrt: Path,\n    arcticdem_elevation_vrt: Path,\n    model_dir: Path,\n    tcvis_model_name: str = \"RTS_v6_tcvis.pt\",\n    notcvis_model_name: str = \"RTS_v6_notcvis.pt\",\n    cache_dir: Path | None = None,\n    ee_project: str | None = None,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n):\n    \"\"\"Search for all PlanetScope scenes in the given directory and runs the segmentation pipeline on them.\n\n    Args:\n        sentinel2_dir (Path): The directory containing the Sentinel 2 scenes.\n        output_data_dir (Path): The \"output\" directory.\n        arcticdem_slope_vrt (Path): The path to the ArcticDEM slope VRT file.\n        arcticdem_elevation_vrt (Path): The path to the ArcticDEM elevation VRT file.\n        model_dir (Path): The path to the models to use for segmentation.\n        tcvis_model_name (str, optional): The name of the model to use for TCVis. Defaults to \"RTS_v6_tcvis.pt\".\n        notcvis_model_name (str, optional): The name of the model to use for not TCVis. Defaults to \"RTS_v6_notcvis.pt\".\n        cache_dir (Path | None, optional): The cache directory. If None, no caching will be used. Defaults to None.\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        batch_size (int, optional): The batch size to use for inference. Defaults to 8.\n        reflection (int, optional): The reflection padding to use for inference. Defaults to 0.\n\n    Todo:\n        Document the structure of the input data dir.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    from darts_ensemble.ensemble_v1 import EnsembleV1\n    from darts_export.inference import InferenceResultWriter\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import load_and_preprocess_sentinel2_scene\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(ee_project)\n\n    # Find all Sentinel 2 scenes\n    for fpath in sentinel2_dir.glob(\"*/\"):\n        scene_id = fpath.name\n        outpath = output_data_dir / scene_id\n        tile = load_and_preprocess_sentinel2_scene(fpath, arcticdem_slope_vrt, arcticdem_elevation_vrt, cache_dir)\n\n        ensemble = EnsembleV1(model_dir / tcvis_model_name, model_dir / notcvis_model_name)\n        tile = ensemble.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n        tile = prepare_export(tile)\n\n        outpath.mkdir(parents=True, exist_ok=True)\n        writer = InferenceResultWriter(tile)\n        writer.export_probabilities(outpath)\n        writer.export_binarized(outpath)\n        writer.export_polygonized(outpath)\n</code></pre>"},{"location":"ref/ensemble/","title":"Ensemble Reference","text":""},{"location":"ref/ensemble/#darts_ensemble","title":"<code>darts_ensemble</code>","text":"<p>Inference and model ensembling for the DARTS dataset.</p>"},{"location":"ref/ensemble/#darts_ensemble.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-ensemble/src/darts_ensemble/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-ensemble!\"\n</code></pre>"},{"location":"ref/export/","title":"Export Reference","text":""},{"location":"ref/export/#darts_export","title":"<code>darts_export</code>","text":"<p>Dataset export for the DARTS dataset.</p>"},{"location":"ref/export/#darts_export.InferenceResultWriter","title":"<code>InferenceResultWriter</code>","text":"<p>Writer class to export inference result datasets.</p> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>class InferenceResultWriter:\n    \"\"\"Writer class to export inference result datasets.\"\"\"\n\n    def __init__(self, ds) -&gt; None:\n        \"\"\"Initialize the dataset.\"\"\"\n        self.ds: xarray.Dataset = ds\n\n    def export_probabilities(self, path: Path, filename=\"pred_probabilities.tif\", tags={}):\n        \"\"\"Export the probabilities layer to a file.\n\n        Args:\n            path (Path): The path where to export to.\n            filename (str, optional): the filename. Defaults to \"pred_probabilities.tif\".\n            tags (dict, optional): optional GeoTIFF metadate to be written. Defaults to no additional metadata.\n\n        Returns:\n            the Path of the written file\n\n        \"\"\"\n        # write the probability layer from the raster to a GeoTiff\n        file_path = path / filename\n        self.ds.probabilities.rio.to_raster(file_path, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n        return file_path\n\n    def export_binarized(self, path: Path, filename=\"pred_binarized.tif\", tags={}):\n        \"\"\"Export the binarized segmentation result of the inference Result.\n\n        Args:\n            path (Path): The path where to export to.\n            filename (str, optional): the filename. Defaults to \"pred_binarized.tif\".\n            tags (dict, optional): optional GeoTIFF metadate to be written. Defaults to no additional metadata.\n\n        Returns:\n            the Path of the written file\n\n        \"\"\"\n        file_path = path / filename\n        self.ds.binarized_segmentation.rio.to_raster(file_path, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n        return file_path\n\n    def export_polygonized(self, path: Path, filename_prefix=\"pred_segments\", minimum_mapping_unit=32):\n        \"\"\"Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.\n\n        Args:\n            path (Path): The path where to export the files\n            filename_prefix (str, optional): the file prefix of the exported files. Defaults to \"pred_segments\".\n            minimum_mapping_unit (int, optional): segments covering less pixel are removed. Defaults to 32.\n\n        \"\"\"\n        polygon_gdf = vectorization.vectorize(self.ds, minimum_mapping_unit=minimum_mapping_unit)\n\n        path_gpkg = path / f\"{filename_prefix}.gpkg\"\n        path_parquet = path / f\"{filename_prefix}.parquet\"\n\n        polygon_gdf.to_file(path_gpkg, layer=filename_prefix)\n        polygon_gdf.to_parquet(path_parquet)\n</code></pre>"},{"location":"ref/export/#darts_export.InferenceResultWriter.ds","title":"<code>ds: xarray.Dataset = ds</code>  <code>instance-attribute</code>","text":""},{"location":"ref/export/#darts_export.InferenceResultWriter.__init__","title":"<code>__init__(ds)</code>","text":"<p>Initialize the dataset.</p> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def __init__(self, ds) -&gt; None:\n    \"\"\"Initialize the dataset.\"\"\"\n    self.ds: xarray.Dataset = ds\n</code></pre>"},{"location":"ref/export/#darts_export.InferenceResultWriter.export_binarized","title":"<code>export_binarized(path, filename='pred_binarized.tif', tags={})</code>","text":"<p>Export the binarized segmentation result of the inference Result.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path where to export to.</p> required <code>filename</code> <code>str</code> <p>the filename. Defaults to \"pred_binarized.tif\".</p> <code>'pred_binarized.tif'</code> <code>tags</code> <code>dict</code> <p>optional GeoTIFF metadate to be written. Defaults to no additional metadata.</p> <code>{}</code> <p>Returns:</p> Type Description <p>the Path of the written file</p> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_binarized(self, path: Path, filename=\"pred_binarized.tif\", tags={}):\n    \"\"\"Export the binarized segmentation result of the inference Result.\n\n    Args:\n        path (Path): The path where to export to.\n        filename (str, optional): the filename. Defaults to \"pred_binarized.tif\".\n        tags (dict, optional): optional GeoTIFF metadate to be written. Defaults to no additional metadata.\n\n    Returns:\n        the Path of the written file\n\n    \"\"\"\n    file_path = path / filename\n    self.ds.binarized_segmentation.rio.to_raster(file_path, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n    return file_path\n</code></pre>"},{"location":"ref/export/#darts_export.InferenceResultWriter.export_polygonized","title":"<code>export_polygonized(path, filename_prefix='pred_segments', minimum_mapping_unit=32)</code>","text":"<p>Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path where to export the files</p> required <code>filename_prefix</code> <code>str</code> <p>the file prefix of the exported files. Defaults to \"pred_segments\".</p> <code>'pred_segments'</code> <code>minimum_mapping_unit</code> <code>int</code> <p>segments covering less pixel are removed. Defaults to 32.</p> <code>32</code> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_polygonized(self, path: Path, filename_prefix=\"pred_segments\", minimum_mapping_unit=32):\n    \"\"\"Export the binarized probabilities as a vector dataset in GeoPackage and GeoParquet format.\n\n    Args:\n        path (Path): The path where to export the files\n        filename_prefix (str, optional): the file prefix of the exported files. Defaults to \"pred_segments\".\n        minimum_mapping_unit (int, optional): segments covering less pixel are removed. Defaults to 32.\n\n    \"\"\"\n    polygon_gdf = vectorization.vectorize(self.ds, minimum_mapping_unit=minimum_mapping_unit)\n\n    path_gpkg = path / f\"{filename_prefix}.gpkg\"\n    path_parquet = path / f\"{filename_prefix}.parquet\"\n\n    polygon_gdf.to_file(path_gpkg, layer=filename_prefix)\n    polygon_gdf.to_parquet(path_parquet)\n</code></pre>"},{"location":"ref/export/#darts_export.InferenceResultWriter.export_probabilities","title":"<code>export_probabilities(path, filename='pred_probabilities.tif', tags={})</code>","text":"<p>Export the probabilities layer to a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path where to export to.</p> required <code>filename</code> <code>str</code> <p>the filename. Defaults to \"pred_probabilities.tif\".</p> <code>'pred_probabilities.tif'</code> <code>tags</code> <code>dict</code> <p>optional GeoTIFF metadate to be written. Defaults to no additional metadata.</p> <code>{}</code> <p>Returns:</p> Type Description <p>the Path of the written file</p> Source code in <code>darts-export/src/darts_export/inference.py</code> <pre><code>def export_probabilities(self, path: Path, filename=\"pred_probabilities.tif\", tags={}):\n    \"\"\"Export the probabilities layer to a file.\n\n    Args:\n        path (Path): The path where to export to.\n        filename (str, optional): the filename. Defaults to \"pred_probabilities.tif\".\n        tags (dict, optional): optional GeoTIFF metadate to be written. Defaults to no additional metadata.\n\n    Returns:\n        the Path of the written file\n\n    \"\"\"\n    # write the probability layer from the raster to a GeoTiff\n    file_path = path / filename\n    self.ds.probabilities.rio.to_raster(file_path, driver=\"GTiff\", tags=tags, compress=\"LZW\")\n    return file_path\n</code></pre>"},{"location":"ref/preprocessing/","title":"Preprocessing Reference","text":""},{"location":"ref/preprocessing/#darts_preprocessing","title":"<code>darts_preprocessing</code>","text":"<p>Data preprocessing and feature engineering for the DARTS dataset.</p>"},{"location":"ref/preprocessing/#darts_preprocessing.load_and_preprocess_planet_scene","title":"<code>load_and_preprocess_planet_scene(planet_scene_path, slope_vrt, elevation_vrt, cache_dir=None)</code>","text":"<p>Load and preprocess a Planet Scene (PSOrthoTile or PSScene) into an xr.Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>planet_scene_path</code> <code>Path</code> <p>path to the Planet Scene</p> required <code>slope_vrt</code> <code>Path</code> <p>path to the ArcticDEM slope VRT file</p> required <code>elevation_vrt</code> <code>Path</code> <p>path to the ArcticDEM elevation VRT file</p> required <code>cache_dir</code> <code>Path | None</code> <p>The cache directory. If None, no caching will be used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: preprocessed Planet Scene</p> <p>Examples:</p>"},{"location":"ref/preprocessing/#darts_preprocessing.load_and_preprocess_planet_scene--ps-orthotile","title":"PS Orthotile","text":"<p>Data directory structure:</p> <pre><code>    data/input\n    \u251c\u2500\u2500 ArcticDEM\n    \u2502   \u251c\u2500\u2500 elevation.vrt\n    \u2502   \u251c\u2500\u2500 slope.vrt\n    \u2502   \u251c\u2500\u2500 relative_elevation\n    \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n    \u2502   \u2514\u2500\u2500 slope\n    \u2502       \u2514\u2500\u2500 4372514_slope.tif\n    \u2514\u2500\u2500 planet\n        \u2514\u2500\u2500 PSOrthoTile\n            \u2514\u2500\u2500 4372514/5790392_4372514_2022-07-16_2459\n                \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_Analytic_metadata.xml\n                \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_DN_udm.tif\n                \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_SR.tif\n                \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_metadata.json\n                \u2514\u2500\u2500 5790392_4372514_2022-07-16_2459_udm2.tif\n</code></pre> <p>Load and preprocess a Planet Scene:</p> <pre><code>    from pathlib import Path\n    from darts_preprocessing.preprocess import load_and_preprocess_planet_scene\n\n    fpath = Path(\"data/input/planet/PSOrthoTile/4372514/5790392_4372514_2022-07-16_2459\")\n    arcticdem_dir = input_data_dir / \"ArcticDEM\"\n    tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n</code></pre>"},{"location":"ref/preprocessing/#darts_preprocessing.load_and_preprocess_planet_scene--ps-scene","title":"PS Scene","text":"<p>Data directory structure:</p> <pre><code>    data/input\n    \u251c\u2500\u2500 ArcticDEM\n    \u2502   \u251c\u2500\u2500 elevation.vrt\n    \u2502   \u251c\u2500\u2500 slope.vrt\n    \u2502   \u251c\u2500\u2500 relative_elevation\n    \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n    \u2502   \u2514\u2500\u2500 slope\n    \u2502       \u2514\u2500\u2500 4372514_slope.tif\n    \u2514\u2500\u2500 planet\n        \u2514\u2500\u2500 PSScene\n            \u2514\u2500\u2500 20230703_194241_43_2427\n                \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_metadata.xml\n                \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_SR.tif\n                \u251c\u2500\u2500 20230703_194241_43_2427_3B_udm2.tif\n                \u251c\u2500\u2500 20230703_194241_43_2427_metadata.json\n                \u2514\u2500\u2500 20230703_194241_43_2427.json\n</code></pre> <p>Load and preprocess a Planet Scene:</p> <pre><code>    from pathlib import Path\n    from darts_preprocessing.preprocess import load_and_preprocess_planet_scene\n\n    fpath = Path(\"data/input/planet/PSOrthoTile/20230703_194241_43_2427\")\n    arcticdem_dir = input_data_dir / \"ArcticDEM\"\n    tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>def load_and_preprocess_planet_scene(\n    planet_scene_path: Path, slope_vrt: Path, elevation_vrt: Path, cache_dir: Path | None = None\n) -&gt; xr.Dataset:\n    \"\"\"Load and preprocess a Planet Scene (PSOrthoTile or PSScene) into an xr.Dataset.\n\n    Args:\n        planet_scene_path (Path): path to the Planet Scene\n        slope_vrt (Path): path to the ArcticDEM slope VRT file\n        elevation_vrt (Path): path to the ArcticDEM elevation VRT file\n        cache_dir (Path | None): The cache directory. If None, no caching will be used. Defaults to None.\n\n    Returns:\n        xr.Dataset: preprocessed Planet Scene\n\n    Examples:\n        ### PS Orthotile\n\n        Data directory structure:\n\n        ```sh\n            data/input\n            \u251c\u2500\u2500 ArcticDEM\n            \u2502   \u251c\u2500\u2500 elevation.vrt\n            \u2502   \u251c\u2500\u2500 slope.vrt\n            \u2502   \u251c\u2500\u2500 relative_elevation\n            \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n            \u2502   \u2514\u2500\u2500 slope\n            \u2502       \u2514\u2500\u2500 4372514_slope.tif\n            \u2514\u2500\u2500 planet\n                \u2514\u2500\u2500 PSOrthoTile\n                    \u2514\u2500\u2500 4372514/5790392_4372514_2022-07-16_2459\n                        \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_Analytic_metadata.xml\n                        \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_DN_udm.tif\n                        \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_SR.tif\n                        \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_metadata.json\n                        \u2514\u2500\u2500 5790392_4372514_2022-07-16_2459_udm2.tif\n        ```\n\n        Load and preprocess a Planet Scene:\n\n        ```python\n            from pathlib import Path\n            from darts_preprocessing.preprocess import load_and_preprocess_planet_scene\n\n            fpath = Path(\"data/input/planet/PSOrthoTile/4372514/5790392_4372514_2022-07-16_2459\")\n            arcticdem_dir = input_data_dir / \"ArcticDEM\"\n            tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n        ```\n\n\n        ### PS Scene\n\n        Data directory structure:\n\n        ```sh\n            data/input\n            \u251c\u2500\u2500 ArcticDEM\n            \u2502   \u251c\u2500\u2500 elevation.vrt\n            \u2502   \u251c\u2500\u2500 slope.vrt\n            \u2502   \u251c\u2500\u2500 relative_elevation\n            \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n            \u2502   \u2514\u2500\u2500 slope\n            \u2502       \u2514\u2500\u2500 4372514_slope.tif\n            \u2514\u2500\u2500 planet\n                \u2514\u2500\u2500 PSScene\n                    \u2514\u2500\u2500 20230703_194241_43_2427\n                        \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_metadata.xml\n                        \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_SR.tif\n                        \u251c\u2500\u2500 20230703_194241_43_2427_3B_udm2.tif\n                        \u251c\u2500\u2500 20230703_194241_43_2427_metadata.json\n                        \u2514\u2500\u2500 20230703_194241_43_2427.json\n        ```\n\n        Load and preprocess a Planet Scene:\n\n        ```python\n            from pathlib import Path\n            from darts_preprocessing.preprocess import load_and_preprocess_planet_scene\n\n            fpath = Path(\"data/input/planet/PSOrthoTile/20230703_194241_43_2427\")\n            arcticdem_dir = input_data_dir / \"ArcticDEM\"\n            tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n        ```\n\n    \"\"\"\n    # load planet scene\n    ds_planet = load_planet_scene(planet_scene_path)\n\n    # calculate xr.dataset ndvi\n    ds_ndvi = calculate_ndvi(ds_planet)\n\n    ds_articdem = load_arcticdem(slope_vrt, elevation_vrt, ds_planet)\n\n    ds_tcvis = load_tcvis(ds_planet, cache_dir)\n\n    # load udm2\n    ds_data_masks = load_planet_masks(planet_scene_path)\n\n    # merge to final dataset\n    ds_merged = xr.merge([ds_planet, ds_ndvi, ds_articdem, ds_tcvis, ds_data_masks])\n\n    return ds_merged\n</code></pre>"},{"location":"ref/preprocessing/#darts_preprocessing.load_and_preprocess_sentinel2_scene","title":"<code>load_and_preprocess_sentinel2_scene(s2_scene_path, slope_vrt, elevation_vrt, cache_dir=None)</code>","text":"<p>Load and preprocess a Sentinel 2 scene into an xr.Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>s2_scene_path</code> <code>Path</code> <p>path to the Sentinel 2 Scene</p> required <code>slope_vrt</code> <code>Path</code> <p>path to the ArcticDEM slope VRT file</p> required <code>elevation_vrt</code> <code>Path</code> <p>path to the ArcticDEM elevation VRT file</p> required <code>cache_dir</code> <code>Path | None</code> <p>The cache directory. If None, no caching will be used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: preprocessed Sentinel Scene</p> <p>Examples:</p> <p>Data directory structure:</p> <pre><code>    data/input\n    \u251c\u2500\u2500 ArcticDEM\n    \u2502   \u251c\u2500\u2500 elevation.vrt\n    \u2502   \u251c\u2500\u2500 slope.vrt\n    \u2502   \u251c\u2500\u2500 relative_elevation\n    \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n    \u2502   \u2514\u2500\u2500 slope\n    \u2502       \u2514\u2500\u2500 4372514_slope.tif\n    \u2514\u2500\u2500 sentinel2\n        \u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ/\n            \u251c\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SCL_clip.tif\n            \u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SR_clip.tif\n</code></pre> <p>Load and preprocess a Sentinel Scene:</p> <pre><code>    from pathlib import Path\n    from darts_preprocessing.preprocess import load_and_preprocess_sentinel2_scene\n\n    fpath = Path(\"data/input/sentinel2/20220826T200911_20220826T200905_T17XMJ\")\n    arcticdem_dir = input_data_dir / \"ArcticDEM\"\n    tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n</code></pre> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>def load_and_preprocess_sentinel2_scene(\n    s2_scene_path: Path, slope_vrt: Path, elevation_vrt: Path, cache_dir: Path | None = None\n) -&gt; xr.Dataset:\n    \"\"\"Load and preprocess a Sentinel 2 scene into an xr.Dataset.\n\n    Args:\n        s2_scene_path (Path): path to the Sentinel 2 Scene\n        slope_vrt (Path): path to the ArcticDEM slope VRT file\n        elevation_vrt (Path): path to the ArcticDEM elevation VRT file\n        cache_dir (Path | None): The cache directory. If None, no caching will be used. Defaults to None.\n\n    Returns:\n        xr.Dataset: preprocessed Sentinel Scene\n\n    Examples:\n        Data directory structure:\n\n        ```sh\n            data/input\n            \u251c\u2500\u2500 ArcticDEM\n            \u2502   \u251c\u2500\u2500 elevation.vrt\n            \u2502   \u251c\u2500\u2500 slope.vrt\n            \u2502   \u251c\u2500\u2500 relative_elevation\n            \u2502   \u2502   \u2514\u2500\u2500 4372514_relative_elevation_100.tif\n            \u2502   \u2514\u2500\u2500 slope\n            \u2502       \u2514\u2500\u2500 4372514_slope.tif\n            \u2514\u2500\u2500 sentinel2\n                \u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ/\n                    \u251c\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SCL_clip.tif\n                    \u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SR_clip.tif\n        ```\n\n        Load and preprocess a Sentinel Scene:\n\n        ```python\n            from pathlib import Path\n            from darts_preprocessing.preprocess import load_and_preprocess_sentinel2_scene\n\n            fpath = Path(\"data/input/sentinel2/20220826T200911_20220826T200905_T17XMJ\")\n            arcticdem_dir = input_data_dir / \"ArcticDEM\"\n            tile = load_and_preprocess_planet_scene(fpath, arcticdem_dir / \"slope.vrt\", arcticdem_dir / \"elevation.vrt\")\n        ```\n\n    \"\"\"\n    # load planet scene\n    ds_s2 = load_s2_scene(s2_scene_path)\n\n    # calculate xr.dataset ndvi\n    ds_ndvi = calculate_ndvi(ds_s2)\n\n    ds_articdem = load_arcticdem(slope_vrt, elevation_vrt, ds_s2)\n\n    ds_tcvis = load_tcvis(ds_s2, cache_dir)\n\n    # load scl\n    ds_data_masks = load_s2_masks(s2_scene_path)\n\n    # merge to final dataset\n    ds_merged = xr.merge([ds_s2, ds_ndvi, ds_articdem, ds_tcvis, ds_data_masks])\n\n    return ds_merged\n</code></pre>"},{"location":"ref/segmentation/","title":"Export Reference","text":""},{"location":"ref/segmentation/#darts_segmentation","title":"<code>darts_segmentation</code>","text":"<p>Image segmentation of thaw-slumps for the DARTS dataset.</p>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter","title":"<code>SMPSegmenter</code>","text":"<p>An actor that keeps a model as its state and segments tiles.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>class SMPSegmenter:\n    \"\"\"An actor that keeps a model as its state and segments tiles.\"\"\"\n\n    config: SMPSegmenterConfig\n    model: nn.Module\n    device: torch.device\n\n    def __init__(self, model_checkpoint: Path | str):\n        \"\"\"Initialize the segmenter.\n\n        Args:\n            model_checkpoint (Path): The path to the model checkpoint.\n\n        \"\"\"\n        self.device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n        ckpt = torch.load(model_checkpoint, map_location=self.device)\n        self.config = validate_config(ckpt[\"config\"])\n        self.model = smp.create_model(**self.config[\"model\"], encoder_weights=None)\n        self.model.to(self.device)\n        self.model.load_state_dict(ckpt[\"statedict\"])\n        self.model.eval()\n\n    def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n        \"\"\"Take a tile and convert it to a pytorch tensor.\n\n        Respects the input combination from the config.\n\n        Returns:\n            A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n        \"\"\"\n        bands = []\n        # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n        # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n        for feature_name in self.config[\"input_combination\"]:\n            norm = self.config[\"norm_factors\"][feature_name]\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n        return torch.stack(bands, dim=0)\n\n    def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n        \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n        Respects the the input combination from the config.\n\n        Returns:\n            A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n        \"\"\"\n        bands = []\n        for feature_name in self.config[\"input_combination\"]:\n            norm = self.config[\"norm_factors\"][feature_name]\n            for tile in tiles:\n                band_data = tile[feature_name]\n                # Normalize the band data\n                band_data = band_data * norm\n                bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n        # TODO: Test this\n        return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n\n    def segment_tile(\n        self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n    ) -&gt; xr.Dataset:\n        \"\"\"Run inference on a tile.\n\n        Args:\n            tile: The input tile, containing preprocessed, harmonized data.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n        \"\"\"\n        # Convert the tile to a tensor\n        tensor_tile = self.tile2tensor(tile)\n\n        # Create a batch dimension, because predict expects it\n        tensor_tile = tensor_tile.unsqueeze(0)\n\n        probabilities = predict_in_patches(\n            self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n        ).squeeze(0)\n\n        # Highly sophisticated DL-based predictor\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\n            \"long_name\": \"Probabilities\",\n        }\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n        return tile\n\n    def segment_tile_batched(\n        self,\n        tiles: list[xr.Dataset],\n        patch_size: int = 1024,\n        overlap: int = 16,\n        batch_size: int = 8,\n        reflection: int = 0,\n    ) -&gt; list[xr.Dataset]:\n        \"\"\"Run inference on a list of tiles.\n\n        Args:\n            tiles: The input tiles, containing preprocessed, harmonized data.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n        \"\"\"\n        # Convert the tiles to tensors\n        # TODO: maybe create a batched tile2tensor function?\n        # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n        tensor_tiles = self.tile2tensor_batched(tiles)\n\n        # Create a batch dimension, because predict expects it\n        tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n        probabilities = predict_in_patches(\n            self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n        )\n\n        # Highly sophisticated DL-based predictor\n        for tile, probs in zip(tiles, probabilities):\n            # TODO: is there a better way to pass metadata?\n            tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n            tile[\"probabilities\"].attrs = {\n                \"long_name\": \"Probabilities\",\n            }\n            tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n        return tiles\n\n    def __call__(\n        self,\n        input: xr.Dataset | list[xr.Dataset],\n        patch_size: int = 1024,\n        overlap: int = 16,\n        batch_size: int = 8,\n        reflection: int = 0,\n    ) -&gt; xr.Dataset | list[xr.Dataset]:\n        \"\"\"Run inference on a single tile or a list of tiles.\n\n        Args:\n            input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n            patch_size (int): The size of the patches. Defaults to 1024.\n            overlap (int): The size of the overlap. Defaults to 16.\n            batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n            reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n        Returns:\n            A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n            Each `probability` has type float32 and range [0, 1].\n\n        Raises:\n            ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n        \"\"\"\n        if isinstance(input, xr.Dataset):\n            return self.segment_tile(\n                input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n            )\n        elif isinstance(input, list):\n            return self.segment_tile_batched(\n                input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n            )\n        else:\n            raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.config","title":"<code>config: SMPSegmenterConfig = validate_config(ckpt['config'])</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.device","title":"<code>device: torch.device = torch.device('cpu') if not torch.cuda.is_available() else torch.device('cuda')</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.model","title":"<code>model: nn.Module = smp.create_model(**self.config['model'], encoder_weights=None)</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.__call__","title":"<code>__call__(input, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a single tile or a list of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Dataset | list[Dataset]</code> <p>A single tile or a list of tiles.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dataset | list[Dataset]</code> <p>A single tile or a list of tiles augmented by a predicted <code>probabilities</code> layer, depending on the input.</p> <code>Dataset | list[Dataset]</code> <p>Each <code>probability</code> has type float32 and range [0, 1].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xr.Dataset | list[xr.Dataset]:\n    \"\"\"Run inference on a single tile or a list of tiles.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n        Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n        Each `probability` has type float32 and range [0, 1].\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    else:\n        raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.__init__","title":"<code>__init__(model_checkpoint)</code>","text":"<p>Initialize the segmenter.</p> <p>Parameters:</p> Name Type Description Default <code>model_checkpoint</code> <code>Path</code> <p>The path to the model checkpoint.</p> required Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __init__(self, model_checkpoint: Path | str):\n    \"\"\"Initialize the segmenter.\n\n    Args:\n        model_checkpoint (Path): The path to the model checkpoint.\n\n    \"\"\"\n    self.device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n    ckpt = torch.load(model_checkpoint, map_location=self.device)\n    self.config = validate_config(ckpt[\"config\"])\n    self.model = smp.create_model(**self.config[\"model\"], encoder_weights=None)\n    self.model.to(self.device)\n    self.model.load_state_dict(ckpt[\"statedict\"])\n    self.model.eval()\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.segment_tile","title":"<code>segment_tile(tile, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a tile.</p> <p>Parameters:</p> Name Type Description Default <code>tile</code> <code>Dataset</code> <p>The input tile, containing preprocessed, harmonized data.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def segment_tile(\n    self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n        Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tile to a tensor\n    tensor_tile = self.tile2tensor(tile)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tile = tensor_tile.unsqueeze(0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n    ).squeeze(0)\n\n    # Highly sophisticated DL-based predictor\n    # TODO: is there a better way to pass metadata?\n    tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n    tile[\"probabilities\"].attrs = {\n        \"long_name\": \"Probabilities\",\n    }\n    tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n    return tile\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.segment_tile_batched","title":"<code>segment_tile_batched(tiles, patch_size=1024, overlap=16, batch_size=8, reflection=0)</code>","text":"<p>Run inference on a list of tiles.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>list[Dataset]</code> <p>The input tiles, containing preprocessed, harmonized data.</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>The size of the overlap. Defaults to 16.</p> <code>16</code> <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles.</p> <code>8</code> <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>list[Dataset]</code> <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n        Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tiles to tensors\n    # TODO: maybe create a batched tile2tensor function?\n    # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n    tensor_tiles = self.tile2tensor_batched(tiles)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n    )\n\n    # Highly sophisticated DL-based predictor\n    for tile, probs in zip(tiles, probabilities):\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\n            \"long_name\": \"Probabilities\",\n        }\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n    return tiles\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.tile2tensor","title":"<code>tile2tensor(tile)</code>","text":"<p>Take a tile and convert it to a pytorch tensor.</p> <p>Respects the input combination from the config.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n    \"\"\"Take a tile and convert it to a pytorch tensor.\n\n    Respects the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n    # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        band_data = tile[feature_name]\n        # Normalize the band data\n        band_data = band_data * norm\n        bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n    return torch.stack(bands, dim=0)\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenter.tile2tensor_batched","title":"<code>tile2tensor_batched(tiles)</code>","text":"<p>Take a list of tiles and convert them to a pytorch tensor.</p> <p>Respects the the input combination from the config.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n    \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n    Respects the the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        for tile in tiles:\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n    # TODO: Test this\n    return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenterConfig","title":"<code>SMPSegmenterConfig</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Configuration for the segmentor.</p> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>class SMPSegmenterConfig(TypedDict):\n    \"\"\"Configuration for the segmentor.\"\"\"\n\n    input_combination: list[str]\n    model: dict[str, Any]\n    norm_factors: dict[str, float]\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenterConfig.input_combination","title":"<code>input_combination: list[str]</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenterConfig.model","title":"<code>model: dict[str, Any]</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.SMPSegmenterConfig.norm_factors","title":"<code>norm_factors: dict[str, float]</code>  <code>instance-attribute</code>","text":""},{"location":"ref/segmentation/#darts_segmentation.create_patches","title":"<code>create_patches(tensor_tiles, patch_size, overlap, return_coords=False)</code>","text":"<p>Create patches from a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_tiles</code> <code>Tensor</code> <p>The input tensor. Shape: (BS, C, H, W).</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches.</p> required <code>overlap</code> <code>int</code> <p>The size of the overlap.</p> required <code>return_coords</code> <code>bool</code> <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    logger.debug(f\"Creating {nh * nw} patches took {time.time() - start_time:.2f}s\")\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.patch_coords","title":"<code>patch_coords(h, w, patch_size, overlap)</code>","text":"<p>Yield patch coordinates based on height, width, patch size and margin size.</p> <p>Parameters:</p> Name Type Description Default <code>h</code> <code>int</code> <p>Height of the image.</p> required <code>w</code> <code>int</code> <p>Width of the image.</p> required <code>patch_size</code> <code>int</code> <p>Patch size.</p> required <code>overlap</code> <code>int</code> <p>Margin size.</p> required <p>Yields:</p> Type Description <code>tuple[int, int, int, int]</code> <p>tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def patch_coords(h: int, w: int, patch_size: int, overlap: int) -&gt; Generator[tuple[int, int, int, int], None, None]:\n    \"\"\"Yield patch coordinates based on height, width, patch size and margin size.\n\n    Args:\n        h (int): Height of the image.\n        w (int): Width of the image.\n        patch_size (int): Patch size.\n        overlap (int): Margin size.\n\n    Yields:\n        tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.\n\n    \"\"\"\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            yield y, x, patch_idx_y, patch_idx_x\n</code></pre>"},{"location":"ref/segmentation/#darts_segmentation.predict_in_patches","title":"<code>predict_in_patches(model, tensor_tiles, patch_size, overlap, batch_size, reflection, device=torch.device, return_weights=False)</code>","text":"<p>Predict on a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to use for prediction.</p> required <code>tensor_tiles</code> <code>Tensor</code> <p>The input tensor. Shape: (BS, C, H, W).</p> required <code>patch_size</code> <code>int</code> <p>The size of the patches.</p> required <code>overlap</code> <code>int</code> <p>The size of the overlap.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> required <code>reflection</code> <code>int</code> <p>Reflection-Padding which will be applied to the edges of the tensor.</p> required <code>device</code> <code>device</code> <p>The device to use for the prediction.</p> <code>device</code> <code>return_weights</code> <code>bool</code> <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The predicted tensor.</p> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    start_time = time.time()\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    for i, batch in track(enumerate(patches), total=len(patches), description=\"Predicting on patches\"):\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n    logger.debug(f\"Predicting took {time.time() - start_time:.2f}s\")\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"ref/superresolution/","title":"Superresolution Reference","text":""},{"location":"ref/superresolution/#darts_superresolution","title":"<code>darts_superresolution</code>","text":"<p>Image superresolution of Sentinel 2 imagery for the DARTS dataset.</p>"},{"location":"ref/superresolution/#darts_superresolution.hello","title":"<code>hello()</code>","text":"<p>Say hello to the user.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Greating message.</p> Source code in <code>darts-superresolution/src/darts_superresolution/__init__.py</code> <pre><code>def hello() -&gt; str:\n    \"\"\"Say hello to the user.\n\n    Returns:\n        str: Greating message.\n\n    \"\"\"\n    return \"Hello from darts-superresolution!\"\n</code></pre>"}]}