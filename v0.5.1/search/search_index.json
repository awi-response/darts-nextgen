{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DARTS nextgen","text":"<p>Panarctic Database of Active Layer Detachment Slides and Retrogressive Thaw Slumps from Deep Learning on High Resolution Satellite Imagery. This is te successor of the thaw-slump-segmentation (pipeline), with which the first version of the DARTS dataset was created.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li> <p>Download source code from the GitHub repository:</p> <pre><code>git clone git@github.com:awi-response/darts-nextgen.git\ncd darts-nextgen\n</code></pre> </li> <li> <p>Install the required dependencies:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> </li> <li> <p>Run the Sentinel 2 based pipeline on an area of interest:</p> <pre><code>uv run darts run-sequential-aoi-sentinel2-pipeline \\\n  --aoi-shapefile path/to/your/aoi.geojson \\\n  --model-files path/to/your/model/checkpoint \\\n  --start-date 2024-07 \\\n  --end-date 2024-09\n</code></pre> </li> </ol> <ul> <li> <p> Overview</p> <p>Get an overview on how this project works and how to run different pipelines.</p> <p> Get Started</p> </li> <li> <p> Install</p> <p>View detailed instructions on how to install the project for different environments and setup, e.g. with CUDA.</p> <p> Install</p> </li> <li> <p> Pipeline Components</p> <p>Learn about the different components of the pipeline and how they work together.</p> <p> Components</p> </li> <li> <p> API Reference</p> <p>View the API reference of the components.</p> <p> Reference</p> </li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>Before contributing please contact one of the authors and make sure to read the Contribution Guidelines.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>This page is also meant for internal documentation.</p>"},{"location":"contribute/#editor-setup","title":"Editor setup","text":"<p>There is only setup files provided for VSCode and no other editor (yet). A list of extensions and some settings can be found in the <code>.vscode</code>. At the first start, VSCode should ask you if you want to install the recommended extension. The settings should be automaticly used by VSCode. Both should provide the developers with a better experience and enforce code-style.</p>"},{"location":"contribute/#environment-setup","title":"Environment setup","text":"<p>Please read and follow the installation guide to setup the environment.</p>"},{"location":"contribute/#writing-docs","title":"Writing docs","text":"<p>The documentation is managed with Material for MkDocs. The documentation related dependencies are separated from the main dependencies and can be installed with:</p> <pre><code>uv sync --group docs\n</code></pre> <p>Note</p> <p>You should combine the <code>--group docs</code> with the extras you previously used, e.g. <code>uv sync --extra training --extra cuda126 --group docs</code>.</p> <p>To start the documentation server for live-update, run:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>In general all mkdocs commands can be run with <code>uv run mkdocs ...</code>.</p>"},{"location":"contribute/#recommended-notebook-header","title":"Recommended Notebook header","text":"<p>The following code snipped can be put in the very first cell of a notebook to already to add logging and initialize earth engine.</p> <pre><code>import logging\n\nfrom rich.logging import RichHandler\nfrom rich import traceback\n\nfrom darts.utils.earthengine import init_ee\nfrom darts.utils.logging import LoggingManager\n\nLoggingManager.setup_logging()\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\ntraceback.install(show_locals=True)  # Change to False if you encounter too large tracebacks\ninit_ee(\"ee-project\")  # Replace with your project\n</code></pre>"},{"location":"overview/","title":"Overview","text":"<p>This is a guide to help you, as a user / data engineer, get started with the project.</p> Table of Contents<ul> <li>Overview<ul> <li>Installation</li> <li>Running stuff via the CLI<ul> <li>Device selection</li> <li>Config files</li> <li>Log files</li> </ul> </li> <li>Running a pipeline based on Sentinel 2 data</li> <li>Running a pipeline based on PLANET data<ul> <li>Create a config file</li> <li>Run a the pipeline</li> </ul> </li> <li>Creating your own pipeline</li> </ul> </li> </ul>"},{"location":"overview/#installation","title":"Installation","text":"<p>To setup the environment for the project, you need to install uv and run the following command, assuming CUDA 12.6 is installed:</p> <pre><code>uv sync --extra cuda126\n</code></pre> <p>For other CUDA versions, see the installation guide.</p> <p>Training specific dependencies are optional and therefore not installed by default. To install them, add <code>--extra training</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> <p>To see if the installation was successful, you can run the following command:</p> <pre><code>uv run darts env-info\n</code></pre>"},{"location":"overview/#running-stuff-via-the-cli","title":"Running stuff via the CLI","text":"<p>The project provides a CLI to run different pipelines, training and other utility functions. Because the environment is setup with <code>uv</code>, you can run the CLI commands with <code>uv run darts ...</code>. If you manually active the environment with <code>source .venv/bin/activate</code>, you can run the CLI commands just via <code>darts ...</code> without <code>uv run</code>.</p> <p>To see a list of all available commands, run:</p> <pre><code>uv run darts --help\n</code></pre> <p>To get help for a specific command, run:</p> <pre><code>uv run darts the-specific-command --help\n</code></pre>"},{"location":"overview/#device-selection","title":"Device selection","text":"<p>By default, the CLI will automatic select the device for functions that support it. To force a specific device, you can use the <code>--device</code> parameter. Read more about the device selection in the device guide.</p>"},{"location":"overview/#config-files","title":"Config files","text":"<p>The CLI supports config files in TOML format to reduce the amount of parameters you need to pass or to safe different configurations. By default the CLI tries to load a <code>config.toml</code> file from the current directory. However, you can specify a different file with the <code>--config-file</code> parameter.</p> <p>As of right now, the CLI tries to match all parameters under the <code>darts</code> key of the config file, skipping not needed ones. For more information about the config file,  see the config guide..</p>"},{"location":"overview/#log-files","title":"Log files","text":"<p>By default the CLI sets up a logging handler at <code>INFO</code> level for the <code>darts</code> specific packages found in this workspace. The log-level can be changed via the <code>--verbose</code> flag of the CLI to set it to <code>DEBUG</code>. Running any command will output a logging file at the logging directory, which can be specified via the <code>--log-dir</code> parameter. The logging file will be named after the command and the current timestamp. If you want to change the logging behavior in python code, you can check out the logging guide.</p>"},{"location":"overview/#running-a-pipeline-based-on-sentinel-2-data","title":"Running a pipeline based on Sentinel 2 data","text":"<p>The <code>run-sequential-aoi-sentinel2-pipeline</code> automatically downloads and processes Sentinel 2 data based on an Area of Interest (AOI) in GeoJSON format. Before running you need access to a trained model. Note, that only special checkpoints can be used, as described in the architecture guide. In future versions, downloading of the model via huggingface will be supported, but for now you need to ask the developers for a valid model checkpoint.</p> <p>To run the pipeline run:</p> <pre><code>uv run darts run-sequential-aoi-sentinel2-pipeline --aoi-shapefile path/to/your/aoi.geojson --model-files path/to/your/model/checkpoint --start-date 2024-07 --end-date 2024-09\n</code></pre> <p>Run <code>uv run darts run-sequential-aoi-sentinel2-pipeline --help</code> for more configuration options.</p> <p>Pipeline v2</p> <p>The Pipeline v2 Guide provides a more in-depth explanation of the pipeline and its components.</p>"},{"location":"overview/#running-a-pipeline-based-on-planet-data","title":"Running a pipeline based on PLANET data","text":"<p>PLANET data cannot be downloaded automatically. Hence, you need to download the data manually and place it a directory of you choice.</p> <p>Example directory structure of a PLANET Orthotile:</p> <pre><code>    data/input/planet/PSOrthoTile/\n    \u251c\u2500\u2500 4372514/\n    \u2502  \u2514\u2500\u2500 5790392_4372514_2022-07-16_2459/\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_Analytic_metadata.xml\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_DN_udm.tif\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_SR.tif\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_metadata.json\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_udm2.tif\n    \u2502      \u2514\u2500\u2500 Thumbs.db\n    \u2514\u2500\u2500 4974017/\n        \u2514\u2500\u2500 5854937_4974017_2022-08-14_2475/\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_Analytic_metadata.xml\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_DN_udm.tif\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_SR.tif\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_metadata.json\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_udm2.tif\n            \u2514\u2500\u2500 Thumbs.db\n</code></pre> <p>Example directory structure of a PLANET Scene:</p> <pre><code>    data/input/planet/PSScene/\n    \u251c\u2500\u2500 20230703_194241_43_2427/\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427.json\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_metadata.xml\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_SR.tif\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_udm2.tif\n    \u2502  \u2514\u2500\u2500 20230703_194241_43_2427_metadata.json\n    \u2514\u2500\u2500 20230703_194243_54_2427/\n       \u251c\u2500\u2500 20230703_194243_54_2427.json\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_AnalyticMS_metadata.xml\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_AnalyticMS_SR.tif\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_udm2.tif\n       \u2514\u2500\u2500 20230703_194243_54_2427_metadata.json\n</code></pre> <p>Backcompatability of Sentinel 2 data</p> <p>For historical reasons, it is possible to run similar pipelines with Sentinel 2 data. For this, the Sentinel 2 data is expected to be in the same directory structure as the PLANET data. Hence, data from Google EarthEngine or from the Copernicus Cloud needs to be adjusted and scaled by the factor of <code>0.0001</code>.</p> <pre><code>data/input/sentinel2/\n\u251c\u2500\u2500 20210818T223529_20210818T223531_T03WXP/\n\u2502  \u251c\u2500\u2500 20210818T223529_20210818T223531_T03WXP_SCL_clip.tif\n\u2502  \u2514\u2500\u2500 20210818T223529_20210818T223531_T03WXP_SR_clip.tif\n\u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ/\n\u251c\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SCL_clip.tif\n\u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SR_clip.tif\n</code></pre>"},{"location":"overview/#create-a-config-file","title":"Create a config file","text":"<p>Because the minimal amount of parameters to pass for the PLANET pipeline, it is recommended to use a config file.</p> <p>An example config file can be found in the root of this repository called <code>config.toml.example</code>. You can copy this file to either <code>configs/</code> or copy and rename it to <code>config.toml</code>, so that you personal config will be ignored by git.</p> <p>Please change  <code>orthotiles-dir</code> and <code>scenes-dir</code> according to your PLANET download directory.</p> <p>You also need to specify the paths the model checkpoints (<code>model-dir</code>, <code>tcvis-model-name</code> and <code>notcvis-model-name</code>) you want to use. Note, that only special checkpoints can be used, as described in the architecture guide By setting <code>notcvis-model-name</code> to <code>None</code>, the pipeline will only use the TCVIS model.</p> <p>Auxiliary data (TCVIS and ArcticDEM) will be downloaded on demand into a datacube, which paths needs to be specified as well (<code>arcticdem-dir</code> and <code>tcvis-dir</code>).</p> <p>Finally, specify an output directory (<code>output-dir</code>), where you want to save the results of the pipeline.</p> <p>Of course you can tweak all other options aswell, also via the CLI. A list of all options can be found in the config guide or by running a command with the <code>--help</code> parameter.</p> Example config file <p>This is how an example config file could look like for the automated Sentinel 2 pipeline:</p> config.toml<pre><code>[darts]\nee-project = \"ee-tobias-hoelzer\"\n\n[darts.aoi]\naoi-shapefile = \"./data/banks_island.shp\"\nstart-date = \"2024-07\"\nend-date = \"2024-10\"\nmax-cloud-cover = 1 # %\n\n[darts.paths]\ninput-cache = \"./data/cache/s2gee\"\noutput-data-dir = \"./data/out\"\narcticdem-dir = \"./data/download/arcticdem\"\ntcvis-dir = \"./data/download/tcvis\"\nmodel-file = \"./models/s2-tcvis-final-large_2025-02-12.ckpt\"\n\n[darts.tiling]\nbatch-size = 8  # Reduce incase of memory issues\npatch-size = 512  # Reduce incase of memory issues\noverlap = 128  # Recommended to be 1/4 of patch-size\n</code></pre>"},{"location":"overview/#run-a-the-pipeline","title":"Run a the pipeline","text":"<p>Finally run the pipeline with the following command. Additional parameters can be passed via the CLI, which will overwrite the config file.</p> <pre><code>rye run darts run-sequential-planet-pipeline-fast --config-file path/to/your/config.toml\n</code></pre>"},{"location":"overview/#creating-your-own-pipeline","title":"Creating your own pipeline","text":"<p>The project was build with the idea in mind, that it is easy to create a new pipeline, with e.g. different parallelisation techniques. The architecture guide provides an overview of the project structure and the key components. A good starting point to understand the components is the intro to components. The build-in pipelines are a good example how the components can be used and put together to create a new pipeline.</p>"},{"location":"dev/arch/","title":"Architecture describtion","text":"<p>This repository is a workspace repository, managed by uv. Read more about workspaces at the uv docs. Each workspace-member starts with <code>darts-*</code> and can be seen as an own package or module, except the <code>darts</code> directory which is the top-level package. Each package has it's own internal functions and it's public facing API. The public facing API of each package MUST follow the API paradigms.</p> Table of Contents<ul> <li>Architecture describtion<ul> <li>Package overview<ul> <li>Conceptual migration from thaw-slump-segmentation</li> <li>Create a new package</li> <li>Versioning</li> </ul> </li> <li>PyTorch Model checkpoints</li> <li>API paradigms<ul> <li>Examples</li> <li>About the Xarray overhead with Ray</li> </ul> </li> </ul> </li> </ul>"},{"location":"dev/arch/#package-overview","title":"Package overview","text":"<p>Main design priciple</p> <p>Each package should provide components - stateless functions or stateful classes - which should be then combined either by the top-level <code>darts</code> package or by the user. Each component should take a Xarray Dataset as input and return a Xarray Dataset as output, with the exception of components of the <code>darts-aquisition</code> and <code>darts-export</code> packages. This way it should be easy to combine different components to build a custom pipeline for different parallelization frameworks and workflows.</p> Package Name Type Description (Major) Dependencies - all need Xarray <code>darts-acquisition</code> Data Fetches data from the data sources and created Xarray Datasets GEE, rasterio, ODC-Geo <code>darts-preprocessing</code> Data Combines Xarray Datasets from different acquisition sources and do some preprocessing on the data Cupy, Xarray-Spatial <code>darts-superresolution</code> Train Run a super-resolution model to scale Sentinel 2 images from 10m to 3m resolution PyTorch <code>darts-segmentation</code> Train Run the segmentation model PyTorch, segmentation_models_pytorch <code>darts-ensemble</code> Ensemble Ensembles the different models and run the multi-stage inference pipeline. PyTorch <code>darts-postprocessing</code> Data Further refines the output from an ensemble or segmentaion and binarizes the probs Scipy, Cucim <code>darts-export</code> Data Saves the results from inference and combines the result to the final DARTS dataset GeoPandas, rasterio <code>darts-utils</code> Data Shared utilities for data processing <p>The packages are currently designed around the v2 Pipeline.</p>"},{"location":"dev/arch/#conceptual-migration-from-thaw-slump-segmentation","title":"Conceptual migration from thaw-slump-segmentation","text":"<ul> <li>The <code>darts-ensemble</code> and <code>darts-postprocessing</code> packages is the successor of the <code>process-02-inference</code> and <code>process-03-ensemble</code> scripts.</li> <li>The <code>darts-preprocessing</code> and <code>darts-acquisition</code> packages are the successors of the <code>setup-raw-data</code> script and manual work of obtaining data.</li> <li>The <code>darts-export</code> package is splitted from the  <code>inference</code> script, should include the previous manual works of combining everything into the final dataset.</li> <li>The <code>darts-superresolution</code> package is the successor of the <code>superresolution</code> repository.</li> <li>The <code>darts-segmentation</code> package is the successor of the <code>train</code> and <code>prepare_data</code> script.</li> </ul> <p>The following diagram visualizes how the new packages are meant to work together. </p> <p>This is a mock</p> <p>This diagram is not realised in any form. It just exists for demonstrational purposes. To see an example of a realized pipeline based on this architecture please see the Pipeline v2 Guide</p>"},{"location":"dev/arch/#create-a-new-package","title":"Create a new package","text":"<p>A new package can easily created with:</p> <pre><code>uv init darts-packagename\n</code></pre> <p>uv creates a minimal project structure for us.</p> <p>The following things needs to be done updated and created:</p> <ol> <li>The <code>pyproject.toml</code> file inside the new package:</li> </ol> <p>Add to the <code>pyproject.toml</code> file inside the new package is the following to enable Ruff:</p> <pre><code>```toml\n[tool.ruff]\n# Extend the `pyproject.toml` file in the parent directory...\nextend = \"../pyproject.toml\"\n```\n\nPlease also provide a description and a list of authors to the file.\n</code></pre> <ol> <li> <p>The docs:     By updating the <code>notebooks/create_api_docs.ipynb</code>, running it and updating the <code>nav</code> section of the <code>mkdocs.yml</code> with the generated text.     To enable code detection, also add the package directory under <code>plugins</code> in the <code>mkdocs.yml</code>.</p> </li> <li> <p>The Readme of the package</p> </li> </ol>"},{"location":"dev/arch/#versioning","title":"Versioning","text":"<p>All packages have at all time the same version. The versioning is done via git-tags and the uv dynamic versioning tool. Hence, the version of the <code>pyproject.toml</code> of each subpackage is ignored and has no meaning.</p>"},{"location":"dev/arch/#pytorch-model-checkpoints","title":"PyTorch Model checkpoints","text":"<p>Each checkpoint is stored as a torch <code>.pt</code> tensor file. The checkpoint MUST have the following structure:</p> <pre><code>{\n    \"config\": {\n        \"model_framework\": \"smp\", # Identifier which framework or model was used\n        \"model\": { ... }, # Model specific hyperparameter which are needed to create the model\n        \"input_combination\": [ ... ], # List of strings of the names with which the model was trained, order is important\n        \"patch_size\": 1024, # Patch size on which the model was trained\n        ... # More model-framework specific parameter, e.g. normalization method and factors\n    },\n    \"statedict\": model.module.state_dict(),\n}\n</code></pre> <p>Pre-Deprecation warning</p> <p>It is planned to switch from our custom structure to huggingface model accessors.</p>"},{"location":"dev/arch/#api-paradigms","title":"API paradigms","text":"<p>The packages should pass the data as Xarray Datasets between each other. Datasets can hold coordinate information aswell as other metadata (like CRS) in a single self-describing object. Since different <code>tiles</code> do not share the same coordinates or metadata, each <code>tile</code> should be represented by a single Xarray <code>Dataset</code>.</p> <ul> <li>Each public facing API function which in some way transforms data should accept a Xarray Dataset as input and return an Xarray Dataset.</li> <li>Data can also be accepted as a list of Xarray Dataset as input and returned as a list of Xarray Datasets for batched processing.     In this case, concattenation should happend internally and on <code>numpy</code> or <code>pytorch</code> level, NOT on <code>xarray</code> abstraction level.     The reason behind this it that the tiles don't share their coordinates, resulting in a lot of empty spaces between the tiles and high memory usage.     The name of the function should then be <code>function_batched</code>.</li> <li>Each public facing API function which loads data should return a single Xarray Dataset for each <code>tile</code>.</li> <li>Data should NOT be saved to file internally, with <code>darts-export</code> as the only exception. Instead, data should returned in-memory as a Xarray Dataset, so the user / pipeline can decide what to save and when.</li> <li>Function names should be verbs, e.g. <code>process</code>, <code>ensemble</code>, <code>do_inference</code>.</li> <li>If a function is stateless it should NOT be part of a class or wrapper</li> <li>If a function is stateful it should be part of a class or wrapper, this is important for Ray</li> <li>Each Tile should be represented as a single <code>xr.Dataset</code> with each feature / band as <code>DataVariable</code>.</li> <li>Each DataVariable should have their <code>data_source</code> documented in the <code>attrs</code>, aswell as <code>long_name</code> and <code>units</code> if any for plotting.</li> <li>A <code>_FillValue</code> should also be set for no-data with <code>.rio.write_nodata(\"no-data-value\")</code>.</li> </ul> <p>Components</p> <p>The goal of these paradigms is to write functions which work as Components. Potential users can then later pick their components and put them together in their custom pipeline, utilizing their own parallelization framework.</p>"},{"location":"dev/arch/#examples","title":"Examples","text":"<p>Here are some examples, how these API paradigms should look like.</p> <p>This is a mock</p> <p>Even if some real packages are shown, these examples use mock-functions / non-existing functions and will not work .</p> <ol> <li> <p>Single transformation</p> <pre><code>import darts-package\nimport xarray as xr\n\n# User loads / creates the dataset (a single tile) by themself\nds = xr.open_dataset(\"...\")\n\n# User calls the function to transform the dataset\nds = darts-package.transform(ds, **kwargs)\n\n# User can decide by themself what to do next, e.g. save\nds.to_netcdf(\"...\")\n</code></pre> </li> <li> <p>Batched transformation</p> <pre><code>import darts_package\nimport xarray as xr\n\n# User loads / creates multiple datasets (hence, multiple tiles) by themself\ndata = [xr.open_dataset(\"...\"), xr.open_dataset(\"...\"), ...]\n\n# User calls the function to transform the dataset\ndata = darts_package.transform_batched(data, **kwargs)\n\n# User can decide by themself what to do next\ndata[0].whatever()\n</code></pre> </li> <li> <p>Load &amp; preprocess some data</p> <pre><code>import darts_package\n\n# User calls the function to transform the dataset\nds = darts_package.load(\"path/to/data\", **kwargs)\n\n# User can decide by themself what to do next\nds.whatever()\n</code></pre> </li> <li> <p>Custom pipeline example</p> <pre><code>from pathlib import Path\nimport darts_preprocessing\nimport darts_ensemble\n\nDATA_DIR = Path(\"./data/\")\nMODEL_FILE = Path(\"./models/model.pt\")\nOUT_DIR = Path(\"./out/\")\n\n# Inference is a stateful transformation, because it needs to load the model\n# Hence, the \nensemble = darts_ensemble.EnsembleV1(MODEL_FILE)\n\n# The data directory contains subfolders which then hold the input data\nfor dir in DATA_DIR:\n    name = dir.name\n\n    # Load the files from the processing directory\n    ds = darts_preprocessing.load_and_preprocess(dir)\n\n    # Do the inferencce\n    ds = ensemble.inference(ds)\n\n    # Save the results\n    ds.to_netcdf(OUT_DIR / f\"{name}-result.nc\")\n</code></pre> </li> <li> <p>Pipeline with Ray</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\nimport ray\nimport darts_preprocess\nimport darts_inference\nimport darts_export\n\nDATA_DIR = Path(\"./data/\")\nMODEL_DIR = Path(\"./models/\")\nOUT_DIR = Path(\"./out/\")\n\nray.init()\n\n# We need to wrap the Xarray dataset in a class, so that Ray can serialize it\n@dataclass\nclass Tile:\n    ds: xr.Dataset\n\n# Wrapper for ray\ndef open_dataset_ray(row: dict[str, Any]) -&gt; dict[str, Any]:\n    data = xr.open_dataset(row[\"path\"])\n    tile = Tile(data)\n    return {\n        \"input\": tile,\n    }\n\n# Wrapper for the preprocessing -&gt; Stateless\ndef preprocess_tile_ray(row: dict[str, Tile]) -&gt; dict[str, Tile]:\n    ds = darts_preprocess.preprocess(row[\"input\"].ds)\n    return {\n        \"preprocessed\": Tile(ds),\n        \"input\": row[\"input\"]\n    }\n\n# Wrapper for the inference -&gt; Statefull\nclass EnsembleRay:\n    def __init__(self):\n        self.ensemble = darts_inference.Ensemble.load(MODEL_DIR)\n\n    def __call__(self, row: dict[str, Tile]) -&gt; dict[str, Tile]:\n        ds = self.ensemble.inference(row[\"preprocessed\"].ds)\n        return {\n            \"output\": Tile(ds),\n            \"preprocessed\": row[\"preprocessed\"],\n            \"input\": row[\"input\"],\n        }\n\n# We need to add 'local:///' to tell ray that we want to use the local filesystem\nfiles = data.glob(\"*.nc\")\nfile_list = [f\"local:////{file.resolve().absolute()}\" for file in files]\n\nds = ray.data.read_binary_files(file_list, include_paths=True)\nds = ds.map(open_dataset_ray) # Lazy open\nds = ds.map(preprocess_tile_ray) # Lazy preprocess\nds = ds.map(EnsembleRay) # Lazy inference\n\n# Save the results\nfor row in ds.iter_rows():\n    darts_export.save(row[\"output\"].ds, OUT_DIR / f\"{row['input'].ds.name}-result.nc\")\n</code></pre> </li> </ol>"},{"location":"dev/arch/#about-the-xarray-overhead-with-ray","title":"About the Xarray overhead with Ray","text":"<p>Ray expects batched data to be in either numpy or pandas format and can't work with Xarray datasets directly. Hence, a wrapper with custom stacking functions is needed. This tradeoff is not small, however, the benefits in terms of maintainability and readability are worth it.</p> <p></p>"},{"location":"dev/auxiliary/","title":"Auxiliary Data and Datacubes","text":"<p>DARTS uses several auxiliary data - data which does not change between different scenes and / or time steps. Raster auxiliary data is stored in Zarr Datacubes.</p> <p>Currently, the following auxiliary data is used:</p> <ul> <li>ArcticDEM</li> <li>Tasseled Cap indices (Brightness, Greenness, Wetness)</li> </ul> <p>with more to come.</p>"},{"location":"dev/auxiliary/#arcticdem","title":"ArcticDEM","text":"<p>The ArcticDEM is downloaded via their STAC server using these extend files.</p> <p>The user can specify the download directory, where the ArcticDEM will be procedurally stored in a Zarr Datacube. The user can also specify the resolution of the ArcticDEM, which is either 2m, 10m or 32m. Each resolution is stored in their own Zarr Datacube.</p>"},{"location":"dev/auxiliary/#darts_acquisition.load_arcticdem","title":"darts_acquisition.load_arcticdem","text":"<pre><code>load_arcticdem(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which the tile should be loaded.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory where the ArcticDEM data is stored.</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in m.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The ArcticDEM tile, with a buffer applied. Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different. Final dataset is NOT matched to the reference CRS and resolution.</p> </li> </ul> Warning <p>Geobox must be in a meter based CRS.</p> Usage <p>Since the API of the <code>load_arcticdem</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_arcticdem\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\narcticdem = load_arcticdem(\n    optical.odc.geobox,\n    \"/path/to/arcticdem-parent-directory\",\n    resolution=2,\n    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n)\n\n# Now we can for example match the resolution and extent of the optical data:\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> <p>The <code>buffer</code> parameter is used to extend the region of interest by a certain amount of pixels. This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the resolution is not supported.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopwatch.f(\"Loading ArcticDEM\", printer=logger.debug, print_kwargs=[\"data_dir\", \"resolution\", \"buffer\", \"persist\"])\ndef load_arcticdem(\n    geobox: GeoBox, data_dir: Path | str, resolution: RESOLUTIONS, buffer: int = 0, persist: bool = True\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded.\n        data_dir (Path | str): The directory where the ArcticDEM data is stored.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in m.\n        buffer (int, optional): The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The ArcticDEM tile, with a buffer applied.\n            Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different.\n            Final dataset is NOT matched to the reference CRS and resolution.\n\n    Warning:\n        Geobox must be in a meter based CRS.\n\n    Usage:\n        Since the API of the `load_arcticdem` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_arcticdem\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        arcticdem = load_arcticdem(\n            optical.odc.geobox,\n            \"/path/to/arcticdem-parent-directory\",\n            resolution=2,\n            buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n        The `buffer` parameter is used to extend the region of interest by a certain amount of pixels.\n        This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.\n\n    Raises:\n        ValueError: If the resolution is not supported.\n\n    \"\"\"  # noqa: E501\n    odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n\n    match resolution:\n        case 2:\n            accessor = smart_geocubes.ArcticDEM2m(data_dir)\n        case 10:\n            accessor = smart_geocubes.ArcticDEM10m(data_dir)\n        case 32:\n            accessor = smart_geocubes.ArcticDEM32m(data_dir)\n        case _:\n            raise ValueError(f\"Resolution {resolution} not supported, only 2m, 10m and 32m are supported\")\n\n    accessor.assert_created()\n\n    arcticdem = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem[\"datamask\"] = arcticdem.datamask.astype(\"uint8\")\n\n    return arcticdem\n</code></pre>"},{"location":"dev/auxiliary/#tasseled-cap-indices-tcvis","title":"Tasseled Cap indices (TCVIS)","text":"<p>The TCVIS data is downloaded from Google Earth-Engine (GEE) using the TCVIS collection from Ingmar Nitze: <code>\"users/ingmarnitze/TCTrend_SR_2000-2019_TCVIS\"</code>.</p>"},{"location":"dev/auxiliary/#darts_acquisition.load_tcvis","title":"darts_acquisition.load_tcvis","text":"<pre><code>load_tcvis(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox to load the data for.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory to store the downloaded data for faster access for consecutive calls.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The TCVIS dataset.</p> </li> </ul> Usage <p>Since the API of the <code>load_tcvis</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_tcvis\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\ntcvis = load_tcvis(\n    optical.odc.geobox,\n    \"/path/to/tcvis-parent-directory\",\n)\n\n# Now we can for example match the resolution and extent of the optical data:\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopwatch.f(\"Loading TCVIS\", printer=logger.debug, print_kwargs=[\"data_dir\", \"buffer\", \"persist\"])\ndef load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the TCVIS for the given geobox, fetch new data from GEE if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox to load the data for.\n        data_dir (Path | str): The directory to store the downloaded data for faster access for consecutive calls.\n        buffer (int, optional): The buffer around the geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset.\n\n    Usage:\n        Since the API of the `load_tcvis` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_tcvis\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        tcvis = load_tcvis(\n            optical.odc.geobox,\n            \"/path/to/tcvis-parent-directory\",\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"  # noqa: E501\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n\n    # We want to assume that the datacube is already created to be save in a multi-process environment\n    accessor.assert_created()\n\n    tcvis = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Rename to follow our conventions\n    tcvis = tcvis.rename_vars(\n        {\n            \"TCB_slope\": \"tc_brightness\",\n            \"TCG_slope\": \"tc_greenness\",\n            \"TCW_slope\": \"tc_wetness\",\n        }\n    )\n\n    return tcvis\n</code></pre>"},{"location":"dev/auxiliary/#why-zarr-datacubes","title":"Why Zarr Datacubes?","text":"<p>Zarr is a file format for storing chunked, compressed, N-dimensional arrays. It is designed to store large arrays of data, and to facilitate fast and efficient IO. Zarr works well integrated with Dask and Xarray.</p> <p>By storing the auxiliary data in Zarr Datacubes, it is much easier and faster to access the data of interest. If we would use GeoTiffs, we would have to first create a Cloud-Optimized GeoTiff (COG), which is basically an ensemble (mosaic) of multiple GeoTiffs. Then we would have to read from the COG, which behind the scenes would open multiple GeoTiffs and crops them to fit the region of interest. E.g. Opening a specific region of interest 10km x 10km from a 2m resolution COG would take up to 2 minutes, if the COGs extend is panarctic. Opening the same region from a Zarr Datacube takes less than 1 second.</p> <p>Inspiration</p> <p>This implementation and concept is heavily inspired by EarthMovers implementation of serverless datacube generation.</p>"},{"location":"dev/auxiliary/#procedural-download","title":"Procedural download","text":"<p>Info</p> <p>The currently used auxiliary data is downloaded on demand, only data actually used is downloaded and stored on your local machine. Hence, the stored datacubes can be thought of as a cache, which is filled with data as needed.</p> <p>There are currently two implementations of the procedural download used: a cloud based STAC download and a download via Google Earth-Engine.</p> <p>Because the single tiles of the STAC mosaic can be overlapping and intersect with multiple Zarr chunks, the STAC download is slightly more complicated. Since Google Earth-Engine allows for exact geoboxes, download of the exact chunks is possible. This reduces the complexity of the download.</p> STAC GEE 1. ROI 2. ROI <p>The above graphics shows the difference between loading data from STAC (left) and Google Earth-Engine (right). With the STAC download, the data is downloaded from a mosaic of tiles, which can be overlapping with each other and cover multiple Zarr chunks. It may occur that a chunk is not fully covered by the STAC mosaic, which results in only partial loaded chunks. In such cases, the missing data in these chunks will be updated if the other intersecting tile is downloaded, which may occur to a later time if a connected ROI is requested. The download process is much easier for GEE, since one can request the exact geoboxes of the Zarr chunks and GEE will handle the rest. Hence, chunks will always be fully covered by the downloaded data.</p> <p>Regarding the open ROI process, both implementations follow the same principle:</p> <ol> <li>Check which Tiles / Chunks intersect with the region of interest</li> <li>Dowload all new Tiles / Chunks</li> <li>Store the new Tiles / Chunks in their specific Zarr chunks</li> <li>Return the region of interest of the Zarr Datacube</li> </ol>"},{"location":"dev/auxiliary/#stac-download","title":"STAC download","text":""},{"location":"dev/auxiliary/#google-earth-engine-download","title":"Google Earth-Engine download","text":""},{"location":"dev/cuda_fixes/","title":"Problems with CUDA","text":"<p>This is a collection of known issues and potential fixes for CUDA-related problems in the codebase.</p>"},{"location":"dev/cuda_fixes/#cucim-import","title":"CUCIM import","text":"<pre><code>python3: cufile_worker_thread.h:57: virtual void CUFileThreadPoolWorker::run(): Assertion `0' failed.\n</code></pre> <p><code>cufile.log</code>:</p> <pre><code> 07-05-2025 20:29:31:49 [pid=747532 tid=748005] ERROR  cufio_core:55 Threadpool Thread ID:  139915782243904 cuDevicePrimaryCtxRetain failed with error 2\n</code></pre> <p>What happend? Probably, another user on the cluster is using a GPU in exclusive mode, which prevents other users from using it. CUCIM is trying to access the GPU, but fails because the GPU is not available.</p> <p>Please read this related community issue:</p> <p>From the snippet pasted here, it looks like the cuda device is not available at this time and as a result the cuDevicePrimaryCtxRetain cuda call fails. CUDA_ERROR_DEVICE_UNAVAILABLE = 46 This indicates that requested CUDA device is unavailable at the current time. Devices are often unavailable due to use of CU_COMPUTEMODE_EXCLUSIVE_PROCESS or CU_COMPUTEMODE_PROHIBITED. My suspicion is that one of the compute mode settings are effected in this environment preventing the thread to retain the context on the same device.</p> <p>How to fix? Just set the <code>CUDA_VISIBLE_DEVICES</code> environment variable to the device you want to use. Don't forget to set the <code>--device</code> argument of the CLI to <code>cuda</code> is such case, since our auto detection will not work in this case.</p> <pre><code>export CUDA_VISIBLE_DEVICES=0\n</code></pre>"},{"location":"dev/cuda_fixes/#nvrtc","title":"NVRTC","text":"<pre><code>Unable to replicate\n</code></pre> <p>What happend? The LD_LIBRARY_PATH is either not set correctly or the <code>cuda-nvrtc</code> package is not installed in the conda environment.</p> <p>How to fix? Create a conda environment with the <code>cuda-nvrtc</code> package installed. You can do this by running the following command:</p> <pre><code>conda create -n cuda_nvrtc -c nvidia cuda-nvrtc\n</code></pre> <p>This environment must NOT be activated, it is just to install the library files somewhere. Check:</p> <pre><code>$ conda list -n cuda120\n# packages in environment at /path/to/.conda/envs/cuda_nvrtc:\n#\n# Name                    Version                   Build  Channel\ncuda-nvrtc                12.1.105                      0    nvidia\n</code></pre> <p>Now you can set the <code>LD_LIBRARY_PATH</code> to include the path to the <code>cuda-nvrtc</code> library:</p> <pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/.conda/envs/cuda120/lib/\n</code></pre> <p>This will allow numba to find the <code>libnvrtc.so</code> library when it is needed:</p> <pre><code>$ ls -l /path/to/.conda/envs/cuda120/lib/\ntotal 62234\nlrwxrwxrwx 1 jokuep001 hpc_user       29 Mar 24 16:25 libnvrtc-builtins.so.12.1 -&gt; libnvrtc-builtins.so.12.1.105\n-rwxrwxr-x 3 jokuep001 hpc_user  6846016 Apr  4  2023 libnvrtc-builtins.so.12.1.105\nlrwxrwxrwx 1 jokuep001 hpc_user       20 Mar 24 16:25 libnvrtc.so.12 -&gt; libnvrtc.so.12.1.105\n-rwxrwxr-x 3 jokuep001 hpc_user 56875328 Apr  4  2023 libnvrtc.so.12.1.105\n</code></pre>"},{"location":"dev/cuda_fixes/#ptxas","title":"PTXAS","text":"<pre><code>LinkerError: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION                                                                                                                                                                                                 \nptxas application ptx input, line 9; fatal   : Unsupported .version 8.7; current version is '8.2'\n</code></pre> <p>What happend? There is a mismatch between the CUDA version used by the projects code and the CUDA version used by the system. This error is caused by a mismatch between the OS and the CUDA version of the system. In our case, the default CUDA version of the system was 12.6 and Ubuntu 22.04. CUDA 12.6 expects for our use case a PTX version of 8.7, but Ubutnu 22.04 only supports PTX version 8.2.</p> <p>How to fix?</p> <p>Write a script to set all the environment variables (these may differ on your system):</p> <pre><code>export CUDA_PATH=\"/usr/local/cuda-12.2\"\nexport CUDA_HOME=\"/usr/local/cuda-12.2\"\nexport PATH=\"/usr/local/cuda-12.2/bin:$PATH\"\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.2/extras/CUPTI/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.2/compat:$LD_LIBRARY_PATH\n\nexport NUMBA_CUDA_DRIVER=\"/usr/local/cuda-12.2/compat/libcuda.so\"\nexport NUMBA_CUDA_INCLUDE_PATH=\"/usr/local/cuda-12.2/include\"\n</code></pre> <p>and then source it:</p> <pre><code>source /path/to/your/script.sh\n</code></pre> <p>Just running <code>sh script.sh</code> will not work, since it will create a new shell and the environment variables will not be set in the current shell. You can also add these lines to your <code>.bashrc</code> file, so they are set automatically when you open a new terminal. However, this may cause problems for other CUDA applications and projects, so be careful with this approach. Of course, you can also set these variables manually in the terminal before running the code.</p>"},{"location":"dev/cuda_fixes/#template","title":"Template","text":"<p>\"Error message\"</p> <p>What happend? ...</p> <p>How to fix? ...</p>"},{"location":"dev/docs/","title":"Documentation organization","text":"<p>Guides should help users to get familar on how to use the pipelines or how they could implement certain parts of our project / components.</p> <p>The dev section should help us developers to understand the codebase and decision making.</p>"},{"location":"dev/docs/#todos","title":"ToDos","text":"<ul> <li> Add components guide</li> <li> Further document them</li> <li> Document the required inputs and output data_vars and attrs<ul> <li> In API ref</li> <li> In Components Guide</li> </ul> </li> </ul>"},{"location":"guides/components/","title":"Introduction to the darts components","text":"<p>Components</p> <p>The idea behind the Architecture of <code>darts-nextgen</code> is to provide <code>Components</code> to the user. Users should pick their components and put them together in their custom pipeline, utilizing their own parallelization framework. Pipeline v2 shows how this could look like for a simple sequential pipeline - hence without parallelization framework.</p> <p>Currently, the implemented components are:</p> Component Statefull? and why? Bound darts_acquisition.load_arcticdem Stateless Network-IO + Disk-IO darts_acquisition.load_tcvis Stateless Network-IO + Disk-IO darts_acquisition.load_planet_scene Stateless Disk-IO darts_acquisition.load_planet_masks Stateless Disk-IO darts_acquisition.load_s2_scene Stateless Disk-IO darts_acquisition.load_s2_from_gee Stateless Network-IO (+ Disk-IO) + CPU darts_acquisition.load_s2_from_stac Stateless Network-IO (+ Disk-IO) darts_acquisition.load_s2_masks Stateless Disk-IO + CPU darts_preprocessing.preprocess_legacy_fast Stateless CPU + GPU darts_preprocessing.preprocess_v2 Stateless CPU + GPU darts_segmentation.segment.SMPSegmenter.segment_tile Statefull: Model-Weights GPU darts_ensemble.EnsembleV1.segment_tile Statefull: Model-Weights GPU darts_postprocessing.prepare_export Stateless CPU + GPU darts_export.export_tile Stateless Disk-IO"},{"location":"guides/components/#component-outputs","title":"Component Outputs","text":"<p>Incomplete</p> <p>This section is incomplete and will be updated in the future.</p> <p>All component-tiles are <code>xr.Datasets</code> which have geospatial coordinates <code>x</code>, <code>y</code> and a spatial reference  <code>spatial_ref</code> (from rioxarray / odc-geo) as coordinates.. The following documents the input and output of each component.</p>"},{"location":"guides/components/#acquisition-load-arcticdem","title":"Acquisition: Load ArcticDEM","text":""},{"location":"guides/components/#input","title":"Input","text":"<p>(Acquisition components do not have an input in form of a <code>xr.Dataset</code>)</p>"},{"location":"guides/components/#output","title":"Output","text":"DataVariable shape dtype no-data attrs note <code>dem</code> (x, y) float32 nan data_source, long_name, units <code>dem_datamask</code> (x, y) bool False data_source, long_name, units"},{"location":"guides/components/#preprocessing-legacy-fast","title":"Preprocessing: Legacy Fast","text":""},{"location":"guides/components/#input_1","title":"Input","text":"DataVariable shape dtype no-data attrs note <code>blue</code> (x, y) uint16 0 data_source, long_name, units <code>green</code> (x, y) uint16 0 data_source, long_name, units <code>red</code> (x, y) uint16 0 data_source, long_name, units <code>nir</code> (x, y) uint16 0 data_source, long_name, units <code>dem</code> (x, y) float32 nan data_source, long_name, units <code>dem_datamask</code> (x, y) bool False data_source, long_name, units <code>tc_brightness</code> (x, y) uint8 - data_source, long_name <code>tc_greenness</code> (x, y) uint8 - data_source, long_name <code>tc_wetness</code> (x, y) uint8 - data_source, long_name"},{"location":"guides/components/#output_1","title":"Output","text":"DataVariable shape dtype no-data attrs note [Input] <code>ndvi</code> (x, y) uint16 0 data_source, long_name Values between 0-20.000 (+1, *1e4) <code>relative_elevation</code> (x, y) int16 0 data_source, long_name, units <code>slope</code> (x, y) float32 nan data_source, long_name"},{"location":"guides/components/#segmentation-ensemble-output","title":"Segmentation / Ensemble Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs [Output from Preprocessing] <code>probabilities</code> (x, y) float32 nan long_name <code>probabilities-model-X*</code> (x, y) float32 nan long_name <p>*: optional intermedia probabilities in an ensemble</p>"},{"location":"guides/components/#postprocessing-output","title":"Postprocessing Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs note [Output from Preprocessing] <code>probabilities_percent</code> (x, y) uint8 255 long_name, units Values between 0-100 <code>binarized_segmentation</code> (x, y) uint8 - long_name"},{"location":"guides/config/","title":"Config Files","text":"<p>The <code>darts</code> CLI support passing parameters via a config file in TOML format. This can be useful to reduce the amount of parameters you need to pass or to safe different configurations. In general, the CLI tries to match all parameters under the <code>darts</code> key of the config file, skipping not needed ones.</p>"},{"location":"guides/config/#example-usage","title":"Example usage","text":"<p>Let's take a closer look with the example command <code>darts hello</code>. This command has the following function signature:</p> <pre><code>def hello(name: str, n: int = 1):\n    \"\"\"Say hello to someone.\n\n    Args:\n        name (str): The name of the person to say hello to\n        n (int, optional): The number of times to say hello. Defaults to 1.\n\n    Raises:\n        ValueError: If n is 3.\n\n    \"\"\"\n    for i in range(n):\n        logger.debug(f\"Currently at {i=}\")\n        if n == 3:\n            raise ValueError(\"I don't like 3\")\n        logger.info(f\"Hello {name}\")\n</code></pre> <p>Let's run the command without making a config file:</p> <pre><code>$ uv run darts hello Alice\nDEBUG Currently at i=0\nINFO Hello Alice\n</code></pre> <p>Now specify a config file <code>config.toml</code>:</p> <pre><code>[darts]\nname = \"Not Alice\"\nn = 2\n</code></pre> <p>And run the same command:</p> <pre><code>$ uv run darts hello Alice\nDEBUG Currently at i=0\nINFO Hello Alice\nDEBUG Currently at i=1\nINFO Hello Alice\n</code></pre> <p>The <code>name</code> parameter is still taken from the CLI, while the <code>n</code> parameter is taken from the config file.</p> <p>Because the CLI utilized a custom TOML parser to parse the config file and pass it to the CLI tool cyclopts, only parameters under the <code>darts</code> key are considered. Subheading keys are not considered, but can be used to structure the config file:</p> <pre><code>[darts]\nname = \"Not Alice\"\n\n[darts.numbers]\nn = 2\n</code></pre> <p>The <code>numbers</code> key is ignored by the CLI, hence <code>n</code> will be add to the command as before.</p> <p>Warning</p> <p>The only parameters not passed from the config file are the <code>--config-file</code>, <code>--verbose</code>, <code>--tracebacks-show-locals</code> and <code>--log-dir</code> parameters. These parameters are evaluated before the config file is parsed, hence it is not possible to specify the logging directory via the config file.</p>"},{"location":"guides/config/#real-world-example-with-sentinel-2-processing","title":"Real world example with Sentinel 2 processing","text":"<p>Sentinel 2 processing via. Area of Interest file:</p> <pre><code>[darts]\nee-project = \"your-ee-project\"\ndask-worker = 4\n\n[darts.paths]\ninput-cache = \"./data/cache/s2gee\"\noutput-data-dir = \"./data/out\"\narcticdem-dir = \"./data/datacubes/arcticdem\"\ntcvis-dir = \"./data/datacubes/tcvis\"\nmodel-file = \"./models/s2-tcvis-final-large_2025-02-12.ckpt\"\n</code></pre> <p>Running the command:</p> <pre><code>uv run darts run-sequential-aoi-sentinel2-pipeline --aoi-shapefile path/to/your/aoi.geojson --start-date 2024-07 --end-date 2024-09\n</code></pre>"},{"location":"guides/debugging/","title":"Debugging","text":"<p>CUDA related</p> <p>There is a complete page about CUDA related errors.</p> <p>To produce more (hopefully) helpful output from the CLI, two flags can be set:</p> <ul> <li><code>--verbose</code> This will set the log-level to DEBUG. The output becomes much more noisy, but contains a lot of useful information.</li> <li><code>--tracebacks-show-locals</code> This will enable the rich traceback feature to not only show the traceback when errors are occuring, but also the local variables in that scope. This, however, is much slower and can potentially output to much information for the terminal to handle.</li> </ul>"},{"location":"guides/devices/","title":"Devices","text":"<p>Supported devices</p> <p>As of right now, only CUDA and CPU devices are supported. How to install a working Python environment for either case please refer to the installation guide.</p> <p>Some functions can be run on the GPU if a CUDA device is available and the python environment is properly installed with CUDA enabled. These functions will automatically detect if a CUDA device is available and will use it if so. It is possible to also force the use of a specific device through the <code>device</code> parameter of the respective function. For most GPU-capable functions it is possible to pass either <code>cpu</code> or <code>cuda</code> as a string to the <code>device</code> parameter. In a multi-GPU setup, the device can be specified by passing the device index as an integer (e.g. <code>0</code> for the first GPU, <code>1</code> for the second GPU, etc.). However, functions which use PyTorch expect the device to be a PyTorch device object, so you need to pass <code>torch.device(\"cuda:0\")</code> instead of just <code>0</code>. Which type of device is expected is documented in the respective function documentation.</p> <p>As of now, the following functions can be run on the GPU:</p> <ul> <li>darts_preprocessing.preprocess_legacy_fast - <code>device</code>: <code>\"cpu\" | \"cuda\" | int</code></li> <li>darts_preprocessing.preprocess_v2 - <code>device</code>: <code>\"cpu\" | \"cuda\" | int</code></li> <li>darts_postprocessing.prepare_export - <code>device</code>: <code>\"cpu\" | \"cuda\" | int</code></li> <li>darts_segmentation.segment.SMPSegmenter - <code>device</code>: <code>torch.device</code></li> <li>darts_ensemble.EnsembleV1 - <code>device</code>: <code>torch.device</code></li> </ul>"},{"location":"guides/installation/","title":"Advanced Installation","text":"<p>Prereq:</p> <ul> <li>uv: <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code></li> <li>gcloud CLI</li> <li>cuda (optional for GPU support)</li> </ul> <p>This project uses <code>uv</code> to manage the python environment. If you are not familiar with <code>uv</code> yet, please read their documentation first. Please don't use <code>pip</code> or <code>conda</code> to install the dependencies, as this often leads to problems. We have spend a lot of time making sure that the install process is easy and quick, but this is only possible with <code>uv</code>. So please use it.</p> <p>In general the environment can be installed with <code>uv sync</code>. However, this project depends on some libraries (torch and torchvision) which don't get installed per default. Therefore you need to specify an extra flag to install the correct dependencies, e.g.</p> <pre><code>uv sync --extra cuda126\n</code></pre> <p>This will install the environment with the correct dependencies for CUDA 12.6. The following sections will explain the different extra flags and groups which can be used to install the environment for different purposes and systems.</p>"},{"location":"guides/installation/#cuda-and-cpu-only-installations","title":"CUDA and CPU-only installations","text":"<p>Several CUDA versions can be used, but it may happen that some problems occur on different systems. Currently CUDA 11.8, 12.1, and 12.6 are supported, but sometimes other versions work as well. We use python extra dependencies, so it is possible to specify the CUDA version via an <code>--extra</code> flag in the <code>uv sync</code> command.</p> <p>You can check the currently installed CUDA version via:</p> <pre><code>nvidia-smi\n# Look at the top right corner for the CUDA version\n</code></pre> <p>Warning</p> <p>If the <code>nvidia-smi</code> command is not found, you might need to install the nvidia drivers. Be very cautious with the installation of the driver, rather read the documentation with care.</p> <p>To install the python environment for a specific CUDA version use one of the following commands respectively:</p> <pre><code>uv sync --extra cuda118\nuv sync --extra cuda121\nuv sync --extra cuda126\n</code></pre> <p>CUDA version missmatch</p> <p>Sometimes it is possible to use a different CUDA version for the python packages than the one installed. E.g. we tested our code on a system with CUDA 12.2 installed, but used the python packages for CUDA 12.1. This is not recommended, but sometimes it works.</p> <p>Install the python environment for CPU-only use:</p> <pre><code>uv sync --extra cpu\n</code></pre> <p>Danger</p> <p>Either <code>--extra cpu</code> or <code>--extra cudaXXX</code> must be specified. Without important libraries like PyTorch will not be installed and the environment will not work.</p>"},{"location":"guides/installation/#workaround-for-cuda-related-errors","title":"Workaround for CUDA related errors","text":"<p>If CUDA is not installed correctly, some CUDA optional packages are missing or the wrong version of CUDA is installed, conda / mamba can be used as a workaround.</p> <p>First create a new conda environment and activate it:</p> <pre><code>mamba create -n darts-nextgen-cuda-env\nmamba activate darts-nextgen-cuda-env\n</code></pre> <p>Then install CUDA toolkit and required system packages via conda / mamba:</p> <pre><code>mamba install cuda-toolkit nvidia::cuda-nvrtc\n...\n</code></pre> <p>Now you can (while the conda / mamba environment is active) sync your uv environment.</p> <pre><code>uv sync --...\n</code></pre>"},{"location":"guides/installation/#training-specific-dependencies","title":"Training specific dependencies","text":"<p>Training specific dependencies are optional and therefore not installed by default. To install them, add <code>--extra training</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> <p>psycopg2</p> <p>The training dependencies depend on psycopg2, which requires postgresql installed on your system.</p>"},{"location":"guides/installation/#packages-for-the-documentation","title":"Packages for the documentation","text":"<p>Packages which are used to create this documentation are not installed by default and are not available via as an extra. Instead they are installed as part of an optional <code>dependency-group</code>, or <code>group</code> for short. To install the documentation dependencies, add <code>--group docs</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training --group docs\n</code></pre>"},{"location":"guides/logging/","title":"Logging Guide","text":"<p>We want to use the python logging module as much as possible to traceback errors and document the pipeline processes. Furthermore, we want to configure each logger with the <code>RichHandler</code>, which prettyfies the output with rich.</p>"},{"location":"guides/logging/#setup-guide","title":"Setup Guide","text":"<p>Currently, all setup related to logging is found in the <code>darts.utils.logging.py</code> file. It contains two functions:</p> <ol> <li>A setup function which sets the log-level for all <code>darts.*</code> logger and add default options to xarray and pytorch to supress arrays. See how to supress arrays.</li> <li>A function which adds a file and a rich log handler.</li> </ol> <p>Both functions are used in the CLI setup but can also be called from e.g. a notebook. The recommended approach for handling logging within a notebook is the following:</p> <pre><code>import logging\nfrom rich.logging import RichHandler\nfrom darts.utils.logging import LoggingManager\n\nLoggingManager.setup_logging(verbose=True)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\n</code></pre> <p>This way the notebook won't spam logfiles everywhere and we still have control over our rich handler.</p>"},{"location":"guides/logging/#usage-guide","title":"Usage Guide","text":"<p>For logging inside a darts-package should be done without any further configuration:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__.replace(\"darts_\", \"darts.\")) # don't replace __name__\n</code></pre> <p>Logging at the top-level <code>darts</code> package can just use a <code>__name__</code> logger:</p> <pre><code>import loggin\n\nlogger = logging.getLogger(__name__) # don't replace __name__\n</code></pre>"},{"location":"guides/logging/#supressing-arrays","title":"Supressing Arrays","text":"<p>When printing or logging large numpy arrays a lot of numbers get truncated, however the array still takes a lot of space. Using <code>lovely_numpy</code> and <code>lovely_tensor</code> can help here:</p> <pre><code>import numyp as np\nimport torch\nimport xarray as xr\nfrom lovely_numpy import lo\nimport lovely_tensors\n\nlovely_tensors.monkey_patch()\nlovely_tensors.set_config(color=False)  # Disable colored output, which is better for logging\nxr.set_options(display_expand_data=False)\n\na = np.zeros((8, 1024, 1024))\nla = lo(a)\nda = xr.DataArray(a)\nt = torch.tensor(a)\n\nlogger.warning(la)\nlogger.warning(da)\nlogger.warning(t)\n</code></pre>"},{"location":"guides/logging/#dev-guide","title":"Dev Guide","text":"<p>This section should cover best practices developing inside the darts package.</p> <ul> <li>Fail fast</li> <li>Logging exceptions to debug</li> </ul>"},{"location":"guides/logging/#exceptions","title":"Exceptions","text":"<p>When logging exceptions from within an component, the complete exception should be logged with the traceback by setting <code>exc_info=True</code> to debug and then re-raised:</p> <pre><code>try:\n    # some code\nexcept Exception as e:\n    logger.debug(e, exc_info=True)\n    raise\n</code></pre>"},{"location":"guides/pipeline-v2/","title":"Pipeline v2","text":"<p>The following document describes the pipeline which lead to the DARTS v1 dataset and will potentially lead to the DARTS v2 dataset. The orginial v1 dataset was not created with this repository, however, a newer, faster version of this pipeline is implemented here, which still uses the exact same pipeline-steps. Hence, it should be possible to re-create the DARTS v1 dataset with this repository. The implemented pipeline in this repository could potentially be used for future iterations and releases of the DARTS dataset.</p> <p>In addition to the PLANET version of the DARTS dataset, the pipeline also supports Sentinel 2 imagery as optical input, resulting in a lower spatial resolution (10m instead of 3m).</p> <p>Note</p> <p>The v1 / v2 pipeline is also aliased by <code>legacy</code> pipeline somewhere deep in the code.</p> <p>As of right now, three basic realisation of the v1 pipeline are implemented:</p> <ul> <li><code>run-sequential-planet-pipeline-fast</code></li> <li><code>run-sequential-sentinel2-pipeline-fast</code></li> <li><code>run-sequential-aoi-sentinel2-pipeline</code></li> </ul> <p>The naming convention has changed a lot and probably will further change with more pipeline realisations becoming implemented. <code>sequential</code> indicates that the pipeline runs without any parallelization framework.</p> <p>The pipeline currently consists of the following steps:</p> <ol> <li>Load the optical and auxiliary data     This step depends on the realisation of the pipeline.     Either darts_acquisition.load_planet_scene, darts_acquisition.load_s2_scene, darts_acquisition.load_s2_from_gee or darts_acquisition.load_s2_from_stac.     Also loads the masks if not loaded from gee or stac: darts_acquisition.load_planet_masks or darts_acquisition.load_s2_masks, for the gee and stac versions the masks are already included.     For the auxiliary data: darts_acquisition.load_arcticdem and darts_acquisition.load_tcvis</li> <li>Preprocess the optical data: darts_preprocessing.preprocess_legacy_fast or darts_preprocessing.preprocess_v2.</li> <li>Segment the optical data: darts_segmentation.segment.SMPSegmenter.segment_tile or darts_ensemble.EnsembleV1.segment_tile.</li> <li>Postprocess the segmentation and make it ready for export: darts_postprocessing.prepare_export.</li> <li>Export the data: darts_export.export_tile.</li> </ol> <p></p> <p>A very simplified version of this implementation looks like this:</p> <pre><code>from darts_acquisition import load_arcticdem, load_tcvis\nfrom darts_segmentation import SMPSegmenter\nfrom darts_export import export_tile, missing_outputs\nfrom darts_postprocessing import prepare_export\nfrom darts_preprocessing import preprocess_legacy_fast\nfrom darts_acquisition.s2 import load_s2_from_gee\n\ns2id = \"20230701T194909_20230701T195350_T11XNA\"\narcticdem_dir = \"/path/to/arcticdem\"\ntcvis_dir = \"/path/to/tcvis\"\nmodel_file = \"/path/to/model.pt\"\noutpath = \"/path/to/output\"\n\nsegmenter = SMPSegmenter(model_file)\n\ntile = load_s2_from_gee(s2id)\n\narcticdem = load_arcticdem(\n    tile.odc.geobox,\n    arcticdem_dir,\n    resolution=10,\n    buffer=ceil(100 / 2 * sqrt(2)),\n)\n\ntcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n\ntile = preprocess_legacy_fast(tile, arcticdem, tcvis)\n\ntile = segmenter.segment_tile(tile)\n\ntile = prepare_export(tile)\n\nexport_tile(tile, outpath)\n</code></pre> <p>Further reading</p> <p>To learn more about how the pipeline and their components steps work, please refer to the following materials:</p> <ul> <li>The Paper about the DARTS Dataset (No link yet)</li> <li>The Components Guide</li> <li>The API Reference</li> </ul> <p>There are further features implemented, which do not come from the components:</p> <ul> <li>Time tracking of processing steps</li> <li>Skipping of already processed tiles</li> <li>Environment debugging info</li> </ul>"},{"location":"guides/pipeline-v2/#minimal-configuration-example","title":"Minimal configuration example","text":"<pre><code>[darts]\nee-project = \"ee-tobias-hoelzer\"\nmodel-files = \"./models/s2-tcvis-final-large_2025-02-12.ckpt\"\naoi-shapefile = \"./data/myaoi.gpkg\"\nstart-date = \"2024-07\"\nend-date = \"2024-09\"\n</code></pre>"},{"location":"guides/pipeline-v2/#full-configuration-explaination","title":"Full configuration explaination","text":"<p>TODO</p>"},{"location":"guides/training/","title":"How a model is born","text":"<p>Recommendations &amp; best practices</p> <p>It is recommended to use a terminal multiplexer like <code>tmux</code>, <code>screen</code> or <code>zellij</code> to run multiple training runs in parallel. This way there is no need to have multiple terminal open over the span of multiple days.</p> <p>Using the folding method <code>\"region-stratified\"</code> enables more data-efficient training because the test split can then be set to random.</p> <p>Multi-scoring strategy should be <code>\"geometric\"</code> for combinations of recall, precision, f1 and jaccard index / iou, else <code>\"harmonic\"</code>.</p> <p>To do a super quick tutorial to get you started go the quickstart guide.</p> <p>The \"model creation process\" (training) is implemented as a three-level hierarchy, meaning the upper level does several calls to the level below it:</p> <ol> <li>Tuning (Hyperparameter-Sweeps)</li> <li>Cross-Validation</li> <li>(Training-) Run</li> </ol> <p></p>"},{"location":"guides/training/#artifacts-and-naming","title":"Artifacts and Naming","text":"<p>Training artefacts are stored in an organized way so that one does not get lost in 1000s of different directories. Especially when tuning hyperparameters, a lot of different runs are created, which can be difficult to track.</p> <p>For organisation, each tune, cv and training run has it's own name, which can be provided manually, but is usually generated automatically. The name can only be provided manually for the call-level - hence when tuning one can only provide the name for the tune, respective cross-validations and training runs are named automatically based on the provided name. If no name is provided, a random, but human-readable name is generated. Further, a random 8-character id is also generated for each run, primarily for tracking purposes with Weights &amp; Biases.</p> <p>The naming scheme is as follows:</p> <ul> <li><code>tune_name</code>: automatically generated or provided</li> <li><code>cv_name</code>: <code>{tune_name}-cv{hp_index}</code> if called by tune, else automatically generated or provided</li> <li><code>run_name</code>: <code>{cv_name}-run-f{fold_index}s{seed}</code> if called by cross-validation (or indirect tune), else automatically generated or provided</li> <li><code>run_id</code>: 8-character id</li> </ul> <p>Artifacts are stored in the following hierarchy:</p> <ul> <li>Created by runs of tunes: <code>{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}</code></li> <li>Created by runs of cross-validations: <code>{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}</code></li> <li>Created by single runs: <code>{artifact_dir}/_runs/{run_name}-{run_id}</code></li> </ul> <p>This way, the top-level <code>artifact_dir</code> is kept clean and organized.</p> <p>Local vs. WandB</p> <p>The training uses a local directory for storing the artifacts, such as metrics or the model. The final directory where these artifacts is always called <code>{run_name}-{run_id}</code>. This way, it should be easy to relate which artifacts belong to which run in wandb, where the url of a run is always <code>https://wandb.ai/{wandb_entity}/{wandb_project}/runs/{run_id}</code>.</p> <p>The cross-validation will not only contain the artifacts from the training runs but also a <code>run_infos.parquet</code> file with information about each run / experiment. This dataframe contains a <code>fold</code>, <code>seed</code>, <code>duration</code>, <code>checkpoint</code>, <code>is_unstable</code>, <code>is_unstable</code> and metrics columns, where the metrics are taken from the <code>trainer.callback_metrics</code>. The <code>is_unstable</code> column indicates whether the score-metrics of the run were unstable (not finite or zero). Further, it also contains a <code>score</code> and a <code>score_is_unstable</code> column, which contains the score and a boolean indicating whether any run of the cross-validation was unstable. These columns contain the same value for every row (run), since they are valid for the complete cross-validation.</p> <p>Weights &amp; Biases is optionally used for further tracking and logging. <code>wandb_project</code> and <code>wandb_entity</code> can be used to specify the project and entity for logging. Wandb will create a run <code>run_id</code> named <code>{run_name}</code>, meaning the id can be used to directly access the run via link and the name can be used for searching a run. For cross-validation and tuning <code>cv_name</code> and <code>tune_name</code> are set as <code>job_type</code> and <code>group</code> to emulate sweeps. This is a workaround and could potentially fixed if wandb will update their client library to allow the manual creation of sweeps.</p>"},{"location":"guides/training/#specifying-the-devices","title":"Specifying the devices","text":"<p>PyTorch Lightning supports different strategies for training on different devices and accelerators. These can be specified by the <code>--accelerator</code>, <code>--strategy</code>, <code>--devices</code> and <code>--num_nodes</code> parameters which are forwarded by the training scripts to the Lightning Trainer. The default values for these parameters are all <code>\"auto\"</code>, except for <code>--num_nodes</code>, which defaults to <code>1</code>. This means, that if no values are provided, Lightning will automatically detect the available devices and use one of them for training. Because of the limitation of the CLI regarding unions of lists and int/str, devices must always be a list.</p> <p>Here are some configurations for common scenarios:</p> Szenario <code>accelerator</code> <code>strategy</code> <code>devices</code> <code>num_nodes</code> Single GPU <code>\"gpu\"</code> <code>\"auto\"</code> (default) <code>[\"auto\"]</code> (default) <code>1</code> (default) Single GPU on Mac <code>\"mpu\"</code> <code>\"auto\"</code> (default) <code>[\"auto\"]</code> (default) <code>1</code> (default) DDP with 4 specified GPUs <code>\"gpu\"</code> <code>\"ddp_fork\"</code> <code>[0, 2, 5, 6]</code> <code>1</code> (default) <p>Please refer to the documentation of PyTorch Lightning:</p> <ul> <li>Trainer API</li> <li>Strategies</li> <li>DDP Example</li> </ul> <p>For the cross-validation and tuning script, two other <code>--strategy</code> options apart from the ones provided by PyTorch Lightning can be specified, which defines how training runs are executed in parallel. In this scenario, instead of running a single training run on multiple devices, multiple training runs are executed in parallel across multiple devices. Note that it is not possible to use any distribbuted strategy like DDP in this case.</p> <ul> <li><code>\"cv-parallel\"</code>: This strategy will run the training runs of a cross-validation in parallel.</li> <li><code>\"tune-parallel\"</code>: This strategy will run the cross-validations of a tune in parallel. In this scenario, the training runs of a cross-validation will be executed in series.</li> </ul> <p>DDP with parallel tuning or cross-validation</p> <p>When running multiple processes in parallel, normal Distributed Data Parallel (DDP) can not be used, since it will call the complete script multiple times. Thus, e.g. for tuning, multiple tunes would be created, which is not intended. Hence, the cross-validation and tuning script disable the DDP strategy by default and use instead the <code>\"ddp_fork\"</code> strategy if more than one <code>\"num_nodes\"</code> or <code>\"devices\"</code> is specified.</p>"},{"location":"guides/training/cv/","title":"Cross-Validation","text":"<pre><code>[uv run] darts cross-validate-smp ...\n</code></pre>"},{"location":"guides/training/cv/#fold-strategies","title":"Fold strategies","text":"<p>While cross-validating, the data can further be split into a training and validation set. One can specify the fraction of the validation set by providing an integer to <code>total_folds</code>. Higher values will result in smaller, validation sets and therefore more fold-combinations. To reduce the number of folds actually run, one can provide the <code>n_folds</code> parameter to limit the number of folds actually run. Thus, some folds will be skipped. The \"folding\" is based on <code>scikit-learn</code> and currently supports the following folding methods, which can be specified by the <code>fold_method</code> parameter:</p> <ul> <li><code>\"kfold\"</code>: Split the data into <code>total_folds</code> folds, where each fold can be used as a validation set. Uses sklearn.model_selection.KFold.</li> <li><code>\"stratified\"</code>: Will use the <code>\"empty\"</code> column of the metadata to create <code>total_folds</code> shuffled folds where each fold contains the same amount of empty and non-empty samples. Uses sklearn.model_selection.StratifiedKFold.</li> <li><code>\"shuffle\"</code>: Similar to <code>\"stratified\"</code>, but the order of the data is shuffled before splitting. Uses sklearn.model_selection.StratifiedShuffleSplit.</li> <li><code>\"region\"</code>: Will use the <code>\"region\"</code> column of the metadata to create <code>total_folds</code> folds where each fold splits the data by one or multiple regions. Uses sklearn.model_selection.GroupShuffleSplit.</li> <li><code>\"region-stratified\"</code>: Merge of the <code>\"region\"</code> and <code>\"stratified\"</code> methods. Uses sklearn.model_selection.StratifiedGroupKFold.</li> </ul> <p>Even in normal training a single KFold split is used to split between training and validation. This can be disabled by setting <code>fold_method</code> to <code>None</code>. In such cases, the validation set becomes equal to the training set, meaning longer validation time and the metrics are always calculated on seen data. This is useful for e.g. the final training of a model before deployment.</p> Using DartsDataModule <p>The data splitting is implemented by the darts_segmentation.training.data.DartsDataModule and can therefore be used in other settings as well.</p>"},{"location":"guides/training/cv/#darts_segmentation.training.data.DartsDataModule","title":"darts_segmentation.training.data.DartsDataModule","text":"<pre><code>DartsDataModule(\n    data_dir: pathlib.Path,\n    batch_size: int,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n    ]\n    | None = \"kfold\",\n    total_folds: int = 5,\n    fold: int = 0,\n    subsample: int | None = None,\n    bands: darts_segmentation.utils.Bands\n    | list[str]\n    | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    num_workers: int = 0,\n    in_memory: bool = False,\n)\n</code></pre> <p>               Bases: <code>lightning.LightningDataModule</code></p> <p>Initialize the data module.</p> <p>Supports spliting the data into train and test set while also defining cv-folds. Folding only applies to the non-test set and splits this into a train and validation set.</p> Example <ol> <li> <p>Normal train-validate. (Can also be used for testing on the complete dataset) <pre><code>dm = DartsDataModule(data_dir, batch_size)\n</code></pre></p> </li> <li> <p>Specifying a test split by random (20% of the data will be used for testing) <pre><code>dm = DartsDataModule(data_dir, batch_size, data_split_method=\"random\")\n</code></pre></p> </li> <li> <p>Specific fold for cross-validation (On the complete dataset, because data_split_method is \"none\"). This will be take the third of a total of7 folds to determine the validation set. <pre><code>dm = DartsDataModule(data_dir, batch_size, fold_method=\"region-stratified\", fold=2, total_folds=7)\n</code></pre></p> </li> </ol> <p>In general this should be used in combination with a cross-validation loop. <pre><code>for fold in range(total_folds):\n    dm = DartsDataModule(\n        data_dir,\n        batch_size,\n        fold_method=\"region-stratified\",\n        fold=fold,\n        total_folds=total_folds)\n    ...\n</code></pre></p> <ol> <li>Don't split anything -&gt; only train <pre><code>dm = DartsDataModule(data_dir, batch_size, fold_method=None)\n</code></pre></li> </ol> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified'] | None</code>, default:                   <code>'kfold'</code> )           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>fold</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Index of the current fold. Defaults to 0.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> <li> <code>bands</code>               (<code>darts_segmentation.utils.Bands | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of bands to use. Expects the data_dir to contain a config.toml with a \"darts.bands\" key, with which the indices of the bands will be mapped to. Defaults to None.</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>Whether to augment the data. Does nothing for testing. Defaults to True.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for data loading. See torch.utils.data.DataLoader. Defaults to 0.</p> </li> <li> <code>in_memory</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to load the data into memory. Defaults to False.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path,\n    batch_size: int,\n    # data_split is for the test split\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str | float] | None = None,\n    # fold is for cross-validation split (train/val)\n    fold_method: Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\"] | None = \"kfold\",\n    total_folds: int = 5,\n    fold: int = 0,\n    subsample: int | None = None,\n    bands: Bands | list[str] | None = None,\n    augment: list[Augmentation] | None = None,  # Not used for val or test\n    num_workers: int = 0,\n    in_memory: bool = False,\n):\n    \"\"\"Initialize the data module.\n\n    Supports spliting the data into train and test set while also defining cv-folds.\n    Folding only applies to the non-test set and splits this into a train and validation set.\n\n    Example:\n        1. Normal train-validate. (Can also be used for testing on the complete dataset)\n        ```py\n        dm = DartsDataModule(data_dir, batch_size)\n        ```\n\n        2. Specifying a test split by random (20% of the data will be used for testing)\n        ```py\n        dm = DartsDataModule(data_dir, batch_size, data_split_method=\"random\")\n        ```\n\n        3. Specific fold for cross-validation (On the complete dataset, because data_split_method is \"none\").\n        This will be take the third of a total of7 folds to determine the validation set.\n        ```py\n        dm = DartsDataModule(data_dir, batch_size, fold_method=\"region-stratified\", fold=2, total_folds=7)\n        ```\n\n        In general this should be used in combination with a cross-validation loop.\n        ```py\n        for fold in range(total_folds):\n            dm = DartsDataModule(\n                data_dir,\n                batch_size,\n                fold_method=\"region-stratified\",\n                fold=fold,\n                total_folds=total_folds)\n            ...\n        ```\n\n        4. Don't split anything -&gt; only train\n        ```py\n        dm = DartsDataModule(data_dir, batch_size, fold_method=None)\n        ```\n\n    Args:\n        data_dir (Path): The path to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        batch_size (int): Batch size for training and validation.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the test size can be specified\n            by providing a list with a single a float between 0 and 1 to data_split_by\n            This will be the fraction of the data to be used for testing.\n            E.g. [0.2] will use 20% of the data for testing.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str | float] | None, optional): Select by which regions/samples to split or\n            the size of test set. Defaults to None.\n        fold_method (Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\"] | None, optional):\n            Method for cross-validation split. Defaults to \"kfold\".\n        total_folds (int, optional): Total number of folds in cross-validation. Defaults to 5.\n        fold (int, optional): Index of the current fold. Defaults to 0.\n        subsample (int | None, optional): If set, will subsample the dataset to this number of samples.\n            This is useful for debugging and testing. Defaults to None.\n        bands (Bands | list[str] | None, optional): List of bands to use.\n            Expects the data_dir to contain a config.toml with a \"darts.bands\" key,\n            with which the indices of the bands will be mapped to.\n            Defaults to None.\n        augment (bool, optional): Whether to augment the data. Does nothing for testing. Defaults to True.\n        num_workers (int, optional): Number of workers for data loading. See torch.utils.data.DataLoader.\n            Defaults to 0.\n        in_memory (bool, optional): Whether to load the data into memory. Defaults to False.\n\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters(ignore=[\"num_workers\", \"in_memory\"])\n    self.data_dir = data_dir\n    self.batch_size = batch_size\n\n    self.fold = fold\n    self.data_split_method = data_split_method\n    self.data_split_by = data_split_by\n    self.fold_method = fold_method\n    self.total_folds = total_folds\n\n    self.subsample = subsample\n    self.augment = augment\n    self.num_workers = num_workers\n    self.in_memory = in_memory\n\n    data_dir = Path(data_dir)\n\n    metadata_file = data_dir / \"metadata.parquet\"\n    assert metadata_file.exists(), f\"Metadata file {metadata_file} not found!\"\n\n    config_file = data_dir / \"config.toml\"\n    assert config_file.exists(), f\"Config file {config_file} not found!\"\n    data_bands = toml.load(config_file)[\"darts\"][\"bands\"]\n    bands = bands.names if isinstance(bands, Bands) else bands\n    self.bands = [data_bands.index(b) for b in bands] if bands else None\n\n    zdir = data_dir / \"data.zarr\"\n    assert zdir.exists(), f\"Data directory {zdir} not found!\"\n    zroot = zarr.group(store=LocalStore(data_dir / \"data.zarr\"))\n    self.nsamples = zroot[\"x\"].shape[0]\n    logger.debug(f\"Data directory {zdir} found with {self.nsamples} samples.\")\n</code></pre>"},{"location":"guides/training/cv/#scoring-strategies","title":"Scoring strategies","text":"<p>To turn the information (metrics) gathered of a single cross-validation into a useful score, we need to somehow aggregate the metrics. In cases we are only interested in a single metric, this is easy: we can easily compute the mean. This metric can be specified by the <code>scoring_metric</code> parameter of the cross validation. It is also possible to use multiple metrics by specifying a list of metrics in the <code>scoring_metric</code> parameter. This, however, makes it a little more complicated.</p> <p>Multi-metric scoring is implemented as combine-then-reduce, meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. The combining strategy can be specified by the <code>multi_score_strategy</code> parameter. As of now, there are four strategies implemented: <code>\"arithmetic\"</code>, <code>\"geometric\"</code>, <code>\"harmonic\"</code> and <code>\"min\"</code>.</p> <p>The following visualization should help visualize how the different strategies work. Note that the loss is interpreted as \"lower is better\" and has also a broader range of possible values, exceeding 1. For the multi-metric scoring with IoU and Loss the arithmetic and geometric strategies are very instable. The scores for very low loss values where so high that the scores needed to be clipped to the range [0, 1] for the visualization to be able to show the behaviour of these strategies. However, especially the geometric mean shows a smoother curve than the harmonic mean for the multi-metric scoring with IoU and Recall. This should show that the strategy should be chosen carefully and in respect to the metrics used.</p> IoU &amp; Loss IoU &amp; Recall Code to reproduce the visualization <p>If you are unsure which strategy to use, you can use this code snippet to make a visualization based on your metrics:</p> <pre><code>import numpy as np\nimport xarray as xr\n\na = np.arange(0, 1, 0.01)\na = xr.DataArray(a, dims=[\"a\"], coords={\"a\": a})\n# 1 / ... indicates \"lower is better\" - replace it if needed\nb = np.arange(0, 2, 0.01)\nb = 1 / xr.DataArray(b, dims=[\"b\"], coords={\"b\": b})\n\ndef viz_strategies(a, b):\n    harmonic = 2 / (1 / a + 1 / b)\n    geometric = np.sqrt(a * b)\n    arithmetic = (a + b) / 2\n    minimum = np.minimum(a, b)\n\n    harmonic = harmonic.rename(\"harmonic mean\")\n    geometric = geometric.rename(\"geometric mean\")\n    arithmetic = arithmetic.rename(\"arithmetic mean\")\n    minimum = minimum.rename(\"minimum\")\n\n    fig, axs = plt.subplots(1, 4, figsize=(25, 5))\n    axs = axs.flatten()\n    harmonic.plot(ax=axs[0])\n    axs[0].set_title(\"Harmonic\")\n    geometric.plot(ax=axs[1], vmax=min(geometric.max(), 1))\n    axs[1].set_title(\"Geometric\")\n    arithmetic.plot(ax=axs[2], vmax=min(arithmetic.max(), 1))\n    axs[2].set_title(\"Arithmetic\")\n    minimum.plot(ax=axs[3])\n    axs[3].set_title(\"Minimum\")\n    return fig\n\nviz_strategies(a, b).show()\n</code></pre> <p>Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>Available metrics</p> <p>The following metrics are visible to the scoring function:</p> <ul> <li><code>'train/time'</code></li> <li><code>'train/device/batches_per_second'</code></li> <li><code>'train/device/samples_per_second'</code></li> <li><code>'train/device/flops_per_second'</code></li> <li><code>'train/device/mfu'</code></li> <li><code>'train/loss'</code></li> <li><code>'train/Accuracy'</code></li> <li><code>'train/CohenKappa'</code></li> <li><code>'train/F1Score'</code></li> <li><code>'train/HammingDistance'</code></li> <li><code>'train/JaccardIndex'</code></li> <li><code>'train/Precision'</code></li> <li><code>'train/Recall'</code></li> <li><code>'train/Specificity'</code></li> <li><code>'val/loss'</code></li> <li><code>'val/Accuracy'</code></li> <li><code>'val/CohenKappa'</code></li> <li><code>'val/F1Score'</code></li> <li><code>'val/HammingDistance'</code></li> <li><code>'val/JaccardIndex'</code></li> <li><code>'val/Precision'</code></li> <li><code>'val/Recall'</code></li> <li><code>'val/Specificity'</code></li> <li><code>'val/AUROC'</code></li> <li><code>'val/AveragePrecision'</code></li> </ul> <p>These are derived from <code>trainer.logged_metrics</code>.</p>"},{"location":"guides/training/cv/#random-state","title":"Random-state","text":"<p>All random state of the tuning and the cross-validation is seeded to 42. Random state of the training can be specified through a parameter. The cross-validation will not only cross-validates along different folds but also over different random seeds. Thus, for a single cross-validation with 5 folds and 3 seeds, 15 runs will be executed.</p>"},{"location":"guides/training/data/","title":"Preprocessing","text":"<p>To preprocess planet data into the necessary structure, you can use the following command:</p> <pre><code>[uv run] darts preprocess-planet-train-data --your-args-here ...\n</code></pre> <p>This will run the v2 preprocessing used by the v2 segmentation pipeline, but instead of passing the preprocessed data it creates patches of a fixed size and stores them into the <code>data.zarr</code> array. Further, it will also create the necessary metadata, config and labels files.</p> <p>The final train data is saved to disk in form of zarr arrays with dimensions <code>[n, c, h, w]</code> and <code>[n, h, w]</code> for the labels respectivly, with chunksizes of <code>n=1</code>. Hence, every sample is saved in a separate chunk and therefore in a seperate file on disk, but all managed by zarr.</p> <p>The preprocessing is done with the same components used in the v2 segmentation pipeline. Hence, the same configuration options are available.</p> You can also use the underlying function directly:"},{"location":"guides/training/data/#darts.training.preprocess_planet_train_data","title":"darts.training.preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def preprocess_planet_train_data(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.json\",\n        function=preprocess_planet_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_segmentation.utils import Bands\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    fpaths = {fpath.stem: fpath for fpath in _legacy_path_gen(data_dir)}\n    footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these because they depend on the preprocessing used\n    bands = Bands.from_dict(\n        {\n            \"red\": (1 / 3000, 0.0),\n            \"green\": (1 / 3000, 0.0),\n            \"blue\": (1 / 3000, 0.0),\n            \"nir\": (1 / 3000, 0.0),\n            \"ndvi\": (1 / 20000, 0.0),\n            \"relative_elevation\": (1 / 30000, 0.0),\n            \"slope\": (1 / 90, 0.0),\n            \"aspect\": (1 / 360, 0.0),\n            \"hillshade\": (1.0, 0.0),\n            \"curvature\": (1 / 10, 0.5),  # TODO: Do we even want shift?\n            \"tc_brightness\": (1 / 255, 0.0),\n            \"tc_greenness\": (1 / 255, 0.0),\n            \"tc_wetness\": (1 / 255, 0.0),\n        }\n    )\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        mask_erosion_size=mask_erosion_size,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache / \"planet_v2\")\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        try:\n            logger.debug(f\"Processing sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping...\")\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile_batched(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {planet_id} ({i + 1} of {len(footprints)}). \\nSkipping...\")\n            logger.exception(e)\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n    timer.summary()\n</code></pre>"},{"location":"guides/training/data/#training-data-structure","title":"Training data structure","text":"<p>The <code>train_data_dir</code> must look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/\n\u2514\u2500\u2500 metadata.parquet\n</code></pre> <p>The <code>data.zarr</code> is expected to be a zarr group containing a <code>x</code> and a <code>y</code> zarr array. This data contains the training data, hence images and labels - the shape of the <code>x</code> array must be <code>[n, c, h, w]</code> and of <code>y</code> <code>[n, h, w]</code> with <code>n</code> beeing the total number of samples, <code>c</code> the number of channels, <code>h</code> the height and <code>w</code> the width. The data must be complete, since splits and cross-validation folds are created on-the-fly by the training scripts.</p> <p><code>metadata.parquet</code> must contain at least the following columns:</p> <ul> <li>\"sample_id\": The id of the sample</li> <li>\"region\": The region the sample belongs to</li> <li>\"empty\": Whether the image has positive pixels The index must refer to the index of the sample in the zarr data.</li> </ul> <p><code>config.toml</code> must contain all configuration parameters necessary to preprocess the data from scratch. Further, it must contain information about the bands (channels) used, which will be used to setup the model. The configuration should lay under the <code>darts</code> key of the file.</p> <p>Ideally, use the preprocessing functions explained above to create this structure.</p>"},{"location":"guides/training/data/#own-preprocessing","title":"Own preprocessing","text":"<p>If you want to use your own preprocessing, you can do so by either creating the data yourself or by using the darts_segmentation.training.prepare_training.TrainDatasetBuilder helper class.</p> <p>The usage is quite simple:</p> <pre><code>from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_segmentation.utils import Bands\n\nbands = Bands.from_dict(\n    {\n        # \"name\": (scale, offset)\n        \"red\": (1 / 3000, 0.0),\n        \"green\": (1 / 3000, 0.0),\n        \"blue\": (1 / 3000, 0.0),\n    }\n)\n\nbuilder = TrainDatasetBuilder(\n    train_data_dir=train_data_dir,  # The directory where the data is stored\n    patch_size=patch_size, # The size of the patches to create\n    overlap=overlap, # The overlap of the patches\n    bands=bands, # The bands to use, expects the Bands class\n    exclude_nopositive=exclude_nopositive, # Whether to exclude samples without positive pixels\n    exclude_nan=exclude_nan, # Whether to exclude samples with NaN values\n    mask_erosion_size=mask_erosion_size, # How much the mask should be eroded (increase size of nan-holes)\n    device=device, # The device to use for preprocessing should be a torch.device\n)\n\nfor tile, labels, region in your_data:\n    # tile (your sample) should be an xarray Dataset with at least the bands specified above as variables\n    # labels should be an geopandas GeoDataFrame\n    builder.add_tile(\n        tile=tile,\n        labels=labels,\n        region=region, # Region should be a str describing the region of your sample, can be arbitrary\n        sample_id=sample_id, # A unique id for the sample\n        metadata={\n            ... # Any metadata you want to store with the sample, not mandatory\n        },\n    )\n\nbuilder.finalize(\n    {\n        ... # Any metadata you want to store with in the final config, not mandatory\n    }\n)\n</code></pre>"},{"location":"guides/training/quickstart/","title":"Quickstart Training","text":"<p>In this tutorial, you should be able to quickly setup the training of a segmentation model on the PLANET data.</p>"},{"location":"guides/training/quickstart/#0-prereq","title":"0. Prereq","text":"<ul> <li>Make sure you have installed the package and all dependencies. See the installation guide for more information.</li> <li>Clone this repository to obtain the labels for the training data.</li> <li>Ask a maintainer for access to the PLANET training data.</li> </ul>"},{"location":"guides/training/quickstart/#1-setup-the-configuration-file","title":"1. Setup the configuration file","text":"<p>Copy this configuration file to your local machine, e.g. under <code>configs/planet-training-quickstart.toml</code>, and adjust</p> <ul> <li>the paths to your needs</li> <li>the account settings of earth engine and wandb</li> </ul> Configuration file configs/planet-training-quickstart.toml<pre><code>[darts.wandb] # (5)\nwandb-project = \"...\"\nwandb-entity = \"...\"\nee-project = \"...\"\n\n[darts.paths] # (3)\ndata-dir = \"/path/to/planet_data\"\narcticdem-dir = \"/path/to/data/datacubes/arcticdem2m.icechunk\"\ntcvis-dir = \"/path/to/data/datacubes/tcvis.icechunk\"\nadmin-dir = \"/path/to/data/aux/admin\"\npreprocess-cache = \"/path/to/data/cache\"\nartifact-dir = \"/path/to/artifacts\"\n\n[darts.training.paths] # (4)\nlabels-dir = \"/path/to/ML_training_labels/retrogressive_thaw_slumps\" # (1)\ntrain-data-dir = \"/path/to/data/training/planet_quickstart\" # (2)\n\n[darts.preprocess]\ntpi-outer-radius = 100\ntpi-inner-radius = 0\nmask-erosion-size = 3\n\n[darts.training]\ndevice = \"auto\"\nnum-workers = 16\nmax-epochs = 6\nlog-every-n-steps = 50\ncheck-val-every-n-epoch = 5\nplot-every-n-val-epochs = 4 # == 20 epochs\nearly-stopping-patience = 0\nbands = [\n    'blue',\n    'green',\n    'red',\n    'nir',\n    'ndvi',\n    'tc_brightness',\n    'tc_greenness',\n    'tc_wetness',\n    'relative_elevation',\n    'slope',\n]\nfold = 0\n\n[darts.test]\ndata-split-method = \"region\"\ndata-split-by = ['Taymyrsky Dolgano-Nenetsky District']\n\n[darts.training.preprocessing]\npatch-size = 896\noverlap = 224 # increase to 64 if exclude-nan = True\nexclude-nopositive = false\nexclude-nan = false\nforce-preprocess = false\n\n# Only used in cross-validation and tuning\n[darts.cross-validation]\nfold-method = \"region-stratified\"\ntotal-folds = 5\nn-folds = 2\nn-randoms = 1\nscoring-metric = [\"val/JaccardIndex\", \"val/Recall\"]\nmulti-score-strategy = \"geometric\"\n\n# Only used in training or cross-validation, not tuning\n[darts.training.hyperparameters]\nlearning-rate = 4e-4\nbatch-size = 6\ngamma = 0.999\nfocal-loss-alpha = 0.92\nfocal-loss-gamma = 1.6\nmodel-arch = \"UPerNet\"\nmodel-encoder = \"tu-maxvit_tiny_rw_224\"\naugment = [\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\"\n]\n\n# Only used for tuning\n[darts.tuning]\nhpconfig = \"configs/planet-training-quickstart.toml\" # link to this file for convinience\nn-trials = 10\n\n# Only used for tuning\n[hyperparameters]\nlearning-rate = {distribution = \"loguniform\", low = 1.0e-5, high = 1.0e-3}\nbatch-size = 6\ngamma = 0.997\nfocal-loss-alpha = {low = 0.8, high = 0.99}\nfocal-loss-gamma = {low = 0.0, high = 2.0}\nmodel-arch = [\"Unet\", \"MAnet\", \"UPerNet\", \"Segformer\"]\nmodel-encoder = [\"resnet50\", \"resnext50_32x4d\", \"mit_b2\", \"tu-convnextv2_tiny\", \"tu-maxvit_tiny_rw_224\"]\naugment = {distribution = \"constant\", value = [\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\"\n]}\n</code></pre> <ol> <li>This should point to the directory of the repository you cloned in step 2.</li> <li>The <code>train-data-dir</code> should point to a fast read-access storage, like a local mounted SSD to speed up the training process.</li> <li>Change these paths to your needs. I recommend to just change the \"/path/to/\" part to have everything in one place.</li> <li>These paths aswell.</li> <li>Change these to your account settings.</li> </ol>"},{"location":"guides/training/quickstart/#2-preprocess-the-data","title":"2. Preprocess the data","text":"<pre><code>[uv run] darts preprocess-planet-train-data --config-file configs/planet-training-quickstart.toml\n</code></pre> <p>This will create the training data in the <code>train-data-dir</code> specified in the configuration file.</p> Take a look at the data <p>If the <code>preprocess-cache</code> directory is specified, the preprocessing will automatically cache the preprocessed data before it is turned into the training data format. You can visualize the data with xarray:</p> <pre><code>import xarray as xr\nfrom pathlib import Path\n\nfpath = list(Path(\"/path/to/data/cache/planet_v2\").glob(\"*.nc\"))[0]\ntile = xr.open_zarr(fpath, decode_coords=\"all\")\ntile\n</code></pre> <pre><code># Visualize the data (reduce the resolution for faster plotting)\ntile.red[::10, ::10].plot.imshow(cmap=\"Reds\")\n</code></pre> <p>To have a look at how the training data looks like, you can use <code>zarr</code> and <code>geopandas</code>:</p> <pre><code>import zarr\n\nzroot = zarr.open(\"/path/to/data/training/planet_quickstart/data.zarr\")\nzroot.tree()\n</code></pre> <pre><code>print(zroot[\"x\"].shape)\n</code></pre> <pre><code>import geopandas as gpd\n\nmetadata = gpd.read_parquet(\"/path/to/data/training/planet_quickstart/metadata.parquet\")\nmetadata.head()\n</code></pre> <pre><code>metadata.explore()\n</code></pre>"},{"location":"guides/training/quickstart/#3-train-the-model","title":"3. Train the model","text":"<pre><code>[uv run] darts train-smp --config-file configs/planet-training-quickstart.toml\n</code></pre>"},{"location":"guides/training/quickstart/#4-test-the-model","title":"4. Test the model","text":"<pre><code>[uv run] darts test-smp --config-file configs/planet-training-quickstart.toml\n</code></pre>"},{"location":"guides/training/quickstart/#5-do-a-cross-validation","title":"5. Do a cross-validation","text":"<p>This will take a while</p> <pre><code>[uv run] darts cross-validation-smp --config-file configs/planet-training-quickstart.toml\n</code></pre>"},{"location":"guides/training/quickstart/#6-hyperparameter-tuning","title":"6. Hyperparameter tuning","text":"<p>This will take a while</p> <pre><code>[uv run] darts tune-smp --config-file configs/planet-training-quickstart.toml\n</code></pre>"},{"location":"guides/training/training/","title":"Training (Binary Segmentation)","text":"<p>To train a simple SMP (Segmentation Model Pytorch) model you can use the command:</p> <pre><code>[uv run] darts train-smp --your-args-here ...\n</code></pre> <p>Model Architecture</p> <p>Configurations for the architecture and encoder can be found in the SMP documentation for model configurations.</p> <p>Change defaults</p> <p>Even though the defaults from the CLI are somewhat useful, it is recommended to create a config file and change the behavior of the training there.</p> <p>This command will train a simple SMP model on the data in the <code>train-data-dir</code> directory. The training relies on PyTorch Lightning, which is a high-level interface for PyTorch. It is recommended to use Weights and Biases (wandb) for the logging, because the training script is heavily influenced by how the organization of wandb works.</p> <p>The training follows the data splitting, decribed in the Understanding internal workings section below. To test the model on the test split, you can use the following command:</p> <pre><code>[uv run] darts test-smp --your-args-here ...\n</code></pre> <p>The checkpoint stored is not usable for the pipeline yet, since it is stored in a different format. To convert the model to a format, you need to convert is first:</p> <pre><code>[uv run] darts convert-lightning-checkpoint --your-args-here ...\n</code></pre> You can also use the underlying functions directly:"},{"location":"guides/training/training/#darts_segmentation.training.train_smp","title":"darts_segmentation.training.train_smp","text":"<pre><code>train_smp(\n    *,\n    run: darts_segmentation.training.train.TrainRunConfig = darts_segmentation.training.train.TrainRunConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n)\n</code></pre> <p>Run the training of the SMP model, specifically binary segmentation.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This training function is meant for single training runs but is also used for cross-validation and hyperparameter tuning by cv.py and tune.py. This strongly affects where artifacts are stored:</p> <ul> <li>Run was created by a tune: <code>{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}</code></li> <li>Run was created by a cross-validation: <code>{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}</code></li> <li>Single runs: <code>{artifact_dir}/_runs/{run_name}-{run_id}</code></li> </ul> <p><code>run_name</code> can be specified by the user, else it is generated automatically. In case of cross-validation, the run name is generated automatically by the cross-validation. <code>run_id</code> is generated automatically by the training function. Both are saved to the final checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Data related parameters for training.</p> </li> <li> <code>run</code>               (<code>darts_segmentation.training.train.TrainRunConfig</code>, default:                   <code>darts_segmentation.training.train.TrainRunConfig()</code> )           \u2013            <p>Run related parameters for training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging related parameters for training.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Training related parameters for training.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the model.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>pl.Trainer: The trainer object used for training. Contains also metrics.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def train_smp(\n    *,\n    run: TrainRunConfig = TrainRunConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n):\n    \"\"\"Run the training of the SMP model, specifically binary segmentation.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This training function is meant for single training runs but is also used for cross-validation and hyperparameter\n    tuning by cv.py and tune.py.\n    This strongly affects where artifacts are stored:\n\n    - Run was created by a tune: `{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}`\n    - Run was created by a cross-validation: `{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}`\n    - Single runs: `{artifact_dir}/_runs/{run_name}-{run_id}`\n\n    `run_name` can be specified by the user, else it is generated automatically.\n    In case of cross-validation, the run name is generated automatically by the cross-validation.\n    `run_id` is generated automatically by the training function.\n    Both are saved to the final checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_config (DataConfig): Data related parameters for training.\n        run (TrainRunConfig): Run related parameters for training.\n        logging_config (LoggingConfig): Logging related parameters for training.\n        device_config (DeviceConfig): Device and distributed strategy related parameters.\n        training_config (TrainingConfig): Training related parameters for training.\n        hparams (Hyperparameters): Hyperparameters for the model.\n\n    Returns:\n        pl.Trainer: The trainer object used for training. Contains also metrics.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from darts_utils.namegen import generate_counted_name, generate_id\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics, BinarySegmentationPreview\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n    from darts_segmentation.utils import Bands\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\", level=logging.INFO)\n\n    tick_fstart = time.perf_counter()\n\n    # Get the right nesting of the artifact directory\n    artifact_dir = logging_config.artifact_dir_at_run(run.cv_name, run.tune_name)\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run.name or generate_counted_name(artifact_dir)\n    run_id = generate_id()  # Needed for wandb\n\n    logger.info(\n        f\"Starting training '{run_name}' ('{run_id}') with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(\n        f\"Using config:\\n\\t{run}\\n\\t{training_config}\\n\\t{data_config}\\n\\t{logging_config}\\n\\t\"\n        f\"{device_config}\\n\\t{hparams}\"\n    )\n    if training_config.continue_from_checkpoint:\n        logger.debug(f\"Continuing from checkpoint '{training_config.continue_from_checkpoint.resolve()}'\")\n\n    lovely_tensors.monkey_patch()\n    lovely_tensors.set_config(color=False)\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(run.random_seed, workers=True, verbose=False)\n\n    dataset_config = toml.load(data_config.train_data_dir / \"config.toml\")[\"darts\"]\n    all_bands = Bands.from_config(dataset_config)\n    bands = all_bands.filter(hparams.bands) if hparams.bands else all_bands\n    config = SMPSegmenterConfig(\n        bands=bands,\n        model={\n            \"arch\": hparams.model_arch,\n            \"encoder_name\": hparams.model_encoder,\n            \"encoder_weights\": hparams.model_encoder_weights,\n            \"in_channels\": len(all_bands) if bands is None else len(bands),\n            \"classes\": 1,\n        },\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=data_config.train_data_dir,\n        batch_size=hparams.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        fold_method=data_config.fold_method,\n        total_folds=data_config.total_folds,\n        fold=run.fold,\n        subsample=data_config.subsample,\n        bands=hparams.bands,\n        augment=hparams.augment,\n        num_workers=training_config.num_workers,\n    )\n    model = LitSMP(\n        config=config,\n        learning_rate=hparams.learning_rate,\n        gamma=hparams.gamma,\n        focal_loss_alpha=hparams.focal_loss_alpha,\n        focal_loss_gamma=hparams.focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        cv_name=run.cv_name or \"none\",\n        tune_name=run.tune_name or \"none\",\n        random_seed=run.random_seed,\n    )\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=None, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        tags = [data_config.train_data_dir.stem]\n        if run.cv_name:\n            tags.append(run.cv_name)\n        if run.tune_name:\n            tags.append(run.tune_name)\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent.parent if run.tune_name or run.cv_name else artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=logging_config.wandb_project,\n            entity=logging_config.wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=run.tune_name or \"none\",\n            job_type=run.cv_name or \"none\",\n            # Using tags to quickly identify the run\n            tags=tags,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{logging_config.wandb_entity}' and project '{logging_config.wandb_project}'\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks and profiler\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            bands=bands,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            is_crossval=bool(run.cv_name),\n            batch_size=hparams.batch_size,\n            patch_size=dataset_config[\"patch_size\"],\n        ),\n        BinarySegmentationPreview(\n            bands=bands,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n        ),\n        # Something does not work well here...\n        # ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0), window_size=log_every_n_steps),\n    ]\n    if training_config.early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {training_config.early_stopping_patience}\")\n        early_stopping = EarlyStopping(\n            monitor=\"val/JaccardIndex\", mode=\"max\", patience=training_config.early_stopping_patience\n        )\n        callbacks.append(early_stopping)\n\n    # Unsupported: https://github.com/Lightning-AI/pytorch-lightning/issues/19983\n    # profiler_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"profiler\"\n    # profiler_dir.mkdir(parents=True, exist_ok=True)\n    # profiler = AdvancedProfiler(dirpath=profiler_dir, filename=\"perf_logs\", dump_stats=True)\n    # logger.debug(f\"Using profiler with output to {profiler.dirpath.resolve()}\")\n\n    logger.debug(\n        f\"Creating lightning-trainer on {device_config.accelerator} with devices {device_config.devices}\"\n        f\" and strategy '{device_config.lightning_strategy}'\"\n    )\n    # Train\n    trainer = L.Trainer(\n        max_epochs=training_config.max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=logging_config.log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n        accelerator=device_config.accelerator,\n        devices=device_config.devices if device_config.devices[0] != \"auto\" else \"auto\",\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        deterministic=False,  # True does not work for some reason\n        # profiler=profiler,\n    )\n    trainer.fit(model, datamodule, ckpt_path=training_config.continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"guides/training/training/#darts_segmentation.training.test_smp","title":"darts_segmentation.training.test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    bands: list[str] | None = None,\n    artifact_dir: pathlib.Path = pathlib.Path(\"artifacts\"),\n    num_workers: int = 0,\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be specified by providing a float between 0 and 1 to data_split_by. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str] | str | float | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which seed/regions/samples split. Defaults to None.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of bands to use. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('artifacts')</code> )           \u2013            <p>Directory to save artifacts. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    bands: list[str] | None = None,\n    artifact_dir: Path = Path(\"artifacts\"),\n    num_workers: int = 0,\n    device_config: DeviceConfig = DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): The path (top-level) to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int): Batch size for training and validation.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be\n            specified by providing a float between 0 and 1 to data_split_by.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str] | str | float | None, optional): Select by which seed/regions/samples split.\n            Defaults to None.\n        bands (list[str] | None, optional): List of bands to use. Defaults to None.\n        artifact_dir (Path, optional): Directory to save artifacts. Defaults to Path(\"lightning_logs\").\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device_config (DeviceConfig, optional): Device and distributed strategy related parameters.\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar, ThroughputMonitor\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n    from darts_segmentation.utils import Bands\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Further nest the artifact directory to avoid cluttering the root directory\n    artifact_dir = artifact_dir / \"_runs\"\n\n    logger.info(\n        f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device_config}\")\n\n    lovely_tensors.set_config(color=False)\n    lovely_tensors.monkey_patch()\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    data_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    all_bands = Bands.from_config(data_config)\n    bands = all_bands.filter(bands) if bands else all_bands\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir,\n        batch_size=batch_size,\n        data_split_method=data_split_method,\n        data_split_by=data_split_by,\n        bands=bands,\n        num_workers=num_workers,\n    )\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    logger.debug(f\"Using model checkpoint at {model_ckp.resolve()}\")\n    model = LitSMP.load_from_checkpoint(model_ckp)\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=\"none\",\n            job_type=\"none\",\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            bands=bands,\n            batch_size=batch_size,\n            patch_size=data_config[\"patch_size\"],\n        ),\n        ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0)),\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=device_config.accelerator,\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        devices=device_config.devices,\n        deterministic=True,\n    )\n\n    trainer.test(model, datamodule, ckpt_path=model_ckp)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"guides/training/training/#darts_segmentation.training.convert_lightning_checkpoint","title":"darts_segmentation.training.convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"guides/training/training/#data-splits","title":"Data splits","text":"<p>The initial training/test data split is performed at train/test time by using the <code>data_split_method</code> and <code>data_split_by</code> parameters. <code>data_split_method</code> can be one of the following:</p> <ul> <li><code>\"random\"</code> will split the data randomly, the seed is always 42 and the size of the test set can be specified by providing a list with a single float between 0 and 1 to <code>data_split_by</code>.</li> <li><code>\"region\"</code> will split the data by one or multiple regions, which can be specified by providing a str or list of str to <code>data_split_by</code>.</li> <li><code>\"sample\"</code> will split the data by sample ids, which can be specified similar to <code>\"region\"</code>.</li> <li><code>None</code>, no split is done and the complete dataset is used for both training and testing.</li> </ul>"},{"location":"guides/training/tune/","title":"Hyperparameter tuning","text":"<p>With the tuning script hyperparameters can be tuned by running a sweep. The sweep uses cross-validation to evaluate the performance of a single hyperparameter configuration.</p> <pre><code>[uv run] darts tune-smp ...\n</code></pre> Use the function <p>How the hyperparameters should be sweeped can be configured in a YAML or Toml file, specified by the <code>hpconfig</code> parameter. This file must contain a key called <code>\"hyperparameters\"</code> containing a list of hyperparameters distributions. These distributions can either be explicit defined by another dictionary containing a <code>\"distribution\"</code> key, or they can be implicit defined by a single value, a list or a dictionary containing a <code>\"low\"</code> and <code>\"high\"</code> key.</p> <p>The following distributions are supported:</p> <ul> <li><code>\"uniform\"</code>: Uniform distribution - must have a <code>\"low\"</code> and <code>\"high\"</code> value</li> <li><code>\"loguniform\"</code>: Log-uniform distribution - must have a <code>\"low\"</code> and <code>\"high\"</code> value</li> <li><code>\"intuniform\"</code>: Integer uniform distribution - must have a <code>\"low\"</code> and <code>\"high\"</code> value (both are inclusive)</li> <li><code>\"choice\"</code>: Choice distribution - must have a list of <code>\"choices\"</code> for explicit case, else just pass a list</li> <li><code>\"value\"</code>: Fixed value distribution - must have a <code>\"value\"</code> key for explicit case, else just pass a value</li> </ul> <p>And the following hyperparameters can be configured:</p> Hyperparameter Type Default model_arch str \"Unet\" model_encoder str \"dpn107\" model_encoder_weights str or None None augment bool True learning_rate float 1e-3 gamma float 0.9 focal_loss_alpha float or None None focal_loss_gamma float 2.0 batch_size int 8 <p>Because the configuration file doesn't use the <code>darts</code> key, it can also be merged into the normal configuration file and specified by the <code>hpconfig</code> parameter to also use that file.</p> Why using a separate configuration file? <ul> <li>It makes creating different sweeps easier</li> <li>It separates the sweep configuration from the normal configuration</li> <li>It allows for using dicts in the config - this is not possible right now due to the way we handle the main configuration file.</li> </ul> <p>Per default, a random search is performed, where the number of samples can be specified by <code>n_trials</code>. If <code>n_trials</code> is set to \"grid\", a grid search is performed instead. However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.</p> <p>Optionally it is possible to retrain and test with the best hyperparameter configuration by setting <code>retrain_and_test</code> to <code>True</code>. This will retrain the model on the complete train split without folding and test the data on the test split.</p>"},{"location":"guides/training/tune/#darts_segmentation.training.tune.tune_smp","title":"darts_segmentation.training.tune.tune_smp","text":"<pre><code>tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | typing.Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    cv_config: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    hpconfig: pathlib.Path | None = None,\n    config_file: pathlib.Path | None = None,\n)\n</code></pre> <p>Tune the hyper-parameters of the model using cross-validation and random states.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This tuning script is designed to sweep over hyperparameters with a cross-validation used to evaluate each hyperparameter configuration. Optionally, by setting <code>retrain_and_test</code> to True, the best hyperparameters are then selected based on the cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.</p> <p>Hyperparameters can be configured using a <code>hpconfig</code> file (YAML or Toml). Please consult the training guide or the documentation of <code>darts_segmentation.training.hparams.parse_hyperparameters</code> to learn how such a file should be structured. Per default, a random search is performed, where the number of samples can be specified by <code>n_trials</code>. If <code>n_trials</code> is set to \"grid\", a grid search is performed instead. However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.</p> <p>To specify on which metric(s) the cv score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\". In such cases, the configuration is not considered for further evaluation.</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning run. Will be generated based on the number of existing directories in the artifact directory if None. Defaults to None.</p> </li> <li> <code>n_trials</code>               (<code>int | typing.Literal['grid']</code>, default:                   <code>100</code> )           \u2013            <p>Number of trials to perform in hyperparameter tuning. If \"grid\", span a grid search over all configured hyperparameters. In a grid search, only constant or choice hyperparameters are allowed. Defaults to 100.</p> </li> <li> <code>retrain_and_test</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to retrain the model with the best hyperparameters and test it. Defaults to False.</p> </li> <li> <code>cv_config</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation. Defaults to CrossValidationConfig().</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for training. Defaults to TrainingConfig().</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for data. Defaults to DataConfig().</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for device. Defaults to DeviceConfig().</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Configuration for logging. Defaults to LoggingConfig().</p> </li> <li> <code>hpconfig</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the hyperparameter configuration file. Please see the documentation of <code>hyperparameters</code> for more information. Defaults to None.</p> </li> <li> <code>config_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the configuration file. If provided, it will be used instead of <code>hpconfig</code> if <code>hpconfig</code> is None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no hyperparameter configuration file is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/tune.py</code> <pre><code>def tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    cv_config: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    hpconfig: Path | None = None,\n    config_file: Annotated[Path | None, cyclopts.Parameter(parse=False)] = None,\n):\n    \"\"\"Tune the hyper-parameters of the model using cross-validation and random states.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This tuning script is designed to sweep over hyperparameters with a cross-validation\n    used to evaluate each hyperparameter configuration.\n    Optionally, by setting `retrain_and_test` to True, the best hyperparameters are then selected based on the\n    cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.\n\n    Hyperparameters can be configured using a `hpconfig` file (YAML or Toml).\n    Please consult the training guide or the documentation of\n    `darts_segmentation.training.hparams.parse_hyperparameters` to learn how such a file should be structured.\n    Per default, a random search is performed, where the number of samples can be specified by `n_trials`.\n    If `n_trials` is set to \"grid\", a grid search is performed instead.\n    However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.\n\n    To specify on which metric(s) the cv score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n    In such cases, the configuration is not considered for further evaluation.\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the tuning run.\n            Will be generated based on the number of existing directories in the artifact directory if None.\n            Defaults to None.\n        n_trials (int | Literal[\"grid\"], optional): Number of trials to perform in hyperparameter tuning.\n            If \"grid\", span a grid search over all configured hyperparameters.\n            In a grid search, only constant or choice hyperparameters are allowed.\n            Defaults to 100.\n        retrain_and_test (bool, optional): Whether to retrain the model with the best hyperparameters and test it.\n            Defaults to False.\n        cv_config (CrossValidationConfig, optional): Configuration for cross-validation.\n            Defaults to CrossValidationConfig().\n        training_config (TrainingConfig, optional): Configuration for training.\n            Defaults to TrainingConfig().\n        data_config (DataConfig, optional): Configuration for data.\n            Defaults to DataConfig().\n        device_config (DeviceConfig, optional): Configuration for device.\n            Defaults to DeviceConfig().\n        logging_config (LoggingConfig, optional): Configuration for logging.\n            Defaults to LoggingConfig().\n        hpconfig (Path | None, optional): Path to the hyperparameter configuration file.\n            Please see the documentation of `hyperparameters` for more information.\n            Defaults to None.\n        config_file (Path | None, optional): Path to the configuration file. If provided,\n            it will be used instead of `hpconfig` if `hpconfig` is None. Defaults to None.\n\n    Returns:\n        tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.\n\n    Raises:\n        ValueError: If no hyperparameter configuration file is provided.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.hparams import parse_hyperparameters, sample_hyperparameters\n    from darts_segmentation.training.scoring import score_from_single_run\n    from darts_segmentation.training.train import test_smp, train_smp\n\n    tick_fstart = time.perf_counter()\n\n    tune_name = name or generate_counted_name(logging_config.artifact_dir)\n    artifact_dir = logging_config.artifact_dir / tune_name\n    run_infos_file = artifact_dir / f\"{tune_name}.parquet\"\n\n    # Check if the artifact directory is empty\n    assert not artifact_dir.exists(), f\"{artifact_dir} already exists.\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    hpconfig = hpconfig or config_file\n    if hpconfig is None:\n        raise ValueError(\n            \"No hyperparameter configuration file provided. Please provide a valid file via the `--hpconfig` flag.\"\n        )\n    param_grid = parse_hyperparameters(hpconfig)\n    logger.debug(f\"Parsed hyperparameter grid: {param_grid}\")\n    param_list = sample_hyperparameters(param_grid, n_trials)\n\n    logger.info(\n        f\"Starting tune '{tune_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_trials*n_randoms*n_folds =\"\n        f\" {len(param_list)}*{cv_config.n_randoms}*{cv_config.n_folds} =\"\n        f\" {len(param_list) * cv_config.n_randoms * cv_config.n_folds} experiments.\"\n    )\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs = [\n        _ProcessInputs(\n            current=i,\n            total=len(param_list),\n            tune_name=tune_name,\n            cv=cv_config,\n            training_config=training_config,\n            logging_config=logging_config,\n            data_config=data_config,\n            device_config=device_config,\n            hparams=hparams,\n        )\n        for i, hparams in enumerate(param_list)\n    ]\n\n    run_infos: list[pd.DataFrame] = []\n    best_score = 0\n    best_hp = None\n\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"tune-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_cv,\n    ):\n        run_infos.append(output.run_infos)\n        if not output.is_unstable and output.score &gt; best_score:\n            best_score = output.score\n            best_hp = inp.hparams\n\n        # Save already here to prevent data loss if something goes wrong\n        pd.concat(run_infos).reset_index(drop=True).to_parquet(run_infos_file)\n        logger.debug(f\"Saved run infos to {run_infos_file}\")\n\n    if len(run_infos) == 0:\n        logger.error(\"No hyperparameters resulted in a valid score. Please check the logs for more information.\")\n        return 0, run_infos\n\n    run_infos = pd.concat(run_infos).reset_index(drop=True)\n\n    tick_fend = time.perf_counter()\n\n    if best_hp is None:\n        logger.warning(\n            f\"Tuning completed in {tick_fend - tick_fstart:.2f}s.\"\n            \" No hyperparameters resulted in a valid score. Please check the logs for more information.\"\n        )\n        return 0, run_infos\n    logger.info(\n        f\"Tuning completed in {tick_fend - tick_fstart:.2f}s. The best score was {best_score:.4f} with {best_hp}.\"\n    )\n\n    # =====================\n    # === End of tuning ===\n    # =====================\n\n    if not retrain_and_test:\n        return 0, run_infos\n\n    logger.info(\"Starting retraining with the best hyperparameters.\")\n\n    tick_fstart = time.perf_counter()\n    trainer = train_smp(\n        run=TrainRunConfig(name=f\"{tune_name}-retrain\"),\n        training_config=training_config,  # TODO: device and strategy\n        data_config=DataConfig(\n            train_data_dir=data_config.train_data_dir,\n            data_split_method=data_config.data_split_method,\n            data_split_by=data_config.data_split_by,\n            fold_method=None,  # No fold method for retraining\n            total_folds=None,  # No folds for retraining\n        ),\n        logging_config=LoggingConfig(\n            artifact_dir=artifact_dir,\n            log_every_n_steps=logging_config.log_every_n_steps,\n            check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            wandb_entity=logging_config.wandb_entity,\n            wandb_project=logging_config.wandb_project,\n        ),\n        hparams=best_hp,\n    )\n    run_id = trainer.lightning_module.hparams[\"run_id\"]\n    trainer = test_smp(\n        train_data_dir=data_config.train_data_dir,\n        run_id=run_id,\n        run_name=f\"{tune_name}-retrain\",\n        model_ckp=trainer.checkpoint_callback.best_model_path,\n        batch_size=best_hp.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        artifact_dir=artifact_dir,\n        num_workers=training_config.num_workers,\n        device_config=device_config,\n        wandb_entity=logging_config.wandb_entity,\n        wandb_project=logging_config.wandb_project,\n    )\n\n    run_info = {k: v.item() for k, v in trainer.callback_metrics.items()}\n    test_scoring_metric = (\n        cv_config.scoring_metric.replace(\"val/\", \"test/\")\n        if isinstance(cv_config.scoring_metric, str)\n        else [sm.replace(\"val/\", \"test/\") for sm in cv_config.scoring_metric]\n    )\n    score = score_from_single_run(run_info, test_scoring_metric, cv_config.multi_score_strategy)\n    is_unstable = check_score_is_unstable(run_info, cv_config.scoring_metric)\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Retraining and testing completed successfully in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, run_infos\n</code></pre>"},{"location":"reference/darts/","title":"darts","text":""},{"location":"reference/darts/#darts","title":"darts","text":"<p>DARTS processing pipeline.</p>"},{"location":"reference/darts/#darts.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts/cli/","title":"darts.cli","text":""},{"location":"reference/darts/cli/#darts.cli","title":"darts.cli","text":"<p>Entrypoint for the darts-pipeline CLI.</p>"},{"location":"reference/darts/cli/#darts.cli.LoggingManager","title":"LoggingManager  <code>module-attribute</code>","text":"<pre><code>LoggingManager = (\n    darts.utils.logging.LoggingManagerSingleton()\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.app","title":"app  <code>module-attribute</code>","text":"<pre><code>app = cyclopts.App(\n    version=darts.__version__,\n    console=rich.get_console(),\n    config=darts.cli.config_parser,\n    help_format=\"plaintext\",\n    version_format=\"plaintext\",\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.config_parser","title":"config_parser  <code>module-attribute</code>","text":"<pre><code>config_parser = darts.utils.config.ConfigParser()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.data_group","title":"data_group  <code>module-attribute</code>","text":"<pre><code>data_group = cyclopts.Group.create_ordered('Data Commands')\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.pipeline_group","title":"pipeline_group  <code>module-attribute</code>","text":"<pre><code>pipeline_group = cyclopts.Group.create_ordered(\n    \"Pipeline Commands\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.root_file","title":"root_file  <code>module-attribute</code>","text":"<pre><code>root_file = pathlib.Path(__file__).resolve()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.train_group","title":"train_group  <code>module-attribute</code>","text":"<pre><code>train_group = cyclopts.Group.create_ordered(\n    \"Training Commands\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline","title":"AOISentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>AOISentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    aoi_shapefile: pathlib.Path = None,\n    start_date: str = None,\n    end_date: str = None,\n    max_cloud_cover: int = 10,\n    input_cache: pathlib.Path = pathlib.Path(\n        \"data/cache/input\"\n    ),\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data based on an area of interest.</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The shapefile containing the area of interest.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for filtering the Sentinel 2 scenes. Defaults to 10.</p> </li> <li> <code>input_cache</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/cache/input')</code> )           \u2013            <p>The directory to use for caching the input data. Defaults to Path(\"data/cache/input\").</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.aoi_shapefile","title":"aoi_shapefile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_shapefile: pathlib.Path = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.input_cache","title":"input_cache  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_cache: pathlib.Path = pathlib.Path(\"data/cache/input\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.AOISentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for AOI Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"AOISentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for AOI Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.AOISentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser","title":"ConfigParser","text":"<pre><code>ConfigParser()\n</code></pre> <p>Parser for cyclopts config.</p> <p>An own implementation is needed to select our own toml structure and source. Implemented as a class to be able to provide the config-file as a parameter of the CLI.</p> <p>Initialize the ConfigParser (no-op).</p> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the ConfigParser (no-op).\"\"\"\n    self._config = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.__call__","title":"__call__","text":"<pre><code>__call__(\n    apps: list[cyclopts.App],\n    commands: tuple[str, ...],\n    arguments: cyclopts.ArgumentCollection,\n)\n</code></pre> <p>Parser for cyclopts config. An own implementation is needed to select our own toml structure.</p> <p>First, the configuration file at \"config.toml\" is loaded. Then, this config is flattened and then mapped to the input arguments of the called function. Hence parent keys are not considered.</p> <p>Parameters:</p> <ul> <li> <code>apps</code>               (<code>list[cyclopts.App]</code>)           \u2013            <p>The cyclopts apps. Unused, but must be provided for the cyclopts hook.</p> </li> <li> <code>commands</code>               (<code>tuple[str, ...]</code>)           \u2013            <p>The commands. Unused, but must be provided for the cyclopts hook.</p> </li> <li> <code>arguments</code>               (<code>cyclopts.ArgumentCollection</code>)           \u2013            <p>The arguments to apply the config to.</p> </li> </ul> <p>Examples:</p>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.__call__--setup-the-cyclopts-app","title":"Setup the cyclopts App","text":"<pre><code>import cyclopts\nfrom darts.utils.config import ConfigParser\n\nconfig_parser = ConfigParser()\napp = cyclopts.App(config=config_parser)\n\n# Intercept the logging behavior to add a file handler\n@app.meta.default\ndef launcher(\n    *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n    log_dir: Path = Path(\"logs\"),\n    config_file: Path = Path(\"config.toml\"),\n):\n    command, bound, _ = app.parse_args(tokens)\n    add_logging_handlers(command.__name__, console, log_dir)\n    return command(*bound.args, **bound.kwargs)\n\nif __name__ == \"__main__\":\n    app.meta()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.__call__--usage","title":"Usage","text":"<p>Config file <code>./config.toml</code>:</p> <pre><code>[darts.hello] # The parent key is completely ignored\nname = \"Tobias\"\n</code></pre> <p>Function signature which is called:</p> <pre><code># ... setup code for cyclopts\n@app.command()\ndef hello(name: str):\n    print(f\"Hello {name}\")\n</code></pre> <p>Calling the function from CLI:</p> <pre><code>$ darts hello\nHello Tobias\n\n$ darts hello --name=Max\nHello Max\n</code></pre> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def __call__(self, apps: list[cyclopts.App], commands: tuple[str, ...], arguments: cyclopts.ArgumentCollection):\n    \"\"\"Parser for cyclopts config. An own implementation is needed to select our own toml structure.\n\n    First, the configuration file at \"config.toml\" is loaded.\n    Then, this config is flattened and then mapped to the input arguments of the called function.\n    Hence parent keys are not considered.\n\n    Args:\n        apps (list[cyclopts.App]): The cyclopts apps. Unused, but must be provided for the cyclopts hook.\n        commands (tuple[str, ...]): The commands. Unused, but must be provided for the cyclopts hook.\n        arguments (cyclopts.ArgumentCollection): The arguments to apply the config to.\n\n    Examples:\n        ### Setup the cyclopts App\n\n        ```python\n        import cyclopts\n        from darts.utils.config import ConfigParser\n\n        config_parser = ConfigParser()\n        app = cyclopts.App(config=config_parser)\n\n        # Intercept the logging behavior to add a file handler\n        @app.meta.default\n        def launcher(\n            *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n            log_dir: Path = Path(\"logs\"),\n            config_file: Path = Path(\"config.toml\"),\n        ):\n            command, bound, _ = app.parse_args(tokens)\n            add_logging_handlers(command.__name__, console, log_dir)\n            return command(*bound.args, **bound.kwargs)\n\n        if __name__ == \"__main__\":\n            app.meta()\n        ```\n\n\n        ### Usage\n\n        Config file `./config.toml`:\n\n        ```toml\n        [darts.hello] # The parent key is completely ignored\n        name = \"Tobias\"\n        ```\n\n        Function signature which is called:\n\n        ```python\n        # ... setup code for cyclopts\n        @app.command()\n        def hello(name: str):\n            print(f\"Hello {name}\")\n        ```\n\n        Calling the function from CLI:\n\n        ```sh\n        $ darts hello\n        Hello Tobias\n\n        $ darts hello --name=Max\n        Hello Max\n        ```\n\n    \"\"\"\n    if self._config is None:\n        config_arg, _, _ = arguments.match(\"--config-file\")\n        config_file = config_arg.convert_and_validate()\n        # Use default config file if not specified\n        if not config_file:\n            config_file = config_arg.field_info.default\n        # else never happens\n        self.open_config(config_file)\n\n    self.apply_config(arguments)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.apply_config","title":"apply_config","text":"<pre><code>apply_config(arguments: cyclopts.ArgumentCollection)\n</code></pre> <p>Apply the loaded config to the cyclopts mapping.</p> <p>Parameters:</p> <ul> <li> <code>arguments</code>               (<code>cyclopts.ArgumentCollection</code>)           \u2013            <p>The arguments to apply the config to.</p> </li> </ul> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def apply_config(self, arguments: cyclopts.ArgumentCollection):\n    \"\"\"Apply the loaded config to the cyclopts mapping.\n\n    Args:\n        arguments (cyclopts.ArgumentCollection): The arguments to apply the config to.\n\n    \"\"\"\n    to_add = []\n    for k in self._config.keys():\n        value = self._config[k][\"value\"]\n\n        try:\n            argument, remaining_keys, _ = arguments.match(f\"--{k}\")\n        except ValueError:\n            # Config key not found in arguments - ignore\n            continue\n\n        # Skip if the argument is not bound to a parameter\n        if argument.tokens or argument.field_info.kind is argument.field_info.VAR_KEYWORD:\n            continue\n\n        # Skip if the argument is from the config file\n        if any(x.source != \"config-file\" for x in argument.tokens):\n            continue\n\n        # Parse value to tuple of strings\n        if not isinstance(value, list):\n            value = (value,)\n        value = tuple(str(x) for x in value)\n        # Add the new tokens to the list\n        for i, v in enumerate(value):\n            to_add.append(\n                (\n                    argument,\n                    cyclopts.Token(keyword=k, value=v, source=\"config-file\", index=i, keys=remaining_keys),\n                )\n            )\n    # Add here after all \"arguments.match\" calls, to avoid changing the list while iterating\n    for argument, token in to_add:\n        argument.append(token)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.ConfigParser.open_config","title":"open_config","text":"<pre><code>open_config(file_path: str | pathlib.Path) -&gt; None\n</code></pre> <p>Open the config file, takes the 'darts' key, flattens the resulting dict and saves as config.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the config file.</p> </li> </ul> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def open_config(self, file_path: str | Path) -&gt; None:\n    \"\"\"Open the config file, takes the 'darts' key, flattens the resulting dict and saves as config.\n\n    Args:\n        file_path (str | Path): The path to the config file.\n\n    \"\"\"\n    file_path = file_path if isinstance(file_path, Path) else Path(file_path)\n\n    if not file_path.exists():\n        logger.warning(f\"No config file found at {file_path.resolve()}\")\n        self._config = {}\n        return\n\n    with file_path.open(\"rb\") as f:\n        config = tomllib.load(f)[\"darts\"]\n\n    # Flatten the config data ()\n    self._config = flatten_dict(config)\n    logger.info(f\"loaded config from '{file_path.resolve()}'\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline","title":"PlanetPipeline  <code>dataclass</code>","text":"<pre><code>PlanetPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    orthotiles_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSOrthoTile\"\n    ),\n    scenes_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSScene\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSOrthoTile')</code> )           \u2013            <p>The directory containing the PlanetScope orthotiles.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSScene')</code> )           \u2013            <p>The directory containing the PlanetScope scenes.</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSOrthoTile\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSScene\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Planet data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetPipeline\"):\n    \"\"\"Run the sequential pipeline for Planet data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.PlanetPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline","title":"Sentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    sentinel2_dir: pathlib.Path = pathlib.Path(\n        \"data/input/sentinel2\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data.</p> <p>Parameters:</p> <ul> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/sentinel2')</code> )           \u2013            <p>The directory containing the Sentinel 2 scenes. Defaults to Path(\"data/input/sentinel2\").</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed. Defaults to None.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.sentinel2_dir","title":"sentinel2_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_dir: pathlib.Path = pathlib.Path(\n    \"data/input/sentinel2\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.Sentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.convert_lightning_checkpoint","title":"convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.cross_validation_smp","title":"cross_validation_smp","text":"<pre><code>cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    cv: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n)\n</code></pre> <p>Perform cross-validation for a model with given hyperparameters.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This cross-validation function is designed to evaluate the performance of a single model configuration. It can be used by a tuning script to tune hyperparameters. It calls the training function, hence most functionality is the same as the training function. In general, it does perform this:</p> <pre><code>for seed in seeds:\n    for fold in folds:\n        train_model(seed=seed, fold=fold, ...)\n</code></pre> <p>and calculates a score from the results.</p> <p>To specify on which metric(s) the score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code> for tunes (meaning if <code>tune_name</code> is not None) else <code>{artifact_dir}/_cross_validation</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation. If None, a name is generated automatically. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>cv</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for the training.</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for the data.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for the devices to use.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable, and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no runs were performed, meaning the configuration is invalid or no data was found.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/cv.py</code> <pre><code>def cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    cv: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n    logging_config: LoggingConfig = LoggingConfig(),\n):\n    \"\"\"Perform cross-validation for a model with given hyperparameters.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This cross-validation function is designed to evaluate the performance of a single model configuration.\n    It can be used by a tuning script to tune hyperparameters.\n    It calls the training function, hence most functionality is the same as the training function.\n    In general, it does perform this:\n\n    ```py\n    for seed in seeds:\n        for fold in folds:\n            train_model(seed=seed, fold=fold, ...)\n    ```\n\n    and calculates a score from the results.\n\n    To specify on which metric(s) the score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}` for tunes (meaning if `tune_name` is not None)\n    else `{artifact_dir}/_cross_validation`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the cross-validation. If None, a name is generated automatically.\n            Defaults to None.\n        tune_name (str | None, optional): Name of the tuning. Should only be specified by a tuning script.\n            Defaults to None.\n        cv (CrossValidationConfig): Configuration for cross-validation.\n        training_config (TrainingConfig): Configuration for the training.\n        data_config (DataConfig): Configuration for the data.\n        device_config (DeviceConfig): Configuration for the devices to use.\n        hparams (Hyperparameters): Hyperparameters for the training.\n        logging_config (LoggingConfig): Logging configuration.\n\n    Returns:\n        tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable,\n            and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)\n\n    Raises:\n        ValueError: If no runs were performed, meaning the configuration is invalid or no data was found.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.scoring import score_from_runs\n\n    tick_fstart = time.perf_counter()\n\n    artifact_dir = logging_config.artifact_dir_at_cv(tune_name)\n    cv_name = name or generate_counted_name(artifact_dir)\n    artifact_dir = artifact_dir / cv_name\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    n_folds = cv.n_folds or data_config.total_folds\n\n    logger.info(\n        f\"Starting cross-validation '{cv_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_randoms*n_folds = {cv.n_randoms}*{n_folds} = {cv.n_randoms * n_folds} experiments.\"\n    )\n\n    seeds = cv.rng_seeds\n    logger.debug(f\"Using seeds: {seeds}\")\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs: list[_ProcessInputs] = []\n    for i, seed in enumerate(seeds):\n        for fold in range(n_folds):\n            current = i * len(seeds) + fold\n            total = n_folds * len(seeds)\n            run = TrainRunConfig(\n                name=f\"{cv_name}-run-f{fold}s{seed}\",\n                cv_name=cv_name,\n                tune_name=tune_name,\n                fold=fold,\n                random_seed=seed,\n            )\n            process_inputs.append(\n                _ProcessInputs(\n                    current=current,\n                    total=total,\n                    seed=seed,\n                    fold=fold,\n                    cv=cv,\n                    run=run,\n                    training_config=training_config,\n                    logging_config=logging_config,\n                    data_config=data_config,\n                    device_config=device_config,\n                    hparams=hparams,\n                )\n            )\n\n    run_infos = []\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"cv-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_training,\n    ):\n        run_infos.append(output.run_info)\n\n    if len(run_infos) == 0:\n        raise ValueError(\n            \"No runs were performed. Please check your configuration and data.\"\n            \" If you are using a tuning script, make sure to specify the correct parameters.\"\n        )\n\n    logger.debug(f\"{run_infos=}\")\n    score = score_from_runs(run_infos, cv.scoring_metric, cv.multi_score_strategy)\n\n    run_infos = pd.DataFrame(run_infos)\n    run_infos[\"score\"] = score\n    is_unstable = run_infos[\"is_unstable\"].any()\n    run_infos[\"score_is_unstable\"] = is_unstable\n    if is_unstable:\n        logger.warning(\"Score is unstable, meaning at least one of the metrics is NaN, Inf, -Inf or 0.\")\n    run_infos.to_parquet(artifact_dir / \"run_infos.parquet\")\n    logger.debug(f\"Saved run infos to {artifact_dir / 'run_infos.parquet'}\")\n\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Finished cross-validation '{cv_name}' in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, is_unstable, run_infos\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.env_info","title":"env_info","text":"<pre><code>env_info()\n</code></pre> <p>Print debug information about the environment.</p> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.command\ndef env_info():\n    \"\"\"Print debug information about the environment.\"\"\"\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.hello","title":"hello","text":"<pre><code>hello(name: str, *, n: int = 1)\n</code></pre> <p>Say hello to someone.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the person to say hello to</p> </li> <li> <code>n</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The number of times to say hello. Defaults to 1.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If n is 3.</p> </li> </ul> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.command\ndef hello(name: str, *, n: int = 1):\n    \"\"\"Say hello to someone.\n\n    Args:\n        name (str): The name of the person to say hello to\n        n (int, optional): The number of times to say hello. Defaults to 1.\n\n    Raises:\n        ValueError: If n is 3.\n\n    \"\"\"\n    for i in range(n):\n        logger.debug(f\"Currently at {i=}\")\n        if n == 3:\n            raise ValueError(\"I don't like 3\")\n        logger.info(f\"Hello {name}\")\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.help","title":"help","text":"<pre><code>help()\n</code></pre> <p>Display the help screen.</p> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.command\ndef help():\n    \"\"\"Display the help screen.\"\"\"\n    app.help_print()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.launcher","title":"launcher","text":"<pre><code>launcher(\n    *tokens: str,\n    log_dir: pathlib.Path = pathlib.Path(\"logs\"),\n    config_file: pathlib.Path = pathlib.Path(\"config.toml\"),\n    verbose: bool = False,\n    tracebacks_show_locals: bool = False,\n)\n</code></pre> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.meta.default\ndef launcher(  # noqa: D103\n    *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n    log_dir: Path = Path(\"logs\"),\n    config_file: Path = Path(\"config.toml\"),\n    verbose: bool = False,\n    tracebacks_show_locals: bool = False,\n):\n    command, bound, ignored = app.parse_args(tokens, verbose=verbose)\n    LoggingManager.add_logging_handlers(command.__name__, log_dir, verbose, tracebacks_show_locals)\n    logger.debug(f\"Running on Python version {sys.version} from {__name__} ({root_file})\")\n    additional_args = {}\n    if \"config_file\" in ignored:\n        additional_args[\"config_file\"] = config_file\n    if \"log_dir\" in ignored:\n        additional_args[\"log_dir\"] = log_dir\n    if \"verbose\" in ignored:\n        additional_args[\"verbose\"] = verbose\n    return command(*bound.args, **bound.kwargs, **additional_args)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.preprocess_planet_train_data","title":"preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def preprocess_planet_train_data(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.json\",\n        function=preprocess_planet_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_segmentation.utils import Bands\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    fpaths = {fpath.stem: fpath for fpath in _legacy_path_gen(data_dir)}\n    footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these because they depend on the preprocessing used\n    bands = Bands.from_dict(\n        {\n            \"red\": (1 / 3000, 0.0),\n            \"green\": (1 / 3000, 0.0),\n            \"blue\": (1 / 3000, 0.0),\n            \"nir\": (1 / 3000, 0.0),\n            \"ndvi\": (1 / 20000, 0.0),\n            \"relative_elevation\": (1 / 30000, 0.0),\n            \"slope\": (1 / 90, 0.0),\n            \"aspect\": (1 / 360, 0.0),\n            \"hillshade\": (1.0, 0.0),\n            \"curvature\": (1 / 10, 0.5),  # TODO: Do we even want shift?\n            \"tc_brightness\": (1 / 255, 0.0),\n            \"tc_greenness\": (1 / 255, 0.0),\n            \"tc_wetness\": (1 / 255, 0.0),\n        }\n    )\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        mask_erosion_size=mask_erosion_size,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache / \"planet_v2\")\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        try:\n            logger.debug(f\"Processing sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping...\")\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile_batched(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {planet_id} ({i + 1} of {len(footprints)}). \\nSkipping...\")\n            logger.exception(e)\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n    timer.summary()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.preprocess_planet_train_data_pingo","title":"preprocess_planet_train_data_pingo","text":"<pre><code>preprocess_planet_train_data_pingo(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def preprocess_planet_train_data_pingo(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.json\",\n        function=preprocess_planet_train_data_pingo,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_segmentation.utils import Bands\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    footprints[\"fpath\"] = footprints.image_id.map(_path_gen(data_dir))\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these because they depend on the preprocessing used\n    bands = Bands.from_dict(\n        {\n            \"red\": (1 / 3000, 0.0),\n            \"green\": (1 / 3000, 0.0),\n            \"blue\": (1 / 3000, 0.0),\n            \"nir\": (1 / 3000, 0.0),\n            \"ndvi\": (1 / 20000, 0.0),\n            \"relative_elevation\": (1 / 30000, 0.0),\n            \"slope\": (1 / 90, 0.0),\n            \"aspect\": (1 / 360, 0.0),\n            \"hillshade\": (1.0, 0.0),\n            \"curvature\": (1 / 10, 0.5),  # TODO: Do we even want shift?\n            \"tc_brightness\": (1 / 255, 0.0),\n            \"tc_greenness\": (1 / 255, 0.0),\n            \"tc_wetness\": (1 / 255, 0.0),\n        }\n    )\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        mask_erosion_size=mask_erosion_size,\n        device=device,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache / \"planet_v2\")\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        try:\n            logger.debug(f\"Processing sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping...\")\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {planet_id} ({i + 1} of {len(footprints)}). \\nSkipping...\")\n            logger.exception(e)\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n    timer.summary()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.shell","title":"shell","text":"<pre><code>shell()\n</code></pre> <p>Open an interactive shell.</p> Source code in <code>darts/src/darts/cli.py</code> <pre><code>@app.command\ndef shell():\n    \"\"\"Open an interactive shell.\"\"\"\n    app.interactive_shell()\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.start_app","title":"start_app","text":"<pre><code>start_app()\n</code></pre> <p>Wrapp to start the app.</p> Source code in <code>darts/src/darts/cli.py</code> <pre><code>def start_app():\n    \"\"\"Wrapp to start the app.\"\"\"\n    try:\n        # First time initialization of the logging manager\n        LoggingManager.setup_logging()\n        app.meta()\n    except KeyboardInterrupt:\n        logger.info(\"Interrupted by user. Closing...\")\n    except SystemExit:\n        logger.info(\"Closing...\")\n    except Exception as e:\n        logger.exception(e)\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.test_smp","title":"test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    bands: list[str] | None = None,\n    artifact_dir: pathlib.Path = pathlib.Path(\"artifacts\"),\n    num_workers: int = 0,\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be specified by providing a float between 0 and 1 to data_split_by. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str] | str | float | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which seed/regions/samples split. Defaults to None.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of bands to use. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('artifacts')</code> )           \u2013            <p>Directory to save artifacts. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    bands: list[str] | None = None,\n    artifact_dir: Path = Path(\"artifacts\"),\n    num_workers: int = 0,\n    device_config: DeviceConfig = DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): The path (top-level) to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int): Batch size for training and validation.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be\n            specified by providing a float between 0 and 1 to data_split_by.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str] | str | float | None, optional): Select by which seed/regions/samples split.\n            Defaults to None.\n        bands (list[str] | None, optional): List of bands to use. Defaults to None.\n        artifact_dir (Path, optional): Directory to save artifacts. Defaults to Path(\"lightning_logs\").\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device_config (DeviceConfig, optional): Device and distributed strategy related parameters.\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar, ThroughputMonitor\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n    from darts_segmentation.utils import Bands\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Further nest the artifact directory to avoid cluttering the root directory\n    artifact_dir = artifact_dir / \"_runs\"\n\n    logger.info(\n        f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device_config}\")\n\n    lovely_tensors.set_config(color=False)\n    lovely_tensors.monkey_patch()\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    data_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    all_bands = Bands.from_config(data_config)\n    bands = all_bands.filter(bands) if bands else all_bands\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir,\n        batch_size=batch_size,\n        data_split_method=data_split_method,\n        data_split_by=data_split_by,\n        bands=bands,\n        num_workers=num_workers,\n    )\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    logger.debug(f\"Using model checkpoint at {model_ckp.resolve()}\")\n    model = LitSMP.load_from_checkpoint(model_ckp)\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=\"none\",\n            job_type=\"none\",\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            bands=bands,\n            batch_size=batch_size,\n            patch_size=data_config[\"patch_size\"],\n        ),\n        ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0)),\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=device_config.accelerator,\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        devices=device_config.devices,\n        deterministic=True,\n    )\n\n    trainer.test(model, datamodule, ckpt_path=model_ckp)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.train_smp","title":"train_smp","text":"<pre><code>train_smp(\n    *,\n    run: darts_segmentation.training.train.TrainRunConfig = darts_segmentation.training.train.TrainRunConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n)\n</code></pre> <p>Run the training of the SMP model, specifically binary segmentation.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This training function is meant for single training runs but is also used for cross-validation and hyperparameter tuning by cv.py and tune.py. This strongly affects where artifacts are stored:</p> <ul> <li>Run was created by a tune: <code>{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}</code></li> <li>Run was created by a cross-validation: <code>{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}</code></li> <li>Single runs: <code>{artifact_dir}/_runs/{run_name}-{run_id}</code></li> </ul> <p><code>run_name</code> can be specified by the user, else it is generated automatically. In case of cross-validation, the run name is generated automatically by the cross-validation. <code>run_id</code> is generated automatically by the training function. Both are saved to the final checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Data related parameters for training.</p> </li> <li> <code>run</code>               (<code>darts_segmentation.training.train.TrainRunConfig</code>, default:                   <code>darts_segmentation.training.train.TrainRunConfig()</code> )           \u2013            <p>Run related parameters for training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging related parameters for training.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Training related parameters for training.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the model.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>pl.Trainer: The trainer object used for training. Contains also metrics.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def train_smp(\n    *,\n    run: TrainRunConfig = TrainRunConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n):\n    \"\"\"Run the training of the SMP model, specifically binary segmentation.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This training function is meant for single training runs but is also used for cross-validation and hyperparameter\n    tuning by cv.py and tune.py.\n    This strongly affects where artifacts are stored:\n\n    - Run was created by a tune: `{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}`\n    - Run was created by a cross-validation: `{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}`\n    - Single runs: `{artifact_dir}/_runs/{run_name}-{run_id}`\n\n    `run_name` can be specified by the user, else it is generated automatically.\n    In case of cross-validation, the run name is generated automatically by the cross-validation.\n    `run_id` is generated automatically by the training function.\n    Both are saved to the final checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_config (DataConfig): Data related parameters for training.\n        run (TrainRunConfig): Run related parameters for training.\n        logging_config (LoggingConfig): Logging related parameters for training.\n        device_config (DeviceConfig): Device and distributed strategy related parameters.\n        training_config (TrainingConfig): Training related parameters for training.\n        hparams (Hyperparameters): Hyperparameters for the model.\n\n    Returns:\n        pl.Trainer: The trainer object used for training. Contains also metrics.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from darts_utils.namegen import generate_counted_name, generate_id\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics, BinarySegmentationPreview\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n    from darts_segmentation.utils import Bands\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\", level=logging.INFO)\n\n    tick_fstart = time.perf_counter()\n\n    # Get the right nesting of the artifact directory\n    artifact_dir = logging_config.artifact_dir_at_run(run.cv_name, run.tune_name)\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run.name or generate_counted_name(artifact_dir)\n    run_id = generate_id()  # Needed for wandb\n\n    logger.info(\n        f\"Starting training '{run_name}' ('{run_id}') with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(\n        f\"Using config:\\n\\t{run}\\n\\t{training_config}\\n\\t{data_config}\\n\\t{logging_config}\\n\\t\"\n        f\"{device_config}\\n\\t{hparams}\"\n    )\n    if training_config.continue_from_checkpoint:\n        logger.debug(f\"Continuing from checkpoint '{training_config.continue_from_checkpoint.resolve()}'\")\n\n    lovely_tensors.monkey_patch()\n    lovely_tensors.set_config(color=False)\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(run.random_seed, workers=True, verbose=False)\n\n    dataset_config = toml.load(data_config.train_data_dir / \"config.toml\")[\"darts\"]\n    all_bands = Bands.from_config(dataset_config)\n    bands = all_bands.filter(hparams.bands) if hparams.bands else all_bands\n    config = SMPSegmenterConfig(\n        bands=bands,\n        model={\n            \"arch\": hparams.model_arch,\n            \"encoder_name\": hparams.model_encoder,\n            \"encoder_weights\": hparams.model_encoder_weights,\n            \"in_channels\": len(all_bands) if bands is None else len(bands),\n            \"classes\": 1,\n        },\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=data_config.train_data_dir,\n        batch_size=hparams.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        fold_method=data_config.fold_method,\n        total_folds=data_config.total_folds,\n        fold=run.fold,\n        subsample=data_config.subsample,\n        bands=hparams.bands,\n        augment=hparams.augment,\n        num_workers=training_config.num_workers,\n    )\n    model = LitSMP(\n        config=config,\n        learning_rate=hparams.learning_rate,\n        gamma=hparams.gamma,\n        focal_loss_alpha=hparams.focal_loss_alpha,\n        focal_loss_gamma=hparams.focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        cv_name=run.cv_name or \"none\",\n        tune_name=run.tune_name or \"none\",\n        random_seed=run.random_seed,\n    )\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=None, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        tags = [data_config.train_data_dir.stem]\n        if run.cv_name:\n            tags.append(run.cv_name)\n        if run.tune_name:\n            tags.append(run.tune_name)\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent.parent if run.tune_name or run.cv_name else artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=logging_config.wandb_project,\n            entity=logging_config.wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=run.tune_name or \"none\",\n            job_type=run.cv_name or \"none\",\n            # Using tags to quickly identify the run\n            tags=tags,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{logging_config.wandb_entity}' and project '{logging_config.wandb_project}'\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks and profiler\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            bands=bands,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            is_crossval=bool(run.cv_name),\n            batch_size=hparams.batch_size,\n            patch_size=dataset_config[\"patch_size\"],\n        ),\n        BinarySegmentationPreview(\n            bands=bands,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n        ),\n        # Something does not work well here...\n        # ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0), window_size=log_every_n_steps),\n    ]\n    if training_config.early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {training_config.early_stopping_patience}\")\n        early_stopping = EarlyStopping(\n            monitor=\"val/JaccardIndex\", mode=\"max\", patience=training_config.early_stopping_patience\n        )\n        callbacks.append(early_stopping)\n\n    # Unsupported: https://github.com/Lightning-AI/pytorch-lightning/issues/19983\n    # profiler_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"profiler\"\n    # profiler_dir.mkdir(parents=True, exist_ok=True)\n    # profiler = AdvancedProfiler(dirpath=profiler_dir, filename=\"perf_logs\", dump_stats=True)\n    # logger.debug(f\"Using profiler with output to {profiler.dirpath.resolve()}\")\n\n    logger.debug(\n        f\"Creating lightning-trainer on {device_config.accelerator} with devices {device_config.devices}\"\n        f\" and strategy '{device_config.lightning_strategy}'\"\n    )\n    # Train\n    trainer = L.Trainer(\n        max_epochs=training_config.max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=logging_config.log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n        accelerator=device_config.accelerator,\n        devices=device_config.devices if device_config.devices[0] != \"auto\" else \"auto\",\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        deterministic=False,  # True does not work for some reason\n        # profiler=profiler,\n    )\n    trainer.fit(model, datamodule, ckpt_path=training_config.continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/cli/#darts.cli.tune_smp","title":"tune_smp","text":"<pre><code>tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | typing.Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    cv_config: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    hpconfig: pathlib.Path | None = None,\n    config_file: pathlib.Path | None = None,\n)\n</code></pre> <p>Tune the hyper-parameters of the model using cross-validation and random states.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This tuning script is designed to sweep over hyperparameters with a cross-validation used to evaluate each hyperparameter configuration. Optionally, by setting <code>retrain_and_test</code> to True, the best hyperparameters are then selected based on the cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.</p> <p>Hyperparameters can be configured using a <code>hpconfig</code> file (YAML or Toml). Please consult the training guide or the documentation of <code>darts_segmentation.training.hparams.parse_hyperparameters</code> to learn how such a file should be structured. Per default, a random search is performed, where the number of samples can be specified by <code>n_trials</code>. If <code>n_trials</code> is set to \"grid\", a grid search is performed instead. However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.</p> <p>To specify on which metric(s) the cv score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\". In such cases, the configuration is not considered for further evaluation.</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning run. Will be generated based on the number of existing directories in the artifact directory if None. Defaults to None.</p> </li> <li> <code>n_trials</code>               (<code>int | typing.Literal['grid']</code>, default:                   <code>100</code> )           \u2013            <p>Number of trials to perform in hyperparameter tuning. If \"grid\", span a grid search over all configured hyperparameters. In a grid search, only constant or choice hyperparameters are allowed. Defaults to 100.</p> </li> <li> <code>retrain_and_test</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to retrain the model with the best hyperparameters and test it. Defaults to False.</p> </li> <li> <code>cv_config</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation. Defaults to CrossValidationConfig().</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for training. Defaults to TrainingConfig().</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for data. Defaults to DataConfig().</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for device. Defaults to DeviceConfig().</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Configuration for logging. Defaults to LoggingConfig().</p> </li> <li> <code>hpconfig</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the hyperparameter configuration file. Please see the documentation of <code>hyperparameters</code> for more information. Defaults to None.</p> </li> <li> <code>config_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the configuration file. If provided, it will be used instead of <code>hpconfig</code> if <code>hpconfig</code> is None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no hyperparameter configuration file is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/tune.py</code> <pre><code>def tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    cv_config: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    hpconfig: Path | None = None,\n    config_file: Annotated[Path | None, cyclopts.Parameter(parse=False)] = None,\n):\n    \"\"\"Tune the hyper-parameters of the model using cross-validation and random states.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This tuning script is designed to sweep over hyperparameters with a cross-validation\n    used to evaluate each hyperparameter configuration.\n    Optionally, by setting `retrain_and_test` to True, the best hyperparameters are then selected based on the\n    cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.\n\n    Hyperparameters can be configured using a `hpconfig` file (YAML or Toml).\n    Please consult the training guide or the documentation of\n    `darts_segmentation.training.hparams.parse_hyperparameters` to learn how such a file should be structured.\n    Per default, a random search is performed, where the number of samples can be specified by `n_trials`.\n    If `n_trials` is set to \"grid\", a grid search is performed instead.\n    However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.\n\n    To specify on which metric(s) the cv score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n    In such cases, the configuration is not considered for further evaluation.\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the tuning run.\n            Will be generated based on the number of existing directories in the artifact directory if None.\n            Defaults to None.\n        n_trials (int | Literal[\"grid\"], optional): Number of trials to perform in hyperparameter tuning.\n            If \"grid\", span a grid search over all configured hyperparameters.\n            In a grid search, only constant or choice hyperparameters are allowed.\n            Defaults to 100.\n        retrain_and_test (bool, optional): Whether to retrain the model with the best hyperparameters and test it.\n            Defaults to False.\n        cv_config (CrossValidationConfig, optional): Configuration for cross-validation.\n            Defaults to CrossValidationConfig().\n        training_config (TrainingConfig, optional): Configuration for training.\n            Defaults to TrainingConfig().\n        data_config (DataConfig, optional): Configuration for data.\n            Defaults to DataConfig().\n        device_config (DeviceConfig, optional): Configuration for device.\n            Defaults to DeviceConfig().\n        logging_config (LoggingConfig, optional): Configuration for logging.\n            Defaults to LoggingConfig().\n        hpconfig (Path | None, optional): Path to the hyperparameter configuration file.\n            Please see the documentation of `hyperparameters` for more information.\n            Defaults to None.\n        config_file (Path | None, optional): Path to the configuration file. If provided,\n            it will be used instead of `hpconfig` if `hpconfig` is None. Defaults to None.\n\n    Returns:\n        tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.\n\n    Raises:\n        ValueError: If no hyperparameter configuration file is provided.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.hparams import parse_hyperparameters, sample_hyperparameters\n    from darts_segmentation.training.scoring import score_from_single_run\n    from darts_segmentation.training.train import test_smp, train_smp\n\n    tick_fstart = time.perf_counter()\n\n    tune_name = name or generate_counted_name(logging_config.artifact_dir)\n    artifact_dir = logging_config.artifact_dir / tune_name\n    run_infos_file = artifact_dir / f\"{tune_name}.parquet\"\n\n    # Check if the artifact directory is empty\n    assert not artifact_dir.exists(), f\"{artifact_dir} already exists.\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    hpconfig = hpconfig or config_file\n    if hpconfig is None:\n        raise ValueError(\n            \"No hyperparameter configuration file provided. Please provide a valid file via the `--hpconfig` flag.\"\n        )\n    param_grid = parse_hyperparameters(hpconfig)\n    logger.debug(f\"Parsed hyperparameter grid: {param_grid}\")\n    param_list = sample_hyperparameters(param_grid, n_trials)\n\n    logger.info(\n        f\"Starting tune '{tune_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_trials*n_randoms*n_folds =\"\n        f\" {len(param_list)}*{cv_config.n_randoms}*{cv_config.n_folds} =\"\n        f\" {len(param_list) * cv_config.n_randoms * cv_config.n_folds} experiments.\"\n    )\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs = [\n        _ProcessInputs(\n            current=i,\n            total=len(param_list),\n            tune_name=tune_name,\n            cv=cv_config,\n            training_config=training_config,\n            logging_config=logging_config,\n            data_config=data_config,\n            device_config=device_config,\n            hparams=hparams,\n        )\n        for i, hparams in enumerate(param_list)\n    ]\n\n    run_infos: list[pd.DataFrame] = []\n    best_score = 0\n    best_hp = None\n\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"tune-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_cv,\n    ):\n        run_infos.append(output.run_infos)\n        if not output.is_unstable and output.score &gt; best_score:\n            best_score = output.score\n            best_hp = inp.hparams\n\n        # Save already here to prevent data loss if something goes wrong\n        pd.concat(run_infos).reset_index(drop=True).to_parquet(run_infos_file)\n        logger.debug(f\"Saved run infos to {run_infos_file}\")\n\n    if len(run_infos) == 0:\n        logger.error(\"No hyperparameters resulted in a valid score. Please check the logs for more information.\")\n        return 0, run_infos\n\n    run_infos = pd.concat(run_infos).reset_index(drop=True)\n\n    tick_fend = time.perf_counter()\n\n    if best_hp is None:\n        logger.warning(\n            f\"Tuning completed in {tick_fend - tick_fstart:.2f}s.\"\n            \" No hyperparameters resulted in a valid score. Please check the logs for more information.\"\n        )\n        return 0, run_infos\n    logger.info(\n        f\"Tuning completed in {tick_fend - tick_fstart:.2f}s. The best score was {best_score:.4f} with {best_hp}.\"\n    )\n\n    # =====================\n    # === End of tuning ===\n    # =====================\n\n    if not retrain_and_test:\n        return 0, run_infos\n\n    logger.info(\"Starting retraining with the best hyperparameters.\")\n\n    tick_fstart = time.perf_counter()\n    trainer = train_smp(\n        run=TrainRunConfig(name=f\"{tune_name}-retrain\"),\n        training_config=training_config,  # TODO: device and strategy\n        data_config=DataConfig(\n            train_data_dir=data_config.train_data_dir,\n            data_split_method=data_config.data_split_method,\n            data_split_by=data_config.data_split_by,\n            fold_method=None,  # No fold method for retraining\n            total_folds=None,  # No folds for retraining\n        ),\n        logging_config=LoggingConfig(\n            artifact_dir=artifact_dir,\n            log_every_n_steps=logging_config.log_every_n_steps,\n            check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            wandb_entity=logging_config.wandb_entity,\n            wandb_project=logging_config.wandb_project,\n        ),\n        hparams=best_hp,\n    )\n    run_id = trainer.lightning_module.hparams[\"run_id\"]\n    trainer = test_smp(\n        train_data_dir=data_config.train_data_dir,\n        run_id=run_id,\n        run_name=f\"{tune_name}-retrain\",\n        model_ckp=trainer.checkpoint_callback.best_model_path,\n        batch_size=best_hp.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        artifact_dir=artifact_dir,\n        num_workers=training_config.num_workers,\n        device_config=device_config,\n        wandb_entity=logging_config.wandb_entity,\n        wandb_project=logging_config.wandb_project,\n    )\n\n    run_info = {k: v.item() for k, v in trainer.callback_metrics.items()}\n    test_scoring_metric = (\n        cv_config.scoring_metric.replace(\"val/\", \"test/\")\n        if isinstance(cv_config.scoring_metric, str)\n        else [sm.replace(\"val/\", \"test/\") for sm in cv_config.scoring_metric]\n    )\n    score = score_from_single_run(run_info, test_scoring_metric, cv_config.multi_score_strategy)\n    is_unstable = check_score_is_unstable(run_info, cv_config.scoring_metric)\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Retraining and testing completed successfully in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, run_infos\n</code></pre>"},{"location":"reference/darts/legacy_training/","title":"darts.legacy_training","text":""},{"location":"reference/darts/legacy_training/#darts.legacy_training","title":"darts.legacy_training","text":"<p>Legacy training module for DARTS.</p>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.convert_lightning_checkpoint","title":"convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/util.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.optuna_sweep_smp","title":"optuna_sweep_smp","text":"<pre><code>optuna_sweep_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    sweep_config: pathlib.Path,\n    n_trials: int = 10,\n    sweep_db: str | None = None,\n    sweep_id: str | None = None,\n    n_folds: int = 5,\n    n_randoms: int = 3,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    augment: bool = True,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n)\n</code></pre> <p>Create an optuna sweep and run it on the specified cuda device, or continue an existing sweep.</p> <p>If <code>sweep_id</code> already exists in <code>sweep_db</code>, the sweep will be continued. Otherwise, a new sweep will be created.</p> <p>If a <code>cuda_device</code> is specified, run an agent on this device. If None, do nothing.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>This will use cross-validation.</p> Example <p>In one terminal, start a sweep: <pre><code>    $ rye run darts sweep-smp --config-file /path/to/sweep-config.toml\n    ...  # Many logs\n    Created sweep with ID 123456789\n    ... # More logs from spawned agent\n</code></pre></p> <p>In another terminal, start an a second agent: <pre><code>    $ rye run darts sweep-smp --sweep-id 123456789\n    ...\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory.</p> </li> <li> <code>sweep_config</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration. Hyperparameters must contain the following fields: <code>model_arch</code>, <code>model_encoder</code>, <code>augment</code>, <code>gamma</code>, <code>batch_size</code>. Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.</p> </li> <li> <code>n_trials</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of runs to execute. Defaults to 10.</p> </li> <li> <code>sweep_db</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the optuna database. If None, a new database will be created.</p> </li> <li> <code>sweep_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The ID of the sweep. If None, a new sweep will be created. Defaults to None.</p> </li> <li> <code>n_folds</code>               (<code>(int, optinoal)</code>, default:                   <code>5</code> )           \u2013            <p>Number of folds in cross-validation. Max 5. Defaults to 5.</p> </li> <li> <code>n_randoms</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of repetitions with different random-seeds. First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline. Rest are pseudo-random generated beforehand, hence always equal. Defaults to 5.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. Defaults to None.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/sweep.py</code> <pre><code>def optuna_sweep_smp(\n    *,\n    # Data and sweep config\n    train_data_dir: Path,\n    sweep_config: Path,\n    n_trials: int = 10,\n    sweep_db: str | None = None,\n    sweep_id: str | None = None,\n    n_folds: int = 5,\n    n_randoms: int = 3,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    # Hyperparameters (default values if not provided by sweep-config)\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n):\n    \"\"\"Create an optuna sweep and run it on the specified cuda device, or continue an existing sweep.\n\n    If `sweep_id` already exists in `sweep_db`, the sweep will be continued. Otherwise, a new sweep will be created.\n\n    If a `cuda_device` is specified, run an agent on this device. If None, do nothing.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    This will use cross-validation.\n\n    Example:\n        In one terminal, start a sweep:\n        ```sh\n            $ rye run darts sweep-smp --config-file /path/to/sweep-config.toml\n            ...  # Many logs\n            Created sweep with ID 123456789\n            ... # More logs from spawned agent\n        ```\n\n        In another terminal, start an a second agent:\n        ```sh\n            $ rye run darts sweep-smp --sweep-id 123456789\n            ...\n        ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory.\n        sweep_config (Path): Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration.\n            Hyperparameters must contain the following fields: `model_arch`, `model_encoder`, `augment`, `gamma`,\n            `batch_size`.\n            Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.\n        n_trials (int, optional): Number of runs to execute. Defaults to 10.\n        sweep_db (str | None, optional): Path to the optuna database. If None, a new database will be created.\n        sweep_id (str | None, optional): The ID of the sweep. If None, a new sweep will be created. Defaults to None.\n        n_folds (int, optinoal): Number of folds in cross-validation. Max 5. Defaults to 5.\n        n_randoms (int, optional): Number of repetitions with different random-seeds.\n            First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline.\n            Rest are pseudo-random generated beforehand, hence always equal.\n            Defaults to 5.\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str | None, optional): The device to run the model on. Defaults to None.\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n\n    \"\"\"\n    import optuna\n    from names_generator import generate_name\n\n    from darts.legacy_training.util import suggest_optuna_params_from_wandb_config\n\n    with sweep_config.open(\"r\") as f:\n        sweep_configuration = yaml.safe_load(f)\n\n    logger.debug(f\"Loaded sweep configuration from {sweep_config.resolve()}:\\n{sweep_configuration}\")\n\n    # Create a new study-id if none is given\n    if sweep_id is None:\n        sweep_id = f\"sweep-{generate_name('hyphen')}\"\n        logger.info(f\"Generated new sweep ID: {sweep_id}\")\n        logger.info(\"To start a sweep agents, use the following command:\")\n        logger.info(f\"$ rye run darts optuna-sweep-smp --sweep-id {sweep_id}\")\n\n    artifact_dir = artifact_dir / sweep_id\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    def objective(trial):\n        hparams = suggest_optuna_params_from_wandb_config(trial, sweep_configuration)\n        logger.info(f\"Running trial with parameters: {hparams}\")\n\n        # Get the trial a more readable name\n        trial_name = f\"{generate_name(style='hyphen')}-{trial.number}\"\n\n        # We set the default weights to None, to be able to use different architectures\n        model_encoder_weights = None\n        # We set early stopping to None, because wandb will handle the early stopping\n        early_stopping_patience = None\n\n        # Overwrite the default values with the suggested ones, if they are present\n        learning_rate_trial = hparams.get(\"learning_rate\", learning_rate)\n        gamma_trial = hparams.get(\"gamma\", gamma)\n        focal_loss_alpha_trial = hparams.get(\"focal_loss_alpha\", focal_loss_alpha)\n        focal_loss_gamma_trial = hparams.get(\"focal_loss_gamma\", focal_loss_gamma)\n        batch_size_trial = hparams.get(\"batch_size\", batch_size)\n        model_arch_trial = hparams.get(\"model_arch\", model_arch)\n        model_encoder_trial = hparams.get(\"model_encoder\", model_encoder)\n        augment_trial = hparams.get(\"augment\", augment)\n\n        crossval_scores = defaultdict(list)\n\n        folds = list(range(n_folds))\n        rng = random.Random(42)\n        seeds = [42, 21, 69]\n        if n_randoms &gt; 3:\n            seeds += rng.sample(range(9999), n_randoms - 3)\n        elif n_randoms &lt; 3:\n            seeds = seeds[:n_randoms]\n\n        for random_seed in seeds:\n            for fold in folds:\n                logger.info(f\"Running cross-validation fold {fold}\")\n                _gather_and_reset_wandb_env()\n                trainer = train_smp(\n                    # Data config\n                    train_data_dir=train_data_dir,\n                    artifact_dir=artifact_dir,\n                    fold=fold,\n                    random_seed=random_seed,\n                    # Hyperparameters\n                    model_arch=model_arch_trial,\n                    model_encoder=model_encoder_trial,\n                    model_encoder_weights=model_encoder_weights,\n                    augment=augment_trial,\n                    learning_rate=learning_rate_trial,\n                    gamma=gamma_trial,\n                    focal_loss_alpha=focal_loss_alpha_trial,\n                    focal_loss_gamma=focal_loss_gamma_trial,\n                    batch_size=batch_size_trial,\n                    # Epoch and Logging config\n                    early_stopping_patience=early_stopping_patience,\n                    max_epochs=max_epochs,\n                    log_every_n_steps=log_every_n_steps,\n                    check_val_every_n_epoch=check_val_every_n_epoch,\n                    plot_every_n_val_epochs=plot_every_n_val_epochs,\n                    # Device and Manager config\n                    num_workers=num_workers,\n                    device=device,\n                    wandb_entity=wandb_entity,\n                    wandb_project=wandb_project,\n                    wandb_group=sweep_id,\n                    trial_name=trial_name,\n                    run_name=f\"{trial_name}-f{fold}r{random_seed}\",\n                )\n                for metric, value in trainer.callback_metrics.items():\n                    crossval_scores[metric].append(value.item())\n\n        logger.debug(f\"Cross-validation scores: {crossval_scores}\")\n        crossval_jaccard = mean(crossval_scores[\"val/JaccardIndex\"])\n        crossval_recall = mean(crossval_scores[\"val/Recall\"])\n\n        return crossval_jaccard, crossval_recall\n\n    study = optuna.create_study(\n        storage=sweep_db,\n        study_name=sweep_id,\n        directions=[\"maximize\", \"maximize\"],\n        load_if_exists=True,\n    )\n\n    if device is None:\n        logger.info(\"No device specified, closing script...\")\n        return\n\n    logger.info(\"Starting optimizing\")\n    study.optimize(objective, n_trials=n_trials)\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.preprocess_planet_train_data","title":"preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    bands: list[str],\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    dask_worker: int = min(\n        16, multiprocessing.cpu_count() - 1\n    ),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/planet.py</code> <pre><code>def preprocess_planet_train_data(\n    *,\n    bands: list[str],\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n        labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n        footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n        footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        planet_paths = sorted(_legacy_path_gen(data_dir))\n        logger.info(f\"Found {len(planet_paths)} PLANET scenes and orthotiles in {data_dir}\")\n        path_gen = split_dataset_paths(\n            planet_paths, footprints, train_data_dir, test_val_split, test_regions, admin_dir\n        )\n\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(planet_paths), console=console\n        ):\n            try:\n                planet_id = fpath.stem\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(planet_paths)}\"\n                    f\" '{fpath.resolve()}' ({planet_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{planet_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{planet_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_planet_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 2\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_planet_masks(fpath)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile=tile,\n                    labels=labels[labels.image_id == planet_id],\n                    bands=bands,\n                    norm_factors=norm_factors,\n                    patch_size=patch_size,\n                    overlap=overlap,\n                    exclude_nopositive=exclude_nopositive,\n                    exclude_nan=exclude_nan,\n                    device=device,\n                    mask_erosion_size=mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(planet_paths)} '{fpath.resolve()}'\"\n                    f\"({planet_id=}) with {patch_id} patches.\"\n                )\n\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.preprocess_s2_train_data","title":"preprocess_s2_train_data","text":"<pre><code>preprocess_s2_train_data(\n    *,\n    bands: list[str],\n    sentinel2_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    dask_worker: int = min(\n        16, multiprocessing.cpu_count() - 1\n    ),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n)\n</code></pre> <p>Preprocess Sentinel 2 data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Sentinel 2 scenes.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/s2.py</code> <pre><code>def preprocess_s2_train_data(\n    *,\n    bands: list[str],\n    sentinel2_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Sentinel 2 data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        sentinel2_dir (Path): The directory containing the Sentinel 2 scenes.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_s2_masks, load_s2_scene, load_tcvis\n    from darts_acquisition.s2 import parse_s2_tile_id\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        s2_paths = sorted(sentinel2_dir.glob(\"*/\"))\n        logger.info(f\"Found {len(s2_paths)} Sentinel 2 scenes in {sentinel2_dir}\")\n        path_gen = split_dataset_paths(s2_paths, train_data_dir, test_val_split, test_regions, admin_dir)\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(s2_paths), console=console\n        ):\n            try:\n                _, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}' ({tile_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{tile_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{tile_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_s2_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 10\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_s2_masks(fpath, optical.odc.geobox)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                labels = gpd.read_file(fpath / f\"{s2_tile_id}.shp\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile,\n                    labels,\n                    bands,\n                    norm_factors,\n                    patch_size,\n                    overlap,\n                    exclude_nopositive,\n                    exclude_nan,\n                    device,\n                    mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}'\"\n                    f\"({tile_id=}) with {patch_id} patches.\"\n                )\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"sentinel2_dir\": sentinel2_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.test_smp","title":"test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size. Defaults to 8.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Directory to save artifacts. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device to use. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/test.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int, optional): Batch size. Defaults to 8.\n        artifact_dir (Path, optional): Directory to save artifacts. Defaults to Path(\"lightning_logs\").\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device (int | str, optional): Device to use. Defaults to \"auto\".\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n    logger.info(f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device=}\")\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    # Data and model\n    datamodule_val_test = DartsDataModule(\n        data_dir=train_data_dir / \"val-test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    datamodule_test = DartsDataModule(\n        data_dir=train_data_dir / \"test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / run_name / run_id / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    model = SMPSegmenter.load_from_checkpoint(model_ckp)\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=run_name, version=run_id),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            id=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    metrics_cb = BinarySegmentationMetrics(\n        input_combination=preprocess_config[\"bands\"],\n    )\n    callbacks = [\n        RichProgressBar(),\n        metrics_cb,\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=True,\n    )\n    # Overwrite the names of the test sets to test agains two separate sets\n    metrics_cb.test_set = \"val-test\"\n    model.test_set = \"val-test\"\n    trainer.test(model, datamodule_val_test, ckpt_path=model_ckp)\n    metrics_cb.test_set = \"test\"\n    model.test_set = \"test\"\n    trainer.test(model, datamodule_test)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.train_smp","title":"train_smp","text":"<pre><code>train_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    fold: int = 0,\n    continue_from_checkpoint: pathlib.Path | None = None,\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the training of the SMP model.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.</p> <p>Each training run is assigned a unique name and id pair and optionally a trial name. The name, which the user can provide, should be used as a grouping mechanism of equal hyperparameter and code. Hence, different versions of the same name should only differ by random state or run settings parameter, like logs. Each version is assigned a unique id. Artifacts (metrics &amp; checkpoints) are then stored under <code>{artifact_dir}/{run_name}/{run_id}</code> in no-crossval runs. If <code>trial_name</code> is specified, the artifacts are stored under <code>{artifact_dir}/{trial_name}/{run_name}-{run_id}</code>. Wandb logs are always stored under <code>{wandb_entity}/{wandb_project}/{run_name}</code>, regardless of <code>trial_name</code>. However, they are further grouped by the <code>trial_name</code> (via job_type), if specified. Both <code>run_name</code> and <code>run_id</code> are also stored in the hparams of each checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>fold</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The current fold to train on. Must be in [0, 4]. Defaults to 0.</p> </li> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to a checkpoint to continue training from. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the encoder weights. Defaults to None.</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>random_seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The device to run the model on. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>wandb_group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Wandb group. Usefull for CV-Sweeps. Defaults to None.</p> </li> <li> <code>run_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of this run, as a further grouping method for logs etc. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>run_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ID of the run. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>trial_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation run / trial. This effects primary logging and artifact storage. If None, will do nothing. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def train_smp(\n    *,\n    # Data config\n    train_data_dir: Path,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    fold: int = 0,\n    continue_from_checkpoint: Path | None = None,\n    # Hyperparameters\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the training of the SMP model.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.\n\n    Each training run is assigned a unique **name** and **id** pair and optionally a trial name.\n    The name, which the user _can_ provide, should be used as a grouping mechanism of equal hyperparameter and code.\n    Hence, different versions of the same name should only differ by random state or run settings parameter, like logs.\n    Each version is assigned a unique id.\n    Artifacts (metrics &amp; checkpoints) are then stored under `{artifact_dir}/{run_name}/{run_id}` in no-crossval runs.\n    If `trial_name` is specified, the artifacts are stored under `{artifact_dir}/{trial_name}/{run_name}-{run_id}`.\n    Wandb logs are always stored under `{wandb_entity}/{wandb_project}/{run_name}`, regardless of `trial_name`.\n    However, they are further grouped by the `trial_name` (via job_type), if specified.\n    Both `run_name` and `run_id` are also stored in the hparams of each checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        fold (int, optional): The current fold to train on. Must be in [0, 4]. Defaults to 0.\n        continue_from_checkpoint (Path | None, optional): Path to a checkpoint to continue training from.\n            Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        model_encoder_weights (str | None, optional): Path to the encoder weights. Defaults to None.\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        early_stopping_patience (int, optional): Number of epochs to wait for improvement before stopping.\n            Defaults to 5.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        random_seed (int, optional): Random seed for deterministic training. Defaults to 42.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str, optional): The device to run the model on. Defaults to \"auto\".\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        wandb_group (str | None, optional): Wandb group. Usefull for CV-Sweeps. Defaults to None.\n        run_name (str | None, optional): Name of this run, as a further grouping method for logs etc.\n            If None, will generate a random one. Defaults to None.\n        run_id (str | None, optional): ID of the run. If None, will generate a random one. Defaults to None.\n        trial_name (str | None, optional): Name of the cross-validation run / trial.\n            This effects primary logging and artifact storage.\n            If None, will do nothing. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.legacy_training.util import generate_id, get_generated_name\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run_name or get_generated_name(artifact_dir)\n    run_id = run_id or generate_id()\n\n    logger.info(f\"Starting training '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(\n        f\"Using config:\\n\\t{model_arch=}\\n\\t{model_encoder=}\\n\\t{model_encoder_weights=}\\n\\t{augment=}\\n\\t\"\n        f\"{learning_rate=}\\n\\t{gamma=}\\n\\t{batch_size=}\\n\\t{max_epochs=}\\n\\t{log_every_n_steps=}\\n\\t\"\n        f\"{check_val_every_n_epoch=}\\n\\t{early_stopping_patience=}\\n\\t{plot_every_n_val_epochs=}\\n\\t{num_workers=}\"\n        f\"\\n\\t{device=}\\n\\t{random_seed=}\"\n    )\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(random_seed, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    config = SMPSegmenterConfig(\n        input_combination=preprocess_config[\"bands\"],\n        model={\n            \"arch\": model_arch,\n            \"encoder_name\": model_encoder,\n            \"encoder_weights\": model_encoder_weights,\n            \"in_channels\": len(preprocess_config[\"bands\"]),\n            \"classes\": 1,\n        },\n        norm_factors=preprocess_config[\"norm_factors\"],\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir / \"cross-val.zarr\",\n        batch_size=batch_size,\n        fold=fold,\n        augment=augment,\n        num_workers=num_workers,\n    )\n    model = SMPSegmenter(\n        config=config,\n        learning_rate=learning_rate,\n        gamma=gamma,\n        focal_loss_alpha=focal_loss_alpha,\n        focal_loss_gamma=focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        trial_name=trial_name,\n        random_seed=random_seed,\n    )\n\n    # Loggers\n    is_crossval = bool(trial_name)\n    trainer_loggers = [\n        CSVLogger(\n            save_dir=artifact_dir,\n            name=run_name if not is_crossval else trial_name,\n            version=run_id if not is_crossval else f\"{run_name}-{run_id}\",\n        ),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            group=wandb_group,\n            job_type=trial_name,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            input_combination=config[\"input_combination\"],\n            val_set=f\"val{fold}\",\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            is_crossval=is_crossval,\n        ),\n    ]\n    if early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {early_stopping_patience}\")\n        early_stopping = EarlyStopping(monitor=\"val/JaccardIndex\", mode=\"max\", patience=early_stopping_patience)\n        callbacks.append(early_stopping)\n\n    # Train\n    trainer = L.Trainer(\n        max_epochs=max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=check_val_every_n_epoch,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=False,\n    )\n    trainer.fit(model, datamodule, ckpt_path=continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.wandb_sweep_smp","title":"wandb_sweep_smp","text":"<pre><code>wandb_sweep_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    sweep_config: pathlib.Path,\n    n_trials: int = 10,\n    sweep_id: str | None = None,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Create a sweep with wandb and run it on the specified cuda device, or continue an existing sweep.</p> <p>If <code>sweep_id</code> is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued. All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}. Since each sweep-configuration has (currently) an own name and id, a single run can be found under: {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.</p> <p>If a <code>cuda_device</code> is specified, run an agent on this device. If None, do nothing.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>This will NOT use cross-validation. For cross-validation, use <code>optuna_sweep_smp</code>.</p> Example <p>In one terminal, start a sweep: <pre><code>    $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n    ...  # Many logs\n    Created sweep with ID 123456789\n    ... # More logs from spawned agent\n</code></pre></p> <p>In another terminal, start an a second agent: <pre><code>    $ rye run darts wandb-sweep-smp --sweep-id 123456789\n    ...\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory.</p> </li> <li> <code>sweep_config</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration. Hyperparameters must contain the following fields: <code>model_arch</code>, <code>model_encoder</code>, <code>augment</code>, <code>gamma</code>, <code>batch_size</code>. Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.</p> </li> <li> <code>n_trials</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of runs to execute. Defaults to 10.</p> </li> <li> <code>sweep_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The ID of the sweep. If None, a new sweep will be created. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. Defaults to None.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def wandb_sweep_smp(\n    *,\n    # Data and sweep config\n    train_data_dir: Path,\n    sweep_config: Path,\n    n_trials: int = 10,\n    sweep_id: str | None = None,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n):\n    \"\"\"Create a sweep with wandb and run it on the specified cuda device, or continue an existing sweep.\n\n    If `sweep_id` is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued.\n    All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}.\n    Since each sweep-configuration has (currently) an own name and id, a single run can be found under:\n    {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.\n\n    If a `cuda_device` is specified, run an agent on this device. If None, do nothing.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    This will NOT use cross-validation. For cross-validation, use `optuna_sweep_smp`.\n\n    Example:\n        In one terminal, start a sweep:\n        ```sh\n            $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n            ...  # Many logs\n            Created sweep with ID 123456789\n            ... # More logs from spawned agent\n        ```\n\n        In another terminal, start an a second agent:\n        ```sh\n            $ rye run darts wandb-sweep-smp --sweep-id 123456789\n            ...\n        ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory.\n        sweep_config (Path): Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration.\n            Hyperparameters must contain the following fields: `model_arch`, `model_encoder`, `augment`, `gamma`,\n            `batch_size`.\n            Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.\n        n_trials (int, optional): Number of runs to execute. Defaults to 10.\n        sweep_id (str | None, optional): The ID of the sweep. If None, a new sweep will be created. Defaults to None.\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str | None, optional): The device to run the model on. Defaults to None.\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n\n    \"\"\"\n    import wandb\n\n    # Wandb has a stupid way of logging (they log per default with click.echo to stdout)\n    # We need to silence this and redirect all possible logs to our logger\n    # wl = wandb.setup({\"silent\": True})\n    # wandb.termsetup(wl.settings, logging.getLogger(\"wandb\"))\n    # LoggingManager.apply_logging_handlers(\"wandb\")\n\n    if sweep_id is not None and device is None:\n        logger.warning(\"Continuing a sweep without specifying a device will not do anything.\")\n\n    with sweep_config.open(\"r\") as f:\n        sweep_configuration = yaml.safe_load(f)\n\n    logger.debug(f\"Loaded sweep configuration from {sweep_config.resolve()}:\\n{sweep_configuration}\")\n\n    if sweep_id is None:\n        sweep_id = wandb.sweep(sweep=sweep_configuration, project=wandb_project, entity=wandb_entity)\n        logger.info(f\"Created sweep with ID {sweep_id}\")\n        logger.info(\"To start a sweep agents, use the following command:\")\n        logger.info(f\"$ rye run darts sweep_smp --sweep-id {sweep_id}\")\n\n    artifact_dir = artifact_dir / f\"sweep-{sweep_id}\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    def run():\n        run = wandb.init(config=sweep_configuration)\n        # We need to manually log the run data since the wandb logger only logs to its own logs and click\n        logger.info(f\"Starting sweep run '{run.settings.run_name}'\")\n        logger.debug(f\"Run data is saved locally in {Path(run.settings.sync_dir).resolve()}\")\n        logger.debug(f\"View project at {run.settings.project_url}\")\n        logger.debug(f\"View sweep at {run.settings.sweep_url}\")\n        logger.debug(f\"View run at {run.settings.run_url}\")\n\n        # We set the default weights to None, to be able to use different architectures\n        model_encoder_weights = None\n        # We set early stopping to None, because wandb will handle the early stopping\n        early_stopping_patience = None\n        learning_rate = wandb.config[\"learning_rate\"]\n        gamma = wandb.config[\"gamma\"]\n        batch_size = wandb.config[\"batch_size\"]\n        model_arch = wandb.config[\"model_arch\"]\n        model_encoder = wandb.config[\"model_encoder\"]\n        augment = wandb.config[\"augment\"]\n        focal_loss_alpha = wandb.config[\"focal_loss_alpha\"]\n        focal_loss_gamma = wandb.config[\"focal_loss_gamma\"]\n        fold = wandb.config.get(\"fold\", 0)\n        random_seed = wandb.config.get(\"random_seed\", 42)\n\n        train_smp(\n            # Data config\n            train_data_dir=train_data_dir,\n            artifact_dir=artifact_dir,\n            fold=fold,\n            # Hyperparameters\n            model_arch=model_arch,\n            model_encoder=model_encoder,\n            model_encoder_weights=model_encoder_weights,\n            augment=augment,\n            learning_rate=learning_rate,\n            gamma=gamma,\n            focal_loss_alpha=focal_loss_alpha,\n            focal_loss_gamma=focal_loss_gamma,\n            batch_size=batch_size,\n            # Epoch and Logging config\n            early_stopping_patience=early_stopping_patience,\n            max_epochs=max_epochs,\n            log_every_n_steps=log_every_n_steps,\n            check_val_every_n_epoch=check_val_every_n_epoch,\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            # Device and Manager config\n            random_seed=random_seed,\n            num_workers=num_workers,\n            device=device,\n            wandb_entity=wandb_entity,\n            wandb_project=wandb_project,\n            run_name=wandb.run.name,\n            run_id=wandb.run.id,\n        )\n\n    if device is None:\n        logger.info(\"No device specified, closing script...\")\n        return\n\n    logger.info(\"Starting a default sweep agent\")\n    wandb.agent(sweep_id, function=run, count=n_trials, project=wandb_project, entity=wandb_entity)\n</code></pre>"},{"location":"reference/darts/legacy_training/preprocess/","title":"darts.legacy_training.preprocess","text":""},{"location":"reference/darts/legacy_training/preprocess/#darts.legacy_training.preprocess","title":"darts.legacy_training.preprocess","text":"<p>Preprocessiong functions.</p>"},{"location":"reference/darts/legacy_training/preprocess/planet/","title":"darts.legacy_training.preprocess.planet","text":""},{"location":"reference/darts/legacy_training/preprocess/planet/#darts.legacy_training.preprocess.planet","title":"darts.legacy_training.preprocess.planet","text":"<p>Preprocess Planet data for training.</p>"},{"location":"reference/darts/legacy_training/preprocess/planet/#darts.legacy_training.preprocess.planet.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/legacy_training/preprocess/planet/#darts.legacy_training.preprocess.planet._legacy_path_gen","title":"_legacy_path_gen","text":"<pre><code>_legacy_path_gen(data_dir: pathlib.Path)\n</code></pre> Source code in <code>darts/src/darts/legacy_training/preprocess/planet.py</code> <pre><code>def _legacy_path_gen(data_dir: Path):\n    for iterdir in data_dir.iterdir():\n        if iterdir.stem == \"iteration001\":\n            for sitedir in (iterdir).iterdir():\n                for imgdir in (sitedir).iterdir():\n                    if not imgdir.is_dir():\n                        continue\n                    try:\n                        yield next(imgdir.glob(\"*_SR.tif\")).parent\n                    except StopIteration:\n                        yield next(imgdir.glob(\"*_SR_clip.tif\")).parent\n        else:\n            for imgdir in (iterdir).iterdir():\n                if not imgdir.is_dir():\n                    continue\n                try:\n                    yield next(imgdir.glob(\"*_SR.tif\")).parent\n                except StopIteration:\n                    yield next(imgdir.glob(\"*_SR_clip.tif\")).parent\n</code></pre>"},{"location":"reference/darts/legacy_training/preprocess/planet/#darts.legacy_training.preprocess.planet.preprocess_planet_train_data","title":"preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    bands: list[str],\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    dask_worker: int = min(\n        16, multiprocessing.cpu_count() - 1\n    ),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/planet.py</code> <pre><code>def preprocess_planet_train_data(\n    *,\n    bands: list[str],\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n        labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n        footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n        footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        planet_paths = sorted(_legacy_path_gen(data_dir))\n        logger.info(f\"Found {len(planet_paths)} PLANET scenes and orthotiles in {data_dir}\")\n        path_gen = split_dataset_paths(\n            planet_paths, footprints, train_data_dir, test_val_split, test_regions, admin_dir\n        )\n\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(planet_paths), console=console\n        ):\n            try:\n                planet_id = fpath.stem\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(planet_paths)}\"\n                    f\" '{fpath.resolve()}' ({planet_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{planet_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{planet_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_planet_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 2\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_planet_masks(fpath)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile=tile,\n                    labels=labels[labels.image_id == planet_id],\n                    bands=bands,\n                    norm_factors=norm_factors,\n                    patch_size=patch_size,\n                    overlap=overlap,\n                    exclude_nopositive=exclude_nopositive,\n                    exclude_nan=exclude_nan,\n                    device=device,\n                    mask_erosion_size=mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(planet_paths)} '{fpath.resolve()}'\"\n                    f\"({planet_id=}) with {patch_id} patches.\"\n                )\n\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/preprocess/planet/#darts.legacy_training.preprocess.planet.split_dataset_paths","title":"split_dataset_paths","text":"<pre><code>split_dataset_paths(\n    data_paths: list[pathlib.Path],\n    footprints: geopandas.GeoDataFrame,\n    train_data_dir: pathlib.Path,\n    test_val_split: float,\n    test_regions: list[str] | None,\n    admin_dir: pathlib.Path,\n)\n</code></pre> <p>Split the dataset into a cross-val, a val-test and a test dataset.</p> <p>Returns a generator with: input-path, output-path and split/mode. The test set is splitted first by the given regions and is meant to be used to evaluate the regional value shift. Then the val-test set is splitted then by random at given size to evaluate the variance value shift.</p> <p>Parameters:</p> <ul> <li> <code>data_paths</code>               (<code>list[pathlib.Path]</code>)           \u2013            <p>All paths found with tiffs.</p> </li> <li> <code>footprints</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The footprints of the images.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output path.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>)           \u2013            <p>val-test ratio.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | None</code>)           \u2013            <p>test regions.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin level shape-files.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>[zip[tuple[Path, Path, str]]]: A generator with input-path, output-path and split/mode.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/planet.py</code> <pre><code>def split_dataset_paths(\n    data_paths: list[Path],\n    footprints: \"gpd.GeoDataFrame\",\n    train_data_dir: Path,\n    test_val_split: float,\n    test_regions: list[str] | None,\n    admin_dir: Path,\n):\n    \"\"\"Split the dataset into a cross-val, a val-test and a test dataset.\n\n    Returns a generator with: input-path, output-path and split/mode.\n    The test set is splitted first by the given regions and is meant to be used to evaluate the regional value shift.\n    Then the val-test set is splitted then by random at given size to evaluate the variance value shift.\n\n    Args:\n        data_paths (list[Path]): All paths found with tiffs.\n        footprints (gpd.GeoDataFrame): The footprints of the images.\n        train_data_dir (Path): Output path.\n        test_val_split (float): val-test ratio.\n        test_regions (list[str] | None): test regions.\n        admin_dir (Path): The directory containing the admin level shape-files.\n\n    Returns:\n        [zip[tuple[Path, Path, str]]]: A generator with input-path, output-path and split/mode.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    from darts_acquisition.admin import download_admin_files\n    from sklearn.model_selection import train_test_split\n\n    train_data_dir.mkdir(exist_ok=True, parents=True)\n\n    # 1. Split regions\n    test_paths: list[Path] = []\n    training_paths: list[Path] = []\n    if test_regions:\n        # Download admin files if they do not exist\n        admin1_fpath = admin_dir / \"geoBoundariesCGAZ_ADM1.shp\"\n        admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n\n        if not admin1_fpath.exists() or not admin2_fpath.exists():\n            download_admin_files(admin_dir)\n\n        # Load the admin files\n        admin1 = gpd.read_file(admin1_fpath)\n        admin2 = gpd.read_file(admin2_fpath)\n\n        # Get the regions from the admin files\n        test_region_geometries_adm1 = admin1[admin1[\"shapeName\"].isin(test_regions)]\n        test_region_geometries_adm2 = admin2[admin2[\"shapeName\"].isin(test_regions)]\n\n        logger.debug(f\"Found {len(test_region_geometries_adm1)} admin1-regions in {admin1_fpath}\")\n        logger.debug(f\"Found {len(test_region_geometries_adm2)} admin2-regions in {admin2_fpath}\")\n\n        for fpath in data_paths:\n            planet_id = fpath.stem\n            footprint = footprints[footprints.image_id == planet_id]\n            # Check if any label is intersecting with the test regions\n            adm1_intersects = footprint.overlay(test_region_geometries_adm1, how=\"intersection\")\n            adm2_intersects = footprint.overlay(test_region_geometries_adm2, how=\"intersection\")\n\n            if (len(adm1_intersects.index) &gt; 0) or (len(adm2_intersects.index) &gt; 0):\n                test_paths.append(fpath)\n            else:\n                training_paths.append(fpath)\n    else:\n        training_paths = data_paths\n\n    # 2. Split by random sampling\n    cross_val_paths: list[Path]\n    val_test_paths: list[Path]\n    if len(training_paths) &gt; 0:\n        cross_val_paths, val_test_paths = train_test_split(training_paths, test_size=test_val_split, random_state=42)\n    else:\n        cross_val_paths, val_test_paths = [], []\n        logger.warning(\"No left over training samples found. Skipping train-val split.\")\n\n    logger.info(\n        f\"Split the data into {len(cross_val_paths)} cross-val (train + val), \"\n        f\"{len(val_test_paths)} val-test (variance) and {len(test_paths)} test (region) samples.\"\n    )\n\n    fpathgen = chain(cross_val_paths, val_test_paths, test_paths)\n    modegen = chain(\n        repeat(\"cross-val\", len(cross_val_paths)),\n        repeat(\"val-test\", len(val_test_paths)),\n        repeat(\"test\", len(test_paths)),\n    )\n\n    return zip(fpathgen, modegen)\n</code></pre>"},{"location":"reference/darts/legacy_training/preprocess/s2/","title":"darts.legacy_training.preprocess.s2","text":""},{"location":"reference/darts/legacy_training/preprocess/s2/#darts.legacy_training.preprocess.s2","title":"darts.legacy_training.preprocess.s2","text":"<p>Preprocessing functions for legacy training.</p>"},{"location":"reference/darts/legacy_training/preprocess/s2/#darts.legacy_training.preprocess.s2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/legacy_training/preprocess/s2/#darts.legacy_training.preprocess.s2.preprocess_s2_train_data","title":"preprocess_s2_train_data","text":"<pre><code>preprocess_s2_train_data(\n    *,\n    bands: list[str],\n    sentinel2_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    dask_worker: int = min(\n        16, multiprocessing.cpu_count() - 1\n    ),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n)\n</code></pre> <p>Preprocess Sentinel 2 data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Sentinel 2 scenes.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/s2.py</code> <pre><code>def preprocess_s2_train_data(\n    *,\n    bands: list[str],\n    sentinel2_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Sentinel 2 data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        sentinel2_dir (Path): The directory containing the Sentinel 2 scenes.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_s2_masks, load_s2_scene, load_tcvis\n    from darts_acquisition.s2 import parse_s2_tile_id\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        s2_paths = sorted(sentinel2_dir.glob(\"*/\"))\n        logger.info(f\"Found {len(s2_paths)} Sentinel 2 scenes in {sentinel2_dir}\")\n        path_gen = split_dataset_paths(s2_paths, train_data_dir, test_val_split, test_regions, admin_dir)\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(s2_paths), console=console\n        ):\n            try:\n                _, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}' ({tile_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{tile_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{tile_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_s2_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 10\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_s2_masks(fpath, optical.odc.geobox)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                labels = gpd.read_file(fpath / f\"{s2_tile_id}.shp\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile,\n                    labels,\n                    bands,\n                    norm_factors,\n                    patch_size,\n                    overlap,\n                    exclude_nopositive,\n                    exclude_nan,\n                    device,\n                    mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}'\"\n                    f\"({tile_id=}) with {patch_id} patches.\"\n                )\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"sentinel2_dir\": sentinel2_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/preprocess/s2/#darts.legacy_training.preprocess.s2.split_dataset_paths","title":"split_dataset_paths","text":"<pre><code>split_dataset_paths(\n    s2_paths: list[pathlib.Path],\n    train_data_dir: pathlib.Path,\n    test_val_split: float,\n    test_regions: list[str] | None,\n    admin_dir: pathlib.Path,\n)\n</code></pre> <p>Split the dataset into a cross-val, a val-test and a test dataset.</p> <p>Returns a generator with: input-path, output-path and split/mode. The test set is splitted first by the given regions and is meant to be used to evaluate the regional value shift. Then the val-test set is splitted then by random at given size to evaluate the variance value shift.</p> <p>Parameters:</p> <ul> <li> <code>s2_paths</code>               (<code>list[pathlib.Path]</code>)           \u2013            <p>All paths found with tiffs.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output path.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>)           \u2013            <p>val-test ratio.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | None</code>)           \u2013            <p>test regions.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin level shape-files.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>[zip[tuple[Path, Path, str]]]: A generator with input-path, output-path and split/mode.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/s2.py</code> <pre><code>def split_dataset_paths(\n    s2_paths: list[Path], train_data_dir: Path, test_val_split: float, test_regions: list[str] | None, admin_dir: Path\n):\n    \"\"\"Split the dataset into a cross-val, a val-test and a test dataset.\n\n    Returns a generator with: input-path, output-path and split/mode.\n    The test set is splitted first by the given regions and is meant to be used to evaluate the regional value shift.\n    Then the val-test set is splitted then by random at given size to evaluate the variance value shift.\n\n    Args:\n        s2_paths (list[Path]): All paths found with tiffs.\n        train_data_dir (Path): Output path.\n        test_val_split (float): val-test ratio.\n        test_regions (list[str] | None): test regions.\n        admin_dir (Path): The directory containing the admin level shape-files.\n\n    Returns:\n        [zip[tuple[Path, Path, str]]]: A generator with input-path, output-path and split/mode.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    from darts_acquisition.admin import download_admin_files\n    from darts_acquisition.s2 import parse_s2_tile_id\n    from sklearn.model_selection import train_test_split\n\n    train_data_dir.mkdir(exist_ok=True, parents=True)\n\n    # 1. Split regions\n    test_paths: list[Path] = []\n    training_paths: list[Path] = []\n    if test_regions:\n        # Download admin files if they do not exist\n        admin1_fpath = admin_dir / \"geoBoundariesCGAZ_ADM1.shp\"\n        admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n\n        if not admin1_fpath.exists() or not admin2_fpath.exists():\n            download_admin_files(admin_dir)\n\n        # Load the admin files\n        admin1 = gpd.read_file(admin1_fpath)\n        admin2 = gpd.read_file(admin2_fpath)\n\n        # Get the regions from the admin files\n        test_region_geometries_adm1 = admin1[admin1[\"shapeName\"].isin(test_regions)]\n        test_region_geometries_adm2 = admin2[admin2[\"shapeName\"].isin(test_regions)]\n\n        logger.debug(f\"Found {len(test_region_geometries_adm1)} admin1-regions in {admin1_fpath}\")\n        logger.debug(f\"Found {len(test_region_geometries_adm2)} admin2-regions in {admin2_fpath}\")\n\n        for fpath in s2_paths:\n            _, s2_tile_id, _ = parse_s2_tile_id(fpath)\n            labels = gpd.read_file(fpath / f\"{s2_tile_id}.shp\").to_crs(\"EPSG:4326\")\n            # Check if any label is intersecting with the test regions\n            adm1_intersects = labels.overlay(test_region_geometries_adm1, how=\"intersection\")\n            adm2_intersects = labels.overlay(test_region_geometries_adm2, how=\"intersection\")\n\n            if (len(adm1_intersects.index) &gt; 0) or (len(adm2_intersects.index) &gt; 0):\n                test_paths.append(fpath)\n            else:\n                training_paths.append(fpath)\n    else:\n        training_paths = s2_paths\n\n    # 2. Split by random sampling\n    cross_val_paths: list[Path]\n    val_test_paths: list[Path]\n    if len(training_paths) &gt; 0:\n        cross_val_paths, val_test_paths = train_test_split(training_paths, test_size=test_val_split, random_state=42)\n    else:\n        cross_val_paths, val_test_paths = [], []\n        logger.warning(\"No left over training samples found. Skipping train-val split.\")\n\n    logger.info(\n        f\"Split the data into {len(cross_val_paths)} cross-val (train + val), \"\n        f\"{len(val_test_paths)} val-test (variance) and {len(test_paths)} test (region) samples.\"\n    )\n\n    fpathgen = chain(cross_val_paths, val_test_paths, test_paths)\n    modegen = chain(\n        repeat(\"cross-val\", len(cross_val_paths)),\n        repeat(\"val-test\", len(val_test_paths)),\n        repeat(\"test\", len(test_paths)),\n    )\n\n    return zip(fpathgen, modegen)\n</code></pre>"},{"location":"reference/darts/legacy_training/sweep/","title":"darts.legacy_training.sweep","text":""},{"location":"reference/darts/legacy_training/sweep/#darts.legacy_training.sweep","title":"darts.legacy_training.sweep","text":"<p>Sweeping scripts for DARTS.</p> <p>Unused yet!</p>"},{"location":"reference/darts/legacy_training/sweep/#darts.legacy_training.sweep.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/legacy_training/sweep/#darts.legacy_training.sweep._apply_wandb_env","title":"_apply_wandb_env","text":"<pre><code>_apply_wandb_env(wandb_env)\n</code></pre> Source code in <code>darts/src/darts/legacy_training/sweep.py</code> <pre><code>def _apply_wandb_env(wandb_env):\n    for k, v in wandb_env.items():\n        os.environ[k] = v\n</code></pre>"},{"location":"reference/darts/legacy_training/sweep/#darts.legacy_training.sweep._gather_and_reset_wandb_env","title":"_gather_and_reset_wandb_env","text":"<pre><code>_gather_and_reset_wandb_env()\n</code></pre> Source code in <code>darts/src/darts/legacy_training/sweep.py</code> <pre><code>def _gather_and_reset_wandb_env():\n    exclude = {\n        \"WANDB_PROJECT\",\n        \"WANDB_ENTITY\",\n        \"WANDB_API_KEY\",\n    }\n    wandb_env = {}\n    for k, v in os.environ.items():\n        if k.startswith(\"WANDB_\") and k not in exclude:\n            wandb_env[k] = v\n            del os.environ[k]\n    return wandb_env\n</code></pre>"},{"location":"reference/darts/legacy_training/sweep/#darts.legacy_training.sweep.optuna_sweep_smp","title":"optuna_sweep_smp","text":"<pre><code>optuna_sweep_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    sweep_config: pathlib.Path,\n    n_trials: int = 10,\n    sweep_db: str | None = None,\n    sweep_id: str | None = None,\n    n_folds: int = 5,\n    n_randoms: int = 3,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    augment: bool = True,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n)\n</code></pre> <p>Create an optuna sweep and run it on the specified cuda device, or continue an existing sweep.</p> <p>If <code>sweep_id</code> already exists in <code>sweep_db</code>, the sweep will be continued. Otherwise, a new sweep will be created.</p> <p>If a <code>cuda_device</code> is specified, run an agent on this device. If None, do nothing.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>This will use cross-validation.</p> Example <p>In one terminal, start a sweep: <pre><code>    $ rye run darts sweep-smp --config-file /path/to/sweep-config.toml\n    ...  # Many logs\n    Created sweep with ID 123456789\n    ... # More logs from spawned agent\n</code></pre></p> <p>In another terminal, start an a second agent: <pre><code>    $ rye run darts sweep-smp --sweep-id 123456789\n    ...\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory.</p> </li> <li> <code>sweep_config</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration. Hyperparameters must contain the following fields: <code>model_arch</code>, <code>model_encoder</code>, <code>augment</code>, <code>gamma</code>, <code>batch_size</code>. Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.</p> </li> <li> <code>n_trials</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of runs to execute. Defaults to 10.</p> </li> <li> <code>sweep_db</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the optuna database. If None, a new database will be created.</p> </li> <li> <code>sweep_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The ID of the sweep. If None, a new sweep will be created. Defaults to None.</p> </li> <li> <code>n_folds</code>               (<code>(int, optinoal)</code>, default:                   <code>5</code> )           \u2013            <p>Number of folds in cross-validation. Max 5. Defaults to 5.</p> </li> <li> <code>n_randoms</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of repetitions with different random-seeds. First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline. Rest are pseudo-random generated beforehand, hence always equal. Defaults to 5.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. Defaults to None.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/sweep.py</code> <pre><code>def optuna_sweep_smp(\n    *,\n    # Data and sweep config\n    train_data_dir: Path,\n    sweep_config: Path,\n    n_trials: int = 10,\n    sweep_db: str | None = None,\n    sweep_id: str | None = None,\n    n_folds: int = 5,\n    n_randoms: int = 3,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    # Hyperparameters (default values if not provided by sweep-config)\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n):\n    \"\"\"Create an optuna sweep and run it on the specified cuda device, or continue an existing sweep.\n\n    If `sweep_id` already exists in `sweep_db`, the sweep will be continued. Otherwise, a new sweep will be created.\n\n    If a `cuda_device` is specified, run an agent on this device. If None, do nothing.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    This will use cross-validation.\n\n    Example:\n        In one terminal, start a sweep:\n        ```sh\n            $ rye run darts sweep-smp --config-file /path/to/sweep-config.toml\n            ...  # Many logs\n            Created sweep with ID 123456789\n            ... # More logs from spawned agent\n        ```\n\n        In another terminal, start an a second agent:\n        ```sh\n            $ rye run darts sweep-smp --sweep-id 123456789\n            ...\n        ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory.\n        sweep_config (Path): Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration.\n            Hyperparameters must contain the following fields: `model_arch`, `model_encoder`, `augment`, `gamma`,\n            `batch_size`.\n            Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.\n        n_trials (int, optional): Number of runs to execute. Defaults to 10.\n        sweep_db (str | None, optional): Path to the optuna database. If None, a new database will be created.\n        sweep_id (str | None, optional): The ID of the sweep. If None, a new sweep will be created. Defaults to None.\n        n_folds (int, optinoal): Number of folds in cross-validation. Max 5. Defaults to 5.\n        n_randoms (int, optional): Number of repetitions with different random-seeds.\n            First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline.\n            Rest are pseudo-random generated beforehand, hence always equal.\n            Defaults to 5.\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str | None, optional): The device to run the model on. Defaults to None.\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n\n    \"\"\"\n    import optuna\n    from names_generator import generate_name\n\n    from darts.legacy_training.util import suggest_optuna_params_from_wandb_config\n\n    with sweep_config.open(\"r\") as f:\n        sweep_configuration = yaml.safe_load(f)\n\n    logger.debug(f\"Loaded sweep configuration from {sweep_config.resolve()}:\\n{sweep_configuration}\")\n\n    # Create a new study-id if none is given\n    if sweep_id is None:\n        sweep_id = f\"sweep-{generate_name('hyphen')}\"\n        logger.info(f\"Generated new sweep ID: {sweep_id}\")\n        logger.info(\"To start a sweep agents, use the following command:\")\n        logger.info(f\"$ rye run darts optuna-sweep-smp --sweep-id {sweep_id}\")\n\n    artifact_dir = artifact_dir / sweep_id\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    def objective(trial):\n        hparams = suggest_optuna_params_from_wandb_config(trial, sweep_configuration)\n        logger.info(f\"Running trial with parameters: {hparams}\")\n\n        # Get the trial a more readable name\n        trial_name = f\"{generate_name(style='hyphen')}-{trial.number}\"\n\n        # We set the default weights to None, to be able to use different architectures\n        model_encoder_weights = None\n        # We set early stopping to None, because wandb will handle the early stopping\n        early_stopping_patience = None\n\n        # Overwrite the default values with the suggested ones, if they are present\n        learning_rate_trial = hparams.get(\"learning_rate\", learning_rate)\n        gamma_trial = hparams.get(\"gamma\", gamma)\n        focal_loss_alpha_trial = hparams.get(\"focal_loss_alpha\", focal_loss_alpha)\n        focal_loss_gamma_trial = hparams.get(\"focal_loss_gamma\", focal_loss_gamma)\n        batch_size_trial = hparams.get(\"batch_size\", batch_size)\n        model_arch_trial = hparams.get(\"model_arch\", model_arch)\n        model_encoder_trial = hparams.get(\"model_encoder\", model_encoder)\n        augment_trial = hparams.get(\"augment\", augment)\n\n        crossval_scores = defaultdict(list)\n\n        folds = list(range(n_folds))\n        rng = random.Random(42)\n        seeds = [42, 21, 69]\n        if n_randoms &gt; 3:\n            seeds += rng.sample(range(9999), n_randoms - 3)\n        elif n_randoms &lt; 3:\n            seeds = seeds[:n_randoms]\n\n        for random_seed in seeds:\n            for fold in folds:\n                logger.info(f\"Running cross-validation fold {fold}\")\n                _gather_and_reset_wandb_env()\n                trainer = train_smp(\n                    # Data config\n                    train_data_dir=train_data_dir,\n                    artifact_dir=artifact_dir,\n                    fold=fold,\n                    random_seed=random_seed,\n                    # Hyperparameters\n                    model_arch=model_arch_trial,\n                    model_encoder=model_encoder_trial,\n                    model_encoder_weights=model_encoder_weights,\n                    augment=augment_trial,\n                    learning_rate=learning_rate_trial,\n                    gamma=gamma_trial,\n                    focal_loss_alpha=focal_loss_alpha_trial,\n                    focal_loss_gamma=focal_loss_gamma_trial,\n                    batch_size=batch_size_trial,\n                    # Epoch and Logging config\n                    early_stopping_patience=early_stopping_patience,\n                    max_epochs=max_epochs,\n                    log_every_n_steps=log_every_n_steps,\n                    check_val_every_n_epoch=check_val_every_n_epoch,\n                    plot_every_n_val_epochs=plot_every_n_val_epochs,\n                    # Device and Manager config\n                    num_workers=num_workers,\n                    device=device,\n                    wandb_entity=wandb_entity,\n                    wandb_project=wandb_project,\n                    wandb_group=sweep_id,\n                    trial_name=trial_name,\n                    run_name=f\"{trial_name}-f{fold}r{random_seed}\",\n                )\n                for metric, value in trainer.callback_metrics.items():\n                    crossval_scores[metric].append(value.item())\n\n        logger.debug(f\"Cross-validation scores: {crossval_scores}\")\n        crossval_jaccard = mean(crossval_scores[\"val/JaccardIndex\"])\n        crossval_recall = mean(crossval_scores[\"val/Recall\"])\n\n        return crossval_jaccard, crossval_recall\n\n    study = optuna.create_study(\n        storage=sweep_db,\n        study_name=sweep_id,\n        directions=[\"maximize\", \"maximize\"],\n        load_if_exists=True,\n    )\n\n    if device is None:\n        logger.info(\"No device specified, closing script...\")\n        return\n\n    logger.info(\"Starting optimizing\")\n    study.optimize(objective, n_trials=n_trials)\n</code></pre>"},{"location":"reference/darts/legacy_training/sweep/#darts.legacy_training.sweep.train_smp","title":"train_smp","text":"<pre><code>train_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    fold: int = 0,\n    continue_from_checkpoint: pathlib.Path | None = None,\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the training of the SMP model.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.</p> <p>Each training run is assigned a unique name and id pair and optionally a trial name. The name, which the user can provide, should be used as a grouping mechanism of equal hyperparameter and code. Hence, different versions of the same name should only differ by random state or run settings parameter, like logs. Each version is assigned a unique id. Artifacts (metrics &amp; checkpoints) are then stored under <code>{artifact_dir}/{run_name}/{run_id}</code> in no-crossval runs. If <code>trial_name</code> is specified, the artifacts are stored under <code>{artifact_dir}/{trial_name}/{run_name}-{run_id}</code>. Wandb logs are always stored under <code>{wandb_entity}/{wandb_project}/{run_name}</code>, regardless of <code>trial_name</code>. However, they are further grouped by the <code>trial_name</code> (via job_type), if specified. Both <code>run_name</code> and <code>run_id</code> are also stored in the hparams of each checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>fold</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The current fold to train on. Must be in [0, 4]. Defaults to 0.</p> </li> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to a checkpoint to continue training from. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the encoder weights. Defaults to None.</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>random_seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The device to run the model on. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>wandb_group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Wandb group. Usefull for CV-Sweeps. Defaults to None.</p> </li> <li> <code>run_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of this run, as a further grouping method for logs etc. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>run_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ID of the run. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>trial_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation run / trial. This effects primary logging and artifact storage. If None, will do nothing. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def train_smp(\n    *,\n    # Data config\n    train_data_dir: Path,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    fold: int = 0,\n    continue_from_checkpoint: Path | None = None,\n    # Hyperparameters\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the training of the SMP model.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.\n\n    Each training run is assigned a unique **name** and **id** pair and optionally a trial name.\n    The name, which the user _can_ provide, should be used as a grouping mechanism of equal hyperparameter and code.\n    Hence, different versions of the same name should only differ by random state or run settings parameter, like logs.\n    Each version is assigned a unique id.\n    Artifacts (metrics &amp; checkpoints) are then stored under `{artifact_dir}/{run_name}/{run_id}` in no-crossval runs.\n    If `trial_name` is specified, the artifacts are stored under `{artifact_dir}/{trial_name}/{run_name}-{run_id}`.\n    Wandb logs are always stored under `{wandb_entity}/{wandb_project}/{run_name}`, regardless of `trial_name`.\n    However, they are further grouped by the `trial_name` (via job_type), if specified.\n    Both `run_name` and `run_id` are also stored in the hparams of each checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        fold (int, optional): The current fold to train on. Must be in [0, 4]. Defaults to 0.\n        continue_from_checkpoint (Path | None, optional): Path to a checkpoint to continue training from.\n            Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        model_encoder_weights (str | None, optional): Path to the encoder weights. Defaults to None.\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        early_stopping_patience (int, optional): Number of epochs to wait for improvement before stopping.\n            Defaults to 5.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        random_seed (int, optional): Random seed for deterministic training. Defaults to 42.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str, optional): The device to run the model on. Defaults to \"auto\".\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        wandb_group (str | None, optional): Wandb group. Usefull for CV-Sweeps. Defaults to None.\n        run_name (str | None, optional): Name of this run, as a further grouping method for logs etc.\n            If None, will generate a random one. Defaults to None.\n        run_id (str | None, optional): ID of the run. If None, will generate a random one. Defaults to None.\n        trial_name (str | None, optional): Name of the cross-validation run / trial.\n            This effects primary logging and artifact storage.\n            If None, will do nothing. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.legacy_training.util import generate_id, get_generated_name\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run_name or get_generated_name(artifact_dir)\n    run_id = run_id or generate_id()\n\n    logger.info(f\"Starting training '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(\n        f\"Using config:\\n\\t{model_arch=}\\n\\t{model_encoder=}\\n\\t{model_encoder_weights=}\\n\\t{augment=}\\n\\t\"\n        f\"{learning_rate=}\\n\\t{gamma=}\\n\\t{batch_size=}\\n\\t{max_epochs=}\\n\\t{log_every_n_steps=}\\n\\t\"\n        f\"{check_val_every_n_epoch=}\\n\\t{early_stopping_patience=}\\n\\t{plot_every_n_val_epochs=}\\n\\t{num_workers=}\"\n        f\"\\n\\t{device=}\\n\\t{random_seed=}\"\n    )\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(random_seed, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    config = SMPSegmenterConfig(\n        input_combination=preprocess_config[\"bands\"],\n        model={\n            \"arch\": model_arch,\n            \"encoder_name\": model_encoder,\n            \"encoder_weights\": model_encoder_weights,\n            \"in_channels\": len(preprocess_config[\"bands\"]),\n            \"classes\": 1,\n        },\n        norm_factors=preprocess_config[\"norm_factors\"],\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir / \"cross-val.zarr\",\n        batch_size=batch_size,\n        fold=fold,\n        augment=augment,\n        num_workers=num_workers,\n    )\n    model = SMPSegmenter(\n        config=config,\n        learning_rate=learning_rate,\n        gamma=gamma,\n        focal_loss_alpha=focal_loss_alpha,\n        focal_loss_gamma=focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        trial_name=trial_name,\n        random_seed=random_seed,\n    )\n\n    # Loggers\n    is_crossval = bool(trial_name)\n    trainer_loggers = [\n        CSVLogger(\n            save_dir=artifact_dir,\n            name=run_name if not is_crossval else trial_name,\n            version=run_id if not is_crossval else f\"{run_name}-{run_id}\",\n        ),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            group=wandb_group,\n            job_type=trial_name,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            input_combination=config[\"input_combination\"],\n            val_set=f\"val{fold}\",\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            is_crossval=is_crossval,\n        ),\n    ]\n    if early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {early_stopping_patience}\")\n        early_stopping = EarlyStopping(monitor=\"val/JaccardIndex\", mode=\"max\", patience=early_stopping_patience)\n        callbacks.append(early_stopping)\n\n    # Train\n    trainer = L.Trainer(\n        max_epochs=max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=check_val_every_n_epoch,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=False,\n    )\n    trainer.fit(model, datamodule, ckpt_path=continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/legacy_training/sweep/#darts.legacy_training.sweep.wandb_cv_sweep_smp","title":"wandb_cv_sweep_smp","text":"<pre><code>wandb_cv_sweep_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    sweep_config: pathlib.Path,\n    n_trials: int = 10,\n    n_folds: int = 5,\n    n_randoms: int = 5,\n    sweep_id: str | None = None,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Create or continue a cross-validation sweep with wandb and run it on the specified cuda device.</p> <p>If <code>sweep_id</code> is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued. All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}. Since each sweep-configuration has (currently) an own name and id, a single run can be found under: {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.</p> <p>If a <code>cuda_device</code> is specified, run an agent on this device. If None, do nothing.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>This will NOT use cross-validation. For cross-validation, use <code>optuna_sweep_smp</code>.</p> Example <p>In one terminal, start a sweep: <pre><code>    $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n    ...  # Many logs\n    Created sweep with ID 123456789\n    ... # More logs from spawned agent\n</code></pre></p> <p>In another terminal, start an a second agent: <pre><code>    $ rye run darts wandb-sweep-smp --sweep-id 123456789\n    ...\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory.</p> </li> <li> <code>sweep_config</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration. Hyperparameters must contain the following fields: <code>model_arch</code>, <code>model_encoder</code>, <code>augment</code>, <code>gamma</code>, <code>batch_size</code>. Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.</p> </li> <li> <code>n_trials</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of runs to execute. Only used for non-grid sweeps. Defaults to 10.</p> </li> <li> <code>n_folds</code>               (<code>(int, optinoal)</code>, default:                   <code>5</code> )           \u2013            <p>Number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>n_randoms</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of repetitions with different random-seeds. First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline. Rest are pseudo-random generated beforehand, hence always equal. Defaults to 5.</p> </li> <li> <code>sweep_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The ID of the sweep. If None, a new sweep will be created. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. Defaults to None.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/sweep.py</code> <pre><code>def wandb_cv_sweep_smp(\n    *,\n    # Data and sweep config\n    train_data_dir: Path,\n    sweep_config: Path,\n    n_trials: int = 10,\n    n_folds: int = 5,\n    n_randoms: int = 5,\n    sweep_id: str | None = None,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n):\n    \"\"\"Create or continue a cross-validation sweep with wandb and run it on the specified cuda device.\n\n    If `sweep_id` is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued.\n    All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}.\n    Since each sweep-configuration has (currently) an own name and id, a single run can be found under:\n    {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.\n\n    If a `cuda_device` is specified, run an agent on this device. If None, do nothing.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    This will NOT use cross-validation. For cross-validation, use `optuna_sweep_smp`.\n\n    Example:\n        In one terminal, start a sweep:\n        ```sh\n            $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n            ...  # Many logs\n            Created sweep with ID 123456789\n            ... # More logs from spawned agent\n        ```\n\n        In another terminal, start an a second agent:\n        ```sh\n            $ rye run darts wandb-sweep-smp --sweep-id 123456789\n            ...\n        ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory.\n        sweep_config (Path): Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration.\n            Hyperparameters must contain the following fields: `model_arch`, `model_encoder`, `augment`, `gamma`,\n            `batch_size`.\n            Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.\n        n_trials (int, optional): Number of runs to execute. Only used for non-grid sweeps. Defaults to 10.\n        n_folds (int, optinoal): Number of folds in cross-validation. Defaults to 5.\n        n_randoms (int, optional): Number of repetitions with different random-seeds.\n            First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline.\n            Rest are pseudo-random generated beforehand, hence always equal.\n            Defaults to 5.\n        sweep_id (str | None, optional): The ID of the sweep. If None, a new sweep will be created. Defaults to None.\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str | None, optional): The device to run the model on. Defaults to None.\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n\n    \"\"\"\n    import wandb\n\n    # Wandb has a stupid way of logging (they log per default with click.echo to stdout)\n    # We need to silence this and redirect all possible logs to our logger\n    # wl = wandb.setup({\"silent\": True})\n    # wandb.termsetup(wl.settings, logging.getLogger(\"wandb\"))\n    # LoggingManager.apply_logging_handlers(\"wandb\")\n\n    if sweep_id is not None and device is None:\n        logger.warning(\"Continuing a sweep without specifying a device will not do anything.\")\n\n    with sweep_config.open(\"r\") as f:\n        sweep_configuration = yaml.safe_load(f)\n\n    score_metric = sweep_configuration[\"metric\"][\"name\"]\n\n    logger.debug(f\"Loaded sweep configuration from {sweep_config.resolve()}:\\n{sweep_configuration}\")\n\n    if sweep_id is None:\n        sweep_id = wandb.sweep(sweep=sweep_configuration, project=wandb_project, entity=wandb_entity)\n        logger.info(f\"Created sweep with ID {sweep_id}\")\n        logger.info(\"To start a sweep agents, use the following command:\")\n        logger.info(f\"$ rye run darts sweep_smp --sweep-id {sweep_id}\")\n\n    # This complete function is a super dubious hack and neither used nor tested yet:\n    # Currently, wandb doesn't provide a way to have multiple runs with same configuration run on the same process\n    # They designed their lib to use multiprocessing or similar. However, using mp is not really an option here,\n    # since that would clash with PyTorch Lightning.\n    # Hence, this function alters the environment variables which are used by the wandb.init to overwrite existing\n    # instances / runs.\n    # A sweep-run (hyperparameter configuration recommended by wandb) is created first. The wandb sweep algo uses this\n    # run as benchmark (logging of the avg. score from all the folds)\n    # Then a fold is run over this configuration, each overwriting the existing wandb-env and creating a new run.\n    # I recommend reading this issue here:\n    # https://github.com/wandb/wandb/issues/5119\n    def _sweep_run():\n        with wandb.init(config=sweep_configuration) as sweep_run:\n            # We need to manually log the run data since the wandb logger only logs to its own logs and click\n            logger.info(f\"Starting sweep run '{sweep_run.settings.run_name}'\")\n            logger.debug(f\"Run data is saved locally in {Path(sweep_run.settings.sync_dir).resolve()}\")\n            logger.debug(f\"View project at {sweep_run.settings.project_url}\")\n            logger.debug(f\"View sweep at {sweep_run.settings.sweep_url}\")\n            logger.debug(f\"View run at {sweep_run.settings.run_url}\")\n\n            # We set the default weights to None, to be able to use different architectures\n            model_encoder_weights = None\n            # We set early stopping to None, because wandb will handle the early stopping\n            early_stopping_patience = None\n            learning_rate = sweep_run.config[\"learning_rate\"]\n            gamma = sweep_run.config[\"gamma\"]\n            batch_size = sweep_run.config[\"batch_size\"]\n            model_arch = sweep_run.config[\"model_arch\"]\n            model_encoder = sweep_run.config[\"model_encoder\"]\n            augment = sweep_run.config[\"augment\"]\n            focal_loss_alpha = sweep_run.config[\"focal_loss_alpha\"]\n            focal_loss_gamma = sweep_run.config[\"focal_loss_gamma\"]\n\n            folds = list(range(n_folds))\n            rng = random.Random(42)\n            seeds = [42, 21, 69]\n            if n_randoms &gt; 3:\n                seeds += rng.sample(range(9999), n_randoms - 3)\n            elif n_randoms &lt; 3:\n                seeds = seeds[:n_randoms]\n\n            sweep_run_name = sweep_run.name\n            sweep_run_env = _gather_and_reset_wandb_env()\n\n            cvscores = []\n            for fold, seed in product(folds, seeds):\n                _gather_and_reset_wandb_env()\n                with wandb.init(group=sweep_run_name, name=f\"{sweep_run_name}-{fold=}-{seed=}\") as cv_run:\n                    trainer = train_smp(\n                        # Data config\n                        train_data_dir=train_data_dir,\n                        artifact_dir=artifact_dir,\n                        current_fold=fold,\n                        # Hyperparameters\n                        model_arch=model_arch,\n                        model_encoder=model_encoder,\n                        model_encoder_weights=model_encoder_weights,\n                        augment=augment,\n                        learning_rate=learning_rate,\n                        gamma=gamma,\n                        focal_loss_alpha=focal_loss_alpha,\n                        focal_loss_gamma=focal_loss_gamma,\n                        batch_size=batch_size,\n                        # Epoch and Logging config\n                        early_stopping_patience=early_stopping_patience,\n                        max_epochs=max_epochs,\n                        log_every_n_steps=log_every_n_steps,\n                        check_val_every_n_epoch=check_val_every_n_epoch,\n                        plot_every_n_val_epochs=plot_every_n_val_epochs,\n                        # Device and Manager config\n                        random_seed=seed,\n                        num_workers=num_workers,\n                        device=device,\n                        wandb_entity=wandb_entity,\n                        wandb_project=wandb_project,\n                        run_name=cv_run.name,\n                    )\n                    score = trainer.logged_metrics[score_metric]\n                    cvscores.append(score)\n\n            _apply_wandb_env(sweep_run_env)\n            sweep_run.log({score_metric: mean(cvscores)})  # TODO: make score-var selectable\n\n    if device is None:\n        logger.info(\"No device specified, closing script...\")\n        return\n\n    logger.info(\"Starting a default sweep agent\")\n    wandb.agent(sweep_id, function=_sweep_run, count=n_trials, project=wandb_project, entity=wandb_entity)\n</code></pre>"},{"location":"reference/darts/legacy_training/test/","title":"darts.legacy_training.test","text":""},{"location":"reference/darts/legacy_training/test/#darts.legacy_training.test","title":"darts.legacy_training.test","text":"<p>Testing scripts for DARTS.</p>"},{"location":"reference/darts/legacy_training/test/#darts.legacy_training.test.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/legacy_training/test/#darts.legacy_training.test.test_smp","title":"test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size. Defaults to 8.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Directory to save artifacts. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device to use. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/test.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int, optional): Batch size. Defaults to 8.\n        artifact_dir (Path, optional): Directory to save artifacts. Defaults to Path(\"lightning_logs\").\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device (int | str, optional): Device to use. Defaults to \"auto\".\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n    logger.info(f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device=}\")\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    # Data and model\n    datamodule_val_test = DartsDataModule(\n        data_dir=train_data_dir / \"val-test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    datamodule_test = DartsDataModule(\n        data_dir=train_data_dir / \"test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / run_name / run_id / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    model = SMPSegmenter.load_from_checkpoint(model_ckp)\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=run_name, version=run_id),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            id=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    metrics_cb = BinarySegmentationMetrics(\n        input_combination=preprocess_config[\"bands\"],\n    )\n    callbacks = [\n        RichProgressBar(),\n        metrics_cb,\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=True,\n    )\n    # Overwrite the names of the test sets to test agains two separate sets\n    metrics_cb.test_set = \"val-test\"\n    model.test_set = \"val-test\"\n    trainer.test(model, datamodule_val_test, ckpt_path=model_ckp)\n    metrics_cb.test_set = \"test\"\n    model.test_set = \"test\"\n    trainer.test(model, datamodule_test)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/legacy_training/train/","title":"darts.legacy_training.train","text":""},{"location":"reference/darts/legacy_training/train/#darts.legacy_training.train","title":"darts.legacy_training.train","text":"<p>Training and sweeping scripts for DARTS.</p>"},{"location":"reference/darts/legacy_training/train/#darts.legacy_training.train.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/legacy_training/train/#darts.legacy_training.train.train_smp","title":"train_smp","text":"<pre><code>train_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    fold: int = 0,\n    continue_from_checkpoint: pathlib.Path | None = None,\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the training of the SMP model.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.</p> <p>Each training run is assigned a unique name and id pair and optionally a trial name. The name, which the user can provide, should be used as a grouping mechanism of equal hyperparameter and code. Hence, different versions of the same name should only differ by random state or run settings parameter, like logs. Each version is assigned a unique id. Artifacts (metrics &amp; checkpoints) are then stored under <code>{artifact_dir}/{run_name}/{run_id}</code> in no-crossval runs. If <code>trial_name</code> is specified, the artifacts are stored under <code>{artifact_dir}/{trial_name}/{run_name}-{run_id}</code>. Wandb logs are always stored under <code>{wandb_entity}/{wandb_project}/{run_name}</code>, regardless of <code>trial_name</code>. However, they are further grouped by the <code>trial_name</code> (via job_type), if specified. Both <code>run_name</code> and <code>run_id</code> are also stored in the hparams of each checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>fold</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The current fold to train on. Must be in [0, 4]. Defaults to 0.</p> </li> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to a checkpoint to continue training from. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the encoder weights. Defaults to None.</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>random_seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The device to run the model on. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>wandb_group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Wandb group. Usefull for CV-Sweeps. Defaults to None.</p> </li> <li> <code>run_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of this run, as a further grouping method for logs etc. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>run_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ID of the run. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>trial_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation run / trial. This effects primary logging and artifact storage. If None, will do nothing. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def train_smp(\n    *,\n    # Data config\n    train_data_dir: Path,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    fold: int = 0,\n    continue_from_checkpoint: Path | None = None,\n    # Hyperparameters\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the training of the SMP model.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.\n\n    Each training run is assigned a unique **name** and **id** pair and optionally a trial name.\n    The name, which the user _can_ provide, should be used as a grouping mechanism of equal hyperparameter and code.\n    Hence, different versions of the same name should only differ by random state or run settings parameter, like logs.\n    Each version is assigned a unique id.\n    Artifacts (metrics &amp; checkpoints) are then stored under `{artifact_dir}/{run_name}/{run_id}` in no-crossval runs.\n    If `trial_name` is specified, the artifacts are stored under `{artifact_dir}/{trial_name}/{run_name}-{run_id}`.\n    Wandb logs are always stored under `{wandb_entity}/{wandb_project}/{run_name}`, regardless of `trial_name`.\n    However, they are further grouped by the `trial_name` (via job_type), if specified.\n    Both `run_name` and `run_id` are also stored in the hparams of each checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        fold (int, optional): The current fold to train on. Must be in [0, 4]. Defaults to 0.\n        continue_from_checkpoint (Path | None, optional): Path to a checkpoint to continue training from.\n            Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        model_encoder_weights (str | None, optional): Path to the encoder weights. Defaults to None.\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        early_stopping_patience (int, optional): Number of epochs to wait for improvement before stopping.\n            Defaults to 5.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        random_seed (int, optional): Random seed for deterministic training. Defaults to 42.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str, optional): The device to run the model on. Defaults to \"auto\".\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        wandb_group (str | None, optional): Wandb group. Usefull for CV-Sweeps. Defaults to None.\n        run_name (str | None, optional): Name of this run, as a further grouping method for logs etc.\n            If None, will generate a random one. Defaults to None.\n        run_id (str | None, optional): ID of the run. If None, will generate a random one. Defaults to None.\n        trial_name (str | None, optional): Name of the cross-validation run / trial.\n            This effects primary logging and artifact storage.\n            If None, will do nothing. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.legacy_training.util import generate_id, get_generated_name\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run_name or get_generated_name(artifact_dir)\n    run_id = run_id or generate_id()\n\n    logger.info(f\"Starting training '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(\n        f\"Using config:\\n\\t{model_arch=}\\n\\t{model_encoder=}\\n\\t{model_encoder_weights=}\\n\\t{augment=}\\n\\t\"\n        f\"{learning_rate=}\\n\\t{gamma=}\\n\\t{batch_size=}\\n\\t{max_epochs=}\\n\\t{log_every_n_steps=}\\n\\t\"\n        f\"{check_val_every_n_epoch=}\\n\\t{early_stopping_patience=}\\n\\t{plot_every_n_val_epochs=}\\n\\t{num_workers=}\"\n        f\"\\n\\t{device=}\\n\\t{random_seed=}\"\n    )\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(random_seed, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    config = SMPSegmenterConfig(\n        input_combination=preprocess_config[\"bands\"],\n        model={\n            \"arch\": model_arch,\n            \"encoder_name\": model_encoder,\n            \"encoder_weights\": model_encoder_weights,\n            \"in_channels\": len(preprocess_config[\"bands\"]),\n            \"classes\": 1,\n        },\n        norm_factors=preprocess_config[\"norm_factors\"],\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir / \"cross-val.zarr\",\n        batch_size=batch_size,\n        fold=fold,\n        augment=augment,\n        num_workers=num_workers,\n    )\n    model = SMPSegmenter(\n        config=config,\n        learning_rate=learning_rate,\n        gamma=gamma,\n        focal_loss_alpha=focal_loss_alpha,\n        focal_loss_gamma=focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        trial_name=trial_name,\n        random_seed=random_seed,\n    )\n\n    # Loggers\n    is_crossval = bool(trial_name)\n    trainer_loggers = [\n        CSVLogger(\n            save_dir=artifact_dir,\n            name=run_name if not is_crossval else trial_name,\n            version=run_id if not is_crossval else f\"{run_name}-{run_id}\",\n        ),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            group=wandb_group,\n            job_type=trial_name,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            input_combination=config[\"input_combination\"],\n            val_set=f\"val{fold}\",\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            is_crossval=is_crossval,\n        ),\n    ]\n    if early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {early_stopping_patience}\")\n        early_stopping = EarlyStopping(monitor=\"val/JaccardIndex\", mode=\"max\", patience=early_stopping_patience)\n        callbacks.append(early_stopping)\n\n    # Train\n    trainer = L.Trainer(\n        max_epochs=max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=check_val_every_n_epoch,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=False,\n    )\n    trainer.fit(model, datamodule, ckpt_path=continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/legacy_training/train/#darts.legacy_training.train.wandb_sweep_smp","title":"wandb_sweep_smp","text":"<pre><code>wandb_sweep_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    sweep_config: pathlib.Path,\n    n_trials: int = 10,\n    sweep_id: str | None = None,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Create a sweep with wandb and run it on the specified cuda device, or continue an existing sweep.</p> <p>If <code>sweep_id</code> is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued. All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}. Since each sweep-configuration has (currently) an own name and id, a single run can be found under: {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.</p> <p>If a <code>cuda_device</code> is specified, run an agent on this device. If None, do nothing.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>This will NOT use cross-validation. For cross-validation, use <code>optuna_sweep_smp</code>.</p> Example <p>In one terminal, start a sweep: <pre><code>    $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n    ...  # Many logs\n    Created sweep with ID 123456789\n    ... # More logs from spawned agent\n</code></pre></p> <p>In another terminal, start an a second agent: <pre><code>    $ rye run darts wandb-sweep-smp --sweep-id 123456789\n    ...\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory.</p> </li> <li> <code>sweep_config</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration. Hyperparameters must contain the following fields: <code>model_arch</code>, <code>model_encoder</code>, <code>augment</code>, <code>gamma</code>, <code>batch_size</code>. Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.</p> </li> <li> <code>n_trials</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of runs to execute. Defaults to 10.</p> </li> <li> <code>sweep_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The ID of the sweep. If None, a new sweep will be created. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. Defaults to None.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def wandb_sweep_smp(\n    *,\n    # Data and sweep config\n    train_data_dir: Path,\n    sweep_config: Path,\n    n_trials: int = 10,\n    sweep_id: str | None = None,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n):\n    \"\"\"Create a sweep with wandb and run it on the specified cuda device, or continue an existing sweep.\n\n    If `sweep_id` is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued.\n    All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}.\n    Since each sweep-configuration has (currently) an own name and id, a single run can be found under:\n    {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.\n\n    If a `cuda_device` is specified, run an agent on this device. If None, do nothing.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    This will NOT use cross-validation. For cross-validation, use `optuna_sweep_smp`.\n\n    Example:\n        In one terminal, start a sweep:\n        ```sh\n            $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n            ...  # Many logs\n            Created sweep with ID 123456789\n            ... # More logs from spawned agent\n        ```\n\n        In another terminal, start an a second agent:\n        ```sh\n            $ rye run darts wandb-sweep-smp --sweep-id 123456789\n            ...\n        ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory.\n        sweep_config (Path): Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration.\n            Hyperparameters must contain the following fields: `model_arch`, `model_encoder`, `augment`, `gamma`,\n            `batch_size`.\n            Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.\n        n_trials (int, optional): Number of runs to execute. Defaults to 10.\n        sweep_id (str | None, optional): The ID of the sweep. If None, a new sweep will be created. Defaults to None.\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str | None, optional): The device to run the model on. Defaults to None.\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n\n    \"\"\"\n    import wandb\n\n    # Wandb has a stupid way of logging (they log per default with click.echo to stdout)\n    # We need to silence this and redirect all possible logs to our logger\n    # wl = wandb.setup({\"silent\": True})\n    # wandb.termsetup(wl.settings, logging.getLogger(\"wandb\"))\n    # LoggingManager.apply_logging_handlers(\"wandb\")\n\n    if sweep_id is not None and device is None:\n        logger.warning(\"Continuing a sweep without specifying a device will not do anything.\")\n\n    with sweep_config.open(\"r\") as f:\n        sweep_configuration = yaml.safe_load(f)\n\n    logger.debug(f\"Loaded sweep configuration from {sweep_config.resolve()}:\\n{sweep_configuration}\")\n\n    if sweep_id is None:\n        sweep_id = wandb.sweep(sweep=sweep_configuration, project=wandb_project, entity=wandb_entity)\n        logger.info(f\"Created sweep with ID {sweep_id}\")\n        logger.info(\"To start a sweep agents, use the following command:\")\n        logger.info(f\"$ rye run darts sweep_smp --sweep-id {sweep_id}\")\n\n    artifact_dir = artifact_dir / f\"sweep-{sweep_id}\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    def run():\n        run = wandb.init(config=sweep_configuration)\n        # We need to manually log the run data since the wandb logger only logs to its own logs and click\n        logger.info(f\"Starting sweep run '{run.settings.run_name}'\")\n        logger.debug(f\"Run data is saved locally in {Path(run.settings.sync_dir).resolve()}\")\n        logger.debug(f\"View project at {run.settings.project_url}\")\n        logger.debug(f\"View sweep at {run.settings.sweep_url}\")\n        logger.debug(f\"View run at {run.settings.run_url}\")\n\n        # We set the default weights to None, to be able to use different architectures\n        model_encoder_weights = None\n        # We set early stopping to None, because wandb will handle the early stopping\n        early_stopping_patience = None\n        learning_rate = wandb.config[\"learning_rate\"]\n        gamma = wandb.config[\"gamma\"]\n        batch_size = wandb.config[\"batch_size\"]\n        model_arch = wandb.config[\"model_arch\"]\n        model_encoder = wandb.config[\"model_encoder\"]\n        augment = wandb.config[\"augment\"]\n        focal_loss_alpha = wandb.config[\"focal_loss_alpha\"]\n        focal_loss_gamma = wandb.config[\"focal_loss_gamma\"]\n        fold = wandb.config.get(\"fold\", 0)\n        random_seed = wandb.config.get(\"random_seed\", 42)\n\n        train_smp(\n            # Data config\n            train_data_dir=train_data_dir,\n            artifact_dir=artifact_dir,\n            fold=fold,\n            # Hyperparameters\n            model_arch=model_arch,\n            model_encoder=model_encoder,\n            model_encoder_weights=model_encoder_weights,\n            augment=augment,\n            learning_rate=learning_rate,\n            gamma=gamma,\n            focal_loss_alpha=focal_loss_alpha,\n            focal_loss_gamma=focal_loss_gamma,\n            batch_size=batch_size,\n            # Epoch and Logging config\n            early_stopping_patience=early_stopping_patience,\n            max_epochs=max_epochs,\n            log_every_n_steps=log_every_n_steps,\n            check_val_every_n_epoch=check_val_every_n_epoch,\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            # Device and Manager config\n            random_seed=random_seed,\n            num_workers=num_workers,\n            device=device,\n            wandb_entity=wandb_entity,\n            wandb_project=wandb_project,\n            run_name=wandb.run.name,\n            run_id=wandb.run.id,\n        )\n\n    if device is None:\n        logger.info(\"No device specified, closing script...\")\n        return\n\n    logger.info(\"Starting a default sweep agent\")\n    wandb.agent(sweep_id, function=run, count=n_trials, project=wandb_project, entity=wandb_entity)\n</code></pre>"},{"location":"reference/darts/legacy_training/util/","title":"darts.legacy_training.util","text":""},{"location":"reference/darts/legacy_training/util/#darts.legacy_training.util","title":"darts.legacy_training.util","text":"<p>Utility functions for legacy training.</p>"},{"location":"reference/darts/legacy_training/util/#darts.legacy_training.util.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/legacy_training/util/#darts.legacy_training.util.convert_lightning_checkpoint","title":"convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/util.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/util/#darts.legacy_training.util.generate_id","title":"generate_id","text":"<pre><code>generate_id(length: int = 8) -&gt; str\n</code></pre> <p>Generate a random base-36 string of <code>length</code> digits.</p> <p>This method is taken from the wandb SDK.</p> <p>There are ~2.8T base-36 8-digit strings. Generating 210k ids will have a ~1% chance of collision.</p> <p>Parameters:</p> <ul> <li> <code>length</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The length of the string. Defaults to 8.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A random base-36 string of <code>length</code> digits.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/util.py</code> <pre><code>def generate_id(length: int = 8) -&gt; str:\n    \"\"\"Generate a random base-36 string of `length` digits.\n\n    This method is taken from the wandb SDK.\n\n    There are ~2.8T base-36 8-digit strings. Generating 210k ids will have a ~1% chance of collision.\n\n    Args:\n        length (int, optional): The length of the string. Defaults to 8.\n\n    Returns:\n        str: A random base-36 string of `length` digits.\n\n    \"\"\"\n    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))\n</code></pre>"},{"location":"reference/darts/legacy_training/util/#darts.legacy_training.util.get_generated_name","title":"get_generated_name","text":"<pre><code>get_generated_name(artifact_dir: pathlib.Path) -&gt; str\n</code></pre> <p>Generate a random name with a count attached.</p> <p>The count is calculated by the number of existing directories in the specified artifact directory. The final name is in the format '{somename}-{somesecondname}-{count+1}'.</p> <p>Parameters:</p> <ul> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory of existing runs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The final name.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/util.py</code> <pre><code>def get_generated_name(artifact_dir: Path) -&gt; str:\n    \"\"\"Generate a random name with a count attached.\n\n    The count is calculated by the number of existing directories in the specified artifact directory.\n    The final name is in the format '{somename}-{somesecondname}-{count+1}'.\n\n    Args:\n        artifact_dir (Path): The directory of existing runs.\n\n    Returns:\n        str: The final name.\n\n    \"\"\"\n    from names_generator import generate_name\n\n    run_name = generate_name(style=\"hyphen\")\n    # Count the number of existing runs in the artifact_dir, increase the number by one and append it to the name\n    run_count = sum(1 for p in artifact_dir.glob(\"*\") if p.is_dir())\n    run_name = f\"{run_name}-{run_count + 1}\"\n    return run_name\n</code></pre>"},{"location":"reference/darts/legacy_training/util/#darts.legacy_training.util.get_value_from_trial","title":"get_value_from_trial","text":"<pre><code>get_value_from_trial(trial, constrains, param: str)\n</code></pre> <p>Get a value from an optuna trial based on the constrains.</p> <p>Parameters:</p> <ul> <li> <code>trial</code>               (<code>optuna.Trial</code>)           \u2013            <p>The optuna trial</p> </li> <li> <code>constrains</code>               (<code>dict</code>)           \u2013            <p>The constrains for the parameter</p> </li> <li> <code>param</code>               (<code>str</code>)           \u2013            <p>The parameter name</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Unknown distribution</p> </li> <li> <code>ValueError</code>             \u2013            <p>Unknown constrains</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>str | float | int: The value suggested by optuna</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/util.py</code> <pre><code>def get_value_from_trial(trial, constrains, param: str):\n    \"\"\"Get a value from an optuna trial based on the constrains.\n\n    Args:\n        trial (optuna.Trial): The optuna trial\n        constrains (dict): The constrains for the parameter\n        param (str): The parameter name\n\n    Raises:\n        ValueError: Unknown distribution\n        ValueError: Unknown constrains\n\n    Returns:\n        str | float | int: The value suggested by optuna\n\n    \"\"\"\n    # Handle bad case first: user didn't specified the \"distribution key\"\n    if \"distribution\" not in constrains.keys():\n        if \"value\" in constrains.keys():\n            res = constrains[\"value\"]\n        elif \"values\" in constrains.keys():\n            res = trial.suggest_categorical(param, constrains[\"values\"])\n        elif \"min\" in constrains.keys() and \"max\" in constrains.keys():\n            res = trial.suggest_float(param, constrains[\"min\"], constrains[\"max\"])\n        else:\n            raise ValueError(f\"Unknown constrains for parameter {param}\")\n\n        return res\n\n    # Now handle the good case where the user specified the distribution\n    distribution = constrains[\"distribution\"]\n    match distribution:\n        case \"categorical\":\n            res = trial.suggest_categorical(param, constrains[\"values\"])\n        case \"int_uniform\":\n            res = trial.suggest_int(param, constrains[\"min\"], constrains[\"max\"])\n        case \"uniform\":\n            res = trial.suggest_float(param, constrains[\"min\"], constrains[\"max\"])\n        case \"q_uniform\":\n            res = trial.suggest_float(param, constrains[\"min\"], constrains[\"max\"], step=constrains[\"q\"])\n        case \"log_uniform_values\":\n            res = trial.suggest_float(param, constrains[\"min\"], constrains[\"max\"], log=True)\n        case _:\n            raise ValueError(f\"Unknown distribution {distribution}\")\n\n    return res\n</code></pre>"},{"location":"reference/darts/legacy_training/util/#darts.legacy_training.util.suggest_optuna_params_from_wandb_config","title":"suggest_optuna_params_from_wandb_config","text":"<pre><code>suggest_optuna_params_from_wandb_config(\n    trial, config: dict\n)\n</code></pre> <p>Get optuna parameters from a wandb sweep config.</p> <p>This functions translate a wandb sweep config to a dict of values, suggested from optuna.</p> <p>Parameters:</p> <ul> <li> <code>trial</code>               (<code>optuna.Trial</code>)           \u2013            <p>The optuna trial</p> </li> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>The wandb sweep config</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>A dict of parameters with the values suggested from optuna.</p> </li> </ul> Example <p>Assume a wandb config which looks like this:</p> <pre><code>    parameters:\n        learning_rate:\n            max: !!float 1e-3\n            min: !!float 1e-7\n            distribution: log_uniform_values\n        batch_size:\n            value: 8\n        gamma:\n            value: 0.9\n        augment:\n            value: True\n        model_arch:\n            values:\n                - UnetPlusPlus\n                - Unet\n        model_encoder:\n            values:\n                - resnext101_32x8d\n                - resnet101\n                - dpn98\n</code></pre> <p>This function will return a dict like this:</p> <pre><code>    {\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-7, 1e-3),\n        \"batch_size\": 8,\n        \"gamma\": 0.9,\n        \"augment\": True,\n        \"model_arch\": trial.suggest_categorical(\"model_arch\", [\"UnetPlusPlus\", \"Unet\"]),\n        \"model_encoder\": trial.suggest_categorical(\n            \"model_encoder\", [\"resnext101_32x8d\", \"resnet101\", \"dpn98\"]\n        ),\n    }\n</code></pre> <p>See https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information on the sweep config.</p> <p>Note: Not all distribution types are supported.</p> Source code in <code>darts/src/darts/legacy_training/util.py</code> <pre><code>def suggest_optuna_params_from_wandb_config(trial, config: dict):\n    \"\"\"Get optuna parameters from a wandb sweep config.\n\n    This functions translate a wandb sweep config to a dict of values, suggested from optuna.\n\n    Args:\n        trial (optuna.Trial): The optuna trial\n        config (dict): The wandb sweep config\n\n    Returns:\n        dict: A dict of parameters with the values suggested from optuna.\n\n    Example:\n        Assume a wandb config which looks like this:\n\n        ```yaml\n            parameters:\n                learning_rate:\n                    max: !!float 1e-3\n                    min: !!float 1e-7\n                    distribution: log_uniform_values\n                batch_size:\n                    value: 8\n                gamma:\n                    value: 0.9\n                augment:\n                    value: True\n                model_arch:\n                    values:\n                        - UnetPlusPlus\n                        - Unet\n                model_encoder:\n                    values:\n                        - resnext101_32x8d\n                        - resnet101\n                        - dpn98\n\n        ```\n\n        This function will return a dict like this:\n\n        ```python\n            {\n                \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-7, 1e-3),\n                \"batch_size\": 8,\n                \"gamma\": 0.9,\n                \"augment\": True,\n                \"model_arch\": trial.suggest_categorical(\"model_arch\", [\"UnetPlusPlus\", \"Unet\"]),\n                \"model_encoder\": trial.suggest_categorical(\n                    \"model_encoder\", [\"resnext101_32x8d\", \"resnet101\", \"dpn98\"]\n                ),\n            }\n        ```\n\n        See https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information on the sweep config.\n\n        Note: Not all distribution types are supported.\n\n    \"\"\"\n    import optuna\n\n    trial: optuna.Trial = trial\n\n    wandb_params: dict[str, dict] = config[\"parameters\"]\n\n    conv = {}\n    for param, constrains in wandb_params.items():\n        conv[param] = get_value_from_trial(trial, constrains, param)\n    return conv\n</code></pre>"},{"location":"reference/darts/pipelines/","title":"darts.pipelines","text":""},{"location":"reference/darts/pipelines/#darts.pipelines","title":"darts.pipelines","text":"<p>Predefined pipelines for DARTS.</p>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline","title":"AOISentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>AOISentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    aoi_shapefile: pathlib.Path = None,\n    start_date: str = None,\n    end_date: str = None,\n    max_cloud_cover: int = 10,\n    input_cache: pathlib.Path = pathlib.Path(\n        \"data/cache/input\"\n    ),\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data based on an area of interest.</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The shapefile containing the area of interest.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for filtering the Sentinel 2 scenes. Defaults to 10.</p> </li> <li> <code>input_cache</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/cache/input')</code> )           \u2013            <p>The directory to use for caching the input data. Defaults to Path(\"data/cache/input\").</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.aoi_shapefile","title":"aoi_shapefile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_shapefile: pathlib.Path = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.input_cache","title":"input_cache  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_cache: pathlib.Path = pathlib.Path(\"data/cache/input\")\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.AOISentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for AOI Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"AOISentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for AOI Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline","title":"PlanetPipeline  <code>dataclass</code>","text":"<pre><code>PlanetPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    orthotiles_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSOrthoTile\"\n    ),\n    scenes_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSScene\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSOrthoTile')</code> )           \u2013            <p>The directory containing the PlanetScope orthotiles.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSScene')</code> )           \u2013            <p>The directory containing the PlanetScope scenes.</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSOrthoTile\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSScene\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Planet data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetPipeline\"):\n    \"\"\"Run the sequential pipeline for Planet data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline","title":"Sentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    sentinel2_dir: pathlib.Path = pathlib.Path(\n        \"data/input/sentinel2\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data.</p> <p>Parameters:</p> <ul> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/sentinel2')</code> )           \u2013            <p>The directory containing the Sentinel 2 scenes. Defaults to Path(\"data/input/sentinel2\").</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed. Defaults to None.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.sentinel2_dir","title":"sentinel2_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_dir: pathlib.Path = pathlib.Path(\n    \"data/input/sentinel2\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/","title":"darts.pipelines.sequential_v2","text":""},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2","title":"darts.pipelines.sequential_v2","text":"<p>Sequential implementation of the v2 pipelines.</p>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline","title":"AOISentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>AOISentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    aoi_shapefile: pathlib.Path = None,\n    start_date: str = None,\n    end_date: str = None,\n    max_cloud_cover: int = 10,\n    input_cache: pathlib.Path = pathlib.Path(\n        \"data/cache/input\"\n    ),\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data based on an area of interest.</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The shapefile containing the area of interest.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for filtering the Sentinel 2 scenes. Defaults to 10.</p> </li> <li> <code>input_cache</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/cache/input')</code> )           \u2013            <p>The directory to use for caching the input data. Defaults to Path(\"data/cache/input\").</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.aoi_shapefile","title":"aoi_shapefile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_shapefile: pathlib.Path = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.input_cache","title":"input_cache  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_cache: pathlib.Path = pathlib.Path(\"data/cache/input\")\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.AOISentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for AOI Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"AOISentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for AOI Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.AOISentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline","title":"PlanetPipeline  <code>dataclass</code>","text":"<pre><code>PlanetPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    orthotiles_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSOrthoTile\"\n    ),\n    scenes_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSScene\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSOrthoTile')</code> )           \u2013            <p>The directory containing the PlanetScope orthotiles.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSScene')</code> )           \u2013            <p>The directory containing the PlanetScope scenes.</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSOrthoTile\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSScene\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Planet data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetPipeline\"):\n    \"\"\"Run the sequential pipeline for Planet data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.PlanetPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline","title":"Sentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    sentinel2_dir: pathlib.Path = pathlib.Path(\n        \"data/input/sentinel2\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data.</p> <p>Parameters:</p> <ul> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/sentinel2')</code> )           \u2013            <p>The directory containing the Sentinel 2 scenes. Defaults to Path(\"data/input/sentinel2\").</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed. Defaults to None.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.sentinel2_dir","title":"sentinel2_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_dir: pathlib.Path = pathlib.Path(\n    \"data/input/sentinel2\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2.Sentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline","title":"_BasePipeline  <code>dataclass</code>","text":"<pre><code>_BasePipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n)\n</code></pre> <p>               Bases: <code>abc.ABC</code></p> <p>Base class for all v2 pipelines.</p> <p>This class provides the run method which is the main entry point for all pipelines.</p> <p>This class is meant to be subclassed by the specific pipelines. These pipeliens must implement the _aqdata_generator method.</p> <p>The main class must be also a dataclass, to fully inherit all parameter of this class (and the mixins).</p>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/sequential_v2/#darts.pipelines.sequential_v2._BasePipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with timer(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with timer(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / arcticdem_resolution * sqrt(2)),\n                )\n            with timer(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with timer(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with timer(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with timer(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with timer(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            if len(timer.durations) &gt; 0:\n                timer.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        timer.summary()\n</code></pre>"},{"location":"reference/darts/training/","title":"darts.training","text":""},{"location":"reference/darts/training/#darts.training","title":"darts.training","text":"<p>Pipeline-related training functions and scripts.</p>"},{"location":"reference/darts/training/#darts.training.preprocess_planet_train_data","title":"preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def preprocess_planet_train_data(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.json\",\n        function=preprocess_planet_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_segmentation.utils import Bands\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    fpaths = {fpath.stem: fpath for fpath in _legacy_path_gen(data_dir)}\n    footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these because they depend on the preprocessing used\n    bands = Bands.from_dict(\n        {\n            \"red\": (1 / 3000, 0.0),\n            \"green\": (1 / 3000, 0.0),\n            \"blue\": (1 / 3000, 0.0),\n            \"nir\": (1 / 3000, 0.0),\n            \"ndvi\": (1 / 20000, 0.0),\n            \"relative_elevation\": (1 / 30000, 0.0),\n            \"slope\": (1 / 90, 0.0),\n            \"aspect\": (1 / 360, 0.0),\n            \"hillshade\": (1.0, 0.0),\n            \"curvature\": (1 / 10, 0.5),  # TODO: Do we even want shift?\n            \"tc_brightness\": (1 / 255, 0.0),\n            \"tc_greenness\": (1 / 255, 0.0),\n            \"tc_wetness\": (1 / 255, 0.0),\n        }\n    )\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        mask_erosion_size=mask_erosion_size,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache / \"planet_v2\")\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        try:\n            logger.debug(f\"Processing sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping...\")\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile_batched(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {planet_id} ({i + 1} of {len(footprints)}). \\nSkipping...\")\n            logger.exception(e)\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n    timer.summary()\n</code></pre>"},{"location":"reference/darts/training/#darts.training.preprocess_planet_train_data_pingo","title":"preprocess_planet_train_data_pingo","text":"<pre><code>preprocess_planet_train_data_pingo(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def preprocess_planet_train_data_pingo(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.json\",\n        function=preprocess_planet_train_data_pingo,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_segmentation.utils import Bands\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    footprints[\"fpath\"] = footprints.image_id.map(_path_gen(data_dir))\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these because they depend on the preprocessing used\n    bands = Bands.from_dict(\n        {\n            \"red\": (1 / 3000, 0.0),\n            \"green\": (1 / 3000, 0.0),\n            \"blue\": (1 / 3000, 0.0),\n            \"nir\": (1 / 3000, 0.0),\n            \"ndvi\": (1 / 20000, 0.0),\n            \"relative_elevation\": (1 / 30000, 0.0),\n            \"slope\": (1 / 90, 0.0),\n            \"aspect\": (1 / 360, 0.0),\n            \"hillshade\": (1.0, 0.0),\n            \"curvature\": (1 / 10, 0.5),  # TODO: Do we even want shift?\n            \"tc_brightness\": (1 / 255, 0.0),\n            \"tc_greenness\": (1 / 255, 0.0),\n            \"tc_wetness\": (1 / 255, 0.0),\n        }\n    )\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        mask_erosion_size=mask_erosion_size,\n        device=device,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache / \"planet_v2\")\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        try:\n            logger.debug(f\"Processing sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping...\")\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {planet_id} ({i + 1} of {len(footprints)}). \\nSkipping...\")\n            logger.exception(e)\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n    timer.summary()\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2/","title":"darts.training.preprocess_planet_v2","text":""},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2","title":"darts.training.preprocess_planet_v2","text":"<p>PLANET preprocessing functions for training with the v2 data preprocessing.</p>"},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2._get_region_name","title":"_get_region_name","text":"<pre><code>_get_region_name(\n    footprint: geopandas.GeoSeries,\n    admin2: geopandas.GeoDataFrame,\n) -&gt; str\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def _get_region_name(footprint: \"gpd.GeoSeries\", admin2: \"gpd.GeoDataFrame\") -&gt; str:\n    # Check if any label is intersecting with the test regions\n    admin2_of_footprint = admin2[admin2.intersects(footprint.geometry)]\n\n    if admin2_of_footprint.empty:\n        raise ValueError(\"No intersection found between labels and admin2 regions\")\n\n    region_name = admin2_of_footprint.iloc[0][\"shapeName\"]\n\n    if len(admin2_of_footprint) &gt; 1:\n        logger.warning(\n            f\"Found multiple regions for footprint {footprint.image_id}: {admin2_of_footprint.shapeName.to_list()}.\"\n            f\" Using the first one ({region_name})\"\n        )\n    return region_name\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2._legacy_path_gen","title":"_legacy_path_gen","text":"<pre><code>_legacy_path_gen(data_dir: pathlib.Path)\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def _legacy_path_gen(data_dir: Path):\n    for iterdir in data_dir.iterdir():\n        if iterdir.stem == \"iteration001\":\n            for sitedir in (iterdir).iterdir():\n                for imgdir in (sitedir).iterdir():\n                    if not imgdir.is_dir():\n                        continue\n                    try:\n                        yield next(imgdir.glob(\"*_SR.tif\")).parent\n                    except StopIteration:\n                        yield next(imgdir.glob(\"*_SR_clip.tif\")).parent\n        else:\n            for imgdir in (iterdir).iterdir():\n                if not imgdir.is_dir():\n                    continue\n                try:\n                    yield next(imgdir.glob(\"*_SR.tif\")).parent\n                except StopIteration:\n                    yield next(imgdir.glob(\"*_SR_clip.tif\")).parent\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2/#darts.training.preprocess_planet_v2.preprocess_planet_train_data","title":"preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>append</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to append the data to the existing data. Defaults to True.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2.py</code> <pre><code>def preprocess_planet_train_data(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    append: bool = True,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        append (bool, optional): Whether to append the data to the existing data. Defaults to True.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.json\",\n        function=preprocess_planet_train_data,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_segmentation.utils import Bands\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    fpaths = {fpath.stem: fpath for fpath in _legacy_path_gen(data_dir)}\n    footprints[\"fpath\"] = footprints.image_id.map(fpaths)\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these because they depend on the preprocessing used\n    bands = Bands.from_dict(\n        {\n            \"red\": (1 / 3000, 0.0),\n            \"green\": (1 / 3000, 0.0),\n            \"blue\": (1 / 3000, 0.0),\n            \"nir\": (1 / 3000, 0.0),\n            \"ndvi\": (1 / 20000, 0.0),\n            \"relative_elevation\": (1 / 30000, 0.0),\n            \"slope\": (1 / 90, 0.0),\n            \"aspect\": (1 / 360, 0.0),\n            \"hillshade\": (1.0, 0.0),\n            \"curvature\": (1 / 10, 0.5),  # TODO: Do we even want shift?\n            \"tc_brightness\": (1 / 255, 0.0),\n            \"tc_greenness\": (1 / 255, 0.0),\n            \"tc_wetness\": (1 / 255, 0.0),\n        }\n    )\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        mask_erosion_size=mask_erosion_size,\n        device=device,\n        append=append,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache / \"planet_v2\")\n\n    if append and (train_data_dir / \"metadata.parquet\").exists():\n        metadata = gpd.read_parquet(train_data_dir / \"metadata.parquet\")\n        already_processed_planet_ids = set(metadata[\"planet_id\"].unique())\n        logger.info(f\"Already processed {len(already_processed_planet_ids)} samples.\")\n        footprints = footprints[~footprints.image_id.isin(already_processed_planet_ids)]\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        try:\n            logger.debug(f\"Processing sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping...\")\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile_batched(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {planet_id} ({i + 1} of {len(footprints)}). \\nSkipping...\")\n            logger.exception(e)\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n    timer.summary()\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/","title":"darts.training.preprocess_planet_v2_pingo","text":""},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo","title":"darts.training.preprocess_planet_v2_pingo","text":"<p>PLANET preprocessing functions for training with the v2 data preprocessing.</p>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo._get_region_name","title":"_get_region_name","text":"<pre><code>_get_region_name(\n    footprint: geopandas.GeoSeries,\n    admin2: geopandas.GeoDataFrame,\n) -&gt; str\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def _get_region_name(footprint: \"gpd.GeoSeries\", admin2: \"gpd.GeoDataFrame\") -&gt; str:\n    # Check if any label is intersecting with the test regions\n    admin2_of_footprint = admin2[admin2.intersects(footprint.geometry)]\n\n    if admin2_of_footprint.empty:\n        raise ValueError(\"No intersection found between labels and admin2 regions\")\n\n    region_name = admin2_of_footprint.iloc[0][\"shapeName\"]\n\n    if len(admin2_of_footprint) &gt; 1:\n        logger.warning(\n            f\"Found multiple regions for footprint {footprint.image_id}: {admin2_of_footprint.shapeName.to_list()}.\"\n            f\" Using the first one ({region_name})\"\n        )\n    return region_name\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo._path_gen","title":"_path_gen","text":"<pre><code>_path_gen(data_dir: pathlib.Path)\n</code></pre> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def _path_gen(data_dir: Path):\n    return {fpath.parent.name: fpath.parent for fpath in data_dir.rglob(\"*_SR.tif\")}\n</code></pre>"},{"location":"reference/darts/training/preprocess_planet_v2_pingo/#darts.training.preprocess_planet_v2_pingo.preprocess_planet_train_data_pingo","title":"preprocess_planet_train_data_pingo","text":"<pre><code>preprocess_planet_train_data_pingo(\n    *,\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    force_preprocess: bool = False,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels and footprints / extents.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>force_preprocess</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to force the preprocessing of the data. Defaults to False.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> </ul> Source code in <code>darts/src/darts/training/preprocess_planet_v2_pingo.py</code> <pre><code>def preprocess_planet_train_data_pingo(\n    *,\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    force_preprocess: bool = False,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 3,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels and footprints / extents.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        force_preprocess (bool, optional): Whether to force the preprocessing of the data. Defaults to False.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n\n    \"\"\"\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting preprocessing at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    train_data_dir.mkdir(parents=True, exist_ok=True)\n    from darts_utils.functools import write_function_args_to_config_file\n\n    write_function_args_to_config_file(\n        fpath=train_data_dir / f\"{current_time}.cli.json\",\n        function=preprocess_planet_train_data_pingo,\n        locals_=locals(),\n    )\n\n    from stopuhr import Chronometer\n\n    timer = Chronometer(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import rich\n    import xarray as xr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_acquisition.admin import download_admin_files\n    from darts_preprocessing import preprocess_v2\n    from darts_segmentation.training.prepare_training import TrainDatasetBuilder\n    from darts_segmentation.utils import Bands\n    from darts_utils.tilecache import XarrayCacheManager\n    from odc.stac import configure_rio\n    from rich.progress import track\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.earthengine import init_ee\n\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n    configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n    logger.info(\"Configured Rasterio\")\n\n    labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n    labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n    footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n    footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n    footprints[\"fpath\"] = footprints.image_id.map(_path_gen(data_dir))\n\n    # Download admin files if they do not exist\n    admin2_fpath = admin_dir / \"geoBoundariesCGAZ_ADM2.shp\"\n    if not admin2_fpath.exists():\n        download_admin_files(admin_dir)\n    admin2 = gpd.read_file(admin2_fpath)\n\n    # We hardcode these because they depend on the preprocessing used\n    bands = Bands.from_dict(\n        {\n            \"red\": (1 / 3000, 0.0),\n            \"green\": (1 / 3000, 0.0),\n            \"blue\": (1 / 3000, 0.0),\n            \"nir\": (1 / 3000, 0.0),\n            \"ndvi\": (1 / 20000, 0.0),\n            \"relative_elevation\": (1 / 30000, 0.0),\n            \"slope\": (1 / 90, 0.0),\n            \"aspect\": (1 / 360, 0.0),\n            \"hillshade\": (1.0, 0.0),\n            \"curvature\": (1 / 10, 0.5),  # TODO: Do we even want shift?\n            \"tc_brightness\": (1 / 255, 0.0),\n            \"tc_greenness\": (1 / 255, 0.0),\n            \"tc_wetness\": (1 / 255, 0.0),\n        }\n    )\n\n    builder = TrainDatasetBuilder(\n        train_data_dir=train_data_dir,\n        patch_size=patch_size,\n        overlap=overlap,\n        bands=bands,\n        exclude_nopositive=exclude_nopositive,\n        exclude_nan=exclude_nan,\n        mask_erosion_size=mask_erosion_size,\n        device=device,\n    )\n    cache_manager = XarrayCacheManager(preprocess_cache / \"planet_v2\")\n\n    for i, footprint in track(\n        footprints.iterrows(), description=\"Processing samples\", total=len(footprints), console=rich.get_console()\n    ):\n        planet_id = footprint.image_id\n        try:\n            logger.debug(f\"Processing sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n            if not footprint.fpath or (not footprint.fpath.exists() and not cache_manager.exists(planet_id)):\n                logger.warning(f\"Footprint image {planet_id} at {footprint.fpath} does not exist. Skipping...\")\n                continue\n\n            def _get_tile():\n                tile = load_planet_scene(footprint.fpath)\n                arctidem_res = 2\n                arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                )\n                tcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n                data_masks = load_planet_masks(footprint.fpath)\n                tile = xr.merge([tile, data_masks])\n\n                tile: xr.Dataset = preprocess_v2(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    tpi_outer_radius,\n                    tpi_inner_radius,\n                    device,\n                )\n                return tile\n\n            with timer(\"Loading tile\"):\n                tile = cache_manager.get_or_create(\n                    identifier=planet_id,\n                    creation_func=_get_tile,\n                    force=force_preprocess,\n                )\n\n            logger.debug(f\"Found tile with size {tile.sizes}\")\n\n            footprint_labels = labels[labels.image_id == planet_id]\n            region = _get_region_name(footprint, admin2)\n\n            with timer(\"Save as patches\"):\n                builder.add_tile(\n                    tile=tile,\n                    labels=footprint_labels,\n                    region=region,\n                    sample_id=planet_id,\n                    metadata={\n                        \"planet_id\": planet_id,\n                        \"fpath\": footprint.fpath,\n                    },\n                )\n\n            logger.info(f\"Processed sample {planet_id} ({i + 1} of {len(footprints)})\")\n\n        except (KeyboardInterrupt, SystemExit, SystemError):\n            logger.info(\"Interrupted by user.\")\n            break\n\n        except Exception as e:\n            logger.warning(f\"Could not process sample {planet_id} ({i + 1} of {len(footprints)}). \\nSkipping...\")\n            logger.exception(e)\n\n    builder.finalize(\n        {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n        }\n    )\n    timer.summary()\n</code></pre>"},{"location":"reference/darts/utils/","title":"darts.utils","text":""},{"location":"reference/darts/utils/#darts.utils","title":"darts.utils","text":"<p>Utilities for the pipeline related parts of the DARTS library.</p>"},{"location":"reference/darts/utils/config/","title":"darts.utils.config","text":""},{"location":"reference/darts/utils/config/#darts.utils.config","title":"darts.utils.config","text":"<p>Utility functions for parsing and handling configuration files.</p>"},{"location":"reference/darts/utils/config/#darts.utils.config.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser","title":"ConfigParser","text":"<pre><code>ConfigParser()\n</code></pre> <p>Parser for cyclopts config.</p> <p>An own implementation is needed to select our own toml structure and source. Implemented as a class to be able to provide the config-file as a parameter of the CLI.</p> <p>Initialize the ConfigParser (no-op).</p> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the ConfigParser (no-op).\"\"\"\n    self._config = None\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.__call__","title":"__call__","text":"<pre><code>__call__(\n    apps: list[cyclopts.App],\n    commands: tuple[str, ...],\n    arguments: cyclopts.ArgumentCollection,\n)\n</code></pre> <p>Parser for cyclopts config. An own implementation is needed to select our own toml structure.</p> <p>First, the configuration file at \"config.toml\" is loaded. Then, this config is flattened and then mapped to the input arguments of the called function. Hence parent keys are not considered.</p> <p>Parameters:</p> <ul> <li> <code>apps</code>               (<code>list[cyclopts.App]</code>)           \u2013            <p>The cyclopts apps. Unused, but must be provided for the cyclopts hook.</p> </li> <li> <code>commands</code>               (<code>tuple[str, ...]</code>)           \u2013            <p>The commands. Unused, but must be provided for the cyclopts hook.</p> </li> <li> <code>arguments</code>               (<code>cyclopts.ArgumentCollection</code>)           \u2013            <p>The arguments to apply the config to.</p> </li> </ul> <p>Examples:</p>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.__call__--setup-the-cyclopts-app","title":"Setup the cyclopts App","text":"<pre><code>import cyclopts\nfrom darts.utils.config import ConfigParser\n\nconfig_parser = ConfigParser()\napp = cyclopts.App(config=config_parser)\n\n# Intercept the logging behavior to add a file handler\n@app.meta.default\ndef launcher(\n    *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n    log_dir: Path = Path(\"logs\"),\n    config_file: Path = Path(\"config.toml\"),\n):\n    command, bound, _ = app.parse_args(tokens)\n    add_logging_handlers(command.__name__, console, log_dir)\n    return command(*bound.args, **bound.kwargs)\n\nif __name__ == \"__main__\":\n    app.meta()\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.__call__--usage","title":"Usage","text":"<p>Config file <code>./config.toml</code>:</p> <pre><code>[darts.hello] # The parent key is completely ignored\nname = \"Tobias\"\n</code></pre> <p>Function signature which is called:</p> <pre><code># ... setup code for cyclopts\n@app.command()\ndef hello(name: str):\n    print(f\"Hello {name}\")\n</code></pre> <p>Calling the function from CLI:</p> <pre><code>$ darts hello\nHello Tobias\n\n$ darts hello --name=Max\nHello Max\n</code></pre> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def __call__(self, apps: list[cyclopts.App], commands: tuple[str, ...], arguments: cyclopts.ArgumentCollection):\n    \"\"\"Parser for cyclopts config. An own implementation is needed to select our own toml structure.\n\n    First, the configuration file at \"config.toml\" is loaded.\n    Then, this config is flattened and then mapped to the input arguments of the called function.\n    Hence parent keys are not considered.\n\n    Args:\n        apps (list[cyclopts.App]): The cyclopts apps. Unused, but must be provided for the cyclopts hook.\n        commands (tuple[str, ...]): The commands. Unused, but must be provided for the cyclopts hook.\n        arguments (cyclopts.ArgumentCollection): The arguments to apply the config to.\n\n    Examples:\n        ### Setup the cyclopts App\n\n        ```python\n        import cyclopts\n        from darts.utils.config import ConfigParser\n\n        config_parser = ConfigParser()\n        app = cyclopts.App(config=config_parser)\n\n        # Intercept the logging behavior to add a file handler\n        @app.meta.default\n        def launcher(\n            *tokens: Annotated[str, cyclopts.Parameter(show=False, allow_leading_hyphen=True)],\n            log_dir: Path = Path(\"logs\"),\n            config_file: Path = Path(\"config.toml\"),\n        ):\n            command, bound, _ = app.parse_args(tokens)\n            add_logging_handlers(command.__name__, console, log_dir)\n            return command(*bound.args, **bound.kwargs)\n\n        if __name__ == \"__main__\":\n            app.meta()\n        ```\n\n\n        ### Usage\n\n        Config file `./config.toml`:\n\n        ```toml\n        [darts.hello] # The parent key is completely ignored\n        name = \"Tobias\"\n        ```\n\n        Function signature which is called:\n\n        ```python\n        # ... setup code for cyclopts\n        @app.command()\n        def hello(name: str):\n            print(f\"Hello {name}\")\n        ```\n\n        Calling the function from CLI:\n\n        ```sh\n        $ darts hello\n        Hello Tobias\n\n        $ darts hello --name=Max\n        Hello Max\n        ```\n\n    \"\"\"\n    if self._config is None:\n        config_arg, _, _ = arguments.match(\"--config-file\")\n        config_file = config_arg.convert_and_validate()\n        # Use default config file if not specified\n        if not config_file:\n            config_file = config_arg.field_info.default\n        # else never happens\n        self.open_config(config_file)\n\n    self.apply_config(arguments)\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.apply_config","title":"apply_config","text":"<pre><code>apply_config(arguments: cyclopts.ArgumentCollection)\n</code></pre> <p>Apply the loaded config to the cyclopts mapping.</p> <p>Parameters:</p> <ul> <li> <code>arguments</code>               (<code>cyclopts.ArgumentCollection</code>)           \u2013            <p>The arguments to apply the config to.</p> </li> </ul> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def apply_config(self, arguments: cyclopts.ArgumentCollection):\n    \"\"\"Apply the loaded config to the cyclopts mapping.\n\n    Args:\n        arguments (cyclopts.ArgumentCollection): The arguments to apply the config to.\n\n    \"\"\"\n    to_add = []\n    for k in self._config.keys():\n        value = self._config[k][\"value\"]\n\n        try:\n            argument, remaining_keys, _ = arguments.match(f\"--{k}\")\n        except ValueError:\n            # Config key not found in arguments - ignore\n            continue\n\n        # Skip if the argument is not bound to a parameter\n        if argument.tokens or argument.field_info.kind is argument.field_info.VAR_KEYWORD:\n            continue\n\n        # Skip if the argument is from the config file\n        if any(x.source != \"config-file\" for x in argument.tokens):\n            continue\n\n        # Parse value to tuple of strings\n        if not isinstance(value, list):\n            value = (value,)\n        value = tuple(str(x) for x in value)\n        # Add the new tokens to the list\n        for i, v in enumerate(value):\n            to_add.append(\n                (\n                    argument,\n                    cyclopts.Token(keyword=k, value=v, source=\"config-file\", index=i, keys=remaining_keys),\n                )\n            )\n    # Add here after all \"arguments.match\" calls, to avoid changing the list while iterating\n    for argument, token in to_add:\n        argument.append(token)\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.ConfigParser.open_config","title":"open_config","text":"<pre><code>open_config(file_path: str | pathlib.Path) -&gt; None\n</code></pre> <p>Open the config file, takes the 'darts' key, flattens the resulting dict and saves as config.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the config file.</p> </li> </ul> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def open_config(self, file_path: str | Path) -&gt; None:\n    \"\"\"Open the config file, takes the 'darts' key, flattens the resulting dict and saves as config.\n\n    Args:\n        file_path (str | Path): The path to the config file.\n\n    \"\"\"\n    file_path = file_path if isinstance(file_path, Path) else Path(file_path)\n\n    if not file_path.exists():\n        logger.warning(f\"No config file found at {file_path.resolve()}\")\n        self._config = {}\n        return\n\n    with file_path.open(\"rb\") as f:\n        config = tomllib.load(f)[\"darts\"]\n\n    # Flatten the config data ()\n    self._config = flatten_dict(config)\n    logger.info(f\"loaded config from '{file_path.resolve()}'\")\n</code></pre>"},{"location":"reference/darts/utils/config/#darts.utils.config.flatten_dict","title":"flatten_dict","text":"<pre><code>flatten_dict(\n    d: dict, parent_key: str = \"\", sep: str = \".\"\n) -&gt; dict[str, dict[str, str]]\n</code></pre> <p>Flatten a nested dictionary.</p> <p>Parameters:</p> <ul> <li> <code>d</code>               (<code>dict</code>)           \u2013            <p>The dictionary to flatten.</p> </li> <li> <code>parent_key</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>The parent key. Defaults to \"\".</p> </li> <li> <code>sep</code>               (<code>str</code>, default:                   <code>'.'</code> )           \u2013            <p>The separator. Defaults to \".\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, dict[str, str]]</code>           \u2013            <p>dict[str, dict[str, str]]: The flattened dictionary. Key is the original key, value is a dictionary with the value and a concatenated key to save parents.</p> </li> </ul> <p>Examples: <pre><code>&gt;&gt;&gt; d = {\n&gt;&gt;&gt;     \"a\": 1,\n&gt;&gt;&gt;     \"b\": {\n&gt;&gt;&gt;         \"c\": 2,\n&gt;&gt;&gt;     },\n&gt;&gt;&gt; }\n&gt;&gt;&gt; print(flatten_dict(d))\n{\n    \"a\": {\"value\": 1, \"key\": \"a\"},\n    \"c\": {\"value\": 2, \"key\": \"b.c\"},\n}\n</code></pre></p> Source code in <code>darts/src/darts/utils/config.py</code> <pre><code>def flatten_dict(d: dict, parent_key: str = \"\", sep: str = \".\") -&gt; dict[str, dict[str, str]]:\n    \"\"\"Flatten a nested dictionary.\n\n    Args:\n        d (dict): The dictionary to flatten.\n        parent_key (str, optional): The parent key. Defaults to \"\".\n        sep (str, optional): The separator. Defaults to \".\".\n\n    Returns:\n        dict[str, dict[str, str]]: The flattened dictionary.\n            Key is the original key, value is a dictionary with the value and a concatenated key to save parents.\n\n    Examples:\n    ```python\n    &gt;&gt;&gt; d = {\n    &gt;&gt;&gt;     \"a\": 1,\n    &gt;&gt;&gt;     \"b\": {\n    &gt;&gt;&gt;         \"c\": 2,\n    &gt;&gt;&gt;     },\n    &gt;&gt;&gt; }\n    &gt;&gt;&gt; print(flatten_dict(d))\n    {\n        \"a\": {\"value\": 1, \"key\": \"a\"},\n        \"c\": {\"value\": 2, \"key\": \"b.c\"},\n    }\n    ```\n\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((k, {\"value\": v, \"key\": new_key}))\n    return dict(items)\n</code></pre>"},{"location":"reference/darts/utils/copernicus/","title":"darts.utils.copernicus","text":""},{"location":"reference/darts/utils/copernicus/#darts.utils.copernicus","title":"darts.utils.copernicus","text":"<p>Copernicus STAC utilities.</p>"},{"location":"reference/darts/utils/copernicus/#darts.utils.copernicus.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/copernicus/#darts.utils.copernicus.init_copernicus","title":"init_copernicus","text":"<pre><code>init_copernicus(profile_name: str = 'default')\n</code></pre> <p>Configure odc.stac and rio to authenticate with Copernicus cloud.</p> <p>This functions expects that credentials are present in the .aws/credentials file. Credentials can be optained from https://eodata-s3keysmanager.dataspace.copernicus.eu/</p> <p>Example credentials file:</p> <pre><code>[default]\nAWS_ACCESS_KEY_ID=...\nAWS_SECRET_ACCESS_KEY=...\n</code></pre> <p>Parameters:</p> <ul> <li> <code>profile_name</code>               (<code>str</code>, default:                   <code>'default'</code> )           \u2013            <p>The boto3 profile name. Defaults to \"default\".</p> </li> </ul> References <ul> <li>S3 access: https://documentation.dataspace.copernicus.eu/APIs/S3.html</li> </ul> Source code in <code>darts/src/darts/utils/copernicus.py</code> <pre><code>def init_copernicus(profile_name: str = \"default\"):\n    \"\"\"Configure odc.stac and rio to authenticate with Copernicus cloud.\n\n    This functions expects that credentials are present in the .aws/credentials file.\n    Credentials can be optained from https://eodata-s3keysmanager.dataspace.copernicus.eu/\n\n    Example credentials file:\n\n    ```\n    [default]\n    AWS_ACCESS_KEY_ID=...\n    AWS_SECRET_ACCESS_KEY=...\n    ```\n\n    Args:\n        profile_name (str, optional): The boto3 profile name. Defaults to \"default\".\n\n    References:\n        - S3 access: https://documentation.dataspace.copernicus.eu/APIs/S3.html\n\n    \"\"\"\n    import boto3\n    import odc.stac\n\n    session = boto3.Session(profile_name=profile_name)\n    credentials = session.get_credentials()\n\n    odc.stac.configure_rio(\n        cloud_defaults=True,\n        verbose=True,\n        aws={\n            \"profile_name\": profile_name,\n            \"aws_access_key_id\": credentials.access_key,\n            \"aws_secret_access_key\": credentials.secret_key,\n            \"region_name\": \"default\",\n            \"endpoint_url\": \"eodata.ams.dataspace.copernicus.eu\",\n        },\n        AWS_VIRTUAL_HOSTING=False,\n    )\n    logger.debug(\"Copernicus STAC initialized\")\n</code></pre>"},{"location":"reference/darts/utils/cuda/","title":"darts.utils.cuda","text":""},{"location":"reference/darts/utils/cuda/#darts.utils.cuda","title":"darts.utils.cuda","text":"<p>Utility functions for working with CUDA devices.</p>"},{"location":"reference/darts/utils/cuda/#darts.utils.cuda.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/cuda/#darts.utils.cuda.debug_info","title":"debug_info","text":"<pre><code>debug_info()\n</code></pre> <p>Print debug information about the CUDA devices and library installations.</p> Source code in <code>darts/src/darts/utils/cuda.py</code> <pre><code>def debug_info():\n    \"\"\"Print debug information about the CUDA devices and library installations.\"\"\"\n    import os\n\n    import torch\n    from xrspatial.utils import has_cuda_and_cupy\n\n    logger.debug(\"=== CUDA DEBUG INFO ===\")\n    logger.debug(f\"PyTorch version: {torch.__version__}\")\n    logger.debug(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n    logger.debug(f\"Cupy+Numba CUDA available: {has_cuda_and_cupy()}\")\n    logger.debug(f\"LD_LIBRARY_PATH: {os.environ.get('LD_LIBRARY_PATH')}\")\n\n    try:\n        from pynvml import (  # type: ignore\n            nvmlDeviceGetCount,\n            nvmlDeviceGetHandleByIndex,\n            nvmlDeviceGetMemoryInfo,\n            nvmlDeviceGetName,\n            nvmlInit,\n            nvmlShutdown,\n            nvmlSystemGetCudaDriverVersion_v2,\n            nvmlSystemGetDriverVersion,\n        )\n\n        nvmlInit()\n        driver_version = nvmlSystemGetDriverVersion().decode()\n        logger.debug(f\"CUDA driver version: {driver_version}\")\n        cuda_driver_version = nvmlSystemGetCudaDriverVersion_v2()\n        logger.debug(f\"CUDA runtime version: {cuda_driver_version}\")\n        ndevices = nvmlDeviceGetCount()\n        logger.debug(f\"Number of CUDA devices: {ndevices}\")\n\n        for i in range(ndevices):\n            handle = nvmlDeviceGetHandleByIndex(i)\n            device_name = nvmlDeviceGetName(handle).decode()\n            meminfo = nvmlDeviceGetMemoryInfo(handle)\n            logger.debug(f\"Device {i} ({device_name}): {meminfo.used / meminfo.total:.2%} memory usage.\")\n        nvmlShutdown()\n\n    except ImportError:\n        logger.debug(\"Module 'pynvml' not found, darts is probably installed without CUDA support.\")\n\n    try:\n        import cupy  # type: ignore\n\n        logger.debug(f\"Cupy version: {cupy.__version__}\")\n        # This is the version which is installed (dynamically linked via PATH or LD_LIBRARY_PATH) in the environment\n        env_runtime_version = cupy.cuda.get_local_runtime_version()\n        # This is the version which is used by cupy (statically linked)\n        cupy_runtime_version = cupy.cuda.runtime.runtimeGetVersion()\n        if env_runtime_version != cupy_runtime_version:\n            logger.warning(\n                \"Cupy CUDA runtime versions don't match!\\n\"\n                f\"Got {env_runtime_version} as local (dynamically linked) runtime version.\\n\"\n                f\"Got {cupy_runtime_version} as by cupy statically linked runtime version.\\n\"\n                \"Cupy will use the statically linked runtime version!\"\n            )\n        else:\n            logger.debug(f\"Cupy CUDA runtime version: {cupy_runtime_version}\")\n        logger.debug(f\"Cupy CUDA driver version: {cupy.cuda.runtime.driverGetVersion()}\")\n    except ImportError:\n        logger.debug(\"Module 'cupy' not found, darts is probably installed without CUDA support.\")\n\n    try:\n        import numba.cuda\n\n        cuda_available = numba.cuda.is_available()\n        logger.debug(f\"Numba CUDA is available: {cuda_available}\")\n        if cuda_available:\n            logger.debug(f\"Numba CUDA runtime: {numba.cuda.runtime.get_version()}\")\n            # logger.debug(f\"Numba CUDA has supported devices: {numba.cuda.detect()}\")\n    except ImportError:\n        logger.debug(\"Module 'numba.cuda' not found, darts is probably installed without CUDA support.\")\n\n    try:\n        import cucim  # type: ignore\n\n        logger.debug(f\"Cucim version: {cucim.__version__}\")\n    except ImportError:\n        logger.debug(\"Module 'cucim' not found, darts is probably installed without CUDA support.\")\n</code></pre>"},{"location":"reference/darts/utils/cuda/#darts.utils.cuda.decide_device","title":"decide_device","text":"<pre><code>decide_device(\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None,\n) -&gt; typing.Literal[\"cuda\", \"cpu\"] | int\n</code></pre> <p>Decide the device based on the input.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int</code>)           \u2013            <p>The device to run the model on.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['cuda', 'cpu'] | int</code>           \u2013            <p>Literal[\"cuda\", \"cpu\"] | int: The device to run the model on.</p> </li> </ul> Source code in <code>darts/src/darts/utils/cuda.py</code> <pre><code>def decide_device(device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None) -&gt; Literal[\"cuda\", \"cpu\"] | int:\n    \"\"\"Decide the device based on the input.\n\n    Args:\n        device (Literal[\"cuda\", \"cpu\", \"auto\"] | int): The device to run the model on.\n\n    Returns:\n        Literal[\"cuda\", \"cpu\"] | int: The device to run the model on.\n\n    \"\"\"\n    import torch\n    from xrspatial.utils import has_cuda_and_cupy\n\n    # We can't provide a default value for device in the parameter list because then we would need to import torch at\n    # top-level, which would make the CLI slow.\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() and has_cuda_and_cupy() else \"cpu\"\n        logger.info(f\"Device not provided. Using {device}.\")\n        return device\n\n    # Automatically select a free GPU (&lt;50% memory usage)\n    if device == \"auto\":\n        logger.info(f\"{device=}. Trying to automatically select a free GPU. (&lt;50% memory usage)\")\n\n        # Check if torch and cupy are available\n        if not has_cuda_and_cupy() or not torch.cuda.is_available():\n            logger.info(\"CUDA not available. Using CPU.\")\n            return \"cpu\"\n\n        try:\n            from pynvml import (  # type: ignore\n                nvmlDeviceGetCount,\n                nvmlDeviceGetHandleByIndex,\n                nvmlDeviceGetMemoryInfo,\n                nvmlInit,\n                nvmlShutdown,\n            )\n        except ImportError:\n            logger.warning(\"Module 'pynvml' not found. Using CPU.\")\n            return \"cpu\"\n\n        nvmlInit()\n        ndevices = nvmlDeviceGetCount()\n\n        # If there are multiple devices, we need to check which one is free\n        for i in range(ndevices):\n            handle = nvmlDeviceGetHandleByIndex(i)\n            meminfo = nvmlDeviceGetMemoryInfo(handle)\n            perc_used = meminfo.used / meminfo.total\n            logger.debug(f\"Device {i}: {perc_used:.2%} memory usage.\")\n            # If the device is less than 50% used, we skip it\n            if perc_used &gt; 0.5:\n                continue\n            else:\n                nvmlShutdown()\n                logger.info(f\"Using free GPU {i} ({perc_used:.2%} memory usage).\")\n                return i\n        else:\n            nvmlShutdown()\n            logger.warning(\n                \"No free GPU found (&lt;50% memory usage). Using CPU. \"\n                \"If you want to use a GPU, please select a device manually with the 'device' parameter.\"\n            )\n            return \"cpu\"\n\n    # If device is int or \"cuda\" or \"cpu\", we just return it\n    logger.info(f\"Using {device=}.\")\n    return device\n</code></pre>"},{"location":"reference/darts/utils/earthengine/","title":"darts.utils.earthengine","text":""},{"location":"reference/darts/utils/earthengine/#darts.utils.earthengine","title":"darts.utils.earthengine","text":"<p>Earth Engine utilities.</p>"},{"location":"reference/darts/utils/earthengine/#darts.utils.earthengine.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/earthengine/#darts.utils.earthengine.init_ee","title":"init_ee","text":"<pre><code>init_ee(\n    project: str | None = None, use_highvolume: bool = True\n) -&gt; None\n</code></pre> <p>Initialize Earth Engine. Authenticate if necessary.</p> <p>Parameters:</p> <ul> <li> <code>project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The project name.</p> </li> <li> <code>use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> </ul> Source code in <code>darts/src/darts/utils/earthengine.py</code> <pre><code>def init_ee(project: str | None = None, use_highvolume: bool = True) -&gt; None:\n    \"\"\"Initialize Earth Engine. Authenticate if necessary.\n\n    Args:\n        project (str): The project name.\n        use_highvolume (bool): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n\n    \"\"\"\n    logger.debug(f\"Initializing Earth Engine with project {project} {'with high volume' if use_highvolume else ''}\")\n    opt_url = \"https://earthengine-highvolume.googleapis.com\" if use_highvolume else None\n    try:\n        ee.Initialize(project=project, opt_url=opt_url)\n        # geemap.ee_initialize(project=project, opt_url=\"https://earthengine-highvolume.googleapis.com\")\n    except Exception:\n        logger.debug(\"Initializing Earth Engine failed, trying to authenticate before\")\n        ee.Authenticate()\n        ee.Initialize(project=project, opt_url=opt_url)\n        # geemap.ee_initialize(project=project, opt_url=\"https://earthengine-highvolume.googleapis.com\")\n    logger.debug(\"Earth Engine initialized\")\n</code></pre>"},{"location":"reference/darts/utils/logging/","title":"darts.utils.logging","text":""},{"location":"reference/darts/utils/logging/#darts.utils.logging","title":"darts.utils.logging","text":"<p>Utility functions for logging.</p>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.DARTS_LEVEL","title":"DARTS_LEVEL  <code>module-attribute</code>","text":"<pre><code>DARTS_LEVEL = logging.INFO\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManager","title":"LoggingManager  <code>module-attribute</code>","text":"<pre><code>LoggingManager = (\n    darts.utils.logging.LoggingManagerSingleton()\n)\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton","title":"LoggingManagerSingleton","text":"<pre><code>LoggingManagerSingleton()\n</code></pre> <p>A singleton class to manage logging handlers for the application.</p> <p>Initialize the LoggingManager.</p> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the LoggingManager.\"\"\"\n    self._rich_handler = None\n    self._file_handler = None\n    self._managed_loggers = []\n    self._log_level = DARTS_LEVEL\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.logger","title":"logger  <code>property</code>","text":"<pre><code>logger\n</code></pre> <p>Get the logger for the application.</p>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create a new instance of the LoggingManager if it does not exist yet.</p> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def __new__(cls):\n    \"\"\"Create a new instance of the LoggingManager if it does not exist yet.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n\n    return cls._instance\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.add_logging_handlers","title":"add_logging_handlers","text":"<pre><code>add_logging_handlers(\n    command: str,\n    log_dir: pathlib.Path,\n    verbose: bool = False,\n    tracebacks_show_locals: bool = False,\n)\n</code></pre> <p>Add logging handlers (rich-console and file) to the application.</p> <p>Parameters:</p> <ul> <li> <code>command</code>               (<code>str</code>)           \u2013            <p>The command that is run.</p> </li> <li> <code>log_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to save the logs to.</p> </li> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to set the log level to DEBUG.</p> </li> <li> <code>tracebacks_show_locals</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to show local variables in tracebacks.</p> </li> </ul> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def add_logging_handlers(\n    self, command: str, log_dir: Path, verbose: bool = False, tracebacks_show_locals: bool = False\n):\n    \"\"\"Add logging handlers (rich-console and file) to the application.\n\n    Args:\n        command (str): The command that is run.\n        log_dir (Path): The directory to save the logs to.\n        verbose (bool): Whether to set the log level to DEBUG.\n        tracebacks_show_locals (bool): Whether to show local variables in tracebacks.\n\n    \"\"\"\n    import distributed\n    import pandas as pd\n    import torch\n    import torch.utils.data\n    import xarray as xr\n\n    try:\n        import lightning as L  # noqa: N812\n    except ImportError:\n        L = None  # noqa: N806\n\n    if self._rich_handler is not None or self._file_handler is not None:\n        logger.warning(\"Logging handlers already added.\")\n        return\n\n    log_dir.mkdir(parents=True, exist_ok=True)\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n    # Configure the rich console handler\n    traceback_suppress = [cyclopts, torch, torch.utils.data, xr, distributed, pd]\n    if L:\n        pass\n    #    traceback_suppress.append(L)\n    rich_handler = RichHandler(\n        console=rich.get_console(),\n        rich_tracebacks=True,\n        tracebacks_suppress=traceback_suppress,\n        tracebacks_show_locals=tracebacks_show_locals,\n    )\n    rich_fmt = (\n        \"%(message)s\"\n        if not verbose\n        else \"%(name)s@%(processName)s(%(process)d)-%(threadName)s(%(thread)d) - %(message)s\"\n    )\n    rich_handler.setFormatter(\n        logging.Formatter(\n            rich_fmt,\n            datefmt=\"[%Y-%m-%d %H:%M:%S]\",\n        )\n    )\n    self._rich_handler = rich_handler\n\n    # Configure the file handler (no fancy)\n    file_handler = logging.FileHandler(log_dir / f\"darts_{command}_{current_time}.log\")\n    file_fmt = \"%(name)s@%(processName)s(%(process)d)-%(threadName)s(%(thread)d):%(levelname)s - %(message)s (in %(filename)s:%(lineno)d)\"  # noqa: E501\n    file_handler.setFormatter(\n        logging.Formatter(\n            file_fmt,\n            datefmt=\"[%Y-%m-%d %H:%M:%S]\",\n        )\n    )\n    self._file_handler = file_handler\n\n    self._log_level = logging.DEBUG if verbose else DARTS_LEVEL\n\n    darts_logger = logging.getLogger(\"darts\")\n    darts_logger.addHandler(rich_handler)\n    darts_logger.addHandler(file_handler)\n    darts_logger.setLevel(self._log_level)\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.apply_logging_handlers","title":"apply_logging_handlers","text":"<pre><code>apply_logging_handlers(\n    *names: str, level: int | None = None\n)\n</code></pre> <p>Apply the logging handlers to a (third-party) logger.</p> <p>Parameters:</p> <ul> <li> <code>names</code>               (<code>str</code>, default:                   <code>()</code> )           \u2013            <p>The names of the loggers to apply the handlers to.</p> </li> <li> <code>level</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The log level to set for the loggers. If None, use the manager level. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def apply_logging_handlers(self, *names: str, level: int | None = None):\n    \"\"\"Apply the logging handlers to a (third-party) logger.\n\n    Args:\n        names (str): The names of the loggers to apply the handlers to.\n        level (int | None, optional): The log level to set for the loggers. If None, use the manager level.\n            Defaults to None.\n\n    \"\"\"\n    if level is None:\n        level = self._log_level\n\n    for name in names:\n        if name in self._managed_loggers:\n            continue\n        third_party_logger = logging.getLogger(name)\n        # Check if logger has a StreamHandler already and remove it if so\n        for handler in third_party_logger.handlers:\n            if isinstance(handler, logging.StreamHandler):\n                third_party_logger.removeHandler(handler)\n        third_party_logger.addHandler(self._rich_handler)\n        third_party_logger.addHandler(self._file_handler)\n        # Set level for all handlers\n        third_party_logger.setLevel(level)\n\n        self._managed_loggers.append(name)\n</code></pre>"},{"location":"reference/darts/utils/logging/#darts.utils.logging.LoggingManagerSingleton.setup_logging","title":"setup_logging","text":"<pre><code>setup_logging(verbose: bool = False)\n</code></pre> <p>Set up logging for the application.</p> <p>Parameters:</p> <ul> <li> <code>verbose</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to set the log level to DEBUG.</p> </li> </ul> Source code in <code>darts/src/darts/utils/logging.py</code> <pre><code>def setup_logging(self, verbose: bool = False):\n    \"\"\"Set up logging for the application.\n\n    Args:\n        verbose (bool): Whether to set the log level to DEBUG.\n\n    \"\"\"\n    # Set up logging for our own modules\n    self._log_level = logging.DEBUG if verbose else DARTS_LEVEL\n    logging.getLogger(\"darts\").setLevel(DARTS_LEVEL)\n    logging.captureWarnings(True)\n</code></pre>"},{"location":"reference/darts_acquisition/","title":"darts_acquisition","text":""},{"location":"reference/darts_acquisition/#darts_acquisition","title":"darts_acquisition","text":"<p>Acquisition of data from various sources for the DARTS dataset.</p>"},{"location":"reference/darts_acquisition/#darts_acquisition.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.download_admin_files","title":"download_admin_files","text":"<pre><code>download_admin_files(admin_dir: pathlib.Path)\n</code></pre> <p>Download the admin files for the regions.</p> <p>Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.</p> <p>Parameters:</p> <ul> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the admin files.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>@stopwatch.f(\"Downloading admin files\", printer=logger.debug)\ndef download_admin_files(admin_dir: Path):\n    \"\"\"Download the admin files for the regions.\n\n    Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.\n\n    Args:\n        admin_dir (Path): The path to the admin files.\n\n    \"\"\"\n    # Download the admin files\n    admin_1_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.zip\"\n    admin_2_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.zip\"\n\n    admin_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.debug(f\"Downloading {admin_1_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_1_url, admin_dir)\n\n    logger.debug(f\"Downloading {admin_2_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_2_url, admin_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_arcticdem","title":"load_arcticdem","text":"<pre><code>load_arcticdem(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which the tile should be loaded.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory where the ArcticDEM data is stored.</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in m.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The ArcticDEM tile, with a buffer applied. Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different. Final dataset is NOT matched to the reference CRS and resolution.</p> </li> </ul> Warning <p>Geobox must be in a meter based CRS.</p> Usage <p>Since the API of the <code>load_arcticdem</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_arcticdem\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\narcticdem = load_arcticdem(\n    optical.odc.geobox,\n    \"/path/to/arcticdem-parent-directory\",\n    resolution=2,\n    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n)\n\n# Now we can for example match the resolution and extent of the optical data:\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> <p>The <code>buffer</code> parameter is used to extend the region of interest by a certain amount of pixels. This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the resolution is not supported.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopwatch.f(\"Loading ArcticDEM\", printer=logger.debug, print_kwargs=[\"data_dir\", \"resolution\", \"buffer\", \"persist\"])\ndef load_arcticdem(\n    geobox: GeoBox, data_dir: Path | str, resolution: RESOLUTIONS, buffer: int = 0, persist: bool = True\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded.\n        data_dir (Path | str): The directory where the ArcticDEM data is stored.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in m.\n        buffer (int, optional): The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The ArcticDEM tile, with a buffer applied.\n            Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different.\n            Final dataset is NOT matched to the reference CRS and resolution.\n\n    Warning:\n        Geobox must be in a meter based CRS.\n\n    Usage:\n        Since the API of the `load_arcticdem` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_arcticdem\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        arcticdem = load_arcticdem(\n            optical.odc.geobox,\n            \"/path/to/arcticdem-parent-directory\",\n            resolution=2,\n            buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n        The `buffer` parameter is used to extend the region of interest by a certain amount of pixels.\n        This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.\n\n    Raises:\n        ValueError: If the resolution is not supported.\n\n    \"\"\"  # noqa: E501\n    odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n\n    match resolution:\n        case 2:\n            accessor = smart_geocubes.ArcticDEM2m(data_dir)\n        case 10:\n            accessor = smart_geocubes.ArcticDEM10m(data_dir)\n        case 32:\n            accessor = smart_geocubes.ArcticDEM32m(data_dir)\n        case _:\n            raise ValueError(f\"Resolution {resolution} not supported, only 2m, 10m and 32m are supported\")\n\n    accessor.assert_created()\n\n    arcticdem = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem[\"datamask\"] = arcticdem.datamask.astype(\"uint8\")\n\n    return arcticdem\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_planet_masks","title":"load_planet_masks","text":"<pre><code>load_planet_masks(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the valid and quality data masks from a Planet scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The file path to the Planet scene from which to derive the masks.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching UDM-2 TIFF file is found in the specified path.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopwatch.f(\"Loading Planet masks\", printer=logger.debug)\ndef load_planet_masks(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Planet scene.\n\n    Args:\n        fpath (str | Path): The file path to the Planet scene from which to derive the masks.\n\n    Raises:\n        FileNotFoundError: If no matching UDM-2 TIFF file is found in the specified path.\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # Get imagepath\n    udm_path = next(fpath.glob(\"*_udm2.tif\"), None)\n    if not udm_path:\n        udm_path = next(fpath.glob(\"*_udm2_clip.tif\"), None)\n    if not udm_path:\n        raise FileNotFoundError(f\"No matching UDM-2 TIFF files found in {fpath.resolve()} (.glob('*_udm2.tif'))\")\n\n    # See udm classes here: https://developers.planet.com/docs/data/udm-2/\n    da_udm = xr.open_dataarray(udm_path)\n\n    invalids = da_udm.sel(band=8).fillna(0) != 0\n    low_quality = da_udm.sel(band=[2, 3, 4, 5, 6]).max(axis=0) == 1\n    high_quality = ~low_quality &amp; ~invalids\n    qa_ds = xr.where(high_quality, 2, 0).where(~low_quality, 1).where(~invalids, 0).to_dataset(name=\"quality_data_mask\")\n    qa_ds[\"quality_data_mask\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Quality data mask\",\n        \"description\": \"0 = Invalid, 1 = Low Quality, 2 = High Quality\",\n    }\n    return qa_ds\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_planet_scene","title":"load_planet_scene","text":"<pre><code>load_planet_scene(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files or a specific path to the TIFF file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopwatch.f(\"Loading Planet scene\", printer=logger.debug)\ndef load_planet_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files or a specific path to the TIFF file.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    # Check if the directory contains a PSOrthoTile or PSScene\n    planet_type = parse_planet_type(fpath)\n    logger.debug(f\"Loading Planet PS {planet_type.capitalize()} from {fpath.resolve()}\")\n\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    # Define band names and corresponding indices\n    planet_da = xr.open_dataarray(ps_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_planet = (\n        planet_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n    )\n    for var in ds_planet.variables:\n        ds_planet[var].assign_attrs(\n            {\n                \"long_name\": f\"PLANET {var.capitalize()}\",\n                \"data_source\": \"planet\",\n                \"planet_type\": planet_type,\n                \"units\": \"Reflectance\",\n            }\n        )\n    ds_planet.attrs = {\"tile_id\": fpath.parent.stem if planet_type == \"orthotile\" else fpath.stem}\n    return ds_planet\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_s2_from_gee","title":"load_s2_from_gee","text":"<pre><code>load_s2_from_gee(\n    img: str | ee.Image,\n    bands_mapping: dict = {\n        \"B2\": \"blue\",\n        \"B3\": \"green\",\n        \"B4\": \"red\",\n        \"B8\": \"nir\",\n    },\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: pathlib.Path | None = None,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel 2 scene from Google Earth Engine and return it as an xarray dataset.</p> <p>Parameters:</p> <ul> <li> <code>img</code>               (<code>str | ee.Image</code>)           \u2013            <p>The Sentinel 2 image ID or the ee image object.</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>, default:                   <code>{'B2': 'blue', 'B3': 'green', 'B4': 'red', 'B8': 'nir'}</code> )           \u2013            <p>A mapping from bands to obtain. Will be renamed to the corresponding band names. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>scale_and_offset</code>               (<code>bool | tuple[float, float]</code>, default:                   <code>True</code> )           \u2013            <p>Whether to apply the scale and offset to the bands. If a tuple is provided, it will be used as the (<code>scale</code>, <code>offset</code>) values with <code>band * scale + offset</code>. If True, use the default values of <code>scale</code> = 0.0001 and <code>offset</code> = 0, taken from ee_extra. Defaults to True.</p> </li> <li> <code>cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cache directory. If None, no caching will be done. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel 2 scene from GEE\", printer=logger.debug, print_kwargs=[\"img\"])\ndef load_s2_from_gee(\n    img: str | ee.Image,\n    bands_mapping: dict = {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"},\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: Path | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 scene from Google Earth Engine and return it as an xarray dataset.\n\n    Args:\n        img (str | ee.Image): The Sentinel 2 image ID or the ee image object.\n        bands_mapping (dict[str, str], optional): A mapping from bands to obtain.\n            Will be renamed to the corresponding band names.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        scale_and_offset (bool | tuple[float, float], optional): Whether to apply the scale and offset to the bands.\n            If a tuple is provided, it will be used as the (`scale`, `offset`) values with `band * scale + offset`.\n            If True, use the default values of `scale` = 0.0001 and `offset` = 0, taken from ee_extra.\n            Defaults to True.\n        cache (Path | None, optional): The path to the cache directory. If None, no caching will be done.\n            Defaults to None.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    \"\"\"\n    if isinstance(img, str):\n        s2id = img\n        img = ee.Image(f\"COPERNICUS/S2_SR_HARMONIZED/{s2id}\")\n    else:\n        s2id = img.id().getInfo().split(\"/\")[-1]\n    logger.debug(f\"Loading Sentinel 2 tile {s2id=} from GEE\")\n\n    if \"SCL\" not in bands_mapping.keys():\n        bands_mapping[\"SCL\"] = \"scl\"\n\n    img = img.select(list(bands_mapping.keys()))\n\n    def _get_tile():\n        ds_s2 = xr.open_dataset(\n            img,\n            engine=\"ee\",\n            geometry=img.geometry(),\n            crs=img.select(0).projection().crs().getInfo(),\n            scale=10,\n        )\n        ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n        ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\").rename({\"X\": \"x\", \"Y\": \"y\"}).transpose(\"y\", \"x\")\n        ds_s2 = ds_s2.odc.assign_crs(ds_s2.attrs[\"crs\"])\n        with stopwatch(f\"Downloading data from GEE for {s2id=}\", printer=logger.debug):\n            ds_s2.load()\n        return ds_s2\n\n    ds_s2 = XarrayCacheManager(cache).get_or_create(\n        identifier=f\"gee-s2srh-{s2id}-{''.join(bands_mapping.keys())}.nc\",\n        creation_func=_get_tile,\n    )\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2-gee\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    ds_s2 = convert_masks(ds_s2)\n\n    # For some reason, there are some spatially random nan values in the data, not only at the borders\n    # To workaround this, set all nan values to 0 and add this information to the quality_data_mask\n    # This workaround is quite computational expensive, but it works for now\n    # TODO: Find other solutions for this problem!\n    with stopwatch(f\"Fixing nan values in {s2id=}\", printer=logger.debug):\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[\"quality_data_mask\"] = xr.where(ds_s2[band].isnull(), 0, ds_s2[\"quality_data_mask\"])\n            ds_s2[band] = ds_s2[band].fillna(0)\n            # Turn real nan values (scl is nan) into invalid data\n            ds_s2[band] = ds_s2[band].where(~ds_s2[\"scl\"].isnull())\n\n    if scale_and_offset:\n        if isinstance(scale_and_offset, tuple):\n            scale, offset = scale_and_offset\n        else:\n            scale, offset = 0.0001, 0\n        logger.debug(f\"Applying {scale=} and {offset=} to {s2id=} optical data\")\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[band] = ds_s2[band] * scale + offset\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_s2_from_stac","title":"load_s2_from_stac","text":"<pre><code>load_s2_from_stac(\n    s2id: str,\n    bands_mapping: dict = {\n        \"B02_10m\": \"blue\",\n        \"B03_10m\": \"green\",\n        \"B04_10m\": \"red\",\n        \"B08_10m\": \"nir\",\n    },\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: pathlib.Path | None = None,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel 2 scene from the Copernicus STAC API and return it as an xarray dataset.</p> <p>Parameters:</p> <ul> <li> <code>s2id</code>               (<code>str</code>)           \u2013            <p>The Sentinel 2 image ID.</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>, default:                   <code>{'B02_10m': 'blue', 'B03_10m': 'green', 'B04_10m': 'red', 'B08_10m': 'nir'}</code> )           \u2013            <p>A mapping from bands to obtain. Will be renamed to the corresponding band names. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>scale_and_offset</code>               (<code>bool | tuple[float, float]</code>, default:                   <code>True</code> )           \u2013            <p>Whether to apply the scale and offset to the bands. If a tuple is provided, it will be used as the (<code>scale</code>, <code>offset</code>) values with <code>band * scale + offset</code>. If True, use the default values of <code>scale</code> = 0.0001 and <code>offset</code> = 0, taken from ee_extra. Defaults to True.</p> </li> <li> <code>cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cache directory. If None, no caching will be done. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel 2 scene from STAC\", printer=logger.debug, print_kwargs=[\"s2id\"])\ndef load_s2_from_stac(\n    s2id: str,\n    bands_mapping: dict = {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"},\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: Path | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 scene from the Copernicus STAC API and return it as an xarray dataset.\n\n    Args:\n        s2id (str): The Sentinel 2 image ID.\n        bands_mapping (dict[str, str], optional): A mapping from bands to obtain.\n            Will be renamed to the corresponding band names.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        scale_and_offset (bool | tuple[float, float], optional): Whether to apply the scale and offset to the bands.\n            If a tuple is provided, it will be used as the (`scale`, `offset`) values with `band * scale + offset`.\n            If True, use the default values of `scale` = 0.0001 and `offset` = 0, taken from ee_extra.\n            Defaults to True.\n        cache (Path | None, optional): The path to the cache directory. If None, no caching will be done.\n            Defaults to None.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    \"\"\"\n    if \"SCL_20m\" not in bands_mapping.keys():\n        bands_mapping[\"SCL_20m\"] = \"scl\"\n\n    catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n    search = catalog.search(\n        collections=[\"sentinel-2-l2a\"],\n        ids=[s2id],\n    )\n\n    def _get_tile():\n        ds_s2 = xr.open_dataset(\n            search,\n            engine=\"stac\",\n            backend_kwargs={\"crs\": \"utm\", \"resolution\": 10, \"bands\": list(bands_mapping.keys())},\n        )\n        ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n        ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\")\n        with stopwatch(f\"Downloading data from STAC for {s2id=}\", printer=logger.debug):\n            ds_s2.load().load()\n        return ds_s2\n\n    ds_s2 = XarrayCacheManager(cache).get_or_create(\n        identifier=f\"stac-s2l2a-{s2id}-{''.join(bands_mapping.keys())}.nc\",\n        creation_func=_get_tile,\n    )\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2-stac\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    ds_s2 = convert_masks(ds_s2)\n\n    if scale_and_offset:\n        if isinstance(scale_and_offset, tuple):\n            scale, offset = scale_and_offset\n        else:\n            scale, offset = 0.0001, 0\n        logger.debug(f\"Applying {scale=} and {offset=} to {s2id=} optical data\")\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[band] = ds_s2[band] * scale + offset\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_s2_masks","title":"load_s2_masks","text":"<pre><code>load_s2_masks(\n    fpath: str | pathlib.Path,\n    reference_geobox: odc.geo.geobox.GeoBox,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the valid and quality data masks from a Sentinel 2 scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> <li> <code>reference_geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The reference geobox to reproject, resample and crop the masks data to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel 2 masks\", printer=logger.debug, print_kwargs=[\"fpath\"])\ndef load_s2_masks(fpath: str | Path, reference_geobox: GeoBox) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Sentinel 2 scene.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n        reference_geobox (GeoBox): The reference geobox to reproject, resample and crop the masks data to.\n\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # TODO: SCL band in SR file\n    try:\n        scl_path = next(fpath.glob(\"*_SCL*.tif\"))\n    except StopIteration:\n        logger.warning(\"Found no data quality mask (SCL). No masking will occur.\")\n        valid_data_mask = (odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\") + 1).to_dataset(name=\"valid_data_mask\")\n        valid_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Valid Data Mask\"}\n        quality_data_mask = odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\").to_dataset(name=\"quality_data_mask\")\n        quality_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Quality Data Mask\"}\n        qa_ds = xr.merge([valid_data_mask, quality_data_mask])\n        return qa_ds\n\n    # See scene classes here: https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/scene-classification/\n    da_scl = xr.open_dataarray(scl_path)\n\n    da_scl = da_scl.odc.reproject(reference_geobox, sampling=\"nearest\")\n\n    # Match crs\n    da_scl = da_scl.rio.write_crs(reference_geobox.crs)\n\n    da_scl = xr.Dataset({\"scl\": da_scl.sel(band=1).fillna(0).drop_vars(\"band\").astype(\"uint8\")})\n    da_scl = convert_masks(da_scl)\n\n    return da_scl\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_s2_scene","title":"load_s2_scene","text":"<pre><code>load_s2_scene(fpath: str | pathlib.Path) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel 2 scene from file\", printer=logger.debug)\ndef load_s2_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading Sentinel 2 scene from {fpath.resolve()}\")\n\n    # Get imagepath\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n\n    # Define band names and corresponding indices\n    s2_da = xr.open_dataarray(s2_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_s2 = s2_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    planet_crop_id, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n    ds_s2.attrs[\"planet_crop_id\"] = planet_crop_id\n    ds_s2.attrs[\"s2_tile_id\"] = s2_tile_id\n    ds_s2.attrs[\"tile_id\"] = tile_id\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_tcvis","title":"load_tcvis","text":"<pre><code>load_tcvis(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox to load the data for.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory to store the downloaded data for faster access for consecutive calls.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The TCVIS dataset.</p> </li> </ul> Usage <p>Since the API of the <code>load_tcvis</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_tcvis\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\ntcvis = load_tcvis(\n    optical.odc.geobox,\n    \"/path/to/tcvis-parent-directory\",\n)\n\n# Now we can for example match the resolution and extent of the optical data:\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopwatch.f(\"Loading TCVIS\", printer=logger.debug, print_kwargs=[\"data_dir\", \"buffer\", \"persist\"])\ndef load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the TCVIS for the given geobox, fetch new data from GEE if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox to load the data for.\n        data_dir (Path | str): The directory to store the downloaded data for faster access for consecutive calls.\n        buffer (int, optional): The buffer around the geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset.\n\n    Usage:\n        Since the API of the `load_tcvis` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_tcvis\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        tcvis = load_tcvis(\n            optical.odc.geobox,\n            \"/path/to/tcvis-parent-directory\",\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"  # noqa: E501\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n\n    # We want to assume that the datacube is already created to be save in a multi-process environment\n    accessor.assert_created()\n\n    tcvis = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Rename to follow our conventions\n    tcvis = tcvis.rename_vars(\n        {\n            \"TCB_slope\": \"tc_brightness\",\n            \"TCG_slope\": \"tc_greenness\",\n            \"TCW_slope\": \"tc_wetness\",\n        }\n    )\n\n    return tcvis\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.parse_planet_type","title":"parse_planet_type","text":"<pre><code>parse_planet_type(\n    fpath: pathlib.Path,\n) -&gt; typing.Literal[\"orthotile\", \"scene\"]\n</code></pre> <p>Parse the type of Planet data from the directory path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory path to the Planet data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['orthotile', 'scene']</code>           \u2013            <p>Literal[\"orthotile\", \"scene\"]: The type of Planet data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the Planet data type cannot be parsed from the file path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def parse_planet_type(fpath: Path) -&gt; Literal[\"orthotile\", \"scene\"]:\n    \"\"\"Parse the type of Planet data from the directory path.\n\n    Args:\n        fpath (Path): The directory path to the Planet data.\n\n    Returns:\n        Literal[\"orthotile\", \"scene\"]: The type of Planet data.\n\n    Raises:\n        ValueError: If the Planet data type cannot be parsed from the file path.\n\n    \"\"\"\n    # Cases for Scenes:\n    # - YYYYMMDD_HHMMSS_NN_XXXX\n    # - YYYYMMDD_HHMMSS_XXXX\n\n    # Cases for Orthotiles:\n    # NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n    # NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n\n    assert fpath.is_dir(), \"fpath must be the parent directory!\"\n\n    ps_name_parts = fpath.stem.split(\"_\")\n\n    if len(ps_name_parts) == 3:\n        # Must be scene or invalid\n        date, time, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and len(ident) == 4:\n            return \"scene\"\n\n    if len(ps_name_parts) == 4:\n        # Assume scene\n        date, time, n, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and n.isdigit() and len(ident) == 4:\n            return \"scene\"\n        # Is not scene, assume orthotile\n        chunkid, tileid, date, ident = ps_name_parts\n        if chunkid.isdigit() and tileid.isdigit() and _is_valid_date(date, \"%Y-%m-%d\") and len(ident) == 4:\n            return \"orthotile\"\n\n    raise ValueError(\n        f\"Could not parse Planet data type from {fpath}.\"\n        f\"Expected a format of YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX for scene, \"\n        \"or NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX or NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX for orthotile.\"\n        f\"Got {fpath.stem} instead.\"\n        \"Please ensure that the parent directory of the file is used, instead of the file itself.\"\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.parse_s2_tile_id","title":"parse_s2_tile_id","text":"<pre><code>parse_s2_tile_id(\n    fpath: str | pathlib.Path,\n) -&gt; tuple[str, str, str]\n</code></pre> <p>Parse the Sentinel 2 tile ID from a file path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, str, str]</code>           \u2013            <p>tuple[str, str, str]: A tuple containing the Planet crop ID, the Sentinel 2 tile ID and the combined tile ID.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>def parse_s2_tile_id(fpath: str | Path) -&gt; tuple[str, str, str]:\n    \"\"\"Parse the Sentinel 2 tile ID from a file path.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        tuple[str, str, str]: A tuple containing the Planet crop ID, the Sentinel 2 tile ID and the combined tile ID.\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n    planet_crop_id = fpath.stem\n    s2_tile_id = \"_\".join(s2_image.stem.split(\"_\")[:3])\n    tile_id = f\"{planet_crop_id}_{s2_tile_id}\"\n    return planet_crop_id, s2_tile_id, tile_id\n</code></pre>"},{"location":"reference/darts_acquisition/admin/","title":"darts_acquisition.admin","text":""},{"location":"reference/darts_acquisition/admin/#darts_acquisition.admin","title":"darts_acquisition.admin","text":"<p>Download of admin level files for the regions.</p>"},{"location":"reference/darts_acquisition/admin/#darts_acquisition.admin.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/admin/#darts_acquisition.admin._download_zip","title":"_download_zip","text":"<pre><code>_download_zip(url: str, admin_dir: pathlib.Path)\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>@stopwatch.f(\"Downloading and extracting zip file\", printer=logger.debug)\ndef _download_zip(url: str, admin_dir: Path):\n    response = requests.get(url)\n\n    # Get the downloaded data as a byte string\n    data = response.content\n    logger.debug(f\"Downloaded {len(data)} bytes\")\n\n    # Create a bytesIO object\n    with io.BytesIO(data) as buffer:\n        # Create a zipfile.ZipFile object and extract the files to a directory\n        admin_dir.mkdir(parents=True, exist_ok=True)\n        with zipfile.ZipFile(buffer, \"r\") as zip_ref:\n            # Extract the files to the specified directory\n            zip_ref.extractall(admin_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/admin/#darts_acquisition.admin.download_admin_files","title":"download_admin_files","text":"<pre><code>download_admin_files(admin_dir: pathlib.Path)\n</code></pre> <p>Download the admin files for the regions.</p> <p>Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.</p> <p>Parameters:</p> <ul> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the admin files.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>@stopwatch.f(\"Downloading admin files\", printer=logger.debug)\ndef download_admin_files(admin_dir: Path):\n    \"\"\"Download the admin files for the regions.\n\n    Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.\n\n    Args:\n        admin_dir (Path): The path to the admin files.\n\n    \"\"\"\n    # Download the admin files\n    admin_1_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.zip\"\n    admin_2_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.zip\"\n\n    admin_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.debug(f\"Downloading {admin_1_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_1_url, admin_dir)\n\n    logger.debug(f\"Downloading {admin_2_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_2_url, admin_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/arcticdem/","title":"darts_acquisition.arcticdem","text":""},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem","title":"darts_acquisition.arcticdem","text":"<p>Downloading and loading related functions for the Zarr-Datacube approach.</p>"},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem.RESOLUTIONS","title":"RESOLUTIONS  <code>module-attribute</code>","text":"<pre><code>RESOLUTIONS = typing.Literal[2, 10, 32]\n</code></pre>"},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/arcticdem/#darts_acquisition.arcticdem.load_arcticdem","title":"load_arcticdem","text":"<pre><code>load_arcticdem(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which the tile should be loaded.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory where the ArcticDEM data is stored.</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in m.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The ArcticDEM tile, with a buffer applied. Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different. Final dataset is NOT matched to the reference CRS and resolution.</p> </li> </ul> Warning <p>Geobox must be in a meter based CRS.</p> Usage <p>Since the API of the <code>load_arcticdem</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_arcticdem\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\narcticdem = load_arcticdem(\n    optical.odc.geobox,\n    \"/path/to/arcticdem-parent-directory\",\n    resolution=2,\n    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n)\n\n# Now we can for example match the resolution and extent of the optical data:\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> <p>The <code>buffer</code> parameter is used to extend the region of interest by a certain amount of pixels. This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the resolution is not supported.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopwatch.f(\"Loading ArcticDEM\", printer=logger.debug, print_kwargs=[\"data_dir\", \"resolution\", \"buffer\", \"persist\"])\ndef load_arcticdem(\n    geobox: GeoBox, data_dir: Path | str, resolution: RESOLUTIONS, buffer: int = 0, persist: bool = True\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded.\n        data_dir (Path | str): The directory where the ArcticDEM data is stored.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in m.\n        buffer (int, optional): The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The ArcticDEM tile, with a buffer applied.\n            Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different.\n            Final dataset is NOT matched to the reference CRS and resolution.\n\n    Warning:\n        Geobox must be in a meter based CRS.\n\n    Usage:\n        Since the API of the `load_arcticdem` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_arcticdem\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        arcticdem = load_arcticdem(\n            optical.odc.geobox,\n            \"/path/to/arcticdem-parent-directory\",\n            resolution=2,\n            buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n        The `buffer` parameter is used to extend the region of interest by a certain amount of pixels.\n        This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.\n\n    Raises:\n        ValueError: If the resolution is not supported.\n\n    \"\"\"  # noqa: E501\n    odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n\n    match resolution:\n        case 2:\n            accessor = smart_geocubes.ArcticDEM2m(data_dir)\n        case 10:\n            accessor = smart_geocubes.ArcticDEM10m(data_dir)\n        case 32:\n            accessor = smart_geocubes.ArcticDEM32m(data_dir)\n        case _:\n            raise ValueError(f\"Resolution {resolution} not supported, only 2m, 10m and 32m are supported\")\n\n    accessor.assert_created()\n\n    arcticdem = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem[\"datamask\"] = arcticdem.datamask.astype(\"uint8\")\n\n    return arcticdem\n</code></pre>"},{"location":"reference/darts_acquisition/planet/","title":"darts_acquisition.planet","text":""},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet","title":"darts_acquisition.planet","text":"<p>PLANET related data loading. Should be used temporary and maybe moved to the acquisition package.</p>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet._is_valid_date","title":"_is_valid_date","text":"<pre><code>_is_valid_date(date_str: str, format: str) -&gt; bool\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def _is_valid_date(date_str: str, format: str) -&gt; bool:\n    try:\n        datetime.strptime(date_str, format)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet.load_planet_masks","title":"load_planet_masks","text":"<pre><code>load_planet_masks(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the valid and quality data masks from a Planet scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The file path to the Planet scene from which to derive the masks.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching UDM-2 TIFF file is found in the specified path.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopwatch.f(\"Loading Planet masks\", printer=logger.debug)\ndef load_planet_masks(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Planet scene.\n\n    Args:\n        fpath (str | Path): The file path to the Planet scene from which to derive the masks.\n\n    Raises:\n        FileNotFoundError: If no matching UDM-2 TIFF file is found in the specified path.\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # Get imagepath\n    udm_path = next(fpath.glob(\"*_udm2.tif\"), None)\n    if not udm_path:\n        udm_path = next(fpath.glob(\"*_udm2_clip.tif\"), None)\n    if not udm_path:\n        raise FileNotFoundError(f\"No matching UDM-2 TIFF files found in {fpath.resolve()} (.glob('*_udm2.tif'))\")\n\n    # See udm classes here: https://developers.planet.com/docs/data/udm-2/\n    da_udm = xr.open_dataarray(udm_path)\n\n    invalids = da_udm.sel(band=8).fillna(0) != 0\n    low_quality = da_udm.sel(band=[2, 3, 4, 5, 6]).max(axis=0) == 1\n    high_quality = ~low_quality &amp; ~invalids\n    qa_ds = xr.where(high_quality, 2, 0).where(~low_quality, 1).where(~invalids, 0).to_dataset(name=\"quality_data_mask\")\n    qa_ds[\"quality_data_mask\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Quality data mask\",\n        \"description\": \"0 = Invalid, 1 = Low Quality, 2 = High Quality\",\n    }\n    return qa_ds\n</code></pre>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet.load_planet_scene","title":"load_planet_scene","text":"<pre><code>load_planet_scene(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files or a specific path to the TIFF file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopwatch.f(\"Loading Planet scene\", printer=logger.debug)\ndef load_planet_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files or a specific path to the TIFF file.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    # Check if the directory contains a PSOrthoTile or PSScene\n    planet_type = parse_planet_type(fpath)\n    logger.debug(f\"Loading Planet PS {planet_type.capitalize()} from {fpath.resolve()}\")\n\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    # Define band names and corresponding indices\n    planet_da = xr.open_dataarray(ps_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_planet = (\n        planet_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n    )\n    for var in ds_planet.variables:\n        ds_planet[var].assign_attrs(\n            {\n                \"long_name\": f\"PLANET {var.capitalize()}\",\n                \"data_source\": \"planet\",\n                \"planet_type\": planet_type,\n                \"units\": \"Reflectance\",\n            }\n        )\n    ds_planet.attrs = {\"tile_id\": fpath.parent.stem if planet_type == \"orthotile\" else fpath.stem}\n    return ds_planet\n</code></pre>"},{"location":"reference/darts_acquisition/planet/#darts_acquisition.planet.parse_planet_type","title":"parse_planet_type","text":"<pre><code>parse_planet_type(\n    fpath: pathlib.Path,\n) -&gt; typing.Literal[\"orthotile\", \"scene\"]\n</code></pre> <p>Parse the type of Planet data from the directory path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory path to the Planet data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['orthotile', 'scene']</code>           \u2013            <p>Literal[\"orthotile\", \"scene\"]: The type of Planet data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the Planet data type cannot be parsed from the file path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def parse_planet_type(fpath: Path) -&gt; Literal[\"orthotile\", \"scene\"]:\n    \"\"\"Parse the type of Planet data from the directory path.\n\n    Args:\n        fpath (Path): The directory path to the Planet data.\n\n    Returns:\n        Literal[\"orthotile\", \"scene\"]: The type of Planet data.\n\n    Raises:\n        ValueError: If the Planet data type cannot be parsed from the file path.\n\n    \"\"\"\n    # Cases for Scenes:\n    # - YYYYMMDD_HHMMSS_NN_XXXX\n    # - YYYYMMDD_HHMMSS_XXXX\n\n    # Cases for Orthotiles:\n    # NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n    # NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n\n    assert fpath.is_dir(), \"fpath must be the parent directory!\"\n\n    ps_name_parts = fpath.stem.split(\"_\")\n\n    if len(ps_name_parts) == 3:\n        # Must be scene or invalid\n        date, time, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and len(ident) == 4:\n            return \"scene\"\n\n    if len(ps_name_parts) == 4:\n        # Assume scene\n        date, time, n, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and n.isdigit() and len(ident) == 4:\n            return \"scene\"\n        # Is not scene, assume orthotile\n        chunkid, tileid, date, ident = ps_name_parts\n        if chunkid.isdigit() and tileid.isdigit() and _is_valid_date(date, \"%Y-%m-%d\") and len(ident) == 4:\n            return \"orthotile\"\n\n    raise ValueError(\n        f\"Could not parse Planet data type from {fpath}.\"\n        f\"Expected a format of YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX for scene, \"\n        \"or NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX or NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX for orthotile.\"\n        f\"Got {fpath.stem} instead.\"\n        \"Please ensure that the parent directory of the file is used, instead of the file itself.\"\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/s2/","title":"darts_acquisition.s2","text":""},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2","title":"darts_acquisition.s2","text":"<p>Sentinel 2 related data loading. Should be used temporary and maybe moved to the acquisition package.</p>"},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2.convert_masks","title":"convert_masks","text":"<pre><code>convert_masks(ds_s2: xarray.Dataset) -&gt; xarray.Dataset\n</code></pre> <p>Convert the Sentinel 2 scl mask into our own mask format inplace.</p> <p>Invalid: S2 SCL \u2192 0,1 Low Quality S2: S2 SCL != 0,1 \u2192 3,8,9,11 High Quality: S2 SCL != 0,1,3,8,9,11 \u2192 Alles andere</p> <p>Parameters:</p> <ul> <li> <code>ds_s2</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Sentinel 2 dataset containing the SCL band.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The modified dataset.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch(\"Converting Sentinel 2 masks\", printer=logger.debug)\ndef convert_masks(ds_s2: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Convert the Sentinel 2 scl mask into our own mask format inplace.\n\n    Invalid: S2 SCL \u2192 0,1\n    Low Quality S2: S2 SCL != 0,1 \u2192 3,8,9,11\n    High Quality: S2 SCL != 0,1,3,8,9,11 \u2192 Alles andere\n\n    Args:\n        ds_s2 (xr.Dataset): The Sentinel 2 dataset containing the SCL band.\n\n    Returns:\n        xr.Dataset: The modified dataset.\n\n    \"\"\"\n    assert \"scl\" in ds_s2.data_vars, \"The dataset does not contain the SCL band.\"\n\n    ds_s2[\"quality_data_mask\"] = xr.zeros_like(ds_s2[\"scl\"], dtype=\"uint8\")\n    # TODO: What about nan values?\n    invalids = ds_s2[\"scl\"].fillna(0).isin([0, 1])\n    low_quality = ds_s2[\"scl\"].isin([3, 8, 9, 11])\n    high_quality = ~invalids &amp; ~low_quality\n    # ds_s2[\"quality_data_mask\"] = ds_s2[\"quality_data_mask\"].where(invalids, 0)\n    ds_s2[\"quality_data_mask\"] = xr.where(low_quality, 1, ds_s2[\"quality_data_mask\"])\n    ds_s2[\"quality_data_mask\"] = xr.where(high_quality, 2, ds_s2[\"quality_data_mask\"])\n\n    ds_s2[\"quality_data_mask\"].attrs[\"data_source\"] = \"s2\"\n    ds_s2[\"quality_data_mask\"].attrs[\"long_name\"] = \"Quality Data Mask\"\n    ds_s2[\"quality_data_mask\"].attrs[\"description\"] = \"0 = Invalid, 1 = Low Quality, 2 = High Quality\"\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2.get_s2ids_from_shape_ee","title":"get_s2ids_from_shape_ee","text":"<pre><code>get_s2ids_from_shape_ee(\n    aoi_shapefile: pathlib.Path,\n    start_date: str,\n    end_date: str,\n    max_cloud_cover: int = 100,\n) -&gt; set[str]\n</code></pre> <p>Search for Sentinel 2 tiles via Earth Engine based on an aoi shapefile.</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>)           \u2013            <p>AOI shapefile path. Can be anything readable by geopandas.</p> </li> <li> <code>start_date</code>               (<code>str</code>)           \u2013            <p>Starting date in a format readable by ee.</p> </li> <li> <code>end_date</code>               (<code>str</code>)           \u2013            <p>Ending date in a format readable by ee.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 100.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>set[str]: Unique Sentinel 2 tile IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Searching for Sentinel 2 tiles via Earth Engine\", printer=logger.debug)\ndef get_s2ids_from_shape_ee(\n    aoi_shapefile: Path,\n    start_date: str,\n    end_date: str,\n    max_cloud_cover: int = 100,\n) -&gt; set[str]:\n    \"\"\"Search for Sentinel 2 tiles via Earth Engine based on an aoi shapefile.\n\n    Args:\n        aoi_shapefile (Path): AOI shapefile path. Can be anything readable by geopandas.\n        start_date (str): Starting date in a format readable by ee.\n        end_date (str): Ending date in a format readable by ee.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 100.\n\n    Returns:\n        set[str]: Unique Sentinel 2 tile IDs.\n\n    \"\"\"\n    aoi = gpd.read_file(aoi_shapefile)\n    aoi = aoi.to_crs(\"EPSG:4326\")\n    s2ids = set()\n    for i, row in aoi.iterrows():\n        geom = ee.Geometry.Polygon(list(row.geometry.exterior.coords))\n        ic = (\n            ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n            .filterBounds(geom)\n            .filterDate(start_date, end_date)\n            .filterMetadata(\"CLOUDY_PIXEL_PERCENTAGE\", \"less_than\", max_cloud_cover)\n        )\n        s2ids.update(ic.aggregate_array(\"system:index\").getInfo())\n    logger.debug(f\"Found {len(s2ids)} Sentinel 2 tiles via ee.\")\n    return s2ids\n</code></pre>"},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2.get_s2ids_from_shape_stac","title":"get_s2ids_from_shape_stac","text":"<pre><code>get_s2ids_from_shape_stac(\n    aoi_shapefile: pathlib.Path,\n    start_date: str,\n    end_date: str,\n    max_cloud_cover: int = 100,\n) -&gt; set[str]\n</code></pre> <p>Search for Sentinel 2 tiles via Earth Engine based on an aoi shapefile.</p> Note <p><code>start_date</code> and <code>end_date</code> will be concatted with a <code>/</code> to form a date range. Read more about the date format here: https://pystac-client.readthedocs.io/en/stable/api.html#pystac_client.Client.search</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>)           \u2013            <p>AOI shapefile path. Can be anything readable by geopandas.</p> </li> <li> <code>start_date</code>               (<code>str</code>)           \u2013            <p>Starting date in a format readable by pystac_client.</p> </li> <li> <code>end_date</code>               (<code>str</code>)           \u2013            <p>Ending date in a format readable by pystac_client.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum percentage of cloud cover. Defaults to 100.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[str]</code>           \u2013            <p>set[str]: Unique Sentinel 2 tile IDs.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Searching for Sentinel 2 tiles via STAC\", printer=logger.debug)\ndef get_s2ids_from_shape_stac(\n    aoi_shapefile: Path,\n    start_date: str,\n    end_date: str,\n    max_cloud_cover: int = 100,\n) -&gt; set[str]:\n    \"\"\"Search for Sentinel 2 tiles via Earth Engine based on an aoi shapefile.\n\n    Note:\n        `start_date` and `end_date` will be concatted with a `/` to form a date range.\n        Read more about the date format here: https://pystac-client.readthedocs.io/en/stable/api.html#pystac_client.Client.search\n\n    Args:\n        aoi_shapefile (Path): AOI shapefile path. Can be anything readable by geopandas.\n        start_date (str): Starting date in a format readable by pystac_client.\n        end_date (str): Ending date in a format readable by pystac_client.\n        max_cloud_cover (int, optional): Maximum percentage of cloud cover. Defaults to 100.\n\n    Returns:\n        set[str]: Unique Sentinel 2 tile IDs.\n\n    \"\"\"\n    aoi = gpd.read_file(aoi_shapefile)\n    catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n    s2ids = set()\n    for i, row in aoi.iterrows():\n        geom = ee.Geometry.Polygon(list(row.geometry.exterior.coords))\n        search = catalog.search(\n            collections=[\"sentinel-2-l2a\"],\n            bbox=geom.bounds,\n            datetime=f\"{start_date}/{end_date}\",\n            query=[f\"eo:cloud_cover&lt;={max_cloud_cover}\"],\n        )\n        s2ids.update(search.get_all_items())\n    logger.debug(f\"Found {len(s2ids)} Sentinel 2 tiles via stac.\")\n    return s2ids\n</code></pre>"},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2.load_s2_from_gee","title":"load_s2_from_gee","text":"<pre><code>load_s2_from_gee(\n    img: str | ee.Image,\n    bands_mapping: dict = {\n        \"B2\": \"blue\",\n        \"B3\": \"green\",\n        \"B4\": \"red\",\n        \"B8\": \"nir\",\n    },\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: pathlib.Path | None = None,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel 2 scene from Google Earth Engine and return it as an xarray dataset.</p> <p>Parameters:</p> <ul> <li> <code>img</code>               (<code>str | ee.Image</code>)           \u2013            <p>The Sentinel 2 image ID or the ee image object.</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>, default:                   <code>{'B2': 'blue', 'B3': 'green', 'B4': 'red', 'B8': 'nir'}</code> )           \u2013            <p>A mapping from bands to obtain. Will be renamed to the corresponding band names. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>scale_and_offset</code>               (<code>bool | tuple[float, float]</code>, default:                   <code>True</code> )           \u2013            <p>Whether to apply the scale and offset to the bands. If a tuple is provided, it will be used as the (<code>scale</code>, <code>offset</code>) values with <code>band * scale + offset</code>. If True, use the default values of <code>scale</code> = 0.0001 and <code>offset</code> = 0, taken from ee_extra. Defaults to True.</p> </li> <li> <code>cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cache directory. If None, no caching will be done. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel 2 scene from GEE\", printer=logger.debug, print_kwargs=[\"img\"])\ndef load_s2_from_gee(\n    img: str | ee.Image,\n    bands_mapping: dict = {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"},\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: Path | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 scene from Google Earth Engine and return it as an xarray dataset.\n\n    Args:\n        img (str | ee.Image): The Sentinel 2 image ID or the ee image object.\n        bands_mapping (dict[str, str], optional): A mapping from bands to obtain.\n            Will be renamed to the corresponding band names.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        scale_and_offset (bool | tuple[float, float], optional): Whether to apply the scale and offset to the bands.\n            If a tuple is provided, it will be used as the (`scale`, `offset`) values with `band * scale + offset`.\n            If True, use the default values of `scale` = 0.0001 and `offset` = 0, taken from ee_extra.\n            Defaults to True.\n        cache (Path | None, optional): The path to the cache directory. If None, no caching will be done.\n            Defaults to None.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    \"\"\"\n    if isinstance(img, str):\n        s2id = img\n        img = ee.Image(f\"COPERNICUS/S2_SR_HARMONIZED/{s2id}\")\n    else:\n        s2id = img.id().getInfo().split(\"/\")[-1]\n    logger.debug(f\"Loading Sentinel 2 tile {s2id=} from GEE\")\n\n    if \"SCL\" not in bands_mapping.keys():\n        bands_mapping[\"SCL\"] = \"scl\"\n\n    img = img.select(list(bands_mapping.keys()))\n\n    def _get_tile():\n        ds_s2 = xr.open_dataset(\n            img,\n            engine=\"ee\",\n            geometry=img.geometry(),\n            crs=img.select(0).projection().crs().getInfo(),\n            scale=10,\n        )\n        ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n        ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\").rename({\"X\": \"x\", \"Y\": \"y\"}).transpose(\"y\", \"x\")\n        ds_s2 = ds_s2.odc.assign_crs(ds_s2.attrs[\"crs\"])\n        with stopwatch(f\"Downloading data from GEE for {s2id=}\", printer=logger.debug):\n            ds_s2.load()\n        return ds_s2\n\n    ds_s2 = XarrayCacheManager(cache).get_or_create(\n        identifier=f\"gee-s2srh-{s2id}-{''.join(bands_mapping.keys())}.nc\",\n        creation_func=_get_tile,\n    )\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2-gee\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    ds_s2 = convert_masks(ds_s2)\n\n    # For some reason, there are some spatially random nan values in the data, not only at the borders\n    # To workaround this, set all nan values to 0 and add this information to the quality_data_mask\n    # This workaround is quite computational expensive, but it works for now\n    # TODO: Find other solutions for this problem!\n    with stopwatch(f\"Fixing nan values in {s2id=}\", printer=logger.debug):\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[\"quality_data_mask\"] = xr.where(ds_s2[band].isnull(), 0, ds_s2[\"quality_data_mask\"])\n            ds_s2[band] = ds_s2[band].fillna(0)\n            # Turn real nan values (scl is nan) into invalid data\n            ds_s2[band] = ds_s2[band].where(~ds_s2[\"scl\"].isnull())\n\n    if scale_and_offset:\n        if isinstance(scale_and_offset, tuple):\n            scale, offset = scale_and_offset\n        else:\n            scale, offset = 0.0001, 0\n        logger.debug(f\"Applying {scale=} and {offset=} to {s2id=} optical data\")\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[band] = ds_s2[band] * scale + offset\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2.load_s2_from_stac","title":"load_s2_from_stac","text":"<pre><code>load_s2_from_stac(\n    s2id: str,\n    bands_mapping: dict = {\n        \"B02_10m\": \"blue\",\n        \"B03_10m\": \"green\",\n        \"B04_10m\": \"red\",\n        \"B08_10m\": \"nir\",\n    },\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: pathlib.Path | None = None,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel 2 scene from the Copernicus STAC API and return it as an xarray dataset.</p> <p>Parameters:</p> <ul> <li> <code>s2id</code>               (<code>str</code>)           \u2013            <p>The Sentinel 2 image ID.</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>, default:                   <code>{'B02_10m': 'blue', 'B03_10m': 'green', 'B04_10m': 'red', 'B08_10m': 'nir'}</code> )           \u2013            <p>A mapping from bands to obtain. Will be renamed to the corresponding band names. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>scale_and_offset</code>               (<code>bool | tuple[float, float]</code>, default:                   <code>True</code> )           \u2013            <p>Whether to apply the scale and offset to the bands. If a tuple is provided, it will be used as the (<code>scale</code>, <code>offset</code>) values with <code>band * scale + offset</code>. If True, use the default values of <code>scale</code> = 0.0001 and <code>offset</code> = 0, taken from ee_extra. Defaults to True.</p> </li> <li> <code>cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cache directory. If None, no caching will be done. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel 2 scene from STAC\", printer=logger.debug, print_kwargs=[\"s2id\"])\ndef load_s2_from_stac(\n    s2id: str,\n    bands_mapping: dict = {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"},\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: Path | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 scene from the Copernicus STAC API and return it as an xarray dataset.\n\n    Args:\n        s2id (str): The Sentinel 2 image ID.\n        bands_mapping (dict[str, str], optional): A mapping from bands to obtain.\n            Will be renamed to the corresponding band names.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        scale_and_offset (bool | tuple[float, float], optional): Whether to apply the scale and offset to the bands.\n            If a tuple is provided, it will be used as the (`scale`, `offset`) values with `band * scale + offset`.\n            If True, use the default values of `scale` = 0.0001 and `offset` = 0, taken from ee_extra.\n            Defaults to True.\n        cache (Path | None, optional): The path to the cache directory. If None, no caching will be done.\n            Defaults to None.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    \"\"\"\n    if \"SCL_20m\" not in bands_mapping.keys():\n        bands_mapping[\"SCL_20m\"] = \"scl\"\n\n    catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n    search = catalog.search(\n        collections=[\"sentinel-2-l2a\"],\n        ids=[s2id],\n    )\n\n    def _get_tile():\n        ds_s2 = xr.open_dataset(\n            search,\n            engine=\"stac\",\n            backend_kwargs={\"crs\": \"utm\", \"resolution\": 10, \"bands\": list(bands_mapping.keys())},\n        )\n        ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n        ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\")\n        with stopwatch(f\"Downloading data from STAC for {s2id=}\", printer=logger.debug):\n            ds_s2.load().load()\n        return ds_s2\n\n    ds_s2 = XarrayCacheManager(cache).get_or_create(\n        identifier=f\"stac-s2l2a-{s2id}-{''.join(bands_mapping.keys())}.nc\",\n        creation_func=_get_tile,\n    )\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2-stac\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    ds_s2 = convert_masks(ds_s2)\n\n    if scale_and_offset:\n        if isinstance(scale_and_offset, tuple):\n            scale, offset = scale_and_offset\n        else:\n            scale, offset = 0.0001, 0\n        logger.debug(f\"Applying {scale=} and {offset=} to {s2id=} optical data\")\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[band] = ds_s2[band] * scale + offset\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2.load_s2_masks","title":"load_s2_masks","text":"<pre><code>load_s2_masks(\n    fpath: str | pathlib.Path,\n    reference_geobox: odc.geo.geobox.GeoBox,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the valid and quality data masks from a Sentinel 2 scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> <li> <code>reference_geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The reference geobox to reproject, resample and crop the masks data to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel 2 masks\", printer=logger.debug, print_kwargs=[\"fpath\"])\ndef load_s2_masks(fpath: str | Path, reference_geobox: GeoBox) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Sentinel 2 scene.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n        reference_geobox (GeoBox): The reference geobox to reproject, resample and crop the masks data to.\n\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # TODO: SCL band in SR file\n    try:\n        scl_path = next(fpath.glob(\"*_SCL*.tif\"))\n    except StopIteration:\n        logger.warning(\"Found no data quality mask (SCL). No masking will occur.\")\n        valid_data_mask = (odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\") + 1).to_dataset(name=\"valid_data_mask\")\n        valid_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Valid Data Mask\"}\n        quality_data_mask = odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\").to_dataset(name=\"quality_data_mask\")\n        quality_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Quality Data Mask\"}\n        qa_ds = xr.merge([valid_data_mask, quality_data_mask])\n        return qa_ds\n\n    # See scene classes here: https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/scene-classification/\n    da_scl = xr.open_dataarray(scl_path)\n\n    da_scl = da_scl.odc.reproject(reference_geobox, sampling=\"nearest\")\n\n    # Match crs\n    da_scl = da_scl.rio.write_crs(reference_geobox.crs)\n\n    da_scl = xr.Dataset({\"scl\": da_scl.sel(band=1).fillna(0).drop_vars(\"band\").astype(\"uint8\")})\n    da_scl = convert_masks(da_scl)\n\n    return da_scl\n</code></pre>"},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2.load_s2_scene","title":"load_s2_scene","text":"<pre><code>load_s2_scene(fpath: str | pathlib.Path) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopwatch.f(\"Loading Sentinel 2 scene from file\", printer=logger.debug)\ndef load_s2_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading Sentinel 2 scene from {fpath.resolve()}\")\n\n    # Get imagepath\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n\n    # Define band names and corresponding indices\n    s2_da = xr.open_dataarray(s2_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_s2 = s2_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    planet_crop_id, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n    ds_s2.attrs[\"planet_crop_id\"] = planet_crop_id\n    ds_s2.attrs[\"s2_tile_id\"] = s2_tile_id\n    ds_s2.attrs[\"tile_id\"] = tile_id\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/s2/#darts_acquisition.s2.parse_s2_tile_id","title":"parse_s2_tile_id","text":"<pre><code>parse_s2_tile_id(\n    fpath: str | pathlib.Path,\n) -&gt; tuple[str, str, str]\n</code></pre> <p>Parse the Sentinel 2 tile ID from a file path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, str, str]</code>           \u2013            <p>tuple[str, str, str]: A tuple containing the Planet crop ID, the Sentinel 2 tile ID and the combined tile ID.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>def parse_s2_tile_id(fpath: str | Path) -&gt; tuple[str, str, str]:\n    \"\"\"Parse the Sentinel 2 tile ID from a file path.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        tuple[str, str, str]: A tuple containing the Planet crop ID, the Sentinel 2 tile ID and the combined tile ID.\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n    planet_crop_id = fpath.stem\n    s2_tile_id = \"_\".join(s2_image.stem.split(\"_\")[:3])\n    tile_id = f\"{planet_crop_id}_{s2_tile_id}\"\n    return planet_crop_id, s2_tile_id, tile_id\n</code></pre>"},{"location":"reference/darts_acquisition/tcvis/","title":"darts_acquisition.tcvis","text":""},{"location":"reference/darts_acquisition/tcvis/#darts_acquisition.tcvis","title":"darts_acquisition.tcvis","text":"<p>Landsat Trends related Data Loading. Should be used temporary and maybe moved to the acquisition package.</p>"},{"location":"reference/darts_acquisition/tcvis/#darts_acquisition.tcvis.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_acquisition/tcvis/#darts_acquisition.tcvis.load_tcvis","title":"load_tcvis","text":"<pre><code>load_tcvis(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox to load the data for.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory to store the downloaded data for faster access for consecutive calls.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The TCVIS dataset.</p> </li> </ul> Usage <p>Since the API of the <code>load_tcvis</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_tcvis\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\ntcvis = load_tcvis(\n    optical.odc.geobox,\n    \"/path/to/tcvis-parent-directory\",\n)\n\n# Now we can for example match the resolution and extent of the optical data:\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopwatch.f(\"Loading TCVIS\", printer=logger.debug, print_kwargs=[\"data_dir\", \"buffer\", \"persist\"])\ndef load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the TCVIS for the given geobox, fetch new data from GEE if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox to load the data for.\n        data_dir (Path | str): The directory to store the downloaded data for faster access for consecutive calls.\n        buffer (int, optional): The buffer around the geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset.\n\n    Usage:\n        Since the API of the `load_tcvis` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_tcvis\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        tcvis = load_tcvis(\n            optical.odc.geobox,\n            \"/path/to/tcvis-parent-directory\",\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"  # noqa: E501\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n\n    # We want to assume that the datacube is already created to be save in a multi-process environment\n    accessor.assert_created()\n\n    tcvis = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Rename to follow our conventions\n    tcvis = tcvis.rename_vars(\n        {\n            \"TCB_slope\": \"tc_brightness\",\n            \"TCG_slope\": \"tc_greenness\",\n            \"TCW_slope\": \"tc_wetness\",\n        }\n    )\n\n    return tcvis\n</code></pre>"},{"location":"reference/darts_ensemble/","title":"darts_ensemble","text":""},{"location":"reference/darts_ensemble/#darts_ensemble","title":"darts_ensemble","text":"<p>Inference and model ensembling for the DARTS dataset.</p>"},{"location":"reference/darts_ensemble/#darts_ensemble.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1","title":"EnsembleV1","text":"<pre><code>EnsembleV1(\n    model_dict,\n    device: torch.device = darts_ensemble.ensemble_v1.DEFAULT_DEVICE,\n)\n</code></pre> <p>DARTS v1 ensemble based on a list of models.</p> <p>Initialize the ensemble.</p> <p>Parameters:</p> <ul> <li> <code>model_dict</code>               (<code>dict</code>)           \u2013            <p>The paths to model checkpoints to ensemble, the key is should be a model identifier to be written to outputs.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_ensemble.ensemble_v1.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __init__(\n    self,\n    model_dict,\n    device: torch.device = DEFAULT_DEVICE,\n):\n    \"\"\"Initialize the ensemble.\n\n    Args:\n        model_dict (dict): The paths to model checkpoints to ensemble, the key is should be a model identifier\n            to be written to outputs.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_paths = {k: Path(v) for k, v in model_dict.items()}\n    logger.debug(\n        \"Loading models:\\n\" + \"\\n\".join([f\" - {k.upper()} model: {v.resolve()}\" for k, v in model_paths.items()])\n    )\n    self.models = {k: SMPSegmenter(v, device=device) for k, v in model_paths.items()}\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = {\n    k: darts_segmentation.segment.SMPSegmenter(\n        v,\n        device=darts_ensemble.ensemble_v1.EnsembleV1(\n            device\n        ),\n    )\n    for (k, v) in model_paths.items()\n}\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run the ensemble on the given tile.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile with the ensemble applied.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run the ensemble on the given tile.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        xr.Dataset: Output tile with the ensemble applied.\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    else:\n        raise ValueError(\"Input must be an xr.Dataset or a list of xr.Dataset.\")\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run inference on a tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>@stopwatch.f(\n    \"Ensemble inference\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\", \"keep_inputs\"],\n)\ndef segment_tile(\n    self,\n    tile: xr.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    probabilities = {}\n    for model_name, model in self.models.items():\n        probabilities[model_name] = model.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )[\"probabilities\"].copy()\n\n    # calculate the mean\n    tile[\"probabilities\"] = xr.concat(probabilities.values(), dim=\"model_probs\").mean(dim=\"model_probs\")\n\n    if keep_inputs:\n        for k, v in probabilities.items():\n            tile[f\"probabilities-{k}\"] = v\n\n    return tile\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    return [\n        self.segment_tile(\n            tile,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n        for tile in tiles\n    ]\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/","title":"darts_ensemble.ensemble_v1","text":""},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1","title":"darts_ensemble.ensemble_v1","text":"<p>DARTS v1 ensemble based on two models, one trained with TCVIS data and the other without.</p>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.DEFAULT_DEVICE","title":"DEFAULT_DEVICE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DEVICE = torch.device(\n    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1","title":"EnsembleV1","text":"<pre><code>EnsembleV1(\n    model_dict,\n    device: torch.device = darts_ensemble.ensemble_v1.DEFAULT_DEVICE,\n)\n</code></pre> <p>DARTS v1 ensemble based on a list of models.</p> <p>Initialize the ensemble.</p> <p>Parameters:</p> <ul> <li> <code>model_dict</code>               (<code>dict</code>)           \u2013            <p>The paths to model checkpoints to ensemble, the key is should be a model identifier to be written to outputs.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_ensemble.ensemble_v1.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __init__(\n    self,\n    model_dict,\n    device: torch.device = DEFAULT_DEVICE,\n):\n    \"\"\"Initialize the ensemble.\n\n    Args:\n        model_dict (dict): The paths to model checkpoints to ensemble, the key is should be a model identifier\n            to be written to outputs.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_paths = {k: Path(v) for k, v in model_dict.items()}\n    logger.debug(\n        \"Loading models:\\n\" + \"\\n\".join([f\" - {k.upper()} model: {v.resolve()}\" for k, v in model_paths.items()])\n    )\n    self.models = {k: SMPSegmenter(v, device=device) for k, v in model_paths.items()}\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = {\n    k: darts_segmentation.segment.SMPSegmenter(\n        v,\n        device=darts_ensemble.ensemble_v1.EnsembleV1(\n            device\n        ),\n    )\n    for (k, v) in model_paths.items()\n}\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run the ensemble on the given tile.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile with the ensemble applied.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run the ensemble on the given tile.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        xr.Dataset: Output tile with the ensemble applied.\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    else:\n        raise ValueError(\"Input must be an xr.Dataset or a list of xr.Dataset.\")\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run inference on a tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>@stopwatch.f(\n    \"Ensemble inference\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\", \"keep_inputs\"],\n)\ndef segment_tile(\n    self,\n    tile: xr.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    probabilities = {}\n    for model_name, model in self.models.items():\n        probabilities[model_name] = model.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )[\"probabilities\"].copy()\n\n    # calculate the mean\n    tile[\"probabilities\"] = xr.concat(probabilities.values(), dim=\"model_probs\").mean(dim=\"model_probs\")\n\n    if keep_inputs:\n        for k, v in probabilities.items():\n            tile[f\"probabilities-{k}\"] = v\n\n    return tile\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.EnsembleV1.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    return [\n        self.segment_tile(\n            tile,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n        for tile in tiles\n    ]\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.SMPSegmenter","title":"SMPSegmenter","text":"<pre><code>SMPSegmenter(\n    model_checkpoint: pathlib.Path | str,\n    device: torch.device = darts_segmentation.segment.DEFAULT_DEVICE,\n)\n</code></pre> <p>An actor that keeps a model as its state and segments tiles.</p> <p>Initialize the segmenter.</p> <p>Parameters:</p> <ul> <li> <code>model_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the model checkpoint.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_segmentation.segment.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __init__(self, model_checkpoint: Path | str, device: torch.device = DEFAULT_DEVICE):\n    \"\"\"Initialize the segmenter.\n\n    Args:\n        model_checkpoint (Path): The path to the model checkpoint.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_checkpoint = model_checkpoint if isinstance(model_checkpoint, Path) else Path(model_checkpoint)\n    self.device = device\n    ckpt = torch.load(model_checkpoint, map_location=self.device)\n    self.config = SMPSegmenterConfig.from_ckpt(ckpt[\"config\"])\n    # Overwrite the encoder weights with None, because we load our own\n    self.config[\"model\"] |= {\"encoder_weights\": None}\n    self.model = smp.create_model(**self.config[\"model\"])\n    self.model.to(self.device)\n    self.model.load_state_dict(ckpt[\"statedict\"])\n    self.model.eval()\n\n    logger.debug(f\"Successfully loaded model from {model_checkpoint.resolve()} with inputs: {self.config['bands']}\")\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.SMPSegmenter.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: darts_segmentation.segment.SMPSegmenterConfig = (\n    darts_segmentation.segment.SMPSegmenterConfig.from_ckpt(\n        ckpt[\"config\"]\n    )\n)\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.SMPSegmenter.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: torch.device = (\n    darts_segmentation.segment.SMPSegmenter(device)\n)\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.SMPSegmenter.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: torch.nn.Module = (\n    segmentation_models_pytorch.create_model(\n        **darts_segmentation.segment.SMPSegmenter(\n            self\n        ).config[\"model\"]\n    )\n)\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.SMPSegmenter.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset | list[xarray.Dataset]\n</code></pre> <p>Run inference on a single tile or a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>A single tile or a list of tiles augmented by a predicted <code>probabilities</code> layer, depending on the input.</p> </li> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>Each <code>probability</code> has type float32 and range [0, 1].</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xr.Dataset | list[xr.Dataset]:\n    \"\"\"Run inference on a single tile or a list of tiles.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n        Each `probability` has type float32 and range [0, 1].\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    else:\n        raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.SMPSegmenter.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run inference on a tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@stopwatch.f(\n    \"Segmenting tile\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\"],\n)\ndef segment_tile(\n    self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tile to a tensor\n    tensor_tile = self.tile2tensor(tile)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tile = tensor_tile.unsqueeze(0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n    ).squeeze(0)\n\n    # Highly sophisticated DL-based predictor\n    # TODO: is there a better way to pass metadata?\n    tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n    tile[\"probabilities\"].attrs = {\"long_name\": \"Probabilities\"}\n    tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tile, probabilities\n    free_torch()\n\n    return tile\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.SMPSegmenter.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@stopwatch.f(\n    \"Segmenting tiles\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\"],\n)\ndef segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tiles to tensors\n    # TODO: maybe create a batched tile2tensor function?\n    # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n    tensor_tiles = self.tile2tensor_batched(tiles)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n    )\n\n    # Highly sophisticated DL-based predictor\n    for tile, probs in zip(tiles, probabilities):\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\"long_name\": \"Probabilities\"}\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tiles, probabilities\n    free_torch()\n\n    return tiles\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.SMPSegmenter.tile2tensor","title":"tile2tensor","text":"<pre><code>tile2tensor(tile: xarray.Dataset) -&gt; torch.Tensor\n</code></pre> <p>Take a tile and convert it to a pytorch tensor.</p> <p>Respects the input combination from the config.</p> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n    \"\"\"Take a tile and convert it to a pytorch tensor.\n\n    Respects the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    # e.g. band.names: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n    # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n    for band in self.config[\"bands\"]:\n        band_data = tile[band.name]\n        # Normalize the band data to the range [0, 1]\n        # Follows CF conventions for scaling and offsetting\n        # decode_values = encoded_values * scale_factor + add_offset\n        # the range [0, 1] is the decoded range\n        band_data = band_data * band.factor + band.offset\n        band_data = band_data.clip(min=0, max=1)\n        bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n    return torch.stack(bands, dim=0)\n</code></pre>"},{"location":"reference/darts_ensemble/ensemble_v1/#darts_ensemble.ensemble_v1.SMPSegmenter.tile2tensor_batched","title":"tile2tensor_batched","text":"<pre><code>tile2tensor_batched(\n    tiles: list[xarray.Dataset],\n) -&gt; torch.Tensor\n</code></pre> <p>Take a list of tiles and convert them to a pytorch tensor.</p> <p>Respects the the input combination from the config.</p> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n    \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n    Respects the the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    for band in self.config[\"bands\"]:\n        for tile in tiles:\n            band_data = tile[band.name]\n            # Normalize the band data\n            band_data = band_data * band.factor + band.offset\n            band_data = band_data.clip(min=0, max=1)\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n    # TODO: Test this\n    return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"bands\"]), *bands[0].shape)\n</code></pre>"},{"location":"reference/darts_export/","title":"darts_export","text":""},{"location":"reference/darts_export/#darts_export","title":"darts_export","text":"<p>Dataset export for the DARTS dataset.</p>"},{"location":"reference/darts_export/#darts_export.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_export/#darts_export.export_tile","title":"export_tile","text":"<pre><code>export_tile(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n)\n</code></pre> <p>Export a tile to a file.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The tile to export.</p> </li> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path where to export to.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the band is not found in the tile.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>@stopwatch.f(\"Exporting tile\", printer=logger.debug, print_kwargs=[\"bands\", \"ensemble_subsets\"])\ndef export_tile(  # noqa: C901\n    tile: xr.Dataset,\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n):\n    \"\"\"Export a tile to a file.\n\n    Args:\n        tile (xr.Dataset): The tile to export.\n        out_dir (Path): The path where to export to.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Raises:\n        ValueError: If the band is not found in the tile.\n\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                _export_polygonized(tile, out_dir, ensemble_subsets)\n            case \"binarized\":\n                _export_binarized(tile, out_dir, ensemble_subsets)\n            case \"probabilities\":\n                _export_probabilities(tile, out_dir, ensemble_subsets)\n            case \"extent\":\n                _export_vector(tile, \"extent\", out_dir, fname=\"prediction_extent\")\n            case \"thumbnail\":\n                _export_thumbnail(tile, out_dir)\n            case \"optical\":\n                _export_raster(tile, [\"red\", \"green\", \"blue\", \"nir\"], out_dir, fname=\"optical\")\n            case \"dem\":\n                _export_raster(tile, [\"slope\", \"relative_elevation\"], out_dir, fname=\"dem\")\n            case \"tcvis\":\n                _export_raster(tile, [\"tc_brightness\", \"tc_greenness\", \"tc_wetness\"], out_dir, fname=\"tcvis\")\n            case _:\n                if band not in tile.data_vars:\n                    raise ValueError(\n                        f\"Band {band} not found in tile for export. Available bands are: {list(tile.data_vars.keys())}\"\n                    )\n                # Export the band as a raster\n                _export_raster(tile, band, out_dir)\n</code></pre>"},{"location":"reference/darts_export/#darts_export.missing_outputs","title":"missing_outputs","text":"<pre><code>missing_outputs(\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n) -&gt; typing.Literal[\"all\", \"some\", \"none\"]\n</code></pre> <p>Check for missing output files in the given directory.</p> <p>Parameters:</p> <ul> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to check for missing files.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['all', 'some', 'none']</code>           \u2013            <p>Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files: - \"none\": No files are missing. - \"some\": Some files are missing, which one will be logged to debug. - \"all\": All files are missing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the output path is not a directory.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/check.py</code> <pre><code>def missing_outputs(  # noqa: C901\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n) -&gt; Literal[\"all\", \"some\", \"none\"]:\n    \"\"\"Check for missing output files in the given directory.\n\n    Args:\n        out_dir (Path): The directory to check for missing files.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Returns:\n        Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files:\n            - \"none\": No files are missing.\n            - \"some\": Some files are missing, which one will be logged to debug.\n            - \"all\": All files are missing.\n\n    Raises:\n        ValueError: If the output path is not a directory.\n\n    \"\"\"\n    if not out_dir.exists():\n        return []\n    if not out_dir.is_dir():\n        raise ValueError(f\"Output path {out_dir} is not a directory.\")\n    expected_files = []\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                expected_files += [\"prediction_segments.gpkg\"] + [\n                    f\"prediction_segments-{es}.gpkg\" for es in ensemble_subsets\n                ]\n                expected_files += [\"prediction_segments.parquet\"] + [\n                    f\"prediction_segments-{es}.parquet\" for es in ensemble_subsets\n                ]\n            case \"binarized\":\n                expected_files += [\"binarized.tif\"] + [f\"binarized-{es}.tif\" for es in ensemble_subsets]\n            case \"probabilities\":\n                expected_files += [\"probabilities.tif\"] + [f\"probabilities-{es}.tif\" for es in ensemble_subsets]\n            case \"extent\":\n                expected_files += [\"extent.gpkg\", \"extent.parquet\"]\n            case \"thumbnail\":\n                expected_files += [\"thumbnail.jpg\"]\n            case _:\n                expected_files += [f\"{band}.tif\"]\n\n    missing_files = _missing_files(out_dir, expected_files)\n    if len(missing_files) == 0:\n        return \"none\"\n    elif len(missing_files) == len(expected_files):\n        return \"all\"\n    else:\n        logger.debug(\n            f\"Missing files in {out_dir}: {', '.join(missing_files)}. Expected files: {', '.join(expected_files)}.\"\n        )\n        return \"some\"\n</code></pre>"},{"location":"reference/darts_export/check/","title":"darts_export.check","text":""},{"location":"reference/darts_export/check/#darts_export.check","title":"darts_export.check","text":"<p>Check if outputpath already contains files.</p>"},{"location":"reference/darts_export/check/#darts_export.check.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_export/check/#darts_export.check._missing_files","title":"_missing_files","text":"<pre><code>_missing_files(\n    output_dir: pathlib.Path, file_names: list[str]\n) -&gt; list[str]\n</code></pre> <p>Check if the given files exist in the output directory.</p> <p>Parameters:</p> <ul> <li> <code>output_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to check for files.</p> </li> <li> <code>file_names</code>               (<code>list[str]</code>)           \u2013            <p>The list of file names to check.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: A list of missing file names.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/check.py</code> <pre><code>def _missing_files(output_dir: Path, file_names: list[str]) -&gt; list[str]:\n    \"\"\"Check if the given files exist in the output directory.\n\n    Args:\n        output_dir (Path): The directory to check for files.\n        file_names (list[str]): The list of file names to check.\n\n    Returns:\n        list[str]: A list of missing file names.\n\n    \"\"\"\n    missing_files = []\n    for file_name in file_names:\n        file_path = output_dir / file_name\n        if not file_path.exists():\n            missing_files.append(file_name)\n    return missing_files\n</code></pre>"},{"location":"reference/darts_export/check/#darts_export.check.missing_outputs","title":"missing_outputs","text":"<pre><code>missing_outputs(\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n) -&gt; typing.Literal[\"all\", \"some\", \"none\"]\n</code></pre> <p>Check for missing output files in the given directory.</p> <p>Parameters:</p> <ul> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to check for missing files.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['all', 'some', 'none']</code>           \u2013            <p>Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files: - \"none\": No files are missing. - \"some\": Some files are missing, which one will be logged to debug. - \"all\": All files are missing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the output path is not a directory.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/check.py</code> <pre><code>def missing_outputs(  # noqa: C901\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n) -&gt; Literal[\"all\", \"some\", \"none\"]:\n    \"\"\"Check for missing output files in the given directory.\n\n    Args:\n        out_dir (Path): The directory to check for missing files.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Returns:\n        Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files:\n            - \"none\": No files are missing.\n            - \"some\": Some files are missing, which one will be logged to debug.\n            - \"all\": All files are missing.\n\n    Raises:\n        ValueError: If the output path is not a directory.\n\n    \"\"\"\n    if not out_dir.exists():\n        return []\n    if not out_dir.is_dir():\n        raise ValueError(f\"Output path {out_dir} is not a directory.\")\n    expected_files = []\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                expected_files += [\"prediction_segments.gpkg\"] + [\n                    f\"prediction_segments-{es}.gpkg\" for es in ensemble_subsets\n                ]\n                expected_files += [\"prediction_segments.parquet\"] + [\n                    f\"prediction_segments-{es}.parquet\" for es in ensemble_subsets\n                ]\n            case \"binarized\":\n                expected_files += [\"binarized.tif\"] + [f\"binarized-{es}.tif\" for es in ensemble_subsets]\n            case \"probabilities\":\n                expected_files += [\"probabilities.tif\"] + [f\"probabilities-{es}.tif\" for es in ensemble_subsets]\n            case \"extent\":\n                expected_files += [\"extent.gpkg\", \"extent.parquet\"]\n            case \"thumbnail\":\n                expected_files += [\"thumbnail.jpg\"]\n            case _:\n                expected_files += [f\"{band}.tif\"]\n\n    missing_files = _missing_files(out_dir, expected_files)\n    if len(missing_files) == 0:\n        return \"none\"\n    elif len(missing_files) == len(expected_files):\n        return \"all\"\n    else:\n        logger.debug(\n            f\"Missing files in {out_dir}: {', '.join(missing_files)}. Expected files: {', '.join(expected_files)}.\"\n        )\n        return \"some\"\n</code></pre>"},{"location":"reference/darts_export/conversion/","title":"darts_export.conversion","text":""},{"location":"reference/darts_export/conversion/#darts_export.conversion","title":"darts_export.conversion","text":"<p>Collection of conversion functions to translate data for export and processing.</p>"},{"location":"reference/darts_export/conversion/#darts_export.conversion.numpy_to_gdal","title":"numpy_to_gdal","text":"<pre><code>numpy_to_gdal(\n    nparray: numpy.ndarray,\n    rio_georef: xarray.DataArray | xarray.Dataset,\n) -&gt; osgeo.gdal.Dataset\n</code></pre> <p>Convert a numpy ndarray into a gdal Dataset.</p> <p>Georeference is to be passed in terms of an xarray object augmented by the rioxarray module, meaning the '.rio' accessor is available.</p> <p>Parameters:</p> <ul> <li> <code>nparray</code>               (<code>numpy.ndarray</code>)           \u2013            <p>The data to convert</p> </li> <li> <code>rio_georef</code>               (<code>xarray.DataArray | xarray.Dataset</code>)           \u2013            <p>an xarray with rio accessor as georeference</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>osgeo.gdal.Dataset</code>           \u2013            <p>gdal.Dataset: description</p> </li> </ul> Source code in <code>darts-export/src/darts_export/conversion.py</code> <pre><code>def numpy_to_gdal(nparray: np.ndarray, rio_georef: xarray.DataArray | xarray.Dataset) -&gt; gdal.Dataset:\n    \"\"\"Convert a numpy ndarray into a gdal Dataset.\n\n    Georeference is to be passed in terms\n    of an xarray object augmented by the rioxarray module, meaning the '.rio' accessor is\n    available.\n\n    Args:\n        nparray (np.ndarray): The data to convert\n        rio_georef (xarray.DataArray | xarray.Dataset): an xarray with rio accessor as georeference\n\n    Returns:\n        gdal.Dataset: _description_\n\n    \"\"\"\n    # convert the xarray to a gdal dataset\n    dta = gdal_array.OpenArray(nparray)\n\n    # copy over the geodata\n    # the transform object of rasterio has to be converted into a tuple\n    affine_transform = rio_georef.rio.transform()\n    geotransform = (\n        affine_transform.c,\n        affine_transform.a,\n        affine_transform.b,\n        affine_transform.f,\n        affine_transform.d,\n        affine_transform.e,\n    )\n    dta.SetGeoTransform(geotransform)\n    dta.SetProjection(rio_georef.rio.crs.to_wkt())\n    return dta\n</code></pre>"},{"location":"reference/darts_export/conversion/#darts_export.conversion.ogrlyr_to_geopandas","title":"ogrlyr_to_geopandas","text":"<pre><code>ogrlyr_to_geopandas(\n    ogr_layer: osgeo.ogr.Layer,\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Convert a GDAL/OGR layer object to a geopandas dataframe.</p> <p>Parameters:</p> <ul> <li> <code>ogr_layer</code>               (<code>osgeo.ogr.Layer</code>)           \u2013            <p>the ogr layer object to convert</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>geopandas.GeoDataFrame</code>           \u2013            <p>gpd.GeoDataFrame: the resulting GeoDataFrame</p> </li> </ul> Source code in <code>darts-export/src/darts_export/conversion.py</code> <pre><code>def ogrlyr_to_geopandas(ogr_layer: ogr.Layer) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert a GDAL/OGR layer object to a geopandas dataframe.\n\n    Args:\n        ogr_layer (ogr.Layer): the ogr layer object to convert\n\n    Returns:\n        gpd.GeoDataFrame: the resulting GeoDataFrame\n\n    \"\"\"\n    # Initialize an empty list to store geometries and attributes\n    features = []\n\n    # Iterate over OGR features in the layer\n    for feature in ogr_layer:\n        geom = feature.GetGeometryRef()\n        # geom_json = geom.ExportToJson()  # Convert to GeoJSON format\n        geom_wkt = geom.ExportToWkt()  # Convert to Well-Known Text (WKT)\n        attributes = feature.items()  # Get attribute data as a dictionary\n\n        # Append a tuple (geometry, attributes)\n        features.append((geom_wkt, attributes))\n\n    # Create a GeoDataFrame from the geometries and attributes\n    gdf = gpd.GeoDataFrame(\n        [attr for geom, attr in features], geometry=gpd.GeoSeries.from_wkt([geom for geom, attr in features])\n    )\n\n    return gdf\n</code></pre>"},{"location":"reference/darts_export/conversion/#darts_export.conversion.rioxarrayds_to_gdal","title":"rioxarrayds_to_gdal","text":"<pre><code>rioxarrayds_to_gdal(\n    rix: xarray.DataArray,\n) -&gt; osgeo.gdal.Dataset\n</code></pre> <p>Convert a rioxarray object to a gdal dataset.</p> <p>Parameters:</p> <ul> <li> <code>rix</code>               (<code>xarray.DataArray</code>)           \u2013            <p>data to convert</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>osgeo.gdal.Dataset</code>           \u2013            <p>gdal.Dataset: the converted data</p> </li> </ul> Source code in <code>darts-export/src/darts_export/conversion.py</code> <pre><code>def rioxarrayds_to_gdal(rix: xarray.DataArray) -&gt; gdal.Dataset:\n    \"\"\"Convert a rioxarray object to a gdal dataset.\n\n    Args:\n        rix (xarray.DataArray): data to convert\n\n    Returns:\n        gdal.Dataset: the converted data\n\n    \"\"\"\n    return numpy_to_gdal(rix.to_numpy(), rix)\n</code></pre>"},{"location":"reference/darts_export/export/","title":"darts_export.export","text":""},{"location":"reference/darts_export/export/#darts_export.export","title":"darts_export.export","text":"<p>Darts export module for inference results.</p>"},{"location":"reference/darts_export/export/#darts_export.export.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_binarized","title":"_export_binarized","text":"<pre><code>_export_binarized(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    ensemble_subsets: list[str] = [],\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_binarized(tile: xr.Dataset, out_dir: Path, ensemble_subsets: list[str] = []):\n    _export_raster(tile, \"binarized_segmentation\", out_dir, fname=\"binarized\")\n    for ensemble_subset in ensemble_subsets:\n        _export_raster(\n            tile,\n            f\"binarized_segmentation-{ensemble_subset}\",\n            out_dir,\n            fname=f\"binarized-{ensemble_subset}\",\n        )\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_polygonized","title":"_export_polygonized","text":"<pre><code>_export_polygonized(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    ensemble_subsets: list[str] = [],\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_polygonized(tile: xr.Dataset, out_dir: Path, ensemble_subsets: list[str] = []):\n    _export_vector(tile, \"binarized_segmentation\", out_dir, fname=\"prediction_segments\")\n    for ensemble_subset in ensemble_subsets:\n        _export_vector(\n            tile,\n            f\"binarized_segmentation-{ensemble_subset}\",\n            out_dir,\n            fname=f\"prediction_segments-{ensemble_subset}\",\n        )\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_probabilities","title":"_export_probabilities","text":"<pre><code>_export_probabilities(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    ensemble_subsets: list[str] = [],\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_probabilities(tile: xr.Dataset, out_dir: Path, ensemble_subsets: list[str] = []):\n    _export_raster(tile, \"probabilities\", out_dir, fname=\"probabilities\")\n    for ensemble_subset in ensemble_subsets:\n        _export_raster(\n            tile,\n            f\"probabilities-{ensemble_subset}\",\n            out_dir,\n            fname=f\"probabilities-{ensemble_subset}\",\n        )\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_raster","title":"_export_raster","text":"<pre><code>_export_raster(\n    tile: xarray.Dataset,\n    name: str,\n    out_dir: pathlib.Path,\n    fname: str | None = None,\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_raster(tile: xr.Dataset, name: str, out_dir: Path, fname: str | None = None):\n    if fname is None:\n        fname = name\n    fpath = out_dir / f\"{fname}.tif\"\n    with stopwatch(f\"Exporting {name} to {fpath.resolve()}\", logger.debug):\n        tile[name].rio.to_raster(fpath, driver=\"GTiff\", compress=\"LZW\")\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_thumbnail","title":"_export_thumbnail","text":"<pre><code>_export_thumbnail(\n    tile: xarray.Dataset, out_dir: pathlib.Path\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_thumbnail(tile: xr.Dataset, out_dir: Path):\n    fpath = out_dir / \"thumbnail.jpg\"\n    with stopwatch(f\"Exporting thumbnail to {fpath}\", logger.debug):\n        fig = miniviz.thumbnail(tile)\n        fig.savefig(fpath)\n        fig.clear()\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export._export_vector","title":"_export_vector","text":"<pre><code>_export_vector(\n    tile: xarray.Dataset,\n    name: str,\n    out_dir: pathlib.Path,\n    fname: str | None = None,\n)\n</code></pre> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>def _export_vector(tile: xr.Dataset, name: str, out_dir: Path, fname: str | None = None):\n    if fname is None:\n        fname = name\n    fpath_gpkg = out_dir / f\"{fname}.gpkg\"\n    fpath_parquet = out_dir / f\"{fname}.parquet\"\n    with stopwatch(f\"Exporting {name} to {fpath_gpkg.resolve()} and {fpath_parquet.resolve()}\", logger.debug):\n        polygon_gdf = vectorization.vectorize(tile, name)\n        polygon_gdf.to_file(fpath_gpkg, layer=f\"{fname}\")\n        polygon_gdf.to_parquet(fpath_parquet)\n</code></pre>"},{"location":"reference/darts_export/export/#darts_export.export.export_tile","title":"export_tile","text":"<pre><code>export_tile(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n)\n</code></pre> <p>Export a tile to a file.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The tile to export.</p> </li> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path where to export to.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the band is not found in the tile.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>@stopwatch.f(\"Exporting tile\", printer=logger.debug, print_kwargs=[\"bands\", \"ensemble_subsets\"])\ndef export_tile(  # noqa: C901\n    tile: xr.Dataset,\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n):\n    \"\"\"Export a tile to a file.\n\n    Args:\n        tile (xr.Dataset): The tile to export.\n        out_dir (Path): The path where to export to.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Raises:\n        ValueError: If the band is not found in the tile.\n\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                _export_polygonized(tile, out_dir, ensemble_subsets)\n            case \"binarized\":\n                _export_binarized(tile, out_dir, ensemble_subsets)\n            case \"probabilities\":\n                _export_probabilities(tile, out_dir, ensemble_subsets)\n            case \"extent\":\n                _export_vector(tile, \"extent\", out_dir, fname=\"prediction_extent\")\n            case \"thumbnail\":\n                _export_thumbnail(tile, out_dir)\n            case \"optical\":\n                _export_raster(tile, [\"red\", \"green\", \"blue\", \"nir\"], out_dir, fname=\"optical\")\n            case \"dem\":\n                _export_raster(tile, [\"slope\", \"relative_elevation\"], out_dir, fname=\"dem\")\n            case \"tcvis\":\n                _export_raster(tile, [\"tc_brightness\", \"tc_greenness\", \"tc_wetness\"], out_dir, fname=\"tcvis\")\n            case _:\n                if band not in tile.data_vars:\n                    raise ValueError(\n                        f\"Band {band} not found in tile for export. Available bands are: {list(tile.data_vars.keys())}\"\n                    )\n                # Export the band as a raster\n                _export_raster(tile, band, out_dir)\n</code></pre>"},{"location":"reference/darts_export/miniviz/","title":"darts_export.miniviz","text":""},{"location":"reference/darts_export/miniviz/#darts_export.miniviz","title":"darts_export.miniviz","text":"<p>Small visuals previews for the output.</p>"},{"location":"reference/darts_export/miniviz/#darts_export.miniviz.thumbnail","title":"thumbnail","text":"<pre><code>thumbnail(tile: xarray.Dataset) -&gt; matplotlib.pyplot.Figure\n</code></pre> <p>Create a thumbnail of the tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The tile to create a thumbnail from.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>matplotlib.pyplot.Figure</code>           \u2013            <p>plt.Figure: The figure with the thumbnail.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/miniviz.py</code> <pre><code>def thumbnail(tile: xr.Dataset) -&gt; plt.Figure:\n    \"\"\"Create a thumbnail of the tile.\n\n    Args:\n        tile (xr.Dataset): The tile to create a thumbnail from.\n\n    Returns:\n        plt.Figure: The figure with the thumbnail.\n\n    \"\"\"\n    prev_res = 512  # Prefered resolution for the thumbnail, will not exactly match\n    orgi_res = max(tile.sizes.values())\n    factor = int(orgi_res / prev_res)\n    tile_lowres = tile.odc.reproject(tile.odc.geobox.zoom_out(factor))\n\n    tile_id = tile.attrs.get(\"s2_id\", \"unknown\")\n\n    # Add some statistics\n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    ax.set_title(f\"Tile {tile_id} (lowres) [epsg:{tile.odc.crs.epsg}]\")\n    rgba = tile_lowres.odc.to_rgba(bands=[\"red\", \"green\", \"blue\"], vmin=0, vmax=0.2)\n    rgba.plot.imshow(ax=ax)\n\n    # Prediction boundaries\n    tile_lowres.probabilities.where(tile_lowres.probabilities != 255).plot.contour(ax=ax, levels=[50])\n    # Validitity mask\n    (tile_lowres.probabilities == 255).plot.contour(ax=ax, levels=[0.5], colors=\"r\", alpha=0.5)\n    return fig\n</code></pre>"},{"location":"reference/darts_export/vectorization/","title":"darts_export.vectorization","text":""},{"location":"reference/darts_export/vectorization/#darts_export.vectorization","title":"darts_export.vectorization","text":"<p>Module for various tasks during export.</p>"},{"location":"reference/darts_export/vectorization/#darts_export.vectorization.gdal_polygonization","title":"gdal_polygonization","text":"<pre><code>gdal_polygonization(\n    labels: numpy.ndarray,\n    rio_georef: xarray.Dataset,\n    as_gdf=True,\n    gpkg_path: pathlib.Path | None = None,\n) -&gt; typing.Union[gdal.ogr.Layer, geopandas.GeoDataFrame]\n</code></pre> <p>Polygonize a numpy array using GDAL.</p> <p>Detects regions with the same value in the numpy array and converts those into polygons. Can return the initial gdal result as ogr.Dataset or converts into a geopandas dataframe if as_gdf is enabled. If <code>gpkg_path</code> is set, the polsgonization result will be written one go into a GeoPackage file, otherwise the OGR dataset will reside purely in memory.</p> <p>Parameters:</p> <ul> <li> <code>labels</code>               (<code>numpy.ndarray</code>)           \u2013            <p>The input dataset as a ndarray with values designating labels</p> </li> <li> <code>rio_georef</code>               (<code>xarray.Dataset</code>)           \u2013            <p>an xarray with the rioxarray accessor to determine the CRS of the dataset</p> </li> <li> <code>as_gdf</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>returns result as geopandas.GeoDataFrame. Defaults to True.</p> </li> <li> <code>gpkg_path</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path where a GPKG file is written backing the OGR dataset. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Union[gdal.ogr.Layer, geopandas.GeoDataFrame]</code>           \u2013            <p>ogr.Layer | gpd.GeoDataFrame: the polyginization result</p> </li> </ul> Source code in <code>darts-export/src/darts_export/vectorization.py</code> <pre><code>def gdal_polygonization(\n    labels: np.ndarray, rio_georef: xarray.Dataset, as_gdf=True, gpkg_path: Path | None = None\n) -&gt; Union[\"gdal.ogr.Layer\", gpd.GeoDataFrame]:  # type: ignore # noqa: F821\n    \"\"\"Polygonize a numpy array using GDAL.\n\n    Detects regions with the same value in the numpy array and converts those\n    into polygons. Can return the initial gdal result as ogr.Dataset or converts into a geopandas dataframe if\n    as_gdf is enabled. If `gpkg_path` is set, the polsgonization result will be written one go\n    into a GeoPackage file, otherwise the OGR dataset will reside purely in memory.\n\n    Args:\n        labels (np.ndarray): The input dataset as a ndarray with values designating labels\n        rio_georef (xarray.Dataset): an xarray with the rioxarray accessor to determine the CRS of the dataset\n        as_gdf (bool, optional): returns result as geopandas.GeoDataFrame. Defaults to True.\n        gpkg_path (Path | None, optional): Path where a GPKG file is written backing the OGR dataset. Defaults to None.\n\n    Returns:\n        ogr.Layer | gpd.GeoDataFrame: the polyginization result\n\n    \"\"\"\n    from osgeo import gdal, ogr\n\n    from darts_export import conversion\n\n    gdal.UseExceptions()\n\n    # convert to a GDAL dataset\n    dta = conversion.numpy_to_gdal(labels, rio_georef)\n\n    # prepare the vector output datasets to write to\n    if gpkg_path is not None:\n        gpkg_path = Path(gpkg_path)\n        ds = ogr.GetDriverByName(\"GPKG\").CreateDataSource(gpkg_path)\n        ogr_layer = ds.CreateLayer(gpkg_path.stem, geom_type=ogr.wkbPolygon, srs=dta.GetSpatialRef())\n    else:\n        # work only in memory\n        ds = ogr.GetDriverByName(\"Memory\").CreateDataSource(\"gdal_polygonization\")\n        ogr_layer = ds.CreateLayer(\"gdal_polygonization\", geom_type=ogr.wkbPolygon, srs=dta.GetSpatialRef())\n\n    # add the field where to store region ID\n    field = ogr.FieldDefn(\"Region_ID\", ogr.OFTInteger)\n    ogr_layer.CreateField(field)\n\n    # do the polygonization\n    gdal.Polygonize(\n        dta.GetRasterBand(1),\n        None,  # no masking, polygonize everything\n        ogr_layer,  # where to write the vector data to\n        0,  # write the polygonization threshold in the first attribute (\"DN\")\n    )\n    # the region with the ID zero is the region unlabelled by measure label\n    # remove features polygonized from that region, that is all features where DN is not 1\n    ogr_layer.SetAttributeFilter(\"Region_ID = 0\")\n    for feature in ogr_layer:\n        feature_id = feature.GetFID()\n        ogr_layer.DeleteFeature(feature_id)\n    ogr_layer.SetAttributeFilter(None)\n\n    if not as_gdf:\n        return ogr_layer\n\n    # convert the gdal vector object zo a geopandas gdf\n    gdf_polygons = conversion.ogrlyr_to_geopandas(ogr_layer)\n    gdf_polygons.set_crs(rio_georef.rio.crs, inplace=True)\n    return gdf_polygons\n</code></pre>"},{"location":"reference/darts_export/vectorization/#darts_export.vectorization.rasterio_polygonization","title":"rasterio_polygonization","text":"<pre><code>rasterio_polygonization(\n    labels: numpy.ndarray, rio_georef: xarray.Dataset\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Polygonize a numpy array with rasterio.</p> <p>Detects regions with the same value in the numpy array and converts those into polygons. The <code>rio_georef</code> agrument determines the final CRS of the returned geopandas GeoDataFrame.</p> <p>Parameters:</p> <ul> <li> <code>labels</code>               (<code>numpy.ndarray</code>)           \u2013            <p>the array of regionalizable labels</p> </li> <li> <code>rio_georef</code>               (<code>xarray.Dataset</code>)           \u2013            <p>the CRS as an xarray/rioxarray Dataset with rio accessor</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>geopandas.GeoDataFrame</code>           \u2013            <p>geopandas.GeoDataFrame: the resolut of the polygonization</p> </li> </ul> Source code in <code>darts-export/src/darts_export/vectorization.py</code> <pre><code>def rasterio_polygonization(labels: np.ndarray, rio_georef: xarray.Dataset) -&gt; gpd.GeoDataFrame:\n    \"\"\"Polygonize a numpy array with rasterio.\n\n    Detects regions with the same value in the numpy array and converts those\n    into polygons. The `rio_georef` agrument determines the final CRS of\n    the returned geopandas GeoDataFrame.\n\n    Args:\n        labels (np.ndarray): the array of regionalizable labels\n        rio_georef (xarray.Dataset): the CRS as an xarray/rioxarray Dataset with rio accessor\n\n    Returns:\n        geopandas.GeoDataFrame: the resolut of the polygonization\n\n    \"\"\"\n    # shapes() needs int32 data, while scikit labels puts out int64\n    # cast with astype()\n    gdf = (\n        gpd.GeoDataFrame(\n            [\n                (shapely.geometry.shape(geom), int(region_Id))\n                for geom, region_Id in shapes(labels.astype(np.int32), transform=rio_georef.rio.transform())\n            ],\n            columns=[\"geometry\", \"Region_ID\"],\n        )\n        .set_crs(rio_georef.rio.crs)\n        .query(\"Region_ID &gt; 0\")\n    )\n    return gdf\n</code></pre>"},{"location":"reference/darts_export/vectorization/#darts_export.vectorization.vectorize","title":"vectorize","text":"<pre><code>vectorize(\n    xdat: xarray.Dataset,\n    layername: str = \"binarized_segmentation\",\n    polygonization_func: str = \"rasterio\",\n) -&gt; geopandas.GeoDataFrame\n</code></pre> <p>Vectorize an inference result dataset.</p> <p>Detects connected regions in the with the same value <code>binarized_segmentation</code> layer, polygonizes this into a vector dataset. Additionally this function writes zonal statistics of the <code>probabilities</code> layer to the polygon attributes.</p> <p>Parameters:</p> <ul> <li> <code>xdat</code>               (<code>xarray.Dataset</code>)           \u2013            <p>the input dataset augmented with the rioxarray <code>rio</code> accessor</p> </li> <li> <code>layername</code>               (<code>str</code>, default:                   <code>'binarized_segmentation'</code> )           \u2013            <p>the name of the layer in <code>xdat</code> to polygonize</p> </li> <li> <code>polygonization_func</code>               (<code>str</code>, default:                   <code>'rasterio'</code> )           \u2013            <p>the method to utilize for polygonization, either 'gdal' or 'rasterio', the default.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>_type_</code> (              <code>geopandas.GeoDataFrame</code> )          \u2013            <p>description</p> </li> </ul> Source code in <code>darts-export/src/darts_export/vectorization.py</code> <pre><code>def vectorize(\n    xdat: xarray.Dataset,\n    layername: str = \"binarized_segmentation\",\n    polygonization_func: str = \"rasterio\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Vectorize an inference result dataset.\n\n    Detects connected regions in the with the same value `binarized_segmentation` layer, polygonizes\n    this into a vector dataset.\n    Additionally this function writes zonal statistics of the `probabilities` layer to the polygon attributes.\n\n    Args:\n        xdat (xarray.Dataset): the input dataset augmented with the rioxarray `rio` accessor\n        layername (str, optional): the name of the layer in `xdat` to polygonize\n        polygonization_func (str, optional): the method to utilize for polygonization, either 'gdal' or 'rasterio',\n            the default.\n\n    Returns:\n        _type_: _description_\n\n    \"\"\"\n    layer = xdat[layername]\n\n    # Turn layer into int8 if bool\n    if layer.dtype == \"bool\":\n        layer = layer.astype(\"uint8\")\n\n    bin_labelled = measure.label(layer)\n\n    if polygonization_func.lower() == \"gdal\":\n        gdf_polygons = gdal_polygonization(bin_labelled, layer)\n    else:\n        gdf_polygons = rasterio_polygonization(bin_labelled, layer)\n\n    # execute the zonal stats:\n    # arguments must be in the specified order, matching regionprops\n    def median_intensity(region, intensities):\n        # note the ddof arg to get the sample var if you so desire!\n        return np.median(intensities[region])\n\n    region_stats = measure.regionprops(\n        bin_labelled, intensity_image=xdat.probabilities.values, extra_properties=[median_intensity]\n    )\n\n    # collect stats data:\n    stats_dict = {}\n    for region in region_stats:\n        stats_dict[region.label] = {\n            \"min\": int(region.min_intensity),\n            \"max\": int(region.max_intensity),\n            \"mean\": region.mean_intensity,\n            \"median\": region.median_intensity,\n            \"std\": region.intensity_std,\n            \"npixel\": region.num_pixels,\n        }\n\n    # add the zonal stats to the GeoPandas DataFrame\n    stats_df = gpd.pd.DataFrame.from_dict(stats_dict, orient=\"index\")\n    return gdf_polygons.merge(stats_df, left_on=\"Region_ID\", right_index=True)\n</code></pre>"},{"location":"reference/darts_preprocessing/","title":"darts_preprocessing","text":""},{"location":"reference/darts_preprocessing/#darts_preprocessing","title":"darts_preprocessing","text":"<p>Data preprocessing and feature engineering for the DARTS dataset.</p>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_aspect","title":"calculate_aspect","text":"<pre><code>calculate_aspect(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the aspect of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated aspect added as a new variable 'aspect'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating aspect\", printer=logger.debug)\ndef calculate_aspect(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the aspect of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated aspect added as a new variable 'aspect'.\n\n    \"\"\"\n    aspect_deg = aspect(arcticdem_ds.dem)\n    aspect_deg.attrs = {\n        \"long_name\": \"Aspect\",\n        \"units\": \"degrees\",\n        \"description\": \"The compass direction that the slope faces, in degrees clockwise from north.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"aspect\"] = aspect_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_curvature","title":"calculate_curvature","text":"<pre><code>calculate_curvature(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the curvature of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated curvature added as a new variable 'curvature'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating curvature\", printer=logger.debug)\ndef calculate_curvature(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the curvature of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated curvature added as a new variable 'curvature'.\n\n    \"\"\"\n    curvature_da = curvature(arcticdem_ds.dem)\n    curvature_da.attrs = {\n        \"long_name\": \"Curvature\",\n        \"units\": \"\",\n        \"description\": \"The curvature of the terrain surface.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"curvature\"] = curvature_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_hillshade","title":"calculate_hillshade","text":"<pre><code>calculate_hillshade(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the hillshade of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated slhillshadeope added as a new variable 'hillshade'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating hillshade\", printer=logger.debug)\ndef calculate_hillshade(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the hillshade of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated slhillshadeope added as a new variable 'hillshade'.\n\n    \"\"\"\n    hillshade_da = hillshade(arcticdem_ds.dem)\n    hillshade_da.attrs = {\n        \"long_name\": \"Hillshade\",\n        \"units\": \"\",\n        \"description\": \"The hillshade based on azimuth 255 and angle_altitude 25.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"hillshade\"] = hillshade_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_ndvi","title":"calculate_ndvi","text":"<pre><code>calculate_ndvi(\n    planet_scene_dataset: xarray.Dataset,\n    nir_band: str = \"nir\",\n    red_band: str = \"red\",\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate NDVI from an xarray Dataset containing spectral bands.</p> Example <pre><code>ndvi_data = calculate_ndvi(planet_scene_dataset)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>planet_scene_dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The xarray Dataset containing the spectral bands, where the bands are indexed along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.</p> </li> <li> <code>nir_band</code>               (<code>str</code>, default:                   <code>'nir'</code> )           \u2013            <p>The name of the NIR band in the Dataset (default is \"nir\"). This name should correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".</p> </li> <li> <code>red_band</code>               (<code>str</code>, default:                   <code>'red'</code> )           \u2013            <p>The name of the Red band in the Dataset (default is \"red\"). This name should correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".</p> </li> </ul> Notes <p>NDVI (Normalized Difference Vegetation Index) is calculated using the formula:     NDVI = (NIR - Red) / (NIR + Red)</p> <p>This index is commonly used in remote sensing to assess vegetation health and density.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch.f(\"Calculating NDVI\", printer=logger.debug, print_kwargs=[\"nir_band\", \"red_band\"])\ndef calculate_ndvi(planet_scene_dataset: xr.Dataset, nir_band: str = \"nir\", red_band: str = \"red\") -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI from an xarray Dataset containing spectral bands.\n\n    Example:\n        ```python\n        ndvi_data = calculate_ndvi(planet_scene_dataset)\n        ```\n\n    Args:\n        planet_scene_dataset (xr.Dataset): The xarray Dataset containing the spectral bands, where the bands are indexed\n            along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.\n        nir_band (str, optional): The name of the NIR band in the Dataset (default is \"nir\"). This name should\n            correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".\n        red_band (str, optional): The name of the Red band in the Dataset (default is \"red\"). This name should\n            correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".\n\n    Returns:\n        xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have\n            dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".\n\n\n    Notes:\n        NDVI (Normalized Difference Vegetation Index) is calculated using the formula:\n            NDVI = (NIR - Red) / (NIR + Red)\n\n        This index is commonly used in remote sensing to assess vegetation health and density.\n\n    \"\"\"\n    # Calculate NDVI using the formula\n    nir = planet_scene_dataset[nir_band].astype(\"float32\")\n    r = planet_scene_dataset[red_band].astype(\"float32\")\n    ndvi = (nir - r) / (nir + r)\n\n    # Scale to 0 - 20000 (for later conversion to uint16)\n    ndvi = (ndvi.clip(-1, 1) + 1) * 1e4\n    # Make nan to 0\n    ndvi = ndvi.fillna(0).rio.write_nodata(0)\n    # Convert to uint16\n    ndvi = ndvi.astype(\"uint16\")\n\n    ndvi = ndvi.assign_attrs({\"data_source\": \"planet\", \"long_name\": \"NDVI\"}).to_dataset(name=\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_slope","title":"calculate_slope","text":"<pre><code>calculate_slope(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the slope of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_topographic_position_index","title":"calculate_topographic_position_index","text":"<pre><code>calculate_topographic_position_index(\n    arcticdem_ds: xarray.Dataset,\n    outer_radius: int,\n    inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel in m.</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in m.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n        outer_radius (int, optional): The outer radius of the annulus kernel in m.\n        inner_radius (int, optional): The inner radius of the annulus kernel in m.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius_m = f\"{outer_radius}m\"\n    outer_radius_px = f\"{ceil(outer_radius / cellsize_x)}px\"\n    if inner_radius &gt; 0:\n        inner_radius_m = f\"{inner_radius}m\"\n        inner_radius_px = f\"{ceil(inner_radius / cellsize_x)}px\"\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius_m, inner_radius_m)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius_px}-{outer_radius_px} cells \"\n            f\"({inner_radius_m}-{outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of \"\n            f\"{inner_radius_px}-{outer_radius_px} ({inner_radius_m}-{outer_radius_m}) cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius_m)\n        attr_cell_description = (\n            f\"within a circle at a distance of {outer_radius_px} cells ({outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with circle kernel of {outer_radius_px} ({outer_radius_m}) cells.\"\n        )\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.preprocess_legacy_fast","title":"preprocess_legacy_fast","text":"<pre><code>preprocess_legacy_fast(\n    ds_merged: xarray.Dataset,\n    ds_arcticdem: xarray.Dataset,\n    ds_tcvis: xarray.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_preprocessing.legacy.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>The main difference to preprocess_legacy is the new data concept of the arcticdem. Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw arcticdem data and calculates slope and relative elevation on the fly.</p> <p>Parameters:</p> <ul> <li> <code>ds_merged</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The TCVIS dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_preprocessing.legacy.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/legacy.py</code> <pre><code>@stopwatch(\"Preprocessing\", printer=logger.debug)\ndef preprocess_legacy_fast(\n    ds_merged: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    The main difference to preprocess_legacy is the new data concept of the arcticdem.\n    Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw\n    arcticdem data and calculates slope and relative elevation on the fly.\n\n    Args:\n        ds_merged (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Calculate NDVI\n    ds_merged[\"ndvi\"] = calculate_ndvi(ds_merged).ndvi\n\n    # Reproject TCVIS to optical data\n    with stopwatch(\"Reprojecting TCVIS\", printer=logger.debug):\n        ds_tcvis = ds_tcvis.odc.reproject(ds_merged.odc.geobox, resampling=\"cubic\")\n\n    ds_merged[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_merged[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_merged[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    with stopwatch(\"Reprojecting ArcticDEM\", printer=logger.debug):\n        ds_arcticdem = ds_arcticdem.odc.reproject(ds_merged.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n\n    ds_arcticdem = preprocess_legacy_arcticdem_fast(ds_arcticdem, tpi_outer_radius, tpi_inner_radius, device)\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_merged.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_merged)\n\n    ds_merged[\"dem\"] = ds_arcticdem.dem\n    ds_merged[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_merged[\"slope\"] = ds_arcticdem.slope\n    ds_merged[\"arcticdem_data_mask\"] = ds_arcticdem.datamask\n\n    # Update datamask with arcticdem mask\n    # with xr.set_options(keep_attrs=True):\n    #     ds_merged[\"quality_data_mask\"] = ds_merged.quality_data_mask * ds_arcticdem.datamask\n    # ds_merged.quality_data_mask.attrs[\"data_source\"] += \" + ArcticDEM\"\n\n    return ds_merged\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.preprocess_v2","title":"preprocess_v2","text":"<pre><code>preprocess_v2(\n    ds_merged: xarray.Dataset,\n    ds_arcticdem: xarray.Dataset,\n    ds_tcvis: xarray.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_preprocessing.v2.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess optical data with modern (DARTS v2) preprocessing steps.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope, hillshade, aspect, curvature and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>Parameters:</p> <ul> <li> <code>ds_merged</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The TCVIS dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_preprocessing.v2.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/v2.py</code> <pre><code>@stopwatch(\"Preprocessing\", printer=logger.debug)\ndef preprocess_v2(\n    ds_merged: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with modern (DARTS v2) preprocessing steps.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope, hillshade, aspect, curvature and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    Args:\n        ds_merged (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Calculate NDVI\n    ds_merged[\"ndvi\"] = calculate_ndvi(ds_merged).ndvi\n\n    # Reproject TCVIS to optical data\n    with stopwatch(\"Reprojecting TCVIS\", printer=logger.debug):\n        ds_tcvis = ds_tcvis.odc.reproject(ds_merged.odc.geobox, resampling=\"cubic\")\n\n    ds_merged[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_merged[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_merged[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    with stopwatch(\"Reprojecting ArcticDEM\", printer=logger.debug):\n        ds_arcticdem = ds_arcticdem.odc.reproject(ds_merged.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n\n    ds_arcticdem = preprocess_arcticdem(ds_arcticdem, tpi_outer_radius, tpi_inner_radius, device)\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_merged.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_merged)\n\n    ds_merged[\"dem\"] = ds_arcticdem.dem\n    ds_merged[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_merged[\"slope\"] = ds_arcticdem.slope\n    ds_merged[\"hillshade\"] = ds_arcticdem.hillshade\n    ds_merged[\"aspect\"] = ds_arcticdem.aspect\n    ds_merged[\"curvature\"] = ds_arcticdem.curvature\n    ds_merged[\"arcticdem_data_mask\"] = ds_arcticdem.datamask\n\n    # Update datamask with arcticdem mask\n    # with xr.set_options(keep_attrs=True):\n    #     ds_merged[\"quality_data_mask\"] = ds_merged.quality_data_mask * ds_arcticdem.datamask\n    # ds_merged.quality_data_mask.attrs[\"data_source\"] += \" + ArcticDEM\"\n\n    return ds_merged\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/","title":"darts_preprocessing.engineering","text":""},{"location":"reference/darts_preprocessing/engineering/#darts_preprocessing.engineering","title":"darts_preprocessing.engineering","text":"<p>Engineered (Pre-Calculated) features.</p>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/","title":"darts_preprocessing.engineering.arcticdem","text":""},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem","title":"darts_preprocessing.engineering.arcticdem","text":"<p>Computation of ArcticDEM derived products.</p>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_aspect","title":"calculate_aspect","text":"<pre><code>calculate_aspect(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the aspect of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated aspect added as a new variable 'aspect'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating aspect\", printer=logger.debug)\ndef calculate_aspect(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the aspect of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated aspect added as a new variable 'aspect'.\n\n    \"\"\"\n    aspect_deg = aspect(arcticdem_ds.dem)\n    aspect_deg.attrs = {\n        \"long_name\": \"Aspect\",\n        \"units\": \"degrees\",\n        \"description\": \"The compass direction that the slope faces, in degrees clockwise from north.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"aspect\"] = aspect_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_curvature","title":"calculate_curvature","text":"<pre><code>calculate_curvature(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the curvature of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated curvature added as a new variable 'curvature'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating curvature\", printer=logger.debug)\ndef calculate_curvature(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the curvature of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated curvature added as a new variable 'curvature'.\n\n    \"\"\"\n    curvature_da = curvature(arcticdem_ds.dem)\n    curvature_da.attrs = {\n        \"long_name\": \"Curvature\",\n        \"units\": \"\",\n        \"description\": \"The curvature of the terrain surface.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"curvature\"] = curvature_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_hillshade","title":"calculate_hillshade","text":"<pre><code>calculate_hillshade(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the hillshade of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated slhillshadeope added as a new variable 'hillshade'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating hillshade\", printer=logger.debug)\ndef calculate_hillshade(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the hillshade of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated slhillshadeope added as a new variable 'hillshade'.\n\n    \"\"\"\n    hillshade_da = hillshade(arcticdem_ds.dem)\n    hillshade_da.attrs = {\n        \"long_name\": \"Hillshade\",\n        \"units\": \"\",\n        \"description\": \"The hillshade based on azimuth 255 and angle_altitude 25.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"hillshade\"] = hillshade_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_slope","title":"calculate_slope","text":"<pre><code>calculate_slope(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the slope of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/arcticdem/#darts_preprocessing.engineering.arcticdem.calculate_topographic_position_index","title":"calculate_topographic_position_index","text":"<pre><code>calculate_topographic_position_index(\n    arcticdem_ds: xarray.Dataset,\n    outer_radius: int,\n    inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel in m.</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in m.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n        outer_radius (int, optional): The outer radius of the annulus kernel in m.\n        inner_radius (int, optional): The inner radius of the annulus kernel in m.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius_m = f\"{outer_radius}m\"\n    outer_radius_px = f\"{ceil(outer_radius / cellsize_x)}px\"\n    if inner_radius &gt; 0:\n        inner_radius_m = f\"{inner_radius}m\"\n        inner_radius_px = f\"{ceil(inner_radius / cellsize_x)}px\"\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius_m, inner_radius_m)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius_px}-{outer_radius_px} cells \"\n            f\"({inner_radius_m}-{outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of \"\n            f\"{inner_radius_px}-{outer_radius_px} ({inner_radius_m}-{outer_radius_m}) cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius_m)\n        attr_cell_description = (\n            f\"within a circle at a distance of {outer_radius_px} cells ({outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with circle kernel of {outer_radius_px} ({outer_radius_m}) cells.\"\n        )\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/","title":"darts_preprocessing.engineering.indices","text":""},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices","title":"darts_preprocessing.engineering.indices","text":"<p>Calculation of spectral indices from optical data.</p>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_preprocessing/engineering/indices/#darts_preprocessing.engineering.indices.calculate_ndvi","title":"calculate_ndvi","text":"<pre><code>calculate_ndvi(\n    planet_scene_dataset: xarray.Dataset,\n    nir_band: str = \"nir\",\n    red_band: str = \"red\",\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate NDVI from an xarray Dataset containing spectral bands.</p> Example <pre><code>ndvi_data = calculate_ndvi(planet_scene_dataset)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>planet_scene_dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The xarray Dataset containing the spectral bands, where the bands are indexed along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.</p> </li> <li> <code>nir_band</code>               (<code>str</code>, default:                   <code>'nir'</code> )           \u2013            <p>The name of the NIR band in the Dataset (default is \"nir\"). This name should correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".</p> </li> <li> <code>red_band</code>               (<code>str</code>, default:                   <code>'red'</code> )           \u2013            <p>The name of the Red band in the Dataset (default is \"red\"). This name should correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".</p> </li> </ul> Notes <p>NDVI (Normalized Difference Vegetation Index) is calculated using the formula:     NDVI = (NIR - Red) / (NIR + Red)</p> <p>This index is commonly used in remote sensing to assess vegetation health and density.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch.f(\"Calculating NDVI\", printer=logger.debug, print_kwargs=[\"nir_band\", \"red_band\"])\ndef calculate_ndvi(planet_scene_dataset: xr.Dataset, nir_band: str = \"nir\", red_band: str = \"red\") -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI from an xarray Dataset containing spectral bands.\n\n    Example:\n        ```python\n        ndvi_data = calculate_ndvi(planet_scene_dataset)\n        ```\n\n    Args:\n        planet_scene_dataset (xr.Dataset): The xarray Dataset containing the spectral bands, where the bands are indexed\n            along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.\n        nir_band (str, optional): The name of the NIR band in the Dataset (default is \"nir\"). This name should\n            correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".\n        red_band (str, optional): The name of the Red band in the Dataset (default is \"red\"). This name should\n            correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".\n\n    Returns:\n        xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have\n            dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".\n\n\n    Notes:\n        NDVI (Normalized Difference Vegetation Index) is calculated using the formula:\n            NDVI = (NIR - Red) / (NIR + Red)\n\n        This index is commonly used in remote sensing to assess vegetation health and density.\n\n    \"\"\"\n    # Calculate NDVI using the formula\n    nir = planet_scene_dataset[nir_band].astype(\"float32\")\n    r = planet_scene_dataset[red_band].astype(\"float32\")\n    ndvi = (nir - r) / (nir + r)\n\n    # Scale to 0 - 20000 (for later conversion to uint16)\n    ndvi = (ndvi.clip(-1, 1) + 1) * 1e4\n    # Make nan to 0\n    ndvi = ndvi.fillna(0).rio.write_nodata(0)\n    # Convert to uint16\n    ndvi = ndvi.astype(\"uint16\")\n\n    ndvi = ndvi.assign_attrs({\"data_source\": \"planet\", \"long_name\": \"NDVI\"}).to_dataset(name=\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/","title":"darts_preprocessing.legacy","text":""},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy","title":"darts_preprocessing.legacy","text":"<p>PLANET scene based preprocessing.</p>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.DEFAULT_DEVICE","title":"DEFAULT_DEVICE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DEVICE = 'cuda'\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.calculate_ndvi","title":"calculate_ndvi","text":"<pre><code>calculate_ndvi(\n    planet_scene_dataset: xarray.Dataset,\n    nir_band: str = \"nir\",\n    red_band: str = \"red\",\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate NDVI from an xarray Dataset containing spectral bands.</p> Example <pre><code>ndvi_data = calculate_ndvi(planet_scene_dataset)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>planet_scene_dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The xarray Dataset containing the spectral bands, where the bands are indexed along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.</p> </li> <li> <code>nir_band</code>               (<code>str</code>, default:                   <code>'nir'</code> )           \u2013            <p>The name of the NIR band in the Dataset (default is \"nir\"). This name should correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".</p> </li> <li> <code>red_band</code>               (<code>str</code>, default:                   <code>'red'</code> )           \u2013            <p>The name of the Red band in the Dataset (default is \"red\"). This name should correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".</p> </li> </ul> Notes <p>NDVI (Normalized Difference Vegetation Index) is calculated using the formula:     NDVI = (NIR - Red) / (NIR + Red)</p> <p>This index is commonly used in remote sensing to assess vegetation health and density.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch.f(\"Calculating NDVI\", printer=logger.debug, print_kwargs=[\"nir_band\", \"red_band\"])\ndef calculate_ndvi(planet_scene_dataset: xr.Dataset, nir_band: str = \"nir\", red_band: str = \"red\") -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI from an xarray Dataset containing spectral bands.\n\n    Example:\n        ```python\n        ndvi_data = calculate_ndvi(planet_scene_dataset)\n        ```\n\n    Args:\n        planet_scene_dataset (xr.Dataset): The xarray Dataset containing the spectral bands, where the bands are indexed\n            along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.\n        nir_band (str, optional): The name of the NIR band in the Dataset (default is \"nir\"). This name should\n            correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".\n        red_band (str, optional): The name of the Red band in the Dataset (default is \"red\"). This name should\n            correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".\n\n    Returns:\n        xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have\n            dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".\n\n\n    Notes:\n        NDVI (Normalized Difference Vegetation Index) is calculated using the formula:\n            NDVI = (NIR - Red) / (NIR + Red)\n\n        This index is commonly used in remote sensing to assess vegetation health and density.\n\n    \"\"\"\n    # Calculate NDVI using the formula\n    nir = planet_scene_dataset[nir_band].astype(\"float32\")\n    r = planet_scene_dataset[red_band].astype(\"float32\")\n    ndvi = (nir - r) / (nir + r)\n\n    # Scale to 0 - 20000 (for later conversion to uint16)\n    ndvi = (ndvi.clip(-1, 1) + 1) * 1e4\n    # Make nan to 0\n    ndvi = ndvi.fillna(0).rio.write_nodata(0)\n    # Convert to uint16\n    ndvi = ndvi.astype(\"uint16\")\n\n    ndvi = ndvi.assign_attrs({\"data_source\": \"planet\", \"long_name\": \"NDVI\"}).to_dataset(name=\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.calculate_slope","title":"calculate_slope","text":"<pre><code>calculate_slope(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the slope of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.calculate_topographic_position_index","title":"calculate_topographic_position_index","text":"<pre><code>calculate_topographic_position_index(\n    arcticdem_ds: xarray.Dataset,\n    outer_radius: int,\n    inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel in m.</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in m.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n        outer_radius (int, optional): The outer radius of the annulus kernel in m.\n        inner_radius (int, optional): The inner radius of the annulus kernel in m.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius_m = f\"{outer_radius}m\"\n    outer_radius_px = f\"{ceil(outer_radius / cellsize_x)}px\"\n    if inner_radius &gt; 0:\n        inner_radius_m = f\"{inner_radius}m\"\n        inner_radius_px = f\"{ceil(inner_radius / cellsize_x)}px\"\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius_m, inner_radius_m)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius_px}-{outer_radius_px} cells \"\n            f\"({inner_radius_m}-{outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of \"\n            f\"{inner_radius_px}-{outer_radius_px} ({inner_radius_m}-{outer_radius_m}) cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius_m)\n        attr_cell_description = (\n            f\"within a circle at a distance of {outer_radius_px} cells ({outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with circle kernel of {outer_radius_px} ({outer_radius_m}) cells.\"\n        )\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.preprocess_legacy_arcticdem_fast","title":"preprocess_legacy_arcticdem_fast","text":"<pre><code>preprocess_legacy_arcticdem_fast(\n    ds_arcticdem: xarray.Dataset,\n    tpi_outer_radius: int,\n    tpi_inner_radius: int,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess the ArcticDEM data with legacy (DARTS v1) preprocessing steps.</p> <p>Parameters:</p> <ul> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in number of cells.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in number of cells.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed ArcticDEM dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/legacy.py</code> <pre><code>@stopwatch.f(\"Preprocessing arcticdem\", printer=logger.debug, print_kwargs=[\"tpi_outer_radius\", \"tpi_inner_radius\"])\ndef preprocess_legacy_arcticdem_fast(\n    ds_arcticdem: xr.Dataset,\n    tpi_outer_radius: int,\n    tpi_inner_radius: int,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess the ArcticDEM data with legacy (DARTS v1) preprocessing steps.\n\n    Args:\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        tpi_outer_radius (int): The outer radius of the annulus kernel for the tpi calculation in number of cells.\n        tpi_inner_radius (int): The inner radius of the annulus kernel for the tpi calculation in number of cells.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n\n    Returns:\n        xr.Dataset: The preprocessed ArcticDEM dataset.\n\n    \"\"\"\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not has_cuda_and_cupy():\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Calculate TPI and slope from ArcticDEM on GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving arcticdem to GPU:{device}.\")\n        # Check if dem is dask, if not persist it, since tpi and slope can't be calculated from cupy-dask arrays\n        if ds_arcticdem.chunks is not None:\n            ds_arcticdem = ds_arcticdem.persist()\n        # Move and calculate on specified device\n        with cp.cuda.Device(device_nr):\n            ds_arcticdem = ds_arcticdem.cupy.as_cupy()\n            ds_arcticdem = calculate_topographic_position_index(ds_arcticdem, tpi_outer_radius, tpi_inner_radius)\n            ds_arcticdem = calculate_slope(ds_arcticdem)\n            ds_arcticdem = ds_arcticdem.cupy.as_numpy()\n            free_cupy()\n\n    # Calculate TPI and slope from ArcticDEM on CPU\n    else:\n        ds_arcticdem = calculate_topographic_position_index(ds_arcticdem, tpi_outer_radius, tpi_inner_radius)\n        ds_arcticdem = calculate_slope(ds_arcticdem)\n\n    # Apply legacy scaling to tpi\n    with xr.set_options(keep_attrs=True):\n        ds_arcticdem[\"tpi\"] = (ds_arcticdem.tpi + 50) * 300\n    return ds_arcticdem\n</code></pre>"},{"location":"reference/darts_preprocessing/legacy/#darts_preprocessing.legacy.preprocess_legacy_fast","title":"preprocess_legacy_fast","text":"<pre><code>preprocess_legacy_fast(\n    ds_merged: xarray.Dataset,\n    ds_arcticdem: xarray.Dataset,\n    ds_tcvis: xarray.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_preprocessing.legacy.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>The main difference to preprocess_legacy is the new data concept of the arcticdem. Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw arcticdem data and calculates slope and relative elevation on the fly.</p> <p>Parameters:</p> <ul> <li> <code>ds_merged</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The TCVIS dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_preprocessing.legacy.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/legacy.py</code> <pre><code>@stopwatch(\"Preprocessing\", printer=logger.debug)\ndef preprocess_legacy_fast(\n    ds_merged: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    The main difference to preprocess_legacy is the new data concept of the arcticdem.\n    Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw\n    arcticdem data and calculates slope and relative elevation on the fly.\n\n    Args:\n        ds_merged (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Calculate NDVI\n    ds_merged[\"ndvi\"] = calculate_ndvi(ds_merged).ndvi\n\n    # Reproject TCVIS to optical data\n    with stopwatch(\"Reprojecting TCVIS\", printer=logger.debug):\n        ds_tcvis = ds_tcvis.odc.reproject(ds_merged.odc.geobox, resampling=\"cubic\")\n\n    ds_merged[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_merged[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_merged[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    with stopwatch(\"Reprojecting ArcticDEM\", printer=logger.debug):\n        ds_arcticdem = ds_arcticdem.odc.reproject(ds_merged.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n\n    ds_arcticdem = preprocess_legacy_arcticdem_fast(ds_arcticdem, tpi_outer_radius, tpi_inner_radius, device)\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_merged.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_merged)\n\n    ds_merged[\"dem\"] = ds_arcticdem.dem\n    ds_merged[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_merged[\"slope\"] = ds_arcticdem.slope\n    ds_merged[\"arcticdem_data_mask\"] = ds_arcticdem.datamask\n\n    # Update datamask with arcticdem mask\n    # with xr.set_options(keep_attrs=True):\n    #     ds_merged[\"quality_data_mask\"] = ds_merged.quality_data_mask * ds_arcticdem.datamask\n    # ds_merged.quality_data_mask.attrs[\"data_source\"] += \" + ArcticDEM\"\n\n    return ds_merged\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/","title":"darts_preprocessing.v2","text":""},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2","title":"darts_preprocessing.v2","text":"<p>PLANET scene based preprocessing.</p>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.DEFAULT_DEVICE","title":"DEFAULT_DEVICE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DEVICE = 'cuda'\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_aspect","title":"calculate_aspect","text":"<pre><code>calculate_aspect(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the aspect of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated aspect added as a new variable 'aspect'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating aspect\", printer=logger.debug)\ndef calculate_aspect(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the aspect of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated aspect added as a new variable 'aspect'.\n\n    \"\"\"\n    aspect_deg = aspect(arcticdem_ds.dem)\n    aspect_deg.attrs = {\n        \"long_name\": \"Aspect\",\n        \"units\": \"degrees\",\n        \"description\": \"The compass direction that the slope faces, in degrees clockwise from north.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"aspect\"] = aspect_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_curvature","title":"calculate_curvature","text":"<pre><code>calculate_curvature(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the curvature of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated curvature added as a new variable 'curvature'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating curvature\", printer=logger.debug)\ndef calculate_curvature(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the curvature of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated curvature added as a new variable 'curvature'.\n\n    \"\"\"\n    curvature_da = curvature(arcticdem_ds.dem)\n    curvature_da.attrs = {\n        \"long_name\": \"Curvature\",\n        \"units\": \"\",\n        \"description\": \"The curvature of the terrain surface.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"curvature\"] = curvature_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_hillshade","title":"calculate_hillshade","text":"<pre><code>calculate_hillshade(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the hillshade of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated slhillshadeope added as a new variable 'hillshade'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating hillshade\", printer=logger.debug)\ndef calculate_hillshade(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the hillshade of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated slhillshadeope added as a new variable 'hillshade'.\n\n    \"\"\"\n    hillshade_da = hillshade(arcticdem_ds.dem)\n    hillshade_da.attrs = {\n        \"long_name\": \"Hillshade\",\n        \"units\": \"\",\n        \"description\": \"The hillshade based on azimuth 255 and angle_altitude 25.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"hillshade\"] = hillshade_da.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_ndvi","title":"calculate_ndvi","text":"<pre><code>calculate_ndvi(\n    planet_scene_dataset: xarray.Dataset,\n    nir_band: str = \"nir\",\n    red_band: str = \"red\",\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate NDVI from an xarray Dataset containing spectral bands.</p> Example <pre><code>ndvi_data = calculate_ndvi(planet_scene_dataset)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>planet_scene_dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The xarray Dataset containing the spectral bands, where the bands are indexed along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.</p> </li> <li> <code>nir_band</code>               (<code>str</code>, default:                   <code>'nir'</code> )           \u2013            <p>The name of the NIR band in the Dataset (default is \"nir\"). This name should correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".</p> </li> <li> <code>red_band</code>               (<code>str</code>, default:                   <code>'red'</code> )           \u2013            <p>The name of the Red band in the Dataset (default is \"red\"). This name should correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".</p> </li> </ul> Notes <p>NDVI (Normalized Difference Vegetation Index) is calculated using the formula:     NDVI = (NIR - Red) / (NIR + Red)</p> <p>This index is commonly used in remote sensing to assess vegetation health and density.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopwatch.f(\"Calculating NDVI\", printer=logger.debug, print_kwargs=[\"nir_band\", \"red_band\"])\ndef calculate_ndvi(planet_scene_dataset: xr.Dataset, nir_band: str = \"nir\", red_band: str = \"red\") -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI from an xarray Dataset containing spectral bands.\n\n    Example:\n        ```python\n        ndvi_data = calculate_ndvi(planet_scene_dataset)\n        ```\n\n    Args:\n        planet_scene_dataset (xr.Dataset): The xarray Dataset containing the spectral bands, where the bands are indexed\n            along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.\n        nir_band (str, optional): The name of the NIR band in the Dataset (default is \"nir\"). This name should\n            correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".\n        red_band (str, optional): The name of the Red band in the Dataset (default is \"red\"). This name should\n            correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".\n\n    Returns:\n        xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have\n            dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".\n\n\n    Notes:\n        NDVI (Normalized Difference Vegetation Index) is calculated using the formula:\n            NDVI = (NIR - Red) / (NIR + Red)\n\n        This index is commonly used in remote sensing to assess vegetation health and density.\n\n    \"\"\"\n    # Calculate NDVI using the formula\n    nir = planet_scene_dataset[nir_band].astype(\"float32\")\n    r = planet_scene_dataset[red_band].astype(\"float32\")\n    ndvi = (nir - r) / (nir + r)\n\n    # Scale to 0 - 20000 (for later conversion to uint16)\n    ndvi = (ndvi.clip(-1, 1) + 1) * 1e4\n    # Make nan to 0\n    ndvi = ndvi.fillna(0).rio.write_nodata(0)\n    # Convert to uint16\n    ndvi = ndvi.astype(\"uint16\")\n\n    ndvi = ndvi.assign_attrs({\"data_source\": \"planet\", \"long_name\": \"NDVI\"}).to_dataset(name=\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_slope","title":"calculate_slope","text":"<pre><code>calculate_slope(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the slope of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.calculate_topographic_position_index","title":"calculate_topographic_position_index","text":"<pre><code>calculate_topographic_position_index(\n    arcticdem_ds: xarray.Dataset,\n    outer_radius: int,\n    inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel in m.</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in m.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopwatch.f(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n        outer_radius (int, optional): The outer radius of the annulus kernel in m.\n        inner_radius (int, optional): The inner radius of the annulus kernel in m.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius_m = f\"{outer_radius}m\"\n    outer_radius_px = f\"{ceil(outer_radius / cellsize_x)}px\"\n    if inner_radius &gt; 0:\n        inner_radius_m = f\"{inner_radius}m\"\n        inner_radius_px = f\"{ceil(inner_radius / cellsize_x)}px\"\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius_m, inner_radius_m)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius_px}-{outer_radius_px} cells \"\n            f\"({inner_radius_m}-{outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of \"\n            f\"{inner_radius_px}-{outer_radius_px} ({inner_radius_m}-{outer_radius_m}) cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius_m)\n        attr_cell_description = (\n            f\"within a circle at a distance of {outer_radius_px} cells ({outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with circle kernel of {outer_radius_px} ({outer_radius_m}) cells.\"\n        )\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.preprocess_arcticdem","title":"preprocess_arcticdem","text":"<pre><code>preprocess_arcticdem(\n    ds_arcticdem: xarray.Dataset,\n    tpi_outer_radius: int,\n    tpi_inner_radius: int,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess the ArcticDEM data with mdoern (DARTS v2) preprocessing steps.</p> <p>Parameters:</p> <ul> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in number of cells.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in number of cells.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed ArcticDEM dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/v2.py</code> <pre><code>@stopwatch.f(\"Preprocessing arcticdem\", printer=logger.debug, print_kwargs=[\"tpi_outer_radius\", \"tpi_inner_radius\"])\ndef preprocess_arcticdem(\n    ds_arcticdem: xr.Dataset,\n    tpi_outer_radius: int,\n    tpi_inner_radius: int,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess the ArcticDEM data with mdoern (DARTS v2) preprocessing steps.\n\n    Args:\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        tpi_outer_radius (int): The outer radius of the annulus kernel for the tpi calculation in number of cells.\n        tpi_inner_radius (int): The inner radius of the annulus kernel for the tpi calculation in number of cells.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n\n    Returns:\n        xr.Dataset: The preprocessed ArcticDEM dataset.\n\n    \"\"\"\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not has_cuda_and_cupy():\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Calculate TPI and slope from ArcticDEM on GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving arcticdem to GPU:{device}.\")\n        # Check if dem is dask, if not persist it, since tpi and slope can't be calculated from cupy-dask arrays\n        if ds_arcticdem.chunks is not None:\n            ds_arcticdem = ds_arcticdem.persist()\n        # Move and calculate on specified device\n        with cp.cuda.Device(device_nr):\n            ds_arcticdem = ds_arcticdem.cupy.as_cupy()\n            ds_arcticdem = calculate_topographic_position_index(ds_arcticdem, tpi_outer_radius, tpi_inner_radius)\n            ds_arcticdem = calculate_slope(ds_arcticdem)\n            ds_arcticdem = calculate_hillshade(ds_arcticdem)\n            ds_arcticdem = calculate_aspect(ds_arcticdem)\n            ds_arcticdem = calculate_curvature(ds_arcticdem)\n            ds_arcticdem = ds_arcticdem.cupy.as_numpy()\n            free_cupy()\n\n    # Calculate TPI and slope from ArcticDEM on CPU\n    else:\n        ds_arcticdem = calculate_topographic_position_index(ds_arcticdem, tpi_outer_radius, tpi_inner_radius)\n        ds_arcticdem = calculate_slope(ds_arcticdem)\n        ds_arcticdem = calculate_hillshade(ds_arcticdem)\n        ds_arcticdem = calculate_aspect(ds_arcticdem)\n        ds_arcticdem = calculate_curvature(ds_arcticdem)\n\n    # Apply legacy scaling to tpi\n    with xr.set_options(keep_attrs=True):\n        ds_arcticdem[\"tpi\"] = (ds_arcticdem.tpi + 50) * 300\n    return ds_arcticdem\n</code></pre>"},{"location":"reference/darts_preprocessing/v2/#darts_preprocessing.v2.preprocess_v2","title":"preprocess_v2","text":"<pre><code>preprocess_v2(\n    ds_merged: xarray.Dataset,\n    ds_arcticdem: xarray.Dataset,\n    ds_tcvis: xarray.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_preprocessing.v2.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess optical data with modern (DARTS v2) preprocessing steps.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope, hillshade, aspect, curvature and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>Parameters:</p> <ul> <li> <code>ds_merged</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The TCVIS dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_preprocessing.v2.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/v2.py</code> <pre><code>@stopwatch(\"Preprocessing\", printer=logger.debug)\ndef preprocess_v2(\n    ds_merged: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with modern (DARTS v2) preprocessing steps.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope, hillshade, aspect, curvature and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    Args:\n        ds_merged (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Calculate NDVI\n    ds_merged[\"ndvi\"] = calculate_ndvi(ds_merged).ndvi\n\n    # Reproject TCVIS to optical data\n    with stopwatch(\"Reprojecting TCVIS\", printer=logger.debug):\n        ds_tcvis = ds_tcvis.odc.reproject(ds_merged.odc.geobox, resampling=\"cubic\")\n\n    ds_merged[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_merged[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_merged[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    with stopwatch(\"Reprojecting ArcticDEM\", printer=logger.debug):\n        ds_arcticdem = ds_arcticdem.odc.reproject(ds_merged.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n\n    ds_arcticdem = preprocess_arcticdem(ds_arcticdem, tpi_outer_radius, tpi_inner_radius, device)\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_merged.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_merged)\n\n    ds_merged[\"dem\"] = ds_arcticdem.dem\n    ds_merged[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_merged[\"slope\"] = ds_arcticdem.slope\n    ds_merged[\"hillshade\"] = ds_arcticdem.hillshade\n    ds_merged[\"aspect\"] = ds_arcticdem.aspect\n    ds_merged[\"curvature\"] = ds_arcticdem.curvature\n    ds_merged[\"arcticdem_data_mask\"] = ds_arcticdem.datamask\n\n    # Update datamask with arcticdem mask\n    # with xr.set_options(keep_attrs=True):\n    #     ds_merged[\"quality_data_mask\"] = ds_merged.quality_data_mask * ds_arcticdem.datamask\n    # ds_merged.quality_data_mask.attrs[\"data_source\"] += \" + ArcticDEM\"\n\n    return ds_merged\n</code></pre>"},{"location":"reference/darts_postprocessing/","title":"darts_postprocessing","text":""},{"location":"reference/darts_postprocessing/#darts_postprocessing","title":"darts_postprocessing","text":"<p>Postprocessing steps for the DARTS dataset.</p>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.binarize","title":"binarize","text":"<pre><code>binarize(\n    probs: xarray.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xarray.DataArray,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.DataArray\n</code></pre> <p>Binarize the probabilities based on a threshold and a mask.</p> Steps for binarization <ol> <li>Dilate the mask. This will dilate the edges of holes in the mask as well as the edges of the tile.</li> <li>Binarize the probabilities based on the threshold.</li> <li>Remove objects at which overlap with either the edge of the tile or the noData mask.</li> <li>Remove small objects.</li> </ol> <p>Parameters:</p> <ul> <li> <code>probs</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Probabilities to binarize.</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Threshold to binarize the probabilities.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>)           \u2013            <p>Minimum object size to keep.</p> </li> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Mask to apply to the binarized probabilities. Expects 0=negative, 1=postitive.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for removing small objects.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Binarized probabilities.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\"Binarizing probabilities\", printer=logger.debug, print_kwargs=[\"threshold\", \"min_object_size\"])\ndef binarize(\n    probs: xr.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xr.DataArray,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xr.DataArray:\n    \"\"\"Binarize the probabilities based on a threshold and a mask.\n\n    Steps for binarization:\n        1. Dilate the mask. This will dilate the edges of holes in the mask as well as the edges of the tile.\n        2. Binarize the probabilities based on the threshold.\n        3. Remove objects at which overlap with either the edge of the tile or the noData mask.\n        4. Remove small objects.\n\n    Args:\n        probs (xr.DataArray): Probabilities to binarize.\n        threshold (float): Threshold to binarize the probabilities.\n        min_object_size (int): Minimum object size to keep.\n        mask (xr.DataArray): Mask to apply to the binarized probabilities. Expects 0=negative, 1=postitive.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for removing small objects.\n\n    Returns:\n        xr.DataArray: Binarized probabilities.\n\n    \"\"\"\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Where the output from the ensemble / segmentation is nan turn it into 0, else threshold it\n    # Also, where there was no valid input data, turn it into 0\n    binarized = (probs.fillna(0) &gt; threshold).astype(\"uint8\")\n\n    # Remove objects at which overlap with either the edge of the tile or the noData mask\n    labels = binarized.copy(data=label(binarized, connectivity=2))\n    edge_label_ids = np.unique(xr.where(~mask, labels, 0))\n    binarized = ~labels.isin(edge_label_ids) &amp; binarized\n\n    # Remove small objects with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving binarized to GPU:{device}.\")\n        # Check if binarized is dask, if not persist it, since remove_small_objects_gpu can't be calculated from\n        # cupy-dask arrays\n        if binarized.chunks is not None:\n            binarized = binarized.persist()\n        with cp.cuda.Device(device_nr):\n            binarized = binarized.cupy.as_cupy()\n            binarized.values = remove_small_objects_gpu(\n                binarized.astype(bool).expand_dims(\"batch\", 0).data, min_size=min_object_size\n            )[0]\n            binarized = binarized.cupy.as_numpy()\n            free_cupy()\n    else:\n        binarized.values = remove_small_objects(\n            binarized.astype(bool).expand_dims(\"batch\", 0).values, min_size=min_object_size\n        )[0]\n\n    # Convert back to int8\n    binarized = binarized.astype(\"uint8\")\n\n    return binarized\n</code></pre>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.erode_mask","title":"erode_mask","text":"<pre><code>erode_mask(\n    mask: xarray.DataArray,\n    size: int,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.DataArray\n</code></pre> <p>Erode the mask, also set the edges to invalid.</p> <p>Parameters:</p> <ul> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>The mask to erode.</p> </li> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The size of the disk to use for erosion and the edge-cropping.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for erosion.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: The dilated and inverted mask.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\"Eroding mask\", printer=logger.debug, print_kwargs=[\"size\"])\ndef erode_mask(mask: xr.DataArray, size: int, device: Literal[\"cuda\", \"cpu\"] | int) -&gt; xr.DataArray:\n    \"\"\"Erode the mask, also set the edges to invalid.\n\n    Args:\n        mask (xr.DataArray): The mask to erode.\n        size (int): The size of the disk to use for erosion and the edge-cropping.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for erosion.\n\n    Returns:\n        xr.DataArray: The dilated and inverted mask.\n\n    \"\"\"\n    # Clone mask to avoid in-place operations\n    mask = mask.copy()\n\n    # Change to dtype uint8 for faster skimage operations\n    mask = mask.astype(\"uint8\")\n\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Dilate the mask with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving mask to GPU:{device}.\")\n        # Check if mask is dask, if not persist it, since dilation can't be calculated from cupy-dask arrays\n        if mask.chunks is not None:\n            mask = mask.persist()\n        with cp.cuda.Device(device_nr):\n            mask = mask.cupy.as_cupy()\n            mask.values = binary_erosion_gpu(mask.data, disk_gpu(size))\n            mask = mask.cupy.as_numpy()\n            free_cupy()\n    else:\n        mask.values = binary_erosion(mask.values, disk(size))\n\n    # Mask edges\n    mask[:size, :] = 0\n    mask[-size:, :] = 0\n    mask[:, :size] = 0\n    mask[:, -size:] = 0\n\n    return mask\n</code></pre>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.prepare_export","title":"prepare_export","text":"<pre><code>prepare_export(\n    tile: xarray.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 0,\n    ensemble_subsets: list[str] = [],\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_postprocessing.postprocess.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Input tile from inference and / or an ensemble.</p> </li> <li> <code>bin_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | str</code>, default:                   <code>0</code> )           \u2013            <p>The quality level to use for the mask. If a string maps to int. high_quality -&gt; 2, low_quality=1, none=0 (apply no masking). Defaults to 0.</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to use for the binarization. Defaults to [].</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_postprocessing.postprocess.DEFAULT_DEVICE</code> )           \u2013            <p>The device to use for dilation. Defaults to \"cuda\" if cuda for cucim is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\n    \"Preparing export\",\n    printer=logger.debug,\n    print_kwargs=[\"bin_threshold\", \"mask_erosion_size\", \"min_object_size\", \"quality_level\", \"ensemble_subsets\"],\n)\ndef prepare_export(\n    tile: xr.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int | Literal[\"high_quality\", \"low_quality\", \"none\"] = 0,\n    ensemble_subsets: list[str] = [],\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.\n\n    Args:\n        tile (xr.Dataset): Input tile from inference and / or an ensemble.\n        bin_threshold (float, optional): The threshold to binarize the probabilities. Defaults to 0.5.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        min_object_size (int, optional): The minimum object size to keep in pixel. Defaults to 32.\n        quality_level (int | str, optional): The quality level to use for the mask. If a string maps to int.\n            high_quality -&gt; 2, low_quality=1, none=0 (apply no masking). Defaults to 0.\n        ensemble_subsets (list[str], optional): The ensemble subsets to use for the binarization.\n            Defaults to [].\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to use for dilation.\n            Defaults to \"cuda\" if cuda for cucim is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: Output tile.\n\n    \"\"\"\n    quality_level = (\n        quality_level\n        if isinstance(quality_level, int)\n        else {\"high_quality\": 2, \"low_quality\": 1, \"none\": 0}[quality_level]\n    )\n    mask = tile[\"quality_data_mask\"] &gt;= quality_level\n    if quality_level &gt; 0:\n        mask = erode_mask(mask, mask_erosion_size, device)  # 0=positive, 1=negative\n    tile[\"extent\"] = mask.copy()\n    tile[\"extent\"].attrs = {\n        \"long_name\": \"Extent of the segmentation\",\n    }\n\n    def _prep_layer(tile, layername, binarized_layer_name):\n        # Binarize the segmentation\n        tile[binarized_layer_name] = binarize(tile[layername], bin_threshold, min_object_size, mask, device)\n        tile[binarized_layer_name].attrs = {\n            \"long_name\": \"Binarized Segmentation\",\n        }\n\n        # Convert the probabilities to uint8\n        # Same but this time with 255 as no-data\n        # But first check if this step was already run\n        if tile[layername].max() &gt; 1:\n            return tile\n\n        intprobs = (tile[layername] * 100).fillna(255).astype(\"uint8\")\n        tile[layername] = xr.where(mask, intprobs, 255)\n        tile[layername].attrs = {\n            \"long_name\": \"Probabilities\",\n            \"units\": \"%\",\n        }\n        tile[layername] = tile[layername].rio.write_nodata(255)\n        return tile\n\n    tile = _prep_layer(tile, \"probabilities\", \"binarized_segmentation\")\n\n    # get the names of the model probabilities if available\n    # for example 'tcvis' from 'probabilities-tcvis'\n    for ensemble_subset in ensemble_subsets:\n        tile = _prep_layer(tile, f\"probabilities-{ensemble_subset}\", f\"binarized_segmentation-{ensemble_subset}\")\n\n    return tile\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/","title":"darts_postprocessing.postprocess","text":""},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess","title":"darts_postprocessing.postprocess","text":"<p>Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.</p>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.CUCIM_AVAILABLE","title":"CUCIM_AVAILABLE  <code>module-attribute</code>","text":"<pre><code>CUCIM_AVAILABLE = True\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.DEFAULT_DEVICE","title":"DEFAULT_DEVICE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DEVICE = 'cuda'\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.binarize","title":"binarize","text":"<pre><code>binarize(\n    probs: xarray.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xarray.DataArray,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.DataArray\n</code></pre> <p>Binarize the probabilities based on a threshold and a mask.</p> Steps for binarization <ol> <li>Dilate the mask. This will dilate the edges of holes in the mask as well as the edges of the tile.</li> <li>Binarize the probabilities based on the threshold.</li> <li>Remove objects at which overlap with either the edge of the tile or the noData mask.</li> <li>Remove small objects.</li> </ol> <p>Parameters:</p> <ul> <li> <code>probs</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Probabilities to binarize.</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Threshold to binarize the probabilities.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>)           \u2013            <p>Minimum object size to keep.</p> </li> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Mask to apply to the binarized probabilities. Expects 0=negative, 1=postitive.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for removing small objects.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Binarized probabilities.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\"Binarizing probabilities\", printer=logger.debug, print_kwargs=[\"threshold\", \"min_object_size\"])\ndef binarize(\n    probs: xr.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xr.DataArray,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xr.DataArray:\n    \"\"\"Binarize the probabilities based on a threshold and a mask.\n\n    Steps for binarization:\n        1. Dilate the mask. This will dilate the edges of holes in the mask as well as the edges of the tile.\n        2. Binarize the probabilities based on the threshold.\n        3. Remove objects at which overlap with either the edge of the tile or the noData mask.\n        4. Remove small objects.\n\n    Args:\n        probs (xr.DataArray): Probabilities to binarize.\n        threshold (float): Threshold to binarize the probabilities.\n        min_object_size (int): Minimum object size to keep.\n        mask (xr.DataArray): Mask to apply to the binarized probabilities. Expects 0=negative, 1=postitive.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for removing small objects.\n\n    Returns:\n        xr.DataArray: Binarized probabilities.\n\n    \"\"\"\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Where the output from the ensemble / segmentation is nan turn it into 0, else threshold it\n    # Also, where there was no valid input data, turn it into 0\n    binarized = (probs.fillna(0) &gt; threshold).astype(\"uint8\")\n\n    # Remove objects at which overlap with either the edge of the tile or the noData mask\n    labels = binarized.copy(data=label(binarized, connectivity=2))\n    edge_label_ids = np.unique(xr.where(~mask, labels, 0))\n    binarized = ~labels.isin(edge_label_ids) &amp; binarized\n\n    # Remove small objects with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving binarized to GPU:{device}.\")\n        # Check if binarized is dask, if not persist it, since remove_small_objects_gpu can't be calculated from\n        # cupy-dask arrays\n        if binarized.chunks is not None:\n            binarized = binarized.persist()\n        with cp.cuda.Device(device_nr):\n            binarized = binarized.cupy.as_cupy()\n            binarized.values = remove_small_objects_gpu(\n                binarized.astype(bool).expand_dims(\"batch\", 0).data, min_size=min_object_size\n            )[0]\n            binarized = binarized.cupy.as_numpy()\n            free_cupy()\n    else:\n        binarized.values = remove_small_objects(\n            binarized.astype(bool).expand_dims(\"batch\", 0).values, min_size=min_object_size\n        )[0]\n\n    # Convert back to int8\n    binarized = binarized.astype(\"uint8\")\n\n    return binarized\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.erode_mask","title":"erode_mask","text":"<pre><code>erode_mask(\n    mask: xarray.DataArray,\n    size: int,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.DataArray\n</code></pre> <p>Erode the mask, also set the edges to invalid.</p> <p>Parameters:</p> <ul> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>The mask to erode.</p> </li> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The size of the disk to use for erosion and the edge-cropping.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for erosion.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: The dilated and inverted mask.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\"Eroding mask\", printer=logger.debug, print_kwargs=[\"size\"])\ndef erode_mask(mask: xr.DataArray, size: int, device: Literal[\"cuda\", \"cpu\"] | int) -&gt; xr.DataArray:\n    \"\"\"Erode the mask, also set the edges to invalid.\n\n    Args:\n        mask (xr.DataArray): The mask to erode.\n        size (int): The size of the disk to use for erosion and the edge-cropping.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for erosion.\n\n    Returns:\n        xr.DataArray: The dilated and inverted mask.\n\n    \"\"\"\n    # Clone mask to avoid in-place operations\n    mask = mask.copy()\n\n    # Change to dtype uint8 for faster skimage operations\n    mask = mask.astype(\"uint8\")\n\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Dilate the mask with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving mask to GPU:{device}.\")\n        # Check if mask is dask, if not persist it, since dilation can't be calculated from cupy-dask arrays\n        if mask.chunks is not None:\n            mask = mask.persist()\n        with cp.cuda.Device(device_nr):\n            mask = mask.cupy.as_cupy()\n            mask.values = binary_erosion_gpu(mask.data, disk_gpu(size))\n            mask = mask.cupy.as_numpy()\n            free_cupy()\n    else:\n        mask.values = binary_erosion(mask.values, disk(size))\n\n    # Mask edges\n    mask[:size, :] = 0\n    mask[-size:, :] = 0\n    mask[:, :size] = 0\n    mask[:, -size:] = 0\n\n    return mask\n</code></pre>"},{"location":"reference/darts_postprocessing/postprocess/#darts_postprocessing.postprocess.prepare_export","title":"prepare_export","text":"<pre><code>prepare_export(\n    tile: xarray.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 0,\n    ensemble_subsets: list[str] = [],\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_postprocessing.postprocess.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Input tile from inference and / or an ensemble.</p> </li> <li> <code>bin_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | str</code>, default:                   <code>0</code> )           \u2013            <p>The quality level to use for the mask. If a string maps to int. high_quality -&gt; 2, low_quality=1, none=0 (apply no masking). Defaults to 0.</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to use for the binarization. Defaults to [].</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_postprocessing.postprocess.DEFAULT_DEVICE</code> )           \u2013            <p>The device to use for dilation. Defaults to \"cuda\" if cuda for cucim is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopwatch.f(\n    \"Preparing export\",\n    printer=logger.debug,\n    print_kwargs=[\"bin_threshold\", \"mask_erosion_size\", \"min_object_size\", \"quality_level\", \"ensemble_subsets\"],\n)\ndef prepare_export(\n    tile: xr.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int | Literal[\"high_quality\", \"low_quality\", \"none\"] = 0,\n    ensemble_subsets: list[str] = [],\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.\n\n    Args:\n        tile (xr.Dataset): Input tile from inference and / or an ensemble.\n        bin_threshold (float, optional): The threshold to binarize the probabilities. Defaults to 0.5.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        min_object_size (int, optional): The minimum object size to keep in pixel. Defaults to 32.\n        quality_level (int | str, optional): The quality level to use for the mask. If a string maps to int.\n            high_quality -&gt; 2, low_quality=1, none=0 (apply no masking). Defaults to 0.\n        ensemble_subsets (list[str], optional): The ensemble subsets to use for the binarization.\n            Defaults to [].\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to use for dilation.\n            Defaults to \"cuda\" if cuda for cucim is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: Output tile.\n\n    \"\"\"\n    quality_level = (\n        quality_level\n        if isinstance(quality_level, int)\n        else {\"high_quality\": 2, \"low_quality\": 1, \"none\": 0}[quality_level]\n    )\n    mask = tile[\"quality_data_mask\"] &gt;= quality_level\n    if quality_level &gt; 0:\n        mask = erode_mask(mask, mask_erosion_size, device)  # 0=positive, 1=negative\n    tile[\"extent\"] = mask.copy()\n    tile[\"extent\"].attrs = {\n        \"long_name\": \"Extent of the segmentation\",\n    }\n\n    def _prep_layer(tile, layername, binarized_layer_name):\n        # Binarize the segmentation\n        tile[binarized_layer_name] = binarize(tile[layername], bin_threshold, min_object_size, mask, device)\n        tile[binarized_layer_name].attrs = {\n            \"long_name\": \"Binarized Segmentation\",\n        }\n\n        # Convert the probabilities to uint8\n        # Same but this time with 255 as no-data\n        # But first check if this step was already run\n        if tile[layername].max() &gt; 1:\n            return tile\n\n        intprobs = (tile[layername] * 100).fillna(255).astype(\"uint8\")\n        tile[layername] = xr.where(mask, intprobs, 255)\n        tile[layername].attrs = {\n            \"long_name\": \"Probabilities\",\n            \"units\": \"%\",\n        }\n        tile[layername] = tile[layername].rio.write_nodata(255)\n        return tile\n\n    tile = _prep_layer(tile, \"probabilities\", \"binarized_segmentation\")\n\n    # get the names of the model probabilities if available\n    # for example 'tcvis' from 'probabilities-tcvis'\n    for ensemble_subset in ensemble_subsets:\n        tile = _prep_layer(tile, f\"probabilities-{ensemble_subset}\", f\"binarized_segmentation-{ensemble_subset}\")\n\n    return tile\n</code></pre>"},{"location":"reference/darts_segmentation/","title":"darts_segmentation","text":""},{"location":"reference/darts_segmentation/#darts_segmentation","title":"darts_segmentation","text":"<p>Image segmentation of thaw-slumps for the DARTS dataset.</p>"},{"location":"reference/darts_segmentation/#darts_segmentation.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/","title":"darts_segmentation.metrics","text":""},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics","title":"darts_segmentation.metrics","text":"<p>Own metrics for segmentation tasks.</p>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU","title":"BinaryBoundaryIoU","text":"<pre><code>BinaryBoundaryIoU(\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Unpack[\n        darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs\n    ],\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Binary Boundary IoU metric for binary segmentation tasks.</p> <p>This metric is similar to the Binary Intersection over Union (IoU or Jaccard Index) metric, but instead of comparing all pixels it only compares the boundaries of each foreground object.</p> <p>Create a new instance of the BinaryBoundaryIoU metric.</p> <p>Please see the torchmetrics docs for more info about the **kwargs.</p> <p>Parameters:</p> <ul> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class.  Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>**kwargs</code>               (<code>typing.Unpack[darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the metric.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>zero_division</code>               (<code>int</code>)           \u2013            <p>Value to return when there is a zero division. Default is 0.</p> </li> <li> <code>compute_on_cpu</code>               (<code>bool</code>)           \u2013            <p>If metric state should be stored on CPU during computations. Only works for list states.</p> </li> <li> <code>dist_sync_on_step</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize on <code>forward()</code>. Default is <code>False</code>.</p> </li> <li> <code>process_group</code>               (<code>str</code>)           \u2013            <p>The process group on which the synchronization is called. Default is the world.</p> </li> <li> <code>dist_sync_fn</code>               (<code>callable</code>)           \u2013            <p>Function that performs the allgather option on the metric state. Default is a custom implementation that calls <code>torch.distributed.all_gather</code> internally.</p> </li> <li> <code>distributed_available_fn</code>               (<code>callable</code>)           \u2013            <p>Function that checks if the distributed backend is available. Defaults to a check of <code>torch.distributed.is_available()</code> and <code>torch.distributed.is_initialized()</code>.</p> </li> <li> <code>sync_on_compute</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize when <code>compute</code> is called. Default is <code>True</code>.</p> </li> <li> <code>compute_with_cache</code>               (<code>bool</code>)           \u2013            <p>If results from <code>compute</code> should be cached. Default is <code>True</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If dilation is not a float or int.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def __init__(\n    self,\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Unpack[BinaryBoundaryIoUKwargs],\n):\n    \"\"\"Create a new instance of the BinaryBoundaryIoU metric.\n\n    Please see the\n    [torchmetrics docs](https://lightning.ai/docs/torchmetrics/stable/pages/overview.html#metric-kwargs)\n    for more info about the **kwargs.\n\n    Args:\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class.  Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        **kwargs: Additional keyword arguments for the metric.\n\n    Keyword Args:\n        zero_division (int):\n            Value to return when there is a zero division. Default is 0.\n        compute_on_cpu (bool):\n            If metric state should be stored on CPU during computations. Only works for list states.\n        dist_sync_on_step (bool):\n            If metric state should synchronize on ``forward()``. Default is ``False``.\n        process_group (str):\n            The process group on which the synchronization is called. Default is the world.\n        dist_sync_fn (callable):\n            Function that performs the allgather option on the metric state. Default is a custom\n            implementation that calls ``torch.distributed.all_gather`` internally.\n        distributed_available_fn (callable):\n            Function that checks if the distributed backend is available. Defaults to a\n            check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n        sync_on_compute (bool):\n            If metric state should synchronize when ``compute`` is called. Default is ``True``.\n        compute_with_cache (bool):\n            If results from ``compute`` should be cached. Default is ``True``.\n\n    Raises:\n        ValueError: If dilation is not a float or int.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super().__init__(**kwargs)\n\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not isinstance(dilation, float | int):\n            raise ValueError(f\"Expected argument `dilation` to be a float or int, but got {dilation}.\")\n\n    self.dilation = dilation\n    self.threshold = threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if multidim_average == \"samplewise\":\n        self.add_state(\"intersection\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"union\", default=[], dist_reduce_fx=\"cat\")\n    else:\n        self.add_state(\"intersection\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.dilation","title":"dilation  <code>instance-attribute</code>","text":"<pre><code>dilation = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    dilation\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.intersection","title":"intersection  <code>instance-attribute</code>","text":"<pre><code>intersection: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.union","title":"union  <code>instance-attribute</code>","text":"<pre><code>union: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>torch.Tensor</code> )          \u2013            <p>The computed metric.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\n\n    Returns:\n        Tensor: The computed metric.\n\n    \"\"\"\n    if self.multidim_average == \"global\":\n        return self.intersection / self.union\n    else:\n        self.intersection = torch.tensor(self.intersection)\n        self.union = torch.tensor(self.union)\n        return self.intersection / self.union\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input arguments are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input shapes are invalid.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If the input arguments are invalid.\n        ValueError: If the input shapes are invalid.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.shape == target.shape:\n            raise ValueError(\n                f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\"\n            )\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.\")\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    target = target.to(torch.uint8)\n    preds = preds.to(torch.uint8)\n\n    target_boundary = get_boundary((target == 1).to(torch.uint8), self.dilation, self.validate_args)\n    preds_boundary = get_boundary(preds, self.dilation, self.validate_args)\n\n    intersection = target_boundary &amp; preds_boundary\n    union = target_boundary | preds_boundary\n\n    if self.ignore_index is not None:\n        # Important that this is NOT the boundary, but the original mask\n        valid_idx = target != self.ignore_index\n        intersection &amp;= valid_idx\n        union &amp;= valid_idx\n\n    intersection = intersection.sum().item()\n    union = union.sum().item()\n\n    if self.multidim_average == \"global\":\n        self.intersection += intersection\n        self.union += union\n    else:\n        self.intersection.append(intersection)\n        self.union.append(union)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy","title":"BinaryInstanceAccuracy","text":"<pre><code>BinaryInstanceAccuracy(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance accuracy metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _accuracy_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision","title":"BinaryInstanceAveragePrecision","text":"<pre><code>BinaryInstanceAveragePrecision(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve</code></p> <p>Compute the average precision for binary instance segmentation.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tensor:  # type: ignore[override]  # noqa: D102\n    return _binary_average_precision_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # type: ignore[override]  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix","title":"BinaryInstanceConfusionMatrix","text":"<pre><code>BinaryInstanceConfusionMatrix(\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance confusion matrix metric.</p> <p>Create a new instance of the BinaryInstanceConfusionMatrix metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, return the confusion matrix normalized by the number of instances. If False, return the confusion matrix without normalization. Defaults to None.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalize</code> is not a bool.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceConfusionMatrix metric.\n\n    Args:\n        normalize (bool, optional): If True, return the confusion matrix normalized by the number of instances.\n            If False, return the confusion matrix without normalization. Defaults to None.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `normalize` is not a bool.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        matching_threshold=matching_threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if normalize is not None and not isinstance(normalize, bool):\n        raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.normalize","title":"normalize  <code>instance-attribute</code>","text":"<pre><code>normalize = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix(\n    normalize\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    # tn is always 0\n    if self.normalize:\n        all = tp + fp + fn\n        return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n    else:\n        return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n    add_text: bool = True,\n    labels: list[str] | None = None,\n    cmap: torchmetrics.utilities.plot._CMAP_TYPE\n    | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n    add_text: bool = True,\n    labels: list[str] | None = None,  # type: ignore\n    cmap: _CMAP_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    val = val or self.compute()\n    if not isinstance(val, Tensor):\n        raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n    fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n    return fig, ax\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score","title":"BinaryInstanceF1Score","text":"<pre><code>BinaryInstanceF1Score(\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore</code></p> <p>Binary instance F1 score metric.</p> <p>Create a new instance of the BinaryInstanceF1Score metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceF1Score metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        beta=1.0,\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=validate_args,\n        zero_division=zero_division,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore","title":"BinaryInstanceFBetaScore","text":"<pre><code>BinaryInstanceFBetaScore(\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance F-beta score metric.</p> <p>Create a new instance of the BinaryInstanceFBetaScore metric.</p> <p>Parameters:</p> <ul> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The beta parameter for the F-beta score.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceFBetaScore metric.\n\n    Args:\n        beta (float): The beta parameter for the F-beta score.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if validate_args:\n        _binary_fbeta_score_arg_validation(beta, threshold, multidim_average, ignore_index, zero_division)\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n    self.beta = beta\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision","title":"BinaryInstancePrecision","text":"<pre><code>BinaryInstancePrecision(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance precision metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"precision\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve","title":"BinaryInstancePrecisionRecallCurve","text":"<pre><code>BinaryInstancePrecisionRecallCurve(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Compute the precision-recall curve for binary instance segmentation.</p> <p>This metric works similar to <code>torchmetrics.classification.PrecisionRecallCurve</code>, with two key differences: 1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.     Instead of calculating the values for each pixel. 2. The \"thresholds\" argument is required.     Calculating the thresholds at the compute stage would cost to much memory for this usecase.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.compute","title":"compute","text":"<pre><code>compute() -&gt; tuple[\n    torch.Tensor, torch.Tensor, torch.Tensor\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; tuple[Tensor, Tensor, Tensor]:  # noqa: D102\n    return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.plot","title":"plot","text":"<pre><code>plot(\n    curve: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n    | None = None,\n    score: torch.Tensor | bool | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    curve: tuple[Tensor, Tensor, Tensor] | None = None,\n    score: Tensor | bool | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    curve_computed = curve or self.compute()\n    # switch order as the standard way is recall along x-axis and precision along y-axis\n    curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n    score = (\n        _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n        if not curve and score is True\n        else None\n    )\n    return plot_curve(\n        curve_computed, score=score, ax=ax, label_names=(\"Recall\", \"Precision\"), name=self.__class__.__name__\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall","title":"BinaryInstanceRecall","text":"<pre><code>BinaryInstanceRecall(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance recall metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"recall\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores","title":"BinaryInstanceStatScores","text":"<pre><code>BinaryInstanceStatScores(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.classification.stat_scores._AbstractStatScores</code></p> <p>Base class for binary instance segmentation metrics.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _binary_stat_scores_compute(tp, fp, tn, fn, self.multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/","title":"darts_segmentation.metrics.binary_instance_prc","text":""},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc","title":"darts_segmentation.metrics.binary_instance_prc","text":"<p>Complex binary instance segmentation metrics.</p>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.MatchingMetric","title":"MatchingMetric  <code>module-attribute</code>","text":"<pre><code>MatchingMetric = typing.Literal['iou', 'boundary']\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision","title":"BinaryInstanceAveragePrecision","text":"<pre><code>BinaryInstanceAveragePrecision(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve</code></p> <p>Compute the average precision for binary instance segmentation.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tensor:  # type: ignore[override]  # noqa: D102\n    return _binary_average_precision_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # type: ignore[override]  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstanceAveragePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve","title":"BinaryInstancePrecisionRecallCurve","text":"<pre><code>BinaryInstancePrecisionRecallCurve(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Compute the precision-recall curve for binary instance segmentation.</p> <p>This metric works similar to <code>torchmetrics.classification.PrecisionRecallCurve</code>, with two key differences: 1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.     Instead of calculating the values for each pixel. 2. The \"thresholds\" argument is required.     Calculating the thresholds at the compute stage would cost to much memory for this usecase.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.compute","title":"compute","text":"<pre><code>compute() -&gt; tuple[\n    torch.Tensor, torch.Tensor, torch.Tensor\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; tuple[Tensor, Tensor, Tensor]:  # noqa: D102\n    return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.plot","title":"plot","text":"<pre><code>plot(\n    curve: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n    | None = None,\n    score: torch.Tensor | bool | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    curve: tuple[Tensor, Tensor, Tensor] | None = None,\n    score: Tensor | bool | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    curve_computed = curve or self.compute()\n    # switch order as the standard way is recall along x-axis and precision along y-axis\n    curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n    score = (\n        _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n        if not curve and score is True\n        else None\n    )\n    return plot_curve(\n        curve_computed, score=score, ax=ax, label_names=(\"Recall\", \"Precision\"), name=self.__class__.__name__\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.mask_to_instances","title":"mask_to_instances","text":"<pre><code>mask_to_instances(\n    x: torch.Tensor, validate_args: bool = False\n) -&gt; list[torch.Tensor]\n</code></pre> <p>Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.</p> <p>Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[torch.Tensor]</code>           \u2013            <p>list[torch.Tensor]: The instance masks. Length of list: batch_size. Shape of a tensor: (height, width), dtype: torch.uint8</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef mask_to_instances(x: torch.Tensor, validate_args: bool = False) -&gt; list[torch.Tensor]:\n    \"\"\"Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.\n\n    Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.\n\n    Args:\n        x (torch.Tensor): The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        list[torch.Tensor]: The instance masks. Length of list: batch_size.\n            Shape of a tensor: (height, width), dtype: torch.uint8\n\n    \"\"\"\n    if validate_args:\n        assert x.dim() == 3, f\"Expected 3 dimensions, got {x.dim()}\"\n        assert x.dtype == torch.uint8, f\"Expected torch.uint8, got {x.dtype}\"\n        assert x.min() &gt;= 0 and x.max() &lt;= 1, f\"Expected binary mask, got {x.min()} and {x.max()}\"\n\n    # A note on using lists as separation between instances instead of using a batched tensor:\n    # Using a batched tensor with instance numbers (1, 2, 3, ...) would indicate that the instances of the samples\n    # are identical. Using a list clearly separates the instances of the samples.\n\n    if CUCIM_AVAILABLE:\n        # Check if device is cuda\n        assert x.device.type == \"cuda\", f\"Expected device to be cuda, got {x.device.type}\"\n        x = cp.asarray(x).astype(cp.uint8)\n\n        instances = []\n        for x_i in x:\n            instances_i = label_gpu(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n\n    else:\n        instances = []\n        for x_i in x:\n            x_i = x_i.cpu().numpy()\n            instances_i = label(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_prc/#darts_segmentation.metrics.binary_instance_prc.match_instances","title":"match_instances","text":"<pre><code>match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.</p> <p>Parameters:</p> <ul> <li> <code>instances_target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the target. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>instances_preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>match_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int, int]</code>           \u2013            <p>tuple[int, int, int]: True positives, false positives, false negatives</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]:\n    \"\"\"Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.\n\n    Args:\n        instances_target (torch.Tensor): The instance mask of the target. Shape: (height, width), dtype: torch.uint8\n        instances_preds (torch.Tensor): The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8\n        match_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        tuple[int, int, int]: True positives, false positives, false negatives\n\n    \"\"\"\n    if validate_args:\n        assert instances_target.dim() == 2, f\"Expected 2 dimensions, got {instances_target.dim()}\"\n        assert instances_preds.dim() == 2, f\"Expected 2 dimensions, got {instances_preds.dim()}\"\n        assert instances_target.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_target.dtype}\"\n        assert instances_preds.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_preds.dtype}\"\n        assert instances_target.shape == instances_preds.shape, (\n            f\"Shapes do not match: {instances_target.shape} and {instances_preds.shape}\"\n        )\n\n    height, width = instances_target.shape\n    ntargets = instances_target.max().item()\n    npreds = instances_preds.max().item()\n    # If target or predictions has no instances, return 0 for their respective metrics.\n    # If none of them has instances, return 0 for all metrics. (This is implied)\n    if ntargets == 0:\n        return 0, npreds, 0\n    if npreds == 0:\n        return 0, 0, ntargets\n\n    # TODO: These are old edge case filter that need revision.\n    # They are probably not necessary, since the instance metrics are meaningless for noisy predictions.\n    # If there are too many predictions, return all as false positives (this happens when the model is very noisy)\n    # if npreds &gt; ntargets * 5:\n    #     return 0, npreds, ntargets\n    # If there is only one prediction, return all as false negatives (this happens when the model is very noisy)\n    # if npreds == 1 and ntargets &gt; 1:\n    #     return 0, 1, ntargets\n\n    # Create one-hot encoding of instances, so that each instance is a channel\n    instances_target_onehot = torch.zeros((ntargets, height, width), dtype=torch.uint8, device=instances_target.device)\n    instances_preds_onehot = torch.zeros((npreds, height, width), dtype=torch.uint8, device=instances_target.device)\n    for i in range(ntargets):\n        instances_target_onehot[i, :, :] = instances_target == (i + 1)\n    for i in range(npreds):\n        instances_preds_onehot[i, :, :] = instances_preds == (i + 1)\n\n    # Now the instances are channels, hence tensors of shape (num_instances, height, width)\n\n    # Calculate IoU (we need to do a n-m intersection and union, therefore we need to broadcast)\n    intersection = (instances_target_onehot.unsqueeze(1) &amp; instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (instances_target_onehot.unsqueeze(1) | instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    # Match instances based on IoU\n    tp = (iou &gt;= match_threshold).sum().item()\n    fp = npreds - tp\n    fn = ntargets - tp\n\n    return tp, fp, fn\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/","title":"darts_segmentation.metrics.binary_instance_stat_scores","text":""},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores","title":"darts_segmentation.metrics.binary_instance_stat_scores","text":"<p>Binary instance segmentation metrics.</p>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy","title":"BinaryInstanceAccuracy","text":"<pre><code>BinaryInstanceAccuracy(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance accuracy metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _accuracy_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceAccuracy.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix","title":"BinaryInstanceConfusionMatrix","text":"<pre><code>BinaryInstanceConfusionMatrix(\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance confusion matrix metric.</p> <p>Create a new instance of the BinaryInstanceConfusionMatrix metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, return the confusion matrix normalized by the number of instances. If False, return the confusion matrix without normalization. Defaults to None.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalize</code> is not a bool.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceConfusionMatrix metric.\n\n    Args:\n        normalize (bool, optional): If True, return the confusion matrix normalized by the number of instances.\n            If False, return the confusion matrix without normalization. Defaults to None.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `normalize` is not a bool.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        matching_threshold=matching_threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if normalize is not None and not isinstance(normalize, bool):\n        raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.normalize","title":"normalize  <code>instance-attribute</code>","text":"<pre><code>normalize = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix(\n    normalize\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    # tn is always 0\n    if self.normalize:\n        all = tp + fp + fn\n        return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n    else:\n        return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n    add_text: bool = True,\n    labels: list[str] | None = None,\n    cmap: torchmetrics.utilities.plot._CMAP_TYPE\n    | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n    add_text: bool = True,\n    labels: list[str] | None = None,  # type: ignore\n    cmap: _CMAP_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    val = val or self.compute()\n    if not isinstance(val, Tensor):\n        raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n    fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n    return fig, ax\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score","title":"BinaryInstanceF1Score","text":"<pre><code>BinaryInstanceF1Score(\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore</code></p> <p>Binary instance F1 score metric.</p> <p>Create a new instance of the BinaryInstanceF1Score metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceF1Score metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        beta=1.0,\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=validate_args,\n        zero_division=zero_division,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceF1Score.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore","title":"BinaryInstanceFBetaScore","text":"<pre><code>BinaryInstanceFBetaScore(\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance F-beta score metric.</p> <p>Create a new instance of the BinaryInstanceFBetaScore metric.</p> <p>Parameters:</p> <ul> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The beta parameter for the F-beta score.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceFBetaScore metric.\n\n    Args:\n        beta (float): The beta parameter for the F-beta score.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if validate_args:\n        _binary_fbeta_score_arg_validation(beta, threshold, multidim_average, ignore_index, zero_division)\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n    self.beta = beta\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision","title":"BinaryInstancePrecision","text":"<pre><code>BinaryInstancePrecision(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance precision metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"precision\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstancePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall","title":"BinaryInstanceRecall","text":"<pre><code>BinaryInstanceRecall(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance recall metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"recall\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceRecall.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores","title":"BinaryInstanceStatScores","text":"<pre><code>BinaryInstanceStatScores(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.classification.stat_scores._AbstractStatScores</code></p> <p>Base class for binary instance segmentation metrics.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _binary_stat_scores_compute(tp, fp, tn, fn, self.multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.mask_to_instances","title":"mask_to_instances","text":"<pre><code>mask_to_instances(\n    x: torch.Tensor, validate_args: bool = False\n) -&gt; list[torch.Tensor]\n</code></pre> <p>Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.</p> <p>Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[torch.Tensor]</code>           \u2013            <p>list[torch.Tensor]: The instance masks. Length of list: batch_size. Shape of a tensor: (height, width), dtype: torch.uint8</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef mask_to_instances(x: torch.Tensor, validate_args: bool = False) -&gt; list[torch.Tensor]:\n    \"\"\"Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.\n\n    Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.\n\n    Args:\n        x (torch.Tensor): The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        list[torch.Tensor]: The instance masks. Length of list: batch_size.\n            Shape of a tensor: (height, width), dtype: torch.uint8\n\n    \"\"\"\n    if validate_args:\n        assert x.dim() == 3, f\"Expected 3 dimensions, got {x.dim()}\"\n        assert x.dtype == torch.uint8, f\"Expected torch.uint8, got {x.dtype}\"\n        assert x.min() &gt;= 0 and x.max() &lt;= 1, f\"Expected binary mask, got {x.min()} and {x.max()}\"\n\n    # A note on using lists as separation between instances instead of using a batched tensor:\n    # Using a batched tensor with instance numbers (1, 2, 3, ...) would indicate that the instances of the samples\n    # are identical. Using a list clearly separates the instances of the samples.\n\n    if CUCIM_AVAILABLE:\n        # Check if device is cuda\n        assert x.device.type == \"cuda\", f\"Expected device to be cuda, got {x.device.type}\"\n        x = cp.asarray(x).astype(cp.uint8)\n\n        instances = []\n        for x_i in x:\n            instances_i = label_gpu(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n\n    else:\n        instances = []\n        for x_i in x:\n            x_i = x_i.cpu().numpy()\n            instances_i = label(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/binary_instance_stat_scores/#darts_segmentation.metrics.binary_instance_stat_scores.match_instances","title":"match_instances","text":"<pre><code>match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.</p> <p>Parameters:</p> <ul> <li> <code>instances_target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the target. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>instances_preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>match_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int, int]</code>           \u2013            <p>tuple[int, int, int]: True positives, false positives, false negatives</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]:\n    \"\"\"Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.\n\n    Args:\n        instances_target (torch.Tensor): The instance mask of the target. Shape: (height, width), dtype: torch.uint8\n        instances_preds (torch.Tensor): The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8\n        match_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        tuple[int, int, int]: True positives, false positives, false negatives\n\n    \"\"\"\n    if validate_args:\n        assert instances_target.dim() == 2, f\"Expected 2 dimensions, got {instances_target.dim()}\"\n        assert instances_preds.dim() == 2, f\"Expected 2 dimensions, got {instances_preds.dim()}\"\n        assert instances_target.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_target.dtype}\"\n        assert instances_preds.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_preds.dtype}\"\n        assert instances_target.shape == instances_preds.shape, (\n            f\"Shapes do not match: {instances_target.shape} and {instances_preds.shape}\"\n        )\n\n    height, width = instances_target.shape\n    ntargets = instances_target.max().item()\n    npreds = instances_preds.max().item()\n    # If target or predictions has no instances, return 0 for their respective metrics.\n    # If none of them has instances, return 0 for all metrics. (This is implied)\n    if ntargets == 0:\n        return 0, npreds, 0\n    if npreds == 0:\n        return 0, 0, ntargets\n\n    # TODO: These are old edge case filter that need revision.\n    # They are probably not necessary, since the instance metrics are meaningless for noisy predictions.\n    # If there are too many predictions, return all as false positives (this happens when the model is very noisy)\n    # if npreds &gt; ntargets * 5:\n    #     return 0, npreds, ntargets\n    # If there is only one prediction, return all as false negatives (this happens when the model is very noisy)\n    # if npreds == 1 and ntargets &gt; 1:\n    #     return 0, 1, ntargets\n\n    # Create one-hot encoding of instances, so that each instance is a channel\n    instances_target_onehot = torch.zeros((ntargets, height, width), dtype=torch.uint8, device=instances_target.device)\n    instances_preds_onehot = torch.zeros((npreds, height, width), dtype=torch.uint8, device=instances_target.device)\n    for i in range(ntargets):\n        instances_target_onehot[i, :, :] = instances_target == (i + 1)\n    for i in range(npreds):\n        instances_preds_onehot[i, :, :] = instances_preds == (i + 1)\n\n    # Now the instances are channels, hence tensors of shape (num_instances, height, width)\n\n    # Calculate IoU (we need to do a n-m intersection and union, therefore we need to broadcast)\n    intersection = (instances_target_onehot.unsqueeze(1) &amp; instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (instances_target_onehot.unsqueeze(1) | instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    # Match instances based on IoU\n    tp = (iou &gt;= match_threshold).sum().item()\n    fp = npreds - tp\n    fn = ntargets - tp\n\n    return tp, fp, fn\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/","title":"darts_segmentation.metrics.boundary_helpers","text":""},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers","title":"darts_segmentation.metrics.boundary_helpers","text":"<p>Helper functions for boundary metrics.</p>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers.MatchingMetric","title":"MatchingMetric  <code>module-attribute</code>","text":"<pre><code>MatchingMetric = typing.Literal['iou', 'boundary']\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers._boundary_arg_validation","title":"_boundary_arg_validation","text":"<pre><code>_boundary_arg_validation(\n    matching_threshold: float = 0.5,\n    matching_metric: darts_segmentation.metrics.boundary_helpers.MatchingMetric = \"iou\",\n    boundary_dilation: float | int = 0.02,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>def _boundary_arg_validation(\n    matching_threshold: float = 0.5,\n    matching_metric: MatchingMetric = \"iou\",\n    boundary_dilation: float | int = 0.02,\n):\n    if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n        raise ValueError(\n            f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n        )\n    if not isinstance(matching_metric, MatchingMetric):\n        raise ValueError(\n            f'Expected argument `matching_metric` to be either \"iou\" or \"boundary\", but got {matching_metric}.'\n        )\n    if matching_metric not in get_args(MatchingMetric):\n        raise ValueError(\n            f'Expected argument `matching_metric` to be either \"iou\" or \"boundary\", but got {matching_metric}.'\n        )\n    if not isinstance(boundary_dilation, float | int) and matching_metric == \"boundary\":\n        raise ValueError(f\"Expected argument `boundary_dilation` to be a float or int, but got {boundary_dilation}.\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers.erode_pytorch","title":"erode_pytorch","text":"<pre><code>erode_pytorch(\n    mask: torch.Tensor,\n    iterations: int = 1,\n    validate_args: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Erodes a binary mask using a square kernel in PyTorch.</p> <p>Parameters:</p> <ul> <li> <code>mask</code>               (<code>torch.Tensor</code>)           \u2013            <p>The binary mask. Shape: (batch_size, height, width) or (batch_size, channels, height, width), dtype: torch.uint8</p> </li> <li> <code>iterations</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The size of the erosion. Defaults to 1.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The eroded mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef erode_pytorch(mask: torch.Tensor, iterations: int = 1, validate_args: bool = False) -&gt; torch.Tensor:\n    \"\"\"Erodes a binary mask using a square kernel in PyTorch.\n\n    Args:\n        mask (torch.Tensor): The binary mask.\n            Shape: (batch_size, height, width) or (batch_size, channels, height, width), dtype: torch.uint8\n        iterations (int, optional): The size of the erosion. Defaults to 1.\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        torch.Tensor: The eroded mask. Shape: (batch_size, height, width), dtype: torch.uint8\n\n    \"\"\"\n    if validate_args:\n        assert mask.dim() not in [3, 4], f\"Expected 3 or 4 dimensions, got {mask.dim()}\"\n        assert mask.dtype == torch.uint8, f\"Expected torch.uint8, got {mask.dtype}\"\n        assert mask.min() &gt;= 0 and mask.max() &lt;= 1, f\"Expected binary mask, got {mask.min()} and {mask.max()}\"\n\n    isbatched = mask.dim() == 4\n    if not isbatched:\n        mask = mask.unsqueeze(1)\n\n    _n, c, _h, _w = mask.shape\n\n    kernel = torch.ones(c, 1, 3, 3, device=mask.device)\n    erode = torch.nn.functional.conv2d(mask.float(), kernel, padding=1, stride=1, groups=c)\n\n    for _ in range(iterations - 1):\n        erode = torch.nn.functional.conv2d(erode, kernel, padding=1, stride=1, groups=c)\n\n    if isbatched:\n        eroded = (erode == erode.max()).to(torch.uint8)\n    else:\n        eroded = (erode == erode.max()).to(torch.uint8).squeeze(1)\n    return eroded\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers.get_boundary","title":"get_boundary","text":"<pre><code>get_boundary(\n    binary_instances: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n)\n</code></pre> <p>Convert instance masks to instance boundaries.</p> <p>Parameters:</p> <ul> <li> <code>binary_instances</code>               (<code>torch.Tensor</code>)           \u2013            <p>Target instance masks. Must be binary. Can be batched, one-hot encoded or both. (3 or 4 dimensions). The last two dimensions must be height and width.</p> </li> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Weather arguments should be validated. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[torch.Tensor, torch.Tensor]: The boundaries of the instances.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef get_boundary(\n    binary_instances: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n):\n    \"\"\"Convert instance masks to instance boundaries.\n\n    Args:\n        binary_instances (torch.Tensor): Target instance masks. Must be binary.\n            Can be batched, one-hot encoded or both. (3 or 4 dimensions).\n            The last two dimensions must be height and width.\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        validate_args (bool, optional): Weather arguments should be validated. Defaults to False.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: The boundaries of the instances.\n\n    \"\"\"\n    if validate_args:\n        assert binary_instances.dim() in [3, 4], f\"Expected 3 or 4 dimensions, got {binary_instances.dim()}\"\n        assert binary_instances.dtype == torch.uint8, f\"Expected torch.uint8, got {binary_instances.dtype}\"\n        assert (\n            binary_instances.min() &gt;= 0 and binary_instances.max() &lt;= 1\n        ), f\"Expected binary mask, got range between {binary_instances.min()} and {binary_instances.max()}\"\n        assert isinstance(dilation, float | int), f\"Expected float or int, got {type(dilation)}\"\n        assert dilation &gt;= 0, f\"Expected dilation &gt;= 0, got {dilation}\"\n\n    if binary_instances.dim() == 3:\n        _n, h, w = binary_instances.shape\n    else:\n        _n, _c, h, w = binary_instances.shape\n\n    if isinstance(dilation, float):\n        img_diag = sqrt(h**2 + w**2)\n        dilation = round(dilation * img_diag)\n        if dilation &lt; 1:\n            dilation = 1\n\n    # Pad the instances to avoid boundary issues\n    pad = torchvision.transforms.Pad(1)\n    binary_instances_padded = pad(binary_instances)\n\n    # Erode the instances to get the boundaries\n    eroded = erode_pytorch(binary_instances_padded, iterations=dilation, validate_args=validate_args)\n\n    # Remove the padding\n    if binary_instances.dim() == 3:\n        eroded = eroded[:, 1:-1, 1:-1]\n    else:\n        eroded = eroded[:, :, 1:-1, 1:-1]\n    # Calculate the boundary of the instances\n    boundaries = binary_instances - eroded\n\n    return boundaries\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_helpers/#darts_segmentation.metrics.boundary_helpers.instance_boundary_iou","title":"instance_boundary_iou","text":"<pre><code>instance_boundary_iou(\n    instances_target_onehot: torch.Tensor,\n    instances_preds_onehot: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Calculate the IoU of the boundaries of instances.</p> <p>Expects non-batched, one-hot encoded input from skimage.measure.label</p> <p>Parameters:</p> <ul> <li> <code>instances_target_onehot</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the target. Shape: (num_instances, height, width), dtype: torch.uint8</p> </li> <li> <code>instances_preds_onehot</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the prediction. Shape: (num_instances, height, width), dtype: torch.uint8</p> </li> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The IoU of the boundaries. Shape: (num_instances,)</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef instance_boundary_iou(\n    instances_target_onehot: torch.Tensor,\n    instances_preds_onehot: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Calculate the IoU of the boundaries of instances.\n\n    Expects non-batched, one-hot encoded input from skimage.measure.label\n\n    Args:\n        instances_target_onehot (torch.Tensor): The instance mask of the target.\n            Shape: (num_instances, height, width), dtype: torch.uint8\n        instances_preds_onehot (torch.Tensor): The instance mask of the prediction.\n            Shape: (num_instances, height, width), dtype: torch.uint8\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        torch.Tensor: The IoU of the boundaries. Shape: (num_instances,)\n\n    \"\"\"\n    # Calculate the boundary of the instances\n    boundaries_target = get_boundary(instances_target_onehot, dilation, validate_args)\n    boundaries_preds = get_boundary(instances_preds_onehot, dilation, validate_args)\n\n    # Calculate the IoU of the boundaries (broadcast because of the different number of instances)\n    intersection = (boundaries_target.unsqueeze(1) &amp; boundaries_preds.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (boundaries_target.unsqueeze(1) | boundaries_preds.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    return iou\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/","title":"darts_segmentation.metrics.boundary_iou","text":""},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou","title":"darts_segmentation.metrics.boundary_iou","text":"<p>Boundary IoU metric for binary segmentation tasks.</p>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.MatchingMetric","title":"MatchingMetric  <code>module-attribute</code>","text":"<pre><code>MatchingMetric = typing.Literal['iou', 'boundary']\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU","title":"BinaryBoundaryIoU","text":"<pre><code>BinaryBoundaryIoU(\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Unpack[\n        darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs\n    ],\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Binary Boundary IoU metric for binary segmentation tasks.</p> <p>This metric is similar to the Binary Intersection over Union (IoU or Jaccard Index) metric, but instead of comparing all pixels it only compares the boundaries of each foreground object.</p> <p>Create a new instance of the BinaryBoundaryIoU metric.</p> <p>Please see the torchmetrics docs for more info about the **kwargs.</p> <p>Parameters:</p> <ul> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class.  Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>**kwargs</code>               (<code>typing.Unpack[darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the metric.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>zero_division</code>               (<code>int</code>)           \u2013            <p>Value to return when there is a zero division. Default is 0.</p> </li> <li> <code>compute_on_cpu</code>               (<code>bool</code>)           \u2013            <p>If metric state should be stored on CPU during computations. Only works for list states.</p> </li> <li> <code>dist_sync_on_step</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize on <code>forward()</code>. Default is <code>False</code>.</p> </li> <li> <code>process_group</code>               (<code>str</code>)           \u2013            <p>The process group on which the synchronization is called. Default is the world.</p> </li> <li> <code>dist_sync_fn</code>               (<code>callable</code>)           \u2013            <p>Function that performs the allgather option on the metric state. Default is a custom implementation that calls <code>torch.distributed.all_gather</code> internally.</p> </li> <li> <code>distributed_available_fn</code>               (<code>callable</code>)           \u2013            <p>Function that checks if the distributed backend is available. Defaults to a check of <code>torch.distributed.is_available()</code> and <code>torch.distributed.is_initialized()</code>.</p> </li> <li> <code>sync_on_compute</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize when <code>compute</code> is called. Default is <code>True</code>.</p> </li> <li> <code>compute_with_cache</code>               (<code>bool</code>)           \u2013            <p>If results from <code>compute</code> should be cached. Default is <code>True</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If dilation is not a float or int.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def __init__(\n    self,\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Unpack[BinaryBoundaryIoUKwargs],\n):\n    \"\"\"Create a new instance of the BinaryBoundaryIoU metric.\n\n    Please see the\n    [torchmetrics docs](https://lightning.ai/docs/torchmetrics/stable/pages/overview.html#metric-kwargs)\n    for more info about the **kwargs.\n\n    Args:\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class.  Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        **kwargs: Additional keyword arguments for the metric.\n\n    Keyword Args:\n        zero_division (int):\n            Value to return when there is a zero division. Default is 0.\n        compute_on_cpu (bool):\n            If metric state should be stored on CPU during computations. Only works for list states.\n        dist_sync_on_step (bool):\n            If metric state should synchronize on ``forward()``. Default is ``False``.\n        process_group (str):\n            The process group on which the synchronization is called. Default is the world.\n        dist_sync_fn (callable):\n            Function that performs the allgather option on the metric state. Default is a custom\n            implementation that calls ``torch.distributed.all_gather`` internally.\n        distributed_available_fn (callable):\n            Function that checks if the distributed backend is available. Defaults to a\n            check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n        sync_on_compute (bool):\n            If metric state should synchronize when ``compute`` is called. Default is ``True``.\n        compute_with_cache (bool):\n            If results from ``compute`` should be cached. Default is ``True``.\n\n    Raises:\n        ValueError: If dilation is not a float or int.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super().__init__(**kwargs)\n\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not isinstance(dilation, float | int):\n            raise ValueError(f\"Expected argument `dilation` to be a float or int, but got {dilation}.\")\n\n    self.dilation = dilation\n    self.threshold = threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if multidim_average == \"samplewise\":\n        self.add_state(\"intersection\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"union\", default=[], dist_reduce_fx=\"cat\")\n    else:\n        self.add_state(\"intersection\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.dilation","title":"dilation  <code>instance-attribute</code>","text":"<pre><code>dilation = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    dilation\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.intersection","title":"intersection  <code>instance-attribute</code>","text":"<pre><code>intersection: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.union","title":"union  <code>instance-attribute</code>","text":"<pre><code>union: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>torch.Tensor</code> )          \u2013            <p>The computed metric.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\n\n    Returns:\n        Tensor: The computed metric.\n\n    \"\"\"\n    if self.multidim_average == \"global\":\n        return self.intersection / self.union\n    else:\n        self.intersection = torch.tensor(self.intersection)\n        self.union = torch.tensor(self.union)\n        return self.intersection / self.union\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input arguments are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input shapes are invalid.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If the input arguments are invalid.\n        ValueError: If the input shapes are invalid.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.shape == target.shape:\n            raise ValueError(\n                f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\"\n            )\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.\")\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    target = target.to(torch.uint8)\n    preds = preds.to(torch.uint8)\n\n    target_boundary = get_boundary((target == 1).to(torch.uint8), self.dilation, self.validate_args)\n    preds_boundary = get_boundary(preds, self.dilation, self.validate_args)\n\n    intersection = target_boundary &amp; preds_boundary\n    union = target_boundary | preds_boundary\n\n    if self.ignore_index is not None:\n        # Important that this is NOT the boundary, but the original mask\n        valid_idx = target != self.ignore_index\n        intersection &amp;= valid_idx\n        union &amp;= valid_idx\n\n    intersection = intersection.sum().item()\n    union = union.sum().item()\n\n    if self.multidim_average == \"global\":\n        self.intersection += intersection\n        self.union += union\n    else:\n        self.intersection.append(intersection)\n        self.union.append(union)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs","title":"BinaryBoundaryIoUKwargs","text":"<p>               Bases: <code>typing.TypedDict</code></p> <p>Keyword arguments for the BinaryBoundaryIoU metric.</p>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.compute_on_cpu","title":"compute_on_cpu  <code>instance-attribute</code>","text":"<pre><code>compute_on_cpu: bool\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.compute_with_cache","title":"compute_with_cache  <code>instance-attribute</code>","text":"<pre><code>compute_with_cache: bool\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.dist_sync_fn","title":"dist_sync_fn  <code>instance-attribute</code>","text":"<pre><code>dist_sync_fn: callable\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.dist_sync_on_step","title":"dist_sync_on_step  <code>instance-attribute</code>","text":"<pre><code>dist_sync_on_step: bool\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.distributed_available_fn","title":"distributed_available_fn  <code>instance-attribute</code>","text":"<pre><code>distributed_available_fn: callable\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.process_group","title":"process_group  <code>instance-attribute</code>","text":"<pre><code>process_group: str\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.sync_on_compute","title":"sync_on_compute  <code>instance-attribute</code>","text":"<pre><code>sync_on_compute: bool\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division: typing.Literal[0, 1]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/boundary_iou/#darts_segmentation.metrics.boundary_iou.get_boundary","title":"get_boundary","text":"<pre><code>get_boundary(\n    binary_instances: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n)\n</code></pre> <p>Convert instance masks to instance boundaries.</p> <p>Parameters:</p> <ul> <li> <code>binary_instances</code>               (<code>torch.Tensor</code>)           \u2013            <p>Target instance masks. Must be binary. Can be batched, one-hot encoded or both. (3 or 4 dimensions). The last two dimensions must be height and width.</p> </li> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Weather arguments should be validated. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[torch.Tensor, torch.Tensor]: The boundaries of the instances.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_helpers.py</code> <pre><code>@torch.no_grad()\ndef get_boundary(\n    binary_instances: torch.Tensor,\n    dilation: float | int = 0.02,\n    validate_args: bool = False,\n):\n    \"\"\"Convert instance masks to instance boundaries.\n\n    Args:\n        binary_instances (torch.Tensor): Target instance masks. Must be binary.\n            Can be batched, one-hot encoded or both. (3 or 4 dimensions).\n            The last two dimensions must be height and width.\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        validate_args (bool, optional): Weather arguments should be validated. Defaults to False.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: The boundaries of the instances.\n\n    \"\"\"\n    if validate_args:\n        assert binary_instances.dim() in [3, 4], f\"Expected 3 or 4 dimensions, got {binary_instances.dim()}\"\n        assert binary_instances.dtype == torch.uint8, f\"Expected torch.uint8, got {binary_instances.dtype}\"\n        assert (\n            binary_instances.min() &gt;= 0 and binary_instances.max() &lt;= 1\n        ), f\"Expected binary mask, got range between {binary_instances.min()} and {binary_instances.max()}\"\n        assert isinstance(dilation, float | int), f\"Expected float or int, got {type(dilation)}\"\n        assert dilation &gt;= 0, f\"Expected dilation &gt;= 0, got {dilation}\"\n\n    if binary_instances.dim() == 3:\n        _n, h, w = binary_instances.shape\n    else:\n        _n, _c, h, w = binary_instances.shape\n\n    if isinstance(dilation, float):\n        img_diag = sqrt(h**2 + w**2)\n        dilation = round(dilation * img_diag)\n        if dilation &lt; 1:\n            dilation = 1\n\n    # Pad the instances to avoid boundary issues\n    pad = torchvision.transforms.Pad(1)\n    binary_instances_padded = pad(binary_instances)\n\n    # Erode the instances to get the boundaries\n    eroded = erode_pytorch(binary_instances_padded, iterations=dilation, validate_args=validate_args)\n\n    # Remove the padding\n    if binary_instances.dim() == 3:\n        eroded = eroded[:, 1:-1, 1:-1]\n    else:\n        eroded = eroded[:, :, 1:-1, 1:-1]\n    # Calculate the boundary of the instances\n    boundaries = binary_instances - eroded\n\n    return boundaries\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/instance_helpers/","title":"darts_segmentation.metrics.instance_helpers","text":""},{"location":"reference/darts_segmentation/metrics/instance_helpers/#darts_segmentation.metrics.instance_helpers","title":"darts_segmentation.metrics.instance_helpers","text":"<p>Helper functions for instance segmentation metrics.</p>"},{"location":"reference/darts_segmentation/metrics/instance_helpers/#darts_segmentation.metrics.instance_helpers.CUCIM_AVAILABLE","title":"CUCIM_AVAILABLE  <code>module-attribute</code>","text":"<pre><code>CUCIM_AVAILABLE = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/instance_helpers/#darts_segmentation.metrics.instance_helpers.mask_to_instances","title":"mask_to_instances","text":"<pre><code>mask_to_instances(\n    x: torch.Tensor, validate_args: bool = False\n) -&gt; list[torch.Tensor]\n</code></pre> <p>Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.</p> <p>Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[torch.Tensor]</code>           \u2013            <p>list[torch.Tensor]: The instance masks. Length of list: batch_size. Shape of a tensor: (height, width), dtype: torch.uint8</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef mask_to_instances(x: torch.Tensor, validate_args: bool = False) -&gt; list[torch.Tensor]:\n    \"\"\"Convert a binary segmentation mask into multiple instance masks. Expects a batched version of the input.\n\n    Currently only supports uint8 tensors, hence a maximum number of 255 instances per mask.\n\n    Args:\n        x (torch.Tensor): The binary segmentation mask. Shape: (batch_size, height, width), dtype: torch.uint8\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        list[torch.Tensor]: The instance masks. Length of list: batch_size.\n            Shape of a tensor: (height, width), dtype: torch.uint8\n\n    \"\"\"\n    if validate_args:\n        assert x.dim() == 3, f\"Expected 3 dimensions, got {x.dim()}\"\n        assert x.dtype == torch.uint8, f\"Expected torch.uint8, got {x.dtype}\"\n        assert x.min() &gt;= 0 and x.max() &lt;= 1, f\"Expected binary mask, got {x.min()} and {x.max()}\"\n\n    # A note on using lists as separation between instances instead of using a batched tensor:\n    # Using a batched tensor with instance numbers (1, 2, 3, ...) would indicate that the instances of the samples\n    # are identical. Using a list clearly separates the instances of the samples.\n\n    if CUCIM_AVAILABLE:\n        # Check if device is cuda\n        assert x.device.type == \"cuda\", f\"Expected device to be cuda, got {x.device.type}\"\n        x = cp.asarray(x).astype(cp.uint8)\n\n        instances = []\n        for x_i in x:\n            instances_i = label_gpu(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n\n    else:\n        instances = []\n        for x_i in x:\n            x_i = x_i.cpu().numpy()\n            instances_i = label(x_i)\n            instances_i = torch.tensor(instances_i, dtype=torch.uint8)\n            instances.append(instances_i)\n        return instances\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/instance_helpers/#darts_segmentation.metrics.instance_helpers.match_instances","title":"match_instances","text":"<pre><code>match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.</p> <p>Parameters:</p> <ul> <li> <code>instances_target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the target. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>instances_preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8</p> </li> <li> <code>match_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to validate the input arguments. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int, int]</code>           \u2013            <p>tuple[int, int, int]: True positives, false positives, false negatives</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/instance_helpers.py</code> <pre><code>@torch.no_grad()\ndef match_instances(\n    instances_target: torch.Tensor,\n    instances_preds: torch.Tensor,\n    match_threshold: float = 0.5,\n    validate_args: bool = False,\n) -&gt; tuple[int, int, int]:\n    \"\"\"Match instances between target and prediction masks. Expects non-batched input from skimage.measure.label.\n\n    Args:\n        instances_target (torch.Tensor): The instance mask of the target. Shape: (height, width), dtype: torch.uint8\n        instances_preds (torch.Tensor): The instance mask of the prediction. Shape: (height, width), dtype: torch.uint8\n        match_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        validate_args (bool, optional): Whether to validate the input arguments. Defaults to False.\n\n    Returns:\n        tuple[int, int, int]: True positives, false positives, false negatives\n\n    \"\"\"\n    if validate_args:\n        assert instances_target.dim() == 2, f\"Expected 2 dimensions, got {instances_target.dim()}\"\n        assert instances_preds.dim() == 2, f\"Expected 2 dimensions, got {instances_preds.dim()}\"\n        assert instances_target.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_target.dtype}\"\n        assert instances_preds.dtype == torch.uint8, f\"Expected torch.uint8, got {instances_preds.dtype}\"\n        assert instances_target.shape == instances_preds.shape, (\n            f\"Shapes do not match: {instances_target.shape} and {instances_preds.shape}\"\n        )\n\n    height, width = instances_target.shape\n    ntargets = instances_target.max().item()\n    npreds = instances_preds.max().item()\n    # If target or predictions has no instances, return 0 for their respective metrics.\n    # If none of them has instances, return 0 for all metrics. (This is implied)\n    if ntargets == 0:\n        return 0, npreds, 0\n    if npreds == 0:\n        return 0, 0, ntargets\n\n    # TODO: These are old edge case filter that need revision.\n    # They are probably not necessary, since the instance metrics are meaningless for noisy predictions.\n    # If there are too many predictions, return all as false positives (this happens when the model is very noisy)\n    # if npreds &gt; ntargets * 5:\n    #     return 0, npreds, ntargets\n    # If there is only one prediction, return all as false negatives (this happens when the model is very noisy)\n    # if npreds == 1 and ntargets &gt; 1:\n    #     return 0, 1, ntargets\n\n    # Create one-hot encoding of instances, so that each instance is a channel\n    instances_target_onehot = torch.zeros((ntargets, height, width), dtype=torch.uint8, device=instances_target.device)\n    instances_preds_onehot = torch.zeros((npreds, height, width), dtype=torch.uint8, device=instances_target.device)\n    for i in range(ntargets):\n        instances_target_onehot[i, :, :] = instances_target == (i + 1)\n    for i in range(npreds):\n        instances_preds_onehot[i, :, :] = instances_preds == (i + 1)\n\n    # Now the instances are channels, hence tensors of shape (num_instances, height, width)\n\n    # Calculate IoU (we need to do a n-m intersection and union, therefore we need to broadcast)\n    intersection = (instances_target_onehot.unsqueeze(1) &amp; instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    union = (instances_target_onehot.unsqueeze(1) | instances_preds_onehot.unsqueeze(0)).sum(\n        dim=(2, 3)\n    )  # Shape: (num_instances_target, num_instances_preds)\n    iou = intersection / union  # Shape: (num_instances_target, num_instances_preds)\n\n    # Match instances based on IoU\n    tp = (iou &gt;= match_threshold).sum().item()\n    fp = npreds - tp\n    fn = ntargets - tp\n\n    return tp, fp, fn\n</code></pre>"},{"location":"reference/darts_segmentation/segment/","title":"darts_segmentation.segment","text":""},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment","title":"darts_segmentation.segment","text":"<p>Functionality for segmenting tiles.</p>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.DEFAULT_DEVICE","title":"DEFAULT_DEVICE  <code>module-attribute</code>","text":"<pre><code>DEFAULT_DEVICE = torch.device(\n    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Band","title":"Band  <code>dataclass</code>","text":"<pre><code>Band(name: str, factor: float = 1.0, offset: float = 0.0)\n</code></pre> <p>Wrapper for the band information.</p>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Band.factor","title":"factor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>factor: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Band.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Band.offset","title":"offset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offset: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands","title":"Bands","text":"<p>               Bases: <code>collections.UserList[darts_segmentation.utils.Band]</code></p> <p>Wrapper for the list of bands.</p>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.factors","title":"factors  <code>property</code>","text":"<pre><code>factors: list[float]\n</code></pre> <p>Get the factors of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The factors of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.names","title":"names  <code>property</code>","text":"<pre><code>names: list[str]\n</code></pre> <p>Get the names of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: The names of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.offsets","title":"offsets  <code>property</code>","text":"<pre><code>offsets: list[float]\n</code></pre> <p>Get the offsets of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The offsets of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.__reduce__","title":"__reduce__","text":"<pre><code>__reduce__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __reduce__(self):  # noqa: D105\n    # This is needed to pickle (and unpickle) the Bands object as a dict\n    # This is needed, because this way we don't need to have this class present when unpickling\n    # a pytorch checkpoint\n    return (dict, (self.to_config(),))\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __repr__(self) -&gt; str:  # noqa: D105\n    band_info = \", \".join([f\"{band.name}(*{band.factor:.5f}+{band.offset:.5f})\" for band in self])\n    return f\"Bands({band_info})\"\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.filter","title":"filter","text":"<pre><code>filter(\n    band_names: list[str],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Filter the bands by name.</p> <p>Parameters:</p> <ul> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>The names of the bands to keep.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The filtered Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def filter(self, band_names: list[str]) -&gt; \"Bands\":\n    \"\"\"Filter the bands by name.\n\n    Args:\n        band_names (list[str]): The names of the bands to keep.\n\n    Returns:\n        Bands: The filtered Bands object.\n\n    \"\"\"\n    return Bands([band for band in self if band.name in band_names])\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: dict[\n        typing.Literal[\n            \"bands\", \"band_factors\", \"band_offsets\"\n        ],\n        list,\n    ]\n    | dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a config dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>The config dictionary containing the band information. Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\", with the values to be lists of the same length.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list] | dict[str, tuple[float, float]],\n) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a config dictionary.\n\n    Args:\n        config (dict): The config dictionary containing the band information.\n            Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\",\n            with the values to be lists of the same length.\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    assert \"bands\" in config and \"band_factors\" in config and \"band_offsets\" in config, (\n        f\"Config must contain keys 'bands', 'band_factors' and 'band_offsets'.Got {config} instead.\"\n    )\n    return cls(\n        [\n            Band(name=name, factor=factor, offset=offset)\n            for name, factor, offset in zip(config[\"bands\"], config[\"band_factors\"], config[\"band_offsets\"])\n        ]\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    config: dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, tuple[float, float]]</code>)           \u2013            <p>The dictionary containing the band information. Expects the keys to be the band names and the values to be tuples of (factor, offset). Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: dict[str, tuple[float, float]]) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a dictionary.\n\n    Args:\n        config (dict[str, tuple[float, float]]): The dictionary containing the band information.\n            Expects the keys to be the band names and the values to be tuples of (factor, offset).\n            Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    return cls([Band(name=name, factor=factor, offset=offset) for name, (factor, offset) in config.items()])\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.to_config","title":"to_config","text":"<pre><code>to_config() -&gt; dict[\n    typing.Literal[\"bands\", \"band_factors\", \"band_offsets\"],\n    list,\n]\n</code></pre> <p>Convert the Bands object to a config dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[typing.Literal['bands', 'band_factors', 'band_offsets'], list]</code> )          \u2013            <p>The config dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_config(self) -&gt; dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list]:\n    \"\"\"Convert the Bands object to a config dictionary.\n\n    Returns:\n        dict: The config dictionary containing the band information.\n\n    \"\"\"\n    return {\n        \"bands\": [band.name for band in self],\n        \"band_factors\": [band.factor for band in self],\n        \"band_offsets\": [band.offset for band in self],\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.Bands.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, tuple[float, float]]\n</code></pre> <p>Convert the Bands object to a dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict[str, tuple[float, float]]</code>           \u2013            <p>dict[str, tuple[float, float]]: The dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_dict(self) -&gt; dict[str, tuple[float, float]]:\n    \"\"\"Convert the Bands object to a dictionary.\n\n    Returns:\n        dict[str, tuple[float, float]]: The dictionary containing the band information.\n\n    \"\"\"\n    return {band.name: (band.factor, band.offset) for band in self}\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter","title":"SMPSegmenter","text":"<pre><code>SMPSegmenter(\n    model_checkpoint: pathlib.Path | str,\n    device: torch.device = darts_segmentation.segment.DEFAULT_DEVICE,\n)\n</code></pre> <p>An actor that keeps a model as its state and segments tiles.</p> <p>Initialize the segmenter.</p> <p>Parameters:</p> <ul> <li> <code>model_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the model checkpoint.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_segmentation.segment.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __init__(self, model_checkpoint: Path | str, device: torch.device = DEFAULT_DEVICE):\n    \"\"\"Initialize the segmenter.\n\n    Args:\n        model_checkpoint (Path): The path to the model checkpoint.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_checkpoint = model_checkpoint if isinstance(model_checkpoint, Path) else Path(model_checkpoint)\n    self.device = device\n    ckpt = torch.load(model_checkpoint, map_location=self.device)\n    self.config = SMPSegmenterConfig.from_ckpt(ckpt[\"config\"])\n    # Overwrite the encoder weights with None, because we load our own\n    self.config[\"model\"] |= {\"encoder_weights\": None}\n    self.model = smp.create_model(**self.config[\"model\"])\n    self.model.to(self.device)\n    self.model.load_state_dict(ckpt[\"statedict\"])\n    self.model.eval()\n\n    logger.debug(f\"Successfully loaded model from {model_checkpoint.resolve()} with inputs: {self.config['bands']}\")\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: darts_segmentation.segment.SMPSegmenterConfig = (\n    darts_segmentation.segment.SMPSegmenterConfig.from_ckpt(\n        ckpt[\"config\"]\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: torch.device = (\n    darts_segmentation.segment.SMPSegmenter(device)\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: torch.nn.Module = (\n    segmentation_models_pytorch.create_model(\n        **darts_segmentation.segment.SMPSegmenter(\n            self\n        ).config[\"model\"]\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset | list[xarray.Dataset]\n</code></pre> <p>Run inference on a single tile or a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>A single tile or a list of tiles augmented by a predicted <code>probabilities</code> layer, depending on the input.</p> </li> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>Each <code>probability</code> has type float32 and range [0, 1].</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xr.Dataset | list[xr.Dataset]:\n    \"\"\"Run inference on a single tile or a list of tiles.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n        Each `probability` has type float32 and range [0, 1].\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    else:\n        raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run inference on a tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@stopwatch.f(\n    \"Segmenting tile\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\"],\n)\ndef segment_tile(\n    self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tile to a tensor\n    tensor_tile = self.tile2tensor(tile)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tile = tensor_tile.unsqueeze(0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n    ).squeeze(0)\n\n    # Highly sophisticated DL-based predictor\n    # TODO: is there a better way to pass metadata?\n    tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n    tile[\"probabilities\"].attrs = {\"long_name\": \"Probabilities\"}\n    tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tile, probabilities\n    free_torch()\n\n    return tile\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@stopwatch.f(\n    \"Segmenting tiles\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\"],\n)\ndef segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tiles to tensors\n    # TODO: maybe create a batched tile2tensor function?\n    # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n    tensor_tiles = self.tile2tensor_batched(tiles)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n    )\n\n    # Highly sophisticated DL-based predictor\n    for tile, probs in zip(tiles, probabilities):\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\"long_name\": \"Probabilities\"}\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tiles, probabilities\n    free_torch()\n\n    return tiles\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.tile2tensor","title":"tile2tensor","text":"<pre><code>tile2tensor(tile: xarray.Dataset) -&gt; torch.Tensor\n</code></pre> <p>Take a tile and convert it to a pytorch tensor.</p> <p>Respects the input combination from the config.</p> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n    \"\"\"Take a tile and convert it to a pytorch tensor.\n\n    Respects the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    # e.g. band.names: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n    # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n    for band in self.config[\"bands\"]:\n        band_data = tile[band.name]\n        # Normalize the band data to the range [0, 1]\n        # Follows CF conventions for scaling and offsetting\n        # decode_values = encoded_values * scale_factor + add_offset\n        # the range [0, 1] is the decoded range\n        band_data = band_data * band.factor + band.offset\n        band_data = band_data.clip(min=0, max=1)\n        bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n    return torch.stack(bands, dim=0)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenter.tile2tensor_batched","title":"tile2tensor_batched","text":"<pre><code>tile2tensor_batched(\n    tiles: list[xarray.Dataset],\n) -&gt; torch.Tensor\n</code></pre> <p>Take a list of tiles and convert them to a pytorch tensor.</p> <p>Respects the the input combination from the config.</p> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n    \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n    Respects the the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    for band in self.config[\"bands\"]:\n        for tile in tiles:\n            band_data = tile[band.name]\n            # Normalize the band data\n            band_data = band_data * band.factor + band.offset\n            band_data = band_data.clip(min=0, max=1)\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n    # TODO: Test this\n    return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"bands\"]), *bands[0].shape)\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenterConfig","title":"SMPSegmenterConfig","text":"<p>               Bases: <code>typing.TypedDict</code></p> <p>Configuration for the segmentor.</p>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenterConfig.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands: darts_segmentation.utils.Bands\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenterConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: dict[str, typing.Any]\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.SMPSegmenterConfig.from_ckpt","title":"from_ckpt  <code>classmethod</code>","text":"<pre><code>from_ckpt(\n    config: dict[str, typing.Any],\n) -&gt; darts_segmentation.segment.SMPSegmenterConfig\n</code></pre> <p>Validate the config for the segmentor.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, typing.Any]</code>)           \u2013            <p>The configuration to validate.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>darts_segmentation.segment.SMPSegmenterConfig</code>           \u2013            <p>The validated configuration.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@classmethod\ndef from_ckpt(cls, config: dict[str, Any]) -&gt; \"SMPSegmenterConfig\":\n    \"\"\"Validate the config for the segmentor.\n\n    Args:\n        config: The configuration to validate.\n\n    Returns:\n        The validated configuration.\n\n    \"\"\"\n    # Handling legacy case that the config contains the old keys\n    if \"input_combination\" in config and \"norm_factors\" in config:\n        # Check if all input_combination features are in norm_factors\n        config[\"bands\"] = Bands([Band(name, config[\"norm_factors\"][name]) for name in config[\"input_combination\"]])\n        config.pop(\"norm_factors\")\n        config.pop(\"input_combination\")\n\n    assert \"model\" in config, \"Model config is missing!\"\n    assert \"bands\" in config, \"Bands config is missing!\"\n    # The Bands object is always pickled as a dict for interoperability, so we need to convert it back\n    if not isinstance(config[\"bands\"], Bands):\n        config[\"bands\"] = Bands.from_config(config[\"bands\"])\n    return config\n</code></pre>"},{"location":"reference/darts_segmentation/segment/#darts_segmentation.segment.predict_in_patches","title":"predict_in_patches","text":"<pre><code>predict_in_patches(\n    model: torch.nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Predict on a tensor.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>torch.nn.Module</code>)           \u2013            <p>The model to use for prediction.</p> </li> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> </li> <li> <code>reflection</code>               (<code>int</code>)           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>torch.device</code> )           \u2013            <p>The device to use for the prediction.</p> </li> <li> <code>return_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>The predicted tensor.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    n_skipped = 0\n    for i, batch in enumerate(patches):\n        # If batch contains only nans, skip it\n        if torch.isnan(batch).all(axis=0).any():\n            patched_probabilities[i * batch_size : (i + 1) * batch_size] = 0\n            n_skipped += 1\n            continue\n        # If batch contains some nans, replace them with zeros\n        batch[torch.isnan(batch)] = 0\n\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    if n_skipped &gt; 0:\n        logger.debug(f\"Skipped {n_skipped} batches because they only contained NaNs\")\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"reference/darts_segmentation/training/","title":"darts_segmentation.training","text":""},{"location":"reference/darts_segmentation/training/#darts_segmentation.training","title":"darts_segmentation.training","text":"<p>Training related functions and classes for Image Segmentation.</p>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.convert_lightning_checkpoint","title":"convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.cross_validation_smp","title":"cross_validation_smp","text":"<pre><code>cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    cv: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n)\n</code></pre> <p>Perform cross-validation for a model with given hyperparameters.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This cross-validation function is designed to evaluate the performance of a single model configuration. It can be used by a tuning script to tune hyperparameters. It calls the training function, hence most functionality is the same as the training function. In general, it does perform this:</p> <pre><code>for seed in seeds:\n    for fold in folds:\n        train_model(seed=seed, fold=fold, ...)\n</code></pre> <p>and calculates a score from the results.</p> <p>To specify on which metric(s) the score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code> for tunes (meaning if <code>tune_name</code> is not None) else <code>{artifact_dir}/_cross_validation</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation. If None, a name is generated automatically. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>cv</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for the training.</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for the data.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for the devices to use.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable, and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no runs were performed, meaning the configuration is invalid or no data was found.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/cv.py</code> <pre><code>def cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    cv: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n    logging_config: LoggingConfig = LoggingConfig(),\n):\n    \"\"\"Perform cross-validation for a model with given hyperparameters.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This cross-validation function is designed to evaluate the performance of a single model configuration.\n    It can be used by a tuning script to tune hyperparameters.\n    It calls the training function, hence most functionality is the same as the training function.\n    In general, it does perform this:\n\n    ```py\n    for seed in seeds:\n        for fold in folds:\n            train_model(seed=seed, fold=fold, ...)\n    ```\n\n    and calculates a score from the results.\n\n    To specify on which metric(s) the score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}` for tunes (meaning if `tune_name` is not None)\n    else `{artifact_dir}/_cross_validation`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the cross-validation. If None, a name is generated automatically.\n            Defaults to None.\n        tune_name (str | None, optional): Name of the tuning. Should only be specified by a tuning script.\n            Defaults to None.\n        cv (CrossValidationConfig): Configuration for cross-validation.\n        training_config (TrainingConfig): Configuration for the training.\n        data_config (DataConfig): Configuration for the data.\n        device_config (DeviceConfig): Configuration for the devices to use.\n        hparams (Hyperparameters): Hyperparameters for the training.\n        logging_config (LoggingConfig): Logging configuration.\n\n    Returns:\n        tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable,\n            and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)\n\n    Raises:\n        ValueError: If no runs were performed, meaning the configuration is invalid or no data was found.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.scoring import score_from_runs\n\n    tick_fstart = time.perf_counter()\n\n    artifact_dir = logging_config.artifact_dir_at_cv(tune_name)\n    cv_name = name or generate_counted_name(artifact_dir)\n    artifact_dir = artifact_dir / cv_name\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    n_folds = cv.n_folds or data_config.total_folds\n\n    logger.info(\n        f\"Starting cross-validation '{cv_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_randoms*n_folds = {cv.n_randoms}*{n_folds} = {cv.n_randoms * n_folds} experiments.\"\n    )\n\n    seeds = cv.rng_seeds\n    logger.debug(f\"Using seeds: {seeds}\")\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs: list[_ProcessInputs] = []\n    for i, seed in enumerate(seeds):\n        for fold in range(n_folds):\n            current = i * len(seeds) + fold\n            total = n_folds * len(seeds)\n            run = TrainRunConfig(\n                name=f\"{cv_name}-run-f{fold}s{seed}\",\n                cv_name=cv_name,\n                tune_name=tune_name,\n                fold=fold,\n                random_seed=seed,\n            )\n            process_inputs.append(\n                _ProcessInputs(\n                    current=current,\n                    total=total,\n                    seed=seed,\n                    fold=fold,\n                    cv=cv,\n                    run=run,\n                    training_config=training_config,\n                    logging_config=logging_config,\n                    data_config=data_config,\n                    device_config=device_config,\n                    hparams=hparams,\n                )\n            )\n\n    run_infos = []\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"cv-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_training,\n    ):\n        run_infos.append(output.run_info)\n\n    if len(run_infos) == 0:\n        raise ValueError(\n            \"No runs were performed. Please check your configuration and data.\"\n            \" If you are using a tuning script, make sure to specify the correct parameters.\"\n        )\n\n    logger.debug(f\"{run_infos=}\")\n    score = score_from_runs(run_infos, cv.scoring_metric, cv.multi_score_strategy)\n\n    run_infos = pd.DataFrame(run_infos)\n    run_infos[\"score\"] = score\n    is_unstable = run_infos[\"is_unstable\"].any()\n    run_infos[\"score_is_unstable\"] = is_unstable\n    if is_unstable:\n        logger.warning(\"Score is unstable, meaning at least one of the metrics is NaN, Inf, -Inf or 0.\")\n    run_infos.to_parquet(artifact_dir / \"run_infos.parquet\")\n    logger.debug(f\"Saved run infos to {artifact_dir / 'run_infos.parquet'}\")\n\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Finished cross-validation '{cv_name}' in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, is_unstable, run_infos\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.test_smp","title":"test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    bands: list[str] | None = None,\n    artifact_dir: pathlib.Path = pathlib.Path(\"artifacts\"),\n    num_workers: int = 0,\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be specified by providing a float between 0 and 1 to data_split_by. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str] | str | float | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which seed/regions/samples split. Defaults to None.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of bands to use. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('artifacts')</code> )           \u2013            <p>Directory to save artifacts. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    bands: list[str] | None = None,\n    artifact_dir: Path = Path(\"artifacts\"),\n    num_workers: int = 0,\n    device_config: DeviceConfig = DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): The path (top-level) to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int): Batch size for training and validation.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be\n            specified by providing a float between 0 and 1 to data_split_by.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str] | str | float | None, optional): Select by which seed/regions/samples split.\n            Defaults to None.\n        bands (list[str] | None, optional): List of bands to use. Defaults to None.\n        artifact_dir (Path, optional): Directory to save artifacts. Defaults to Path(\"lightning_logs\").\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device_config (DeviceConfig, optional): Device and distributed strategy related parameters.\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar, ThroughputMonitor\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n    from darts_segmentation.utils import Bands\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Further nest the artifact directory to avoid cluttering the root directory\n    artifact_dir = artifact_dir / \"_runs\"\n\n    logger.info(\n        f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device_config}\")\n\n    lovely_tensors.set_config(color=False)\n    lovely_tensors.monkey_patch()\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    data_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    all_bands = Bands.from_config(data_config)\n    bands = all_bands.filter(bands) if bands else all_bands\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir,\n        batch_size=batch_size,\n        data_split_method=data_split_method,\n        data_split_by=data_split_by,\n        bands=bands,\n        num_workers=num_workers,\n    )\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    logger.debug(f\"Using model checkpoint at {model_ckp.resolve()}\")\n    model = LitSMP.load_from_checkpoint(model_ckp)\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=\"none\",\n            job_type=\"none\",\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            bands=bands,\n            batch_size=batch_size,\n            patch_size=data_config[\"patch_size\"],\n        ),\n        ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0)),\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=device_config.accelerator,\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        devices=device_config.devices,\n        deterministic=True,\n    )\n\n    trainer.test(model, datamodule, ckpt_path=model_ckp)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.train_smp","title":"train_smp","text":"<pre><code>train_smp(\n    *,\n    run: darts_segmentation.training.train.TrainRunConfig = darts_segmentation.training.train.TrainRunConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n)\n</code></pre> <p>Run the training of the SMP model, specifically binary segmentation.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This training function is meant for single training runs but is also used for cross-validation and hyperparameter tuning by cv.py and tune.py. This strongly affects where artifacts are stored:</p> <ul> <li>Run was created by a tune: <code>{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}</code></li> <li>Run was created by a cross-validation: <code>{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}</code></li> <li>Single runs: <code>{artifact_dir}/_runs/{run_name}-{run_id}</code></li> </ul> <p><code>run_name</code> can be specified by the user, else it is generated automatically. In case of cross-validation, the run name is generated automatically by the cross-validation. <code>run_id</code> is generated automatically by the training function. Both are saved to the final checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Data related parameters for training.</p> </li> <li> <code>run</code>               (<code>darts_segmentation.training.train.TrainRunConfig</code>, default:                   <code>darts_segmentation.training.train.TrainRunConfig()</code> )           \u2013            <p>Run related parameters for training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging related parameters for training.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Training related parameters for training.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the model.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>pl.Trainer: The trainer object used for training. Contains also metrics.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def train_smp(\n    *,\n    run: TrainRunConfig = TrainRunConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n):\n    \"\"\"Run the training of the SMP model, specifically binary segmentation.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This training function is meant for single training runs but is also used for cross-validation and hyperparameter\n    tuning by cv.py and tune.py.\n    This strongly affects where artifacts are stored:\n\n    - Run was created by a tune: `{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}`\n    - Run was created by a cross-validation: `{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}`\n    - Single runs: `{artifact_dir}/_runs/{run_name}-{run_id}`\n\n    `run_name` can be specified by the user, else it is generated automatically.\n    In case of cross-validation, the run name is generated automatically by the cross-validation.\n    `run_id` is generated automatically by the training function.\n    Both are saved to the final checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_config (DataConfig): Data related parameters for training.\n        run (TrainRunConfig): Run related parameters for training.\n        logging_config (LoggingConfig): Logging related parameters for training.\n        device_config (DeviceConfig): Device and distributed strategy related parameters.\n        training_config (TrainingConfig): Training related parameters for training.\n        hparams (Hyperparameters): Hyperparameters for the model.\n\n    Returns:\n        pl.Trainer: The trainer object used for training. Contains also metrics.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from darts_utils.namegen import generate_counted_name, generate_id\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics, BinarySegmentationPreview\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n    from darts_segmentation.utils import Bands\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\", level=logging.INFO)\n\n    tick_fstart = time.perf_counter()\n\n    # Get the right nesting of the artifact directory\n    artifact_dir = logging_config.artifact_dir_at_run(run.cv_name, run.tune_name)\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run.name or generate_counted_name(artifact_dir)\n    run_id = generate_id()  # Needed for wandb\n\n    logger.info(\n        f\"Starting training '{run_name}' ('{run_id}') with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(\n        f\"Using config:\\n\\t{run}\\n\\t{training_config}\\n\\t{data_config}\\n\\t{logging_config}\\n\\t\"\n        f\"{device_config}\\n\\t{hparams}\"\n    )\n    if training_config.continue_from_checkpoint:\n        logger.debug(f\"Continuing from checkpoint '{training_config.continue_from_checkpoint.resolve()}'\")\n\n    lovely_tensors.monkey_patch()\n    lovely_tensors.set_config(color=False)\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(run.random_seed, workers=True, verbose=False)\n\n    dataset_config = toml.load(data_config.train_data_dir / \"config.toml\")[\"darts\"]\n    all_bands = Bands.from_config(dataset_config)\n    bands = all_bands.filter(hparams.bands) if hparams.bands else all_bands\n    config = SMPSegmenterConfig(\n        bands=bands,\n        model={\n            \"arch\": hparams.model_arch,\n            \"encoder_name\": hparams.model_encoder,\n            \"encoder_weights\": hparams.model_encoder_weights,\n            \"in_channels\": len(all_bands) if bands is None else len(bands),\n            \"classes\": 1,\n        },\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=data_config.train_data_dir,\n        batch_size=hparams.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        fold_method=data_config.fold_method,\n        total_folds=data_config.total_folds,\n        fold=run.fold,\n        subsample=data_config.subsample,\n        bands=hparams.bands,\n        augment=hparams.augment,\n        num_workers=training_config.num_workers,\n    )\n    model = LitSMP(\n        config=config,\n        learning_rate=hparams.learning_rate,\n        gamma=hparams.gamma,\n        focal_loss_alpha=hparams.focal_loss_alpha,\n        focal_loss_gamma=hparams.focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        cv_name=run.cv_name or \"none\",\n        tune_name=run.tune_name or \"none\",\n        random_seed=run.random_seed,\n    )\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=None, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        tags = [data_config.train_data_dir.stem]\n        if run.cv_name:\n            tags.append(run.cv_name)\n        if run.tune_name:\n            tags.append(run.tune_name)\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent.parent if run.tune_name or run.cv_name else artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=logging_config.wandb_project,\n            entity=logging_config.wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=run.tune_name or \"none\",\n            job_type=run.cv_name or \"none\",\n            # Using tags to quickly identify the run\n            tags=tags,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{logging_config.wandb_entity}' and project '{logging_config.wandb_project}'\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks and profiler\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            bands=bands,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            is_crossval=bool(run.cv_name),\n            batch_size=hparams.batch_size,\n            patch_size=dataset_config[\"patch_size\"],\n        ),\n        BinarySegmentationPreview(\n            bands=bands,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n        ),\n        # Something does not work well here...\n        # ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0), window_size=log_every_n_steps),\n    ]\n    if training_config.early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {training_config.early_stopping_patience}\")\n        early_stopping = EarlyStopping(\n            monitor=\"val/JaccardIndex\", mode=\"max\", patience=training_config.early_stopping_patience\n        )\n        callbacks.append(early_stopping)\n\n    # Unsupported: https://github.com/Lightning-AI/pytorch-lightning/issues/19983\n    # profiler_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"profiler\"\n    # profiler_dir.mkdir(parents=True, exist_ok=True)\n    # profiler = AdvancedProfiler(dirpath=profiler_dir, filename=\"perf_logs\", dump_stats=True)\n    # logger.debug(f\"Using profiler with output to {profiler.dirpath.resolve()}\")\n\n    logger.debug(\n        f\"Creating lightning-trainer on {device_config.accelerator} with devices {device_config.devices}\"\n        f\" and strategy '{device_config.lightning_strategy}'\"\n    )\n    # Train\n    trainer = L.Trainer(\n        max_epochs=training_config.max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=logging_config.log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n        accelerator=device_config.accelerator,\n        devices=device_config.devices if device_config.devices[0] != \"auto\" else \"auto\",\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        deterministic=False,  # True does not work for some reason\n        # profiler=profiler,\n    )\n    trainer.fit(model, datamodule, ckpt_path=training_config.continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.tune_smp","title":"tune_smp","text":"<pre><code>tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | typing.Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    cv_config: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    hpconfig: pathlib.Path | None = None,\n    config_file: pathlib.Path | None = None,\n)\n</code></pre> <p>Tune the hyper-parameters of the model using cross-validation and random states.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This tuning script is designed to sweep over hyperparameters with a cross-validation used to evaluate each hyperparameter configuration. Optionally, by setting <code>retrain_and_test</code> to True, the best hyperparameters are then selected based on the cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.</p> <p>Hyperparameters can be configured using a <code>hpconfig</code> file (YAML or Toml). Please consult the training guide or the documentation of <code>darts_segmentation.training.hparams.parse_hyperparameters</code> to learn how such a file should be structured. Per default, a random search is performed, where the number of samples can be specified by <code>n_trials</code>. If <code>n_trials</code> is set to \"grid\", a grid search is performed instead. However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.</p> <p>To specify on which metric(s) the cv score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\". In such cases, the configuration is not considered for further evaluation.</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning run. Will be generated based on the number of existing directories in the artifact directory if None. Defaults to None.</p> </li> <li> <code>n_trials</code>               (<code>int | typing.Literal['grid']</code>, default:                   <code>100</code> )           \u2013            <p>Number of trials to perform in hyperparameter tuning. If \"grid\", span a grid search over all configured hyperparameters. In a grid search, only constant or choice hyperparameters are allowed. Defaults to 100.</p> </li> <li> <code>retrain_and_test</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to retrain the model with the best hyperparameters and test it. Defaults to False.</p> </li> <li> <code>cv_config</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation. Defaults to CrossValidationConfig().</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for training. Defaults to TrainingConfig().</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for data. Defaults to DataConfig().</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for device. Defaults to DeviceConfig().</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Configuration for logging. Defaults to LoggingConfig().</p> </li> <li> <code>hpconfig</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the hyperparameter configuration file. Please see the documentation of <code>hyperparameters</code> for more information. Defaults to None.</p> </li> <li> <code>config_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the configuration file. If provided, it will be used instead of <code>hpconfig</code> if <code>hpconfig</code> is None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no hyperparameter configuration file is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/tune.py</code> <pre><code>def tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    cv_config: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    hpconfig: Path | None = None,\n    config_file: Annotated[Path | None, cyclopts.Parameter(parse=False)] = None,\n):\n    \"\"\"Tune the hyper-parameters of the model using cross-validation and random states.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This tuning script is designed to sweep over hyperparameters with a cross-validation\n    used to evaluate each hyperparameter configuration.\n    Optionally, by setting `retrain_and_test` to True, the best hyperparameters are then selected based on the\n    cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.\n\n    Hyperparameters can be configured using a `hpconfig` file (YAML or Toml).\n    Please consult the training guide or the documentation of\n    `darts_segmentation.training.hparams.parse_hyperparameters` to learn how such a file should be structured.\n    Per default, a random search is performed, where the number of samples can be specified by `n_trials`.\n    If `n_trials` is set to \"grid\", a grid search is performed instead.\n    However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.\n\n    To specify on which metric(s) the cv score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n    In such cases, the configuration is not considered for further evaluation.\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the tuning run.\n            Will be generated based on the number of existing directories in the artifact directory if None.\n            Defaults to None.\n        n_trials (int | Literal[\"grid\"], optional): Number of trials to perform in hyperparameter tuning.\n            If \"grid\", span a grid search over all configured hyperparameters.\n            In a grid search, only constant or choice hyperparameters are allowed.\n            Defaults to 100.\n        retrain_and_test (bool, optional): Whether to retrain the model with the best hyperparameters and test it.\n            Defaults to False.\n        cv_config (CrossValidationConfig, optional): Configuration for cross-validation.\n            Defaults to CrossValidationConfig().\n        training_config (TrainingConfig, optional): Configuration for training.\n            Defaults to TrainingConfig().\n        data_config (DataConfig, optional): Configuration for data.\n            Defaults to DataConfig().\n        device_config (DeviceConfig, optional): Configuration for device.\n            Defaults to DeviceConfig().\n        logging_config (LoggingConfig, optional): Configuration for logging.\n            Defaults to LoggingConfig().\n        hpconfig (Path | None, optional): Path to the hyperparameter configuration file.\n            Please see the documentation of `hyperparameters` for more information.\n            Defaults to None.\n        config_file (Path | None, optional): Path to the configuration file. If provided,\n            it will be used instead of `hpconfig` if `hpconfig` is None. Defaults to None.\n\n    Returns:\n        tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.\n\n    Raises:\n        ValueError: If no hyperparameter configuration file is provided.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.hparams import parse_hyperparameters, sample_hyperparameters\n    from darts_segmentation.training.scoring import score_from_single_run\n    from darts_segmentation.training.train import test_smp, train_smp\n\n    tick_fstart = time.perf_counter()\n\n    tune_name = name or generate_counted_name(logging_config.artifact_dir)\n    artifact_dir = logging_config.artifact_dir / tune_name\n    run_infos_file = artifact_dir / f\"{tune_name}.parquet\"\n\n    # Check if the artifact directory is empty\n    assert not artifact_dir.exists(), f\"{artifact_dir} already exists.\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    hpconfig = hpconfig or config_file\n    if hpconfig is None:\n        raise ValueError(\n            \"No hyperparameter configuration file provided. Please provide a valid file via the `--hpconfig` flag.\"\n        )\n    param_grid = parse_hyperparameters(hpconfig)\n    logger.debug(f\"Parsed hyperparameter grid: {param_grid}\")\n    param_list = sample_hyperparameters(param_grid, n_trials)\n\n    logger.info(\n        f\"Starting tune '{tune_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_trials*n_randoms*n_folds =\"\n        f\" {len(param_list)}*{cv_config.n_randoms}*{cv_config.n_folds} =\"\n        f\" {len(param_list) * cv_config.n_randoms * cv_config.n_folds} experiments.\"\n    )\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs = [\n        _ProcessInputs(\n            current=i,\n            total=len(param_list),\n            tune_name=tune_name,\n            cv=cv_config,\n            training_config=training_config,\n            logging_config=logging_config,\n            data_config=data_config,\n            device_config=device_config,\n            hparams=hparams,\n        )\n        for i, hparams in enumerate(param_list)\n    ]\n\n    run_infos: list[pd.DataFrame] = []\n    best_score = 0\n    best_hp = None\n\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"tune-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_cv,\n    ):\n        run_infos.append(output.run_infos)\n        if not output.is_unstable and output.score &gt; best_score:\n            best_score = output.score\n            best_hp = inp.hparams\n\n        # Save already here to prevent data loss if something goes wrong\n        pd.concat(run_infos).reset_index(drop=True).to_parquet(run_infos_file)\n        logger.debug(f\"Saved run infos to {run_infos_file}\")\n\n    if len(run_infos) == 0:\n        logger.error(\"No hyperparameters resulted in a valid score. Please check the logs for more information.\")\n        return 0, run_infos\n\n    run_infos = pd.concat(run_infos).reset_index(drop=True)\n\n    tick_fend = time.perf_counter()\n\n    if best_hp is None:\n        logger.warning(\n            f\"Tuning completed in {tick_fend - tick_fstart:.2f}s.\"\n            \" No hyperparameters resulted in a valid score. Please check the logs for more information.\"\n        )\n        return 0, run_infos\n    logger.info(\n        f\"Tuning completed in {tick_fend - tick_fstart:.2f}s. The best score was {best_score:.4f} with {best_hp}.\"\n    )\n\n    # =====================\n    # === End of tuning ===\n    # =====================\n\n    if not retrain_and_test:\n        return 0, run_infos\n\n    logger.info(\"Starting retraining with the best hyperparameters.\")\n\n    tick_fstart = time.perf_counter()\n    trainer = train_smp(\n        run=TrainRunConfig(name=f\"{tune_name}-retrain\"),\n        training_config=training_config,  # TODO: device and strategy\n        data_config=DataConfig(\n            train_data_dir=data_config.train_data_dir,\n            data_split_method=data_config.data_split_method,\n            data_split_by=data_config.data_split_by,\n            fold_method=None,  # No fold method for retraining\n            total_folds=None,  # No folds for retraining\n        ),\n        logging_config=LoggingConfig(\n            artifact_dir=artifact_dir,\n            log_every_n_steps=logging_config.log_every_n_steps,\n            check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            wandb_entity=logging_config.wandb_entity,\n            wandb_project=logging_config.wandb_project,\n        ),\n        hparams=best_hp,\n    )\n    run_id = trainer.lightning_module.hparams[\"run_id\"]\n    trainer = test_smp(\n        train_data_dir=data_config.train_data_dir,\n        run_id=run_id,\n        run_name=f\"{tune_name}-retrain\",\n        model_ckp=trainer.checkpoint_callback.best_model_path,\n        batch_size=best_hp.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        artifact_dir=artifact_dir,\n        num_workers=training_config.num_workers,\n        device_config=device_config,\n        wandb_entity=logging_config.wandb_entity,\n        wandb_project=logging_config.wandb_project,\n    )\n\n    run_info = {k: v.item() for k, v in trainer.callback_metrics.items()}\n    test_scoring_metric = (\n        cv_config.scoring_metric.replace(\"val/\", \"test/\")\n        if isinstance(cv_config.scoring_metric, str)\n        else [sm.replace(\"val/\", \"test/\") for sm in cv_config.scoring_metric]\n    )\n    score = score_from_single_run(run_info, test_scoring_metric, cv_config.multi_score_strategy)\n    is_unstable = check_score_is_unstable(run_info, cv_config.scoring_metric)\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Retraining and testing completed successfully in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, run_infos\n</code></pre>"},{"location":"reference/darts_segmentation/training/adp/","title":"darts_segmentation.training.adp","text":""},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp","title":"darts_segmentation.training.adp","text":"<p>Abstract Data Parallelism (ADP) module for DARTS Segmentation.</p>"},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp.RunI","title":"RunI  <code>module-attribute</code>","text":"<pre><code>RunI = typing.TypeVar('I')\n</code></pre>"},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp.RunO","title":"RunO  <code>module-attribute</code>","text":"<pre><code>RunO = typing.TypeVar('O')\n</code></pre>"},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/adp/#darts_segmentation.training.adp._adp","title":"_adp","text":"<pre><code>_adp(\n    process_inputs: list[\n        darts_segmentation.training.adp.RunI\n    ],\n    is_parallel: bool,\n    devices: list[int],\n    available_devices: multiprocessing.Queue,\n    _run: collections.abc.Callable[\n        [darts_segmentation.training.adp.RunI],\n        darts_segmentation.training.adp.RunO,\n    ],\n) -&gt; collections.abc.Generator[\n    tuple[\n        darts_segmentation.training.adp.RunI,\n        darts_segmentation.training.adp.RunO,\n    ],\n    None,\n    None,\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/adp.py</code> <pre><code>def _adp(\n    process_inputs: list[RunI],\n    is_parallel: bool,\n    devices: list[int],\n    available_devices: Queue,\n    _run: Callable[[RunI], RunO],\n) -&gt; Generator[tuple[RunI, RunO], None, None]:\n    # Handling different parallelization strategies\n    if is_parallel:\n        logger.debug(\"Using parallel strategy for ADP\")\n        for device in devices:\n            logger.debug(f\"Adding device {device} to available devices queue\")\n            available_devices.put(device)\n        with ProcessPoolExecutor(max_workers=len(devices)) as executor:\n            futures = {executor.submit(_run, inp): inp for inp in process_inputs}\n\n            for future in as_completed(futures):\n                inp = futures[future]\n                try:\n                    output = future.result()\n                except Exception as e:\n                    logger.error(f\"Error in {inp}: {e}\", exc_info=True)\n                    continue\n\n                yield inp, output\n    else:\n        logger.debug(\"Using serial strategy for ADP\")\n        for inp in process_inputs:\n            try:\n                output = _run(inp)\n            except Exception as e:\n                logger.error(f\"Error in {inp}: {e}\", exc_info=True)\n                continue\n            yield inp, output\n</code></pre>"},{"location":"reference/darts_segmentation/training/augmentations/","title":"darts_segmentation.training.augmentations","text":""},{"location":"reference/darts_segmentation/training/augmentations/#darts_segmentation.training.augmentations","title":"darts_segmentation.training.augmentations","text":"<p>Augmentations for segmentation tasks.</p>"},{"location":"reference/darts_segmentation/training/augmentations/#darts_segmentation.training.augmentations.Augmentation","title":"Augmentation  <code>module-attribute</code>","text":"<pre><code>Augmentation = typing.Literal[\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\",\n    \"Posterize\",\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/augmentations/#darts_segmentation.training.augmentations.get_augmentation","title":"get_augmentation","text":"<pre><code>get_augmentation(\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None,\n) -&gt; albumentations.Compose | None\n</code></pre> <p>Get augmentations for segmentation tasks.</p> <p>Parameters:</p> <ul> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply. If None or emtpy, no augmentations are applied. If not empty, augmentations are applied in the order they are listed. Available augmentations:     - HorizontalFlip     - VerticalFlip     - RandomRotate90     - Blur     - RandomBrightnessContrast     - MultiplicativeNoise</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown augmentation is provided.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>albumentations.Compose | None</code>           \u2013            <p>A.Compose | None: A Compose object containing the augmentations. If no augmentations are provided, returns None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/augmentations.py</code> <pre><code>def get_augmentation(augment: list[Augmentation] | None) -&gt; \"A.Compose | None\":\n    \"\"\"Get augmentations for segmentation tasks.\n\n    Args:\n        augment (list[Augmentation] | None): List of augmentations to apply.\n            If None or emtpy, no augmentations are applied.\n            If not empty, augmentations are applied in the order they are listed.\n            Available augmentations:\n                - HorizontalFlip\n                - VerticalFlip\n                - RandomRotate90\n                - Blur\n                - RandomBrightnessContrast\n                - MultiplicativeNoise\n\n    Raises:\n        ValueError: If an unknown augmentation is provided.\n\n    Returns:\n        A.Compose | None: A Compose object containing the augmentations.\n            If no augmentations are provided, returns None.\n\n    \"\"\"\n    import albumentations as A  # noqa: N812\n\n    if not isinstance(augment, list) or len(augment) == 0:\n        return None\n    transforms = []\n    for aug in augment:\n        match aug:\n            case \"HorizontalFlip\":\n                transforms.append(A.HorizontalFlip())\n            case \"VerticalFlip\":\n                transforms.append(A.VerticalFlip())\n            case \"RandomRotate90\":\n                transforms.append(A.RandomRotate90())\n            case \"Blur\":\n                transforms.append(A.Blur())\n            case \"RandomBrightnessContrast\":\n                transforms.append(A.RandomBrightnessContrast())\n            case \"MultiplicativeNoise\":\n                transforms.append(A.MultiplicativeNoise(per_channel=True, elementwise=True))\n            case \"Posterize\":\n                # First convert to uint8, then apply posterization, then convert back to float32\n                # * Note: This does only work for float32 images.\n                transforms += [\n                    A.FromFloat(dtype=\"uint8\"),\n                    A.Posterize(num_bits=6, p=1.0),\n                    A.ToFloat(),\n                ]\n            case _:\n                raise ValueError(f\"Unknown augmentation: {aug}\")\n    return A.Compose(transforms)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/","title":"darts_segmentation.training.callbacks","text":""},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks","title":"darts_segmentation.training.callbacks","text":"<p>PyTorch Lightning Callbacks for training and validation.</p>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Stage","title":"Stage  <code>module-attribute</code>","text":"<pre><code>Stage = typing.Literal[\"fit\", \"validate\", \"test\", \"predict\"]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands","title":"Bands","text":"<p>               Bases: <code>collections.UserList[darts_segmentation.utils.Band]</code></p> <p>Wrapper for the list of bands.</p>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.factors","title":"factors  <code>property</code>","text":"<pre><code>factors: list[float]\n</code></pre> <p>Get the factors of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The factors of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.names","title":"names  <code>property</code>","text":"<pre><code>names: list[str]\n</code></pre> <p>Get the names of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: The names of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.offsets","title":"offsets  <code>property</code>","text":"<pre><code>offsets: list[float]\n</code></pre> <p>Get the offsets of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The offsets of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.__reduce__","title":"__reduce__","text":"<pre><code>__reduce__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __reduce__(self):  # noqa: D105\n    # This is needed to pickle (and unpickle) the Bands object as a dict\n    # This is needed, because this way we don't need to have this class present when unpickling\n    # a pytorch checkpoint\n    return (dict, (self.to_config(),))\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __repr__(self) -&gt; str:  # noqa: D105\n    band_info = \", \".join([f\"{band.name}(*{band.factor:.5f}+{band.offset:.5f})\" for band in self])\n    return f\"Bands({band_info})\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.filter","title":"filter","text":"<pre><code>filter(\n    band_names: list[str],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Filter the bands by name.</p> <p>Parameters:</p> <ul> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>The names of the bands to keep.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The filtered Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def filter(self, band_names: list[str]) -&gt; \"Bands\":\n    \"\"\"Filter the bands by name.\n\n    Args:\n        band_names (list[str]): The names of the bands to keep.\n\n    Returns:\n        Bands: The filtered Bands object.\n\n    \"\"\"\n    return Bands([band for band in self if band.name in band_names])\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: dict[\n        typing.Literal[\n            \"bands\", \"band_factors\", \"band_offsets\"\n        ],\n        list,\n    ]\n    | dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a config dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>The config dictionary containing the band information. Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\", with the values to be lists of the same length.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list] | dict[str, tuple[float, float]],\n) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a config dictionary.\n\n    Args:\n        config (dict): The config dictionary containing the band information.\n            Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\",\n            with the values to be lists of the same length.\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    assert \"bands\" in config and \"band_factors\" in config and \"band_offsets\" in config, (\n        f\"Config must contain keys 'bands', 'band_factors' and 'band_offsets'.Got {config} instead.\"\n    )\n    return cls(\n        [\n            Band(name=name, factor=factor, offset=offset)\n            for name, factor, offset in zip(config[\"bands\"], config[\"band_factors\"], config[\"band_offsets\"])\n        ]\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    config: dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, tuple[float, float]]</code>)           \u2013            <p>The dictionary containing the band information. Expects the keys to be the band names and the values to be tuples of (factor, offset). Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: dict[str, tuple[float, float]]) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a dictionary.\n\n    Args:\n        config (dict[str, tuple[float, float]]): The dictionary containing the band information.\n            Expects the keys to be the band names and the values to be tuples of (factor, offset).\n            Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    return cls([Band(name=name, factor=factor, offset=offset) for name, (factor, offset) in config.items()])\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.to_config","title":"to_config","text":"<pre><code>to_config() -&gt; dict[\n    typing.Literal[\"bands\", \"band_factors\", \"band_offsets\"],\n    list,\n]\n</code></pre> <p>Convert the Bands object to a config dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[typing.Literal['bands', 'band_factors', 'band_offsets'], list]</code> )          \u2013            <p>The config dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_config(self) -&gt; dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list]:\n    \"\"\"Convert the Bands object to a config dictionary.\n\n    Returns:\n        dict: The config dictionary containing the band information.\n\n    \"\"\"\n    return {\n        \"bands\": [band.name for band in self],\n        \"band_factors\": [band.factor for band in self],\n        \"band_offsets\": [band.offset for band in self],\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.Bands.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, tuple[float, float]]\n</code></pre> <p>Convert the Bands object to a dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict[str, tuple[float, float]]</code>           \u2013            <p>dict[str, tuple[float, float]]: The dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_dict(self) -&gt; dict[str, tuple[float, float]]:\n    \"\"\"Convert the Bands object to a dictionary.\n\n    Returns:\n        dict[str, tuple[float, float]]: The dictionary containing the band information.\n\n    \"\"\"\n    return {band.name: (band.factor, band.offset) for band in self}\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU","title":"BinaryBoundaryIoU","text":"<pre><code>BinaryBoundaryIoU(\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Unpack[\n        darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs\n    ],\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Binary Boundary IoU metric for binary segmentation tasks.</p> <p>This metric is similar to the Binary Intersection over Union (IoU or Jaccard Index) metric, but instead of comparing all pixels it only compares the boundaries of each foreground object.</p> <p>Create a new instance of the BinaryBoundaryIoU metric.</p> <p>Please see the torchmetrics docs for more info about the **kwargs.</p> <p>Parameters:</p> <ul> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class.  Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>**kwargs</code>               (<code>typing.Unpack[darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the metric.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>zero_division</code>               (<code>int</code>)           \u2013            <p>Value to return when there is a zero division. Default is 0.</p> </li> <li> <code>compute_on_cpu</code>               (<code>bool</code>)           \u2013            <p>If metric state should be stored on CPU during computations. Only works for list states.</p> </li> <li> <code>dist_sync_on_step</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize on <code>forward()</code>. Default is <code>False</code>.</p> </li> <li> <code>process_group</code>               (<code>str</code>)           \u2013            <p>The process group on which the synchronization is called. Default is the world.</p> </li> <li> <code>dist_sync_fn</code>               (<code>callable</code>)           \u2013            <p>Function that performs the allgather option on the metric state. Default is a custom implementation that calls <code>torch.distributed.all_gather</code> internally.</p> </li> <li> <code>distributed_available_fn</code>               (<code>callable</code>)           \u2013            <p>Function that checks if the distributed backend is available. Defaults to a check of <code>torch.distributed.is_available()</code> and <code>torch.distributed.is_initialized()</code>.</p> </li> <li> <code>sync_on_compute</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize when <code>compute</code> is called. Default is <code>True</code>.</p> </li> <li> <code>compute_with_cache</code>               (<code>bool</code>)           \u2013            <p>If results from <code>compute</code> should be cached. Default is <code>True</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If dilation is not a float or int.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def __init__(\n    self,\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Unpack[BinaryBoundaryIoUKwargs],\n):\n    \"\"\"Create a new instance of the BinaryBoundaryIoU metric.\n\n    Please see the\n    [torchmetrics docs](https://lightning.ai/docs/torchmetrics/stable/pages/overview.html#metric-kwargs)\n    for more info about the **kwargs.\n\n    Args:\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class.  Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        **kwargs: Additional keyword arguments for the metric.\n\n    Keyword Args:\n        zero_division (int):\n            Value to return when there is a zero division. Default is 0.\n        compute_on_cpu (bool):\n            If metric state should be stored on CPU during computations. Only works for list states.\n        dist_sync_on_step (bool):\n            If metric state should synchronize on ``forward()``. Default is ``False``.\n        process_group (str):\n            The process group on which the synchronization is called. Default is the world.\n        dist_sync_fn (callable):\n            Function that performs the allgather option on the metric state. Default is a custom\n            implementation that calls ``torch.distributed.all_gather`` internally.\n        distributed_available_fn (callable):\n            Function that checks if the distributed backend is available. Defaults to a\n            check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n        sync_on_compute (bool):\n            If metric state should synchronize when ``compute`` is called. Default is ``True``.\n        compute_with_cache (bool):\n            If results from ``compute`` should be cached. Default is ``True``.\n\n    Raises:\n        ValueError: If dilation is not a float or int.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super().__init__(**kwargs)\n\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not isinstance(dilation, float | int):\n            raise ValueError(f\"Expected argument `dilation` to be a float or int, but got {dilation}.\")\n\n    self.dilation = dilation\n    self.threshold = threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if multidim_average == \"samplewise\":\n        self.add_state(\"intersection\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"union\", default=[], dist_reduce_fx=\"cat\")\n    else:\n        self.add_state(\"intersection\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.dilation","title":"dilation  <code>instance-attribute</code>","text":"<pre><code>dilation = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    dilation\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.intersection","title":"intersection  <code>instance-attribute</code>","text":"<pre><code>intersection: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.union","title":"union  <code>instance-attribute</code>","text":"<pre><code>union: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>torch.Tensor</code> )          \u2013            <p>The computed metric.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\n\n    Returns:\n        Tensor: The computed metric.\n\n    \"\"\"\n    if self.multidim_average == \"global\":\n        return self.intersection / self.union\n    else:\n        self.intersection = torch.tensor(self.intersection)\n        self.union = torch.tensor(self.union)\n        return self.intersection / self.union\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryBoundaryIoU.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input arguments are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input shapes are invalid.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If the input arguments are invalid.\n        ValueError: If the input shapes are invalid.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.shape == target.shape:\n            raise ValueError(\n                f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\"\n            )\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.\")\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    target = target.to(torch.uint8)\n    preds = preds.to(torch.uint8)\n\n    target_boundary = get_boundary((target == 1).to(torch.uint8), self.dilation, self.validate_args)\n    preds_boundary = get_boundary(preds, self.dilation, self.validate_args)\n\n    intersection = target_boundary &amp; preds_boundary\n    union = target_boundary | preds_boundary\n\n    if self.ignore_index is not None:\n        # Important that this is NOT the boundary, but the original mask\n        valid_idx = target != self.ignore_index\n        intersection &amp;= valid_idx\n        union &amp;= valid_idx\n\n    intersection = intersection.sum().item()\n    union = union.sum().item()\n\n    if self.multidim_average == \"global\":\n        self.intersection += intersection\n        self.union += union\n    else:\n        self.intersection.append(intersection)\n        self.union.append(union)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy","title":"BinaryInstanceAccuracy","text":"<pre><code>BinaryInstanceAccuracy(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance accuracy metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _accuracy_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAccuracy.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision","title":"BinaryInstanceAveragePrecision","text":"<pre><code>BinaryInstanceAveragePrecision(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve</code></p> <p>Compute the average precision for binary instance segmentation.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tensor:  # type: ignore[override]  # noqa: D102\n    return _binary_average_precision_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # type: ignore[override]  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceAveragePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix","title":"BinaryInstanceConfusionMatrix","text":"<pre><code>BinaryInstanceConfusionMatrix(\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance confusion matrix metric.</p> <p>Create a new instance of the BinaryInstanceConfusionMatrix metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, return the confusion matrix normalized by the number of instances. If False, return the confusion matrix without normalization. Defaults to None.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalize</code> is not a bool.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceConfusionMatrix metric.\n\n    Args:\n        normalize (bool, optional): If True, return the confusion matrix normalized by the number of instances.\n            If False, return the confusion matrix without normalization. Defaults to None.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `normalize` is not a bool.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        matching_threshold=matching_threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if normalize is not None and not isinstance(normalize, bool):\n        raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.normalize","title":"normalize  <code>instance-attribute</code>","text":"<pre><code>normalize = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix(\n    normalize\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    # tn is always 0\n    if self.normalize:\n        all = tp + fp + fn\n        return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n    else:\n        return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n    add_text: bool = True,\n    labels: list[str] | None = None,\n    cmap: torchmetrics.utilities.plot._CMAP_TYPE\n    | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n    add_text: bool = True,\n    labels: list[str] | None = None,  # type: ignore\n    cmap: _CMAP_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    val = val or self.compute()\n    if not isinstance(val, Tensor):\n        raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n    fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n    return fig, ax\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceConfusionMatrix.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score","title":"BinaryInstanceF1Score","text":"<pre><code>BinaryInstanceF1Score(\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore</code></p> <p>Binary instance F1 score metric.</p> <p>Create a new instance of the BinaryInstanceF1Score metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceF1Score metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        beta=1.0,\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=validate_args,\n        zero_division=zero_division,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceF1Score.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision","title":"BinaryInstancePrecision","text":"<pre><code>BinaryInstancePrecision(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance precision metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"precision\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve","title":"BinaryInstancePrecisionRecallCurve","text":"<pre><code>BinaryInstancePrecisionRecallCurve(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Compute the precision-recall curve for binary instance segmentation.</p> <p>This metric works similar to <code>torchmetrics.classification.PrecisionRecallCurve</code>, with two key differences: 1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.     Instead of calculating the values for each pixel. 2. The \"thresholds\" argument is required.     Calculating the thresholds at the compute stage would cost to much memory for this usecase.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.compute","title":"compute","text":"<pre><code>compute() -&gt; tuple[\n    torch.Tensor, torch.Tensor, torch.Tensor\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; tuple[Tensor, Tensor, Tensor]:  # noqa: D102\n    return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.plot","title":"plot","text":"<pre><code>plot(\n    curve: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n    | None = None,\n    score: torch.Tensor | bool | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    curve: tuple[Tensor, Tensor, Tensor] | None = None,\n    score: Tensor | bool | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    curve_computed = curve or self.compute()\n    # switch order as the standard way is recall along x-axis and precision along y-axis\n    curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n    score = (\n        _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n        if not curve and score is True\n        else None\n    )\n    return plot_curve(\n        curve_computed, score=score, ax=ax, label_names=(\"Recall\", \"Precision\"), name=self.__class__.__name__\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstancePrecisionRecallCurve.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall","title":"BinaryInstanceRecall","text":"<pre><code>BinaryInstanceRecall(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance recall metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"recall\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinaryInstanceRecall.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics","title":"BinarySegmentationMetrics","text":"<pre><code>BinarySegmentationMetrics(\n    *,\n    bands: darts_segmentation.utils.Bands,\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n    is_crossval: bool = False,\n    batch_size: int = 8,\n    patch_size: int = 512,\n)\n</code></pre> <p>               Bases: <code>lightning.pytorch.callbacks.Callback</code></p> <p>Callback for validation metrics and visualizations.</p> <p>Initialize the ValidationCallback.</p> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>darts_segmentation.utils.Bands</code>)           \u2013            <p>List of bands to combine for the visualization.</p> </li> <li> <code>val_set</code>               (<code>str</code>, default:                   <code>'val'</code> )           \u2013            <p>Name of the validation set. Only used for naming the validation metrics. Defaults to \"val\".</p> </li> <li> <code>test_set</code>               (<code>str</code>, default:                   <code>'test'</code> )           \u2013            <p>Name of the test set. Only used for naming the test metrics. Defaults to \"test\".</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>is_crossval</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether the training is done with cross-validation. This will change the logging behavior of scalar metrics from logging to {val_set} to just \"val\". The logging behaviour of the samples is not affected. Defaults to False.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size. Needed for throughput measurements. Defaults to 8.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <p>Patch size. Needed for throughput measurements. Defaults to 512.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    *,\n    bands: Bands,\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n    is_crossval: bool = False,\n    batch_size: int = 8,\n    patch_size: int = 512,\n):\n    \"\"\"Initialize the ValidationCallback.\n\n    Args:\n        bands (Bands): List of bands to combine for the visualization.\n        val_set (str, optional): Name of the validation set. Only used for naming the validation metrics.\n            Defaults to \"val\".\n        test_set (str, optional): Name of the test set. Only used for naming the test metrics. Defaults to \"test\".\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        is_crossval (bool, optional): Whether the training is done with cross-validation.\n            This will change the logging behavior of scalar metrics from logging to {val_set} to just \"val\".\n            The logging behaviour of the samples is not affected.\n            Defaults to False.\n        batch_size (int, optional): Batch size. Needed for throughput measurements. Defaults to 8.\n        patch_size (int, optional): Patch size. Needed for throughput measurements. Defaults to 512.\n\n    \"\"\"\n    assert \"/\" not in val_set, \"val_set must not contain '/'\"\n    assert \"/\" not in test_set, \"test_set must not contain '/'\"\n    self.val_set = val_set\n    self.test_set = test_set\n    self.plot_every_n_val_epochs = plot_every_n_val_epochs\n    self.band_names = bands.names\n    self.is_crossval = is_crossval\n    self.batch_size = batch_size\n    self.patch_size = patch_size\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.band_names","title":"band_names  <code>instance-attribute</code>","text":"<pre><code>band_names = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    bands\n).names\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    batch_size\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.is_crossval","title":"is_crossval  <code>instance-attribute</code>","text":"<pre><code>is_crossval = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    is_crossval\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    patch_size\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.pl_module","title":"pl_module  <code>instance-attribute</code>","text":"<pre><code>pl_module: lightning.LightningModule\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    plot_every_n_val_epochs\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.stage","title":"stage  <code>instance-attribute</code>","text":"<pre><code>stage: darts_segmentation.training.callbacks.Stage\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_cmx","title":"test_cmx  <code>instance-attribute</code>","text":"<pre><code>test_cmx: torchmetrics.ConfusionMatrix\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_instance_cmx","title":"test_instance_cmx  <code>instance-attribute</code>","text":"<pre><code>test_instance_cmx: (\n    darts_segmentation.metrics.BinaryInstanceConfusionMatrix\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_instance_prc","title":"test_instance_prc  <code>instance-attribute</code>","text":"<pre><code>test_instance_prc: darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_metrics","title":"test_metrics  <code>instance-attribute</code>","text":"<pre><code>test_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_prc","title":"test_prc  <code>instance-attribute</code>","text":"<pre><code>test_prc: torchmetrics.PrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_roc","title":"test_roc  <code>instance-attribute</code>","text":"<pre><code>test_roc: torchmetrics.ROC\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.test_set","title":"test_set  <code>instance-attribute</code>","text":"<pre><code>test_set = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    test_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.train_metrics","title":"train_metrics  <code>instance-attribute</code>","text":"<pre><code>train_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: lightning.Trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_cmx","title":"val_cmx  <code>instance-attribute</code>","text":"<pre><code>val_cmx: torchmetrics.ConfusionMatrix\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_metrics","title":"val_metrics  <code>instance-attribute</code>","text":"<pre><code>val_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_prc","title":"val_prc  <code>instance-attribute</code>","text":"<pre><code>val_prc: torchmetrics.PrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_roc","title":"val_roc  <code>instance-attribute</code>","text":"<pre><code>val_roc: torchmetrics.ROC\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.val_set","title":"val_set  <code>instance-attribute</code>","text":"<pre><code>val_set = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    val_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.is_val_plot_epoch","title":"is_val_plot_epoch","text":"<pre><code>is_val_plot_epoch(\n    current_epoch: int, check_val_every_n_epoch: int | None\n) -&gt; bool\n</code></pre> <p>Check if the current epoch is an epoch where validation samples should be plotted.</p> <p>Parameters:</p> <ul> <li> <code>current_epoch</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int | None</code>)           \u2013            <p>The number of epochs to check for plotting. If None, no plotting is done.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the current epoch is a plot epoch, False otherwise.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def is_val_plot_epoch(self, current_epoch: int, check_val_every_n_epoch: int | None) -&gt; bool:\n    \"\"\"Check if the current epoch is an epoch where validation samples should be plotted.\n\n    Args:\n        current_epoch (int): The current epoch.\n        check_val_every_n_epoch (int | None): The number of epochs to check for plotting.\n            If None, no plotting is done.\n\n    Returns:\n        bool: True if the current epoch is a plot epoch, False otherwise.\n\n    \"\"\"\n    if check_val_every_n_epoch is None:\n        return False\n    n = self.plot_every_n_val_epochs * check_val_every_n_epoch\n    return ((current_epoch + 1) % n) == 0 or current_epoch == 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_test_batch_end","title":"on_test_batch_end","text":"<pre><code>on_test_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    pl_module.log(f\"{self.test_set}/loss\", outputs[\"loss\"])\n    _x, y = batch\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'test_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    pl_module.test_metrics.update(y_hat, y)\n    pl_module.test_roc.update(y_hat, y)\n    pl_module.test_prc.update(y_hat, y)\n    pl_module.test_cmx.update(y_hat, y)\n    pl_module.test_instance_prc.update(y_hat, y)\n    pl_module.test_instance_cmx.update(y_hat, y)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    pl_module.test_cmx.compute()\n    pl_module.test_roc.compute()\n    pl_module.test_prc.compute()\n    pl_module.test_instance_prc.compute()\n    pl_module.test_instance_cmx.compute()\n\n    # Plot roc, prc and confusion matrix to disk and wandb\n    fig_cmx, _ = pl_module.test_cmx.plot(cmap=\"Blues\")\n    fig_roc, _ = pl_module.test_roc.plot(score=True)\n    fig_prc, _ = pl_module.test_prc.plot(score=True)\n    fig_instance_cmx, _ = pl_module.test_instance_cmx.plot(cmap=\"Blues\")\n    fig_instance_prc, _ = pl_module.test_instance_prc.plot(score=True)\n\n    # Check for a wandb or csv logger to log the images\n    for pllogger in pl_module.loggers:\n        if isinstance(pllogger, CSVLogger):\n            fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.test_set}-samples\"\n            fig_dir.mkdir(exist_ok=True, parents=True)\n            fig_cmx.savefig(fig_dir / f\"cmx_{pl_module.global_step}.png\")\n            fig_roc.savefig(fig_dir / f\"roc_{pl_module.global_step}.png\")\n            fig_prc.savefig(fig_dir / f\"prc_{pl_module.global_step}.png\")\n            fig_instance_cmx.savefig(fig_dir / f\"instance_cmx_{pl_module.global_step}.png\")\n            fig_instance_prc.savefig(fig_dir / f\"instance_prc_{pl_module.global_step}.png\")\n        if isinstance(pllogger, WandbLogger):\n            wandb_run: Run = pllogger.experiment\n            wandb_run.log({f\"{self.test_set}/cmx\": wandb.Image(fig_cmx)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/roc\": wandb.Image(fig_roc)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/prc\": wandb.Image(fig_prc)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/instance_cmx\": wandb.Image(fig_instance_cmx)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/instance_prc\": wandb.Image(fig_instance_prc)}, commit=False)\n\n    fig_cmx.clear()\n    fig_roc.clear()\n    fig_prc.clear()\n    fig_instance_cmx.clear()\n    fig_instance_prc.clear()\n    plt.close(\"all\")\n\n    # This will also commit the accumulated plots\n    pl_module.log_dict(pl_module.test_metrics.compute())\n\n    pl_module.test_metrics.reset()\n    pl_module.test_roc.reset()\n    pl_module.test_prc.reset()\n    pl_module.test_cmx.reset()\n    pl_module.test_instance_prc.reset()\n    pl_module.test_instance_cmx.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_train_batch_end","title":"on_train_batch_end","text":"<pre><code>on_train_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_train_batch_end(self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx):  # noqa: D102\n    pl_module.log(\"train/loss\", outputs[\"loss\"])\n    _, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'training_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n    pl_module.train_metrics(y_hat, y)\n    pl_module.log_dict(pl_module.train_metrics, on_step=True, on_epoch=False)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    pl_module.train_metrics.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_validation_batch_end","title":"on_validation_batch_end","text":"<pre><code>on_validation_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    pl_module.log(f\"{self._val_prefix}/loss\", outputs[\"loss\"])\n    _x, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'validation_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    pl_module.val_metrics.update(y_hat, y)\n    pl_module.val_roc.update(y_hat, y)\n    pl_module.val_prc.update(y_hat, y)\n    pl_module.val_cmx.update(y_hat, y)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    # Only do this every self.plot_every_n_val_epochs epochs\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if is_val_plot_epoch and trainer.state.stage != \"sanity_check\":\n        pl_module.val_cmx.compute()\n        pl_module.val_roc.compute()\n        pl_module.val_prc.compute()\n\n        # Plot roc, prc and confusion matrix to disk and wandb\n        fig_cmx, _ = pl_module.val_cmx.plot(cmap=\"Blues\")\n        fig_roc, _ = pl_module.val_roc.plot(score=True)\n        fig_prc, _ = pl_module.val_prc.plot(score=True)\n\n        # Check for a wandb or csv logger to log the images\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self._val_prefix}-samples\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig_cmx.savefig(fig_dir / f\"cmx_{pl_module.global_step}png\")\n                fig_roc.savefig(fig_dir / f\"roc_{pl_module.global_step}png\")\n                fig_prc.savefig(fig_dir / f\"prc_{pl_module.global_step}.png\")\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                wandb_run.log({f\"{self._val_prefix}/cmx\": wandb.Image(fig_cmx)}, commit=False)\n                wandb_run.log({f\"{self._val_prefix}/roc\": wandb.Image(fig_roc)}, commit=False)\n                wandb_run.log({f\"{self._val_prefix}/prc\": wandb.Image(fig_prc)}, commit=False)\n\n        fig_cmx.clear()\n        fig_roc.clear()\n        fig_prc.clear()\n        plt.close(\"all\")\n\n    # This will also commit the accumulated plots\n    pl_module.log_dict(pl_module.val_metrics.compute())\n\n    pl_module.val_metrics.reset()\n    pl_module.val_roc.reset()\n    pl_module.val_prc.reset()\n    pl_module.val_cmx.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.setup","title":"setup","text":"<pre><code>setup(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> <p>Setups the callback.</p> <p>Creates metrics required for the specific stage:</p> <ul> <li>For the \"fit\" stage, creates training and validation metrics and visualizations.</li> <li>For the \"validate\" stage, only creates validation metrics and visualizations.</li> <li>For the \"test\" stage, only creates test metrics and visualizations.</li> <li>For the \"predict\" stage, no metrics or visualizations are created.</li> </ul> <p>Always maps the trainer and pl_module to the callback.</p> <p>Training and validation metrics are \"simple\" metrics from torchmetrics. The validation visualizations are more complex metrics from torchmetrics. The test metrics and vsiualizations are the same as the validation ones, and also include custom \"Instance\" metrics.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>lightning.Trainer</code>)           \u2013            <p>The lightning trainer.</p> </li> <li> <code>pl_module</code>               (<code>lightning.LightningModule</code>)           \u2013            <p>The lightning module.</p> </li> <li> <code>stage</code>               (<code>typing.Literal['fit', 'validate', 'test', 'predict']</code>)           \u2013            <p>The current stage. One of: \"fit\", \"validate\", \"test\", \"predict\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):\n    \"\"\"Setups the callback.\n\n    Creates metrics required for the specific stage:\n\n    - For the \"fit\" stage, creates training and validation metrics and visualizations.\n    - For the \"validate\" stage, only creates validation metrics and visualizations.\n    - For the \"test\" stage, only creates test metrics and visualizations.\n    - For the \"predict\" stage, no metrics or visualizations are created.\n\n    Always maps the trainer and pl_module to the callback.\n\n    Training and validation metrics are \"simple\" metrics from torchmetrics.\n    The validation visualizations are more complex metrics from torchmetrics.\n    The test metrics and vsiualizations are the same as the validation ones,\n    and also include custom \"Instance\" metrics.\n\n    Args:\n        trainer (Trainer): The lightning trainer.\n        pl_module (LightningModule): The lightning module.\n        stage (Literal[\"fit\", \"validate\", \"test\", \"predict\"]): The current stage.\n            One of: \"fit\", \"validate\", \"test\", \"predict\".\n\n    \"\"\"\n    # Save references to the trainer and pl_module\n    self.trainer = trainer\n    self.pl_module = pl_module\n    self.stage = stage\n\n    # We don't want to use memory in the predict stage\n    if stage == \"predict\":\n        return\n\n    # Add throughput metric, meant to be consumed by the ThroughputMonitor callback\n    # ! This will assume that the batch size does not change during training!\n    with torch.device(\"meta\"):\n        model: torch.Module = copy.deepcopy(self.pl_module.model).to(device=\"meta\")\n\n        def sample_forward():\n            batch = torch.randn(\n                self.batch_size,\n                len(self.band_names),\n                self.patch_size,\n                self.patch_size,\n                device=\"meta\",\n            )\n            return model(batch)\n\n        if stage == \"fit\":\n            # We use sum as a dummy loss function because we don't have a second input available\n            self.pl_module.flops_per_batch = measure_flops(model, sample_forward, loss_fn=torch.Tensor.sum)\n        else:\n            # Don't compute backward pass for validation and test\n            self.pl_module.flops_per_batch = measure_flops(model, sample_forward)\n        logger.debug(f\"FLOPS per batch: {self.pl_module.flops_per_batch}\")\n\n    metric_kwargs = {\"task\": \"binary\", \"validate_args\": False, \"ignore_index\": 2}\n    metrics = MetricCollection(\n        {\n            \"Accuracy\": Accuracy(**metric_kwargs),\n            \"Precision\": Precision(**metric_kwargs),\n            \"Specificity\": Specificity(**metric_kwargs),\n            \"Recall\": Recall(**metric_kwargs),\n            \"F1Score\": F1Score(**metric_kwargs),\n            \"JaccardIndex\": JaccardIndex(**metric_kwargs),\n            \"CohenKappa\": CohenKappa(**metric_kwargs),\n            \"HammingDistance\": HammingDistance(**metric_kwargs),\n        }\n    )\n\n    added_metrics: defaultdict[str] = defaultdict(list)\n\n    # Train metrics only for the fit stage\n    if stage == \"fit\":\n        pl_module.train_metrics = metrics.clone(prefix=\"train/\")\n        added_metrics[\"train\"] += list(pl_module.train_metrics.keys(keep_base=True))\n    # Validation metrics and visualizations for the fit and validate stages\n    if stage == \"fit\" or stage == \"validate\":\n        pl_module.val_metrics = metrics.clone(prefix=f\"{self._val_prefix}/\")\n        pl_module.val_metrics.add_metrics(\n            {\n                \"AUROC\": AUROC(thresholds=20, **metric_kwargs),\n                \"AveragePrecision\": AveragePrecision(thresholds=20, **metric_kwargs),\n            }\n        )\n        pl_module.val_roc = ROC(thresholds=20, **metric_kwargs)\n        pl_module.val_prc = PrecisionRecallCurve(thresholds=20, **metric_kwargs)\n        pl_module.val_cmx = ConfusionMatrix(normalize=\"true\", **metric_kwargs)\n        added_metrics[self._val_prefix] += list(pl_module.val_metrics.keys(keep_base=True))\n        added_metrics[self._val_prefix] += [\"roc\", \"prc\", \"cmx\"]\n\n    # Test metrics and visualizations for the test stage\n    if stage == \"test\":\n        pl_module.test_metrics = metrics.clone(prefix=f\"{self.test_set}/\")\n        pl_module.test_metrics.add_metrics(\n            {\n                \"AUROC\": AUROC(thresholds=20, **metric_kwargs),\n                \"AveragePrecision\": AveragePrecision(thresholds=20, **metric_kwargs),\n            }\n        )\n        pl_module.test_roc = ROC(thresholds=20, **metric_kwargs)\n        pl_module.test_prc = PrecisionRecallCurve(thresholds=20, **metric_kwargs)\n        pl_module.test_cmx = ConfusionMatrix(normalize=\"true\", **metric_kwargs)\n\n        # Instance Metrics\n        instance_metric_kwargs = {\"validate_args\": False, \"ignore_index\": 2, \"matching_threshold\": 0.3}\n        pl_module.test_metrics.add_metrics(\n            {\n                \"InstanceAccuracy\": BinaryInstanceAccuracy(**instance_metric_kwargs),\n                \"InstancePrecision\": BinaryInstancePrecision(**instance_metric_kwargs),\n                \"InstanceRecall\": BinaryInstanceRecall(**instance_metric_kwargs),\n                \"InstanceF1Score\": BinaryInstanceF1Score(**instance_metric_kwargs),\n                \"InstanceAveragePrecision\": BinaryInstanceAveragePrecision(thresholds=20, **instance_metric_kwargs),\n            }\n        )\n        boundary_metric_kwargs = {\"validate_args\": False, \"ignore_index\": 2}\n        pl_module.test_metrics.add_metrics(\n            {\n                \"InstanceBoundaryIoU\": BinaryBoundaryIoU(**boundary_metric_kwargs),\n            }\n        )\n        pl_module.test_instance_prc = BinaryInstancePrecisionRecallCurve(thresholds=20, **instance_metric_kwargs)\n        pl_module.test_instance_cmx = BinaryInstanceConfusionMatrix(normalize=True, **instance_metric_kwargs)\n\n        added_metrics[self.test_set] += list(pl_module.test_metrics.keys(keep_base=True))\n        added_metrics[self.test_set] += [\"roc\", \"prc\", \"cmx\", \"instance_prc\", \"instance_cmx\"]\n\n    # Log the added metrics\n    added_metrics = {k: str(sorted(v)) for k, v in added_metrics.items()}\n    logger.debug(f\"Added metrics:{added_metrics}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationMetrics.teardown","title":"teardown","text":"<pre><code>teardown(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def teardown(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):  # noqa: D102\n    # Delete the references to the trainer and pl_module\n    del self.trainer\n    del self.pl_module\n    del self.stage\n\n    # No need to delete anything if we are in the predict stage\n    if stage == \"predict\":\n        return\n\n    if stage == \"fit\":\n        del pl_module.train_metrics\n\n    if stage == \"fit\" or stage == \"validate\":\n        del pl_module.val_metrics\n        del pl_module.val_roc\n        del pl_module.val_prc\n        del pl_module.val_cmx\n\n    if stage == \"test\":\n        del pl_module.test_metrics\n        del pl_module.test_roc\n        del pl_module.test_prc\n        del pl_module.test_cmx\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview","title":"BinarySegmentationPreview","text":"<pre><code>BinarySegmentationPreview(\n    *,\n    bands: darts_segmentation.utils.Bands,\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n)\n</code></pre> <p>               Bases: <code>lightning.pytorch.callbacks.Callback</code></p> <p>Callback for validation metrics and visualizations.</p> <p>Initialize the ValidationCallback.</p> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>darts_segmentation.utils.Bands</code>)           \u2013            <p>List of bands to combine for the visualization.</p> </li> <li> <code>val_set</code>               (<code>str</code>, default:                   <code>'val'</code> )           \u2013            <p>Name of the validation set. Only used for naming the validation metrics. Defaults to \"val\".</p> </li> <li> <code>test_set</code>               (<code>str</code>, default:                   <code>'test'</code> )           \u2013            <p>Name of the test set. Only used for naming the test metrics. Defaults to \"test\".</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    *,\n    bands: Bands,\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n):\n    \"\"\"Initialize the ValidationCallback.\n\n    Args:\n        bands (Bands): List of bands to combine for the visualization.\n        val_set (str, optional): Name of the validation set. Only used for naming the validation metrics.\n            Defaults to \"val\".\n        test_set (str, optional): Name of the test set. Only used for naming the test metrics. Defaults to \"test\".\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n\n    \"\"\"\n    assert \"/\" not in val_set, \"val_set must not contain '/'\"\n    assert \"/\" not in test_set, \"test_set must not contain '/'\"\n    self.val_set = val_set\n    self.test_set = test_set\n    self.plot_every_n_val_epochs = plot_every_n_val_epochs\n    self.band_names = bands.names\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.band_names","title":"band_names  <code>instance-attribute</code>","text":"<pre><code>band_names = darts_segmentation.training.callbacks.BinarySegmentationPreview(\n    bands\n).names\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.pl_module","title":"pl_module  <code>instance-attribute</code>","text":"<pre><code>pl_module: lightning.LightningModule\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs = darts_segmentation.training.callbacks.BinarySegmentationPreview(\n    plot_every_n_val_epochs\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.stage","title":"stage  <code>instance-attribute</code>","text":"<pre><code>stage: darts_segmentation.training.callbacks.Stage\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.test_set","title":"test_set  <code>instance-attribute</code>","text":"<pre><code>test_set = darts_segmentation.training.callbacks.BinarySegmentationPreview(\n    test_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: lightning.Trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.val_set","title":"val_set  <code>instance-attribute</code>","text":"<pre><code>val_set = darts_segmentation.training.callbacks.BinarySegmentationPreview(\n    val_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.is_val_plot_epoch","title":"is_val_plot_epoch","text":"<pre><code>is_val_plot_epoch(\n    current_epoch: int, check_val_every_n_epoch: int | None\n) -&gt; bool\n</code></pre> <p>Check if the current epoch is an epoch where validation samples should be plotted.</p> <p>Parameters:</p> <ul> <li> <code>current_epoch</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int | None</code>)           \u2013            <p>The number of epochs to check for plotting. If None, no plotting is done.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the current epoch is a plot epoch, False otherwise.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def is_val_plot_epoch(self, current_epoch: int, check_val_every_n_epoch: int | None) -&gt; bool:\n    \"\"\"Check if the current epoch is an epoch where validation samples should be plotted.\n\n    Args:\n        current_epoch (int): The current epoch.\n        check_val_every_n_epoch (int | None): The number of epochs to check for plotting.\n            If None, no plotting is done.\n\n    Returns:\n        bool: True if the current epoch is a plot epoch, False otherwise.\n\n    \"\"\"\n    if check_val_every_n_epoch is None:\n        return False\n\n    n = self.plot_every_n_val_epochs * check_val_every_n_epoch\n    return ((current_epoch + 1) % n) == 0 or current_epoch == 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.on_test_batch_end","title":"on_test_batch_end","text":"<pre><code>on_test_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    # Only do this every self.plot_every_n_val_epochs epochs\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if not is_val_plot_epoch:\n        return\n\n    x, y = batch\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'test_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    # Create figures for the samples (plot at maximum 30)\n    # We want to plot at max 20 POSITIVE samples and 10 NEGATIVE samples in a single epoch\n    # These should also be the same over all epochs\n    for i in range(x.shape[0]):\n        if self._test_pos_visualizations &gt;= 20 and self._test_neg_visualizations &gt;= 10:\n            break\n\n        # Plot positive sample\n        if y[i].sum() &gt; 0 and self._test_pos_visualizations &lt; 20:\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.band_names)\n            self._test_pos_visualizations += 1\n        # Plot negative sample\n        elif y[i].sum() == 0 and self._test_neg_visualizations &lt; 10:\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.band_names)\n            self._test_neg_visualizations += 1\n        # Either the number of positive or negative samples is already full\n        else:\n            continue\n\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.test_set}-samples\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig.savefig(fig_dir / f\"sample_{pl_module.global_step}_{batch_idx}_{i}.png\")\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                # We don't commit the log yet, so that the step is increased with the next lightning log\n                # Which happens at the end of the validation epoch\n                img_name = f\"{self.test_set}-samples/sample_{batch_idx}_{i}\"\n                wandb_run.log({img_name: wandb.Image(fig)}, commit=False)\n        fig.clear()\n        plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    self._test_pos_visualizations = 0\n    self._test_neg_visualizations = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.on_validation_batch_end","title":"on_validation_batch_end","text":"<pre><code>on_validation_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    # Only do this every self.plot_every_n_val_epochs epochs\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if not is_val_plot_epoch:\n        return\n\n    x, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'validation_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    # Create figures for the samples (plot at maximum 30)\n    # We want to plot at max 20 POSITIVE samples and 10 NEGATIVE samples in a single epoch\n    # These should also be the same over all epochs\n    for i in range(x.shape[0]):\n        if self._val_pos_visualizations &gt;= 20 and self._val_neg_visualizations &gt;= 10:\n            break\n\n        # Don't plot in sanity check\n        if trainer.state.stage == \"sanity_check\":\n            break\n\n        # Plot positive sample\n        is_postive = (y[i] == 1).sum() &gt; 0\n        if is_postive and self._val_pos_visualizations &lt; 20:\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.band_names)\n            self._val_pos_visualizations += 1\n        # Plot negative sample\n        elif not is_postive and self._val_neg_visualizations &lt; 10:\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.band_names)\n            self._val_neg_visualizations += 1\n        # Either the number of positive or negative samples is already full\n        else:\n            continue\n\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.val_set}-samples\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig.savefig(fig_dir / f\"sample_{pl_module.global_step}_{batch_idx}_{i}.png\")\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                # We don't commit the log yet, so that the step is increased with the next lightning log\n                # Which happens at the end of the validation epoch\n                img_name = f\"{self.val_set}-samples/sample_{batch_idx}_{i}\"\n                wandb_run.log({img_name: wandb.Image(fig)}, commit=False)\n        fig.clear()\n        plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    self._val_pos_visualizations = 0\n    self._val_neg_visualizations = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.BinarySegmentationPreview.setup","title":"setup","text":"<pre><code>setup(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> <p>Setups the callback.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>lightning.Trainer</code>)           \u2013            <p>The lightning trainer.</p> </li> <li> <code>pl_module</code>               (<code>lightning.LightningModule</code>)           \u2013            <p>The lightning module.</p> </li> <li> <code>stage</code>               (<code>typing.Literal['fit', 'validate', 'test', 'predict']</code>)           \u2013            <p>The current stage. One of: \"fit\", \"validate\", \"test\", \"predict\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):\n    \"\"\"Setups the callback.\n\n    Args:\n        trainer (Trainer): The lightning trainer.\n        pl_module (LightningModule): The lightning module.\n        stage (Literal[\"fit\", \"validate\", \"test\", \"predict\"]): The current stage.\n            One of: \"fit\", \"validate\", \"test\", \"predict\".\n\n    \"\"\"\n    # We don't want to use memory in the predict stage\n    if stage == \"predict\":\n        return\n\n    # Validation metrics and visualizations for the fit and validate stages\n    if stage == \"fit\" or stage == \"validate\":\n        # Internal state to track how many visualizations have been generated in an epoch\n        self._val_pos_visualizations = 0\n        self._val_neg_visualizations = 0\n\n    # Test metrics and visualizations for the test stage\n    if stage == \"test\":\n        # Internal state to track how many visualizations have been generated in an epoch\n        self._test_pos_visualizations = 0\n        self._test_neg_visualizations = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/callbacks/#darts_segmentation.training.callbacks.plot_sample","title":"plot_sample","text":"<pre><code>plot_sample(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    y_pred: torch.Tensor,\n    band_names: list[str],\n) -&gt; tuple[\n    matplotlib.pyplot.Figure,\n    dict[str, matplotlib.pyplot.Axes],\n]\n</code></pre> <p>Plot a single sample with the input, the ground truth and the prediction.</p> <p>This function does a few expections on the input: - The input is expected to be normalized to 0-1. - The prediction is expected to be converted from logits to prediction. - The target is expected to be a int or long tensor with values of:     0 (negative class)     1 (positive class) and     2 (invalid pixels).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor [C, H, W] (float).</p> </li> <li> <code>y</code>               (<code>torch.Tensor</code>)           \u2013            <p>The ground truth tensor [H, W] (int).</p> </li> <li> <code>y_pred</code>               (<code>torch.Tensor</code>)           \u2013            <p>The prediction tensor [H, W] (float).</p> </li> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>The combinations of the input bands.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[matplotlib.pyplot.Figure, dict[str, matplotlib.pyplot.Axes]]</code>           \u2013            <p>tuple[Figure, dict[str, Axes]]: The figure and the axes of the plot.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/viz.py</code> <pre><code>def plot_sample(\n    x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor, band_names: list[str]\n) -&gt; tuple[plt.Figure, dict[str, plt.Axes]]:\n    \"\"\"Plot a single sample with the input, the ground truth and the prediction.\n\n    This function does a few expections on the input:\n    - The input is expected to be normalized to 0-1.\n    - The prediction is expected to be converted from logits to prediction.\n    - The target is expected to be a int or long tensor with values of:\n        0 (negative class)\n        1 (positive class) and\n        2 (invalid pixels).\n\n    Args:\n        x (torch.Tensor): The input tensor [C, H, W] (float).\n        y (torch.Tensor): The ground truth tensor [H, W] (int).\n        y_pred (torch.Tensor): The prediction tensor [H, W] (float).\n        band_names (list[str]): The combinations of the input bands.\n\n    Returns:\n        tuple[Figure, dict[str, Axes]]: The figure and the axes of the plot.\n\n    \"\"\"\n    x = x.cpu()\n    y = y.cpu()\n    y_pred = y_pred.detach().cpu()\n\n    # Make y class 2 invalids (replace 2 with nan)\n    x = x.where(y != 2, torch.nan)\n    y_pred = y_pred.where(y != 2, torch.nan)\n    y = y.where(y != 2, torch.nan)\n\n    # pred == 0, y == 0 -&gt; 0 (true negative)\n    # pred == 1, y == 0 -&gt; 1 (false positive)\n    # pred == 0, y == 1 -&gt; 2 (false negative)\n    # pred == 1, y == 1 -&gt; 3 (true positive)\n    classification_labels = (y_pred &gt; 0.5).int() + y * 2\n    classification_labels = classification_labels.where(classification_labels != 0, torch.nan)\n\n    # Calculate f1 and iou\n    true_positive = (classification_labels == 3).sum()\n    false_positive = (classification_labels == 1).sum()\n    false_negative = (classification_labels == 2).sum()\n    true_negative = (classification_labels == 0).sum()\n    acc = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n    f1 = 2 * true_positive / (2 * true_positive + false_positive + false_negative)\n    iou = true_positive / (true_positive + false_positive + false_negative)\n\n    cmap = mcolors.ListedColormap([\"#cd43b2\", \"#3e0f2f\", \"#6cd875\"])\n    fig, axs = plt.subplot_mosaic(\n        # [[\"rgb\", \"rgb\", \"ndvi\", \"tcvis\", \"stats\"], [\"rgb\", \"rgb\", \"pred\", \"slope\", \"elev\"]],\n        [[\"rgb\", \"rgb\", \"pred\", \"tcvis\"], [\"rgb\", \"rgb\", \"ndvi\", \"slope\"], [\"none\", \"stats\", \"stats\", \"stats\"]],\n        # layout=\"constrained\",\n        figsize=(11, 8),\n    )\n\n    # Disable none plot\n    axs[\"none\"].axis(\"off\")\n\n    # RGB Plot\n    ax_rgb = axs[\"rgb\"]\n    # disable axis\n    ax_rgb.axis(\"off\")\n    is_rgb = \"red\" in band_names and \"green\" in band_names and \"blue\" in band_names\n    if is_rgb:\n        red_band = band_names.index(\"red\")\n        green_band = band_names.index(\"green\")\n        blue_band = band_names.index(\"blue\")\n        rgb = x[[red_band, green_band, blue_band]].transpose(0, 2).transpose(0, 1)\n        ax_rgb.imshow(rgb ** (1 / 1.4))\n        ax_rgb.set_title(f\"Acc: {acc:.1%} F1: {f1:.1%} IoU: {iou:.1%}\")\n    else:\n        # Plot empty with message that RGB is not provided\n        ax_rgb.set_title(\"No RGB values are provided!\")\n    ax_rgb.imshow(classification_labels, alpha=0.6, cmap=cmap, vmin=1, vmax=3)\n    # Add a legend\n    patches = [\n        mpatches.Patch(color=\"#6cd875\", label=\"True Positive\"),\n        mpatches.Patch(color=\"#3e0f2f\", label=\"False Negative\"),\n        mpatches.Patch(color=\"#cd43b2\", label=\"False Positive\"),\n    ]\n    ax_rgb.legend(handles=patches, loc=\"upper left\")\n\n    # NDVI Plot\n    ax_ndvi = axs[\"ndvi\"]\n    ax_ndvi.axis(\"off\")\n    is_ndvi = \"ndvi\" in band_names\n    if is_ndvi:\n        ndvi_band = band_names.index(\"ndvi\")\n        ndvi = x[ndvi_band]\n        ax_ndvi.imshow(ndvi, vmin=0, vmax=1, cmap=\"RdYlGn\")\n        ax_ndvi.set_title(\"NDVI\")\n    else:\n        # Plot empty with message that NDVI is not provided\n        ax_ndvi.set_title(\"No NDVI values are provided!\")\n\n    # TCVIS Plot\n    ax_tcv = axs[\"tcvis\"]\n    ax_tcv.axis(\"off\")\n    is_tcvis = \"tc_brightness\" in band_names and \"tc_greenness\" in band_names and \"tc_wetness\" in band_names\n    if is_tcvis:\n        tcb_band = band_names.index(\"tc_brightness\")\n        tcg_band = band_names.index(\"tc_greenness\")\n        tcw_band = band_names.index(\"tc_wetness\")\n        tcvis = x[[tcb_band, tcg_band, tcw_band]].transpose(0, 2).transpose(0, 1)\n        ax_tcv.imshow(tcvis)\n        ax_tcv.set_title(\"TCVIS\")\n    else:\n        ax_tcv.set_title(\"No TCVIS values are provided!\")\n\n    # Statistics Plot\n    ax_stat = axs[\"stats\"]\n    if (y == 1).sum() &gt; 0:\n        n_bands = x.shape[0]\n        n_pixel = x.shape[1] * x.shape[2]\n        x_flat = x.flatten().cpu()\n        y_flat = y.flatten().repeat(n_bands).cpu()\n        bands = list(itertools.chain.from_iterable([band_names[i]] * n_pixel for i in range(n_bands)))\n        plot_data = pd.DataFrame({\"x\": x_flat, \"y\": y_flat, \"band\": bands})\n        if len(plot_data) &gt; 50000:\n            plot_data = plot_data.sample(50000)\n        plot_data = plot_data.sort_values(\"band\")\n        sns.violinplot(\n            x=\"x\",\n            y=\"band\",\n            hue=\"y\",\n            data=plot_data,\n            split=True,\n            inner=\"quart\",\n            fill=False,\n            palette={1: \"g\", 0: \".35\"},\n            density_norm=\"width\",\n            ax=ax_stat,\n        )\n        ax_stat.set_title(\"Band Statistics\")\n    else:\n        ax_stat.set_title(\"No positive labels in this sample!\")\n        ax_stat.axis(\"off\")\n\n    # Prediction Plot\n    ax_mask = axs[\"pred\"]\n    ax_mask.imshow(y_pred, vmin=0, vmax=1)\n    ax_mask.axis(\"off\")\n    ax_mask.set_title(\"Model Output\")\n\n    # Slope Plot\n    ax_slope = axs[\"slope\"]\n    ax_slope.axis(\"off\")\n    is_slope = \"slope\" in band_names\n    if is_slope:\n        slope_band = band_names.index(\"slope\")\n        slope = x[slope_band]\n        ax_slope.imshow(slope, cmap=\"cividis\")\n        # Add TPI as contour lines\n        is_rel_elev = \"relative_elevation\" in band_names\n        if is_rel_elev:\n            rel_elev_band = band_names.index(\"relative_elevation\")\n            rel_elev = x[rel_elev_band]\n            cs = ax_slope.contour(rel_elev, [0], colors=\"red\", linewidths=0.3, alpha=0.6)\n            ax_slope.clabel(cs, inline=True, fontsize=5, fmt=\"%.1f\")\n\n        ax_slope.set_title(\"Slope\")\n    else:\n        # Plot empty with message that slope is not provided\n        ax_slope.set_title(\"No Slope values are provided!\")\n\n    # Relative Elevation Plot\n    # rel_elev_band = band_names.index(\"relative_elevation\")\n    # rel_elev = x[rel_elev_band]\n    # ax_rel_elev = axs[\"elev\"]\n    # ax_rel_elev.imshow(rel_elev, cmap=\"cividis\")\n    # ax_rel_elev.axis(\"off\")\n    # ax_rel_elev.set_title(\"Relative Elevation\")\n\n    return fig, axs\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/","title":"darts_segmentation.training.cv","text":""},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv","title":"darts_segmentation.training.cv","text":"<p>Cross-validation implementation for binary segmentation.</p>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.available_devices","title":"available_devices  <code>module-attribute</code>","text":"<pre><code>available_devices = multiprocessing.Queue()\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig","title":"CrossValidationConfig  <code>dataclass</code>","text":"<pre><code>CrossValidationConfig(\n    n_folds: int | None = None,\n    n_randoms: int = 3,\n    scoring_metric: list[str] = lambda: [\n        \"val/JaccardIndex\",\n        \"val/Recall\",\n    ](),\n    multi_score_strategy: typing.Literal[\n        \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n    ] = \"harmonic\",\n)\n</code></pre> <p>Configuration for cross-validation.</p> <p>This is used to configure the cross-validation process. It is used by the <code>cross_validation_smp</code> function.</p> <p>Attributes:</p> <ul> <li> <code>n_folds</code>               (<code>int | None</code>)           \u2013            <p>Number of folds to perform in cross-validation. If None, all folds (total_folds) will be used. Defaults to None.</p> </li> <li> <code>n_randoms</code>               (<code>int</code>)           \u2013            <p>Number of random seeds to perform in cross-validation. First three seeds are always 42, 21, 69, further seeds are deterministic generated. Defaults to 3.</p> </li> <li> <code>scoring_metric</code>               (<code>list[str]</code>)           \u2013            <p>Metric(s) to use for scoring. Defaults to [\"val/JaccardIndex\", \"val/Recall\"].</p> </li> <li> <code>multi_score_strategy</code>               (<code>typing.Literal['harmonic', 'arithmetic', 'geometric', 'min']</code>)           \u2013            <p>Strategy for combining multiple metrics. Defaults to \"harmonic\".</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.multi_score_strategy","title":"multi_score_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>multi_score_strategy: typing.Literal[\n    \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n] = \"harmonic\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.n_folds","title":"n_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_folds: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.n_randoms","title":"n_randoms  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_randoms: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.rng_seeds","title":"rng_seeds  <code>property</code>","text":"<pre><code>rng_seeds: list[int]\n</code></pre> <p>Generate a list of seeds for cross-validation.</p> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>list[int]: A list of seeds for cross-validation.</p> </li> <li> <code>list[int]</code>           \u2013            <p>The first three seeds are always 42, 21, 69, further seeds are deterministically generated.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.CrossValidationConfig.scoring_metric","title":"scoring_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scoring_metric: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"val/JaccardIndex\",\n        \"val/Recall\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig","title":"DataConfig  <code>dataclass</code>","text":"<pre><code>DataConfig(\n    train_data_dir: pathlib.Path = pathlib.Path(\"train\"),\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n    ] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n)\n</code></pre> <p>Data related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script. Defaults to \"train\".</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>)           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>)           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified']</code>)           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>)           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>)           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.data_split_by","title":"data_split_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_by: list[str | float] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.data_split_method","title":"data_split_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_method: (\n    typing.Literal[\"random\", \"region\", \"sample\"] | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.fold_method","title":"fold_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold_method: typing.Literal[\n    \"kfold\",\n    \"shuffle\",\n    \"stratified\",\n    \"region\",\n    \"region-stratified\",\n] = \"kfold\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.subsample","title":"subsample  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subsample: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.total_folds","title":"total_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_folds: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DataConfig.train_data_dir","title":"train_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_data_dir: pathlib.Path = pathlib.Path('train')\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig","title":"DeviceConfig  <code>dataclass</code>","text":"<pre><code>DeviceConfig(\n    accelerator: typing.Literal[\n        \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n    ] = \"auto\",\n    strategy: typing.Literal[\n        \"auto\",\n        \"ddp\",\n        \"ddp_fork\",\n        \"ddp_notebook\",\n        \"fsdp\",\n        \"cv-parallel\",\n        \"tune-parallel\",\n    ] = \"auto\",\n    devices: list[int | str] = lambda: [\"auto\"](),\n    num_nodes: int = 1,\n)\n</code></pre> <p>Device and Distributed Strategy related parameters.</p> <p>Attributes:</p> <ul> <li> <code>accelerator</code>               (<code>typing.Literal['auto', 'cpu', 'gpu', 'mps', 'tpu']</code>)           \u2013            <p>Accelerator to use. Defaults to \"auto\".</p> </li> <li> <code>strategy</code>               (<code>typing.Literal['auto', 'ddp', 'ddp_fork', 'ddp_notebook', 'fsdp', 'cv-parallel', 'tune-parallel', 'cv-parallel', 'tune-parallel']</code>)           \u2013            <p>Distributed strategy to use. Defaults to \"auto\".</p> </li> <li> <code>devices</code>               (<code>list[int | str]</code>)           \u2013            <p>List of devices to use. Defaults to [\"auto\"].</p> </li> <li> <code>num_nodes</code>               (<code>int</code>)           \u2013            <p>Number of nodes to use for distributed training. Defaults to 1.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.accelerator","title":"accelerator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>accelerator: typing.Literal[\n    \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int | str] = dataclasses.field(\n    default_factory=lambda: [\"auto\"]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.lightning_strategy","title":"lightning_strategy  <code>property</code>","text":"<pre><code>lightning_strategy: str\n</code></pre> <p>Get the Lightning strategy for the current configuration.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The Lightning strategy to use.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.num_nodes","title":"num_nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_nodes: int = 1\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.strategy","title":"strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strategy: typing.Literal[\n    \"auto\",\n    \"ddp\",\n    \"ddp_fork\",\n    \"ddp_notebook\",\n    \"fsdp\",\n    \"cv-parallel\",\n    \"tune-parallel\",\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.DeviceConfig.in_parallel","title":"in_parallel","text":"<pre><code>in_parallel(\n    device: int | str | None = None,\n) -&gt; darts_segmentation.training.train.DeviceConfig\n</code></pre> <p>Turn the current configuration into a suitable configuration for parallel training.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to use for parallel training. If None, assumes non-multiprocessing parallel training and propagate all devices. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceConfig</code> (              <code>darts_segmentation.training.train.DeviceConfig</code> )          \u2013            <p>A new DeviceConfig instance that is suitable for parallel training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def in_parallel(self, device: int | str | None = None) -&gt; \"DeviceConfig\":\n    \"\"\"Turn the current configuration into a suitable configuration for parallel training.\n\n    Args:\n        device (int | str | None, optional): The device to use for parallel training.\n            If None, assumes non-multiprocessing parallel training and propagate all devices.\n            Defaults to None.\n\n    Returns:\n        DeviceConfig: A new DeviceConfig instance that is suitable for parallel training.\n\n    \"\"\"\n    # In case of parallel training via multiprocessing, only few strategies are allowed.\n    if self.strategy in [\"ddp\", \"ddp_fork\", \"ddp_notebook\", \"fsdp\"]:\n        logger.warning(\"Using 'ddp_fork' instead of 'ddp' for multiprocessing.\")\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=\"ddp_fork\",  # Fork is the only supported strategy for multiprocessing\n            devices=self.devices,\n            num_nodes=self.num_nodes,\n        )\n    elif device is not None:\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=self.strategy,\n            # If a device is specified, we assume that we want to run on a single device\n            devices=[device],\n            num_nodes=1,\n        )\n    else:\n        return self\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters","title":"Hyperparameters  <code>dataclass</code>","text":"<pre><code>Hyperparameters(\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Hyperparameters for Cyclopts CLI.</p> <p>Attributes:</p> <ul> <li> <code>model_arch</code>               (<code>str</code>)           \u2013            <p>Architecture of the model to use.</p> </li> <li> <code>model_encoder</code>               (<code>str</code>)           \u2013            <p>Encoder type for the model.</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>)           \u2013            <p>Weights for the encoder, if any.</p> </li> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>)           \u2013            <p>Learning rate for training.</p> </li> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Decay factor for learning rate.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float | None</code>)           \u2013            <p>Alpha parameter for focal loss, if using.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>)           \u2013            <p>Gamma parameter for focal loss.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>)           \u2013            <p>List of bands to use. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.augment","title":"augment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>augment: (\n    list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.bands","title":"bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bands: list[str] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.focal_loss_alpha","title":"focal_loss_alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_alpha: float | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.focal_loss_gamma","title":"focal_loss_gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_gamma: float = 2.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.9\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.001\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.model_arch","title":"model_arch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_arch: str = 'Unet'\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.model_encoder","title":"model_encoder  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder: str = 'dpn107'\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.Hyperparameters.model_encoder_weights","title":"model_encoder_weights  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder_weights: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig","title":"LoggingConfig  <code>dataclass</code>","text":"<pre><code>LoggingConfig(\n    artifact_dir: pathlib.Path = pathlib.Path(\"artifacts\"),\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Logging related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Top-level path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"artifacts\").</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>)           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>)           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>)           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.artifact_dir","title":"artifact_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>artifact_dir: pathlib.Path = pathlib.Path('artifacts')\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.check_val_every_n_epoch","title":"check_val_every_n_epoch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>check_val_every_n_epoch: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.log_every_n_steps","title":"log_every_n_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_every_n_steps: int = 10\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.wandb_entity","title":"wandb_entity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_entity: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.wandb_project","title":"wandb_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_project: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.artifact_dir_at_cv","title":"artifact_dir_at_cv","text":"<pre><code>artifact_dir_at_cv(tune_name: str | None) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory for cross-validation runs.</p> <p>Similar to <code>parse_artifact_dir_for_run</code>, but meant to be used by the cross-validation script.</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning, if applicable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path for cross-validation runs.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_cv(self, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory for cross-validation runs.\n\n    Similar to `parse_artifact_dir_for_run`, but meant to be used by the cross-validation script.\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        tune_name (str | None): Name of the tuning, if applicable.\n\n    Returns:\n        Path: The nested artifact directory path for cross-validation runs.\n\n    \"\"\"\n    artifact_dir = self.artifact_dir / tune_name if tune_name else self.artifact_dir / \"_cross_validations\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.LoggingConfig.artifact_dir_at_run","title":"artifact_dir_at_run","text":"<pre><code>artifact_dir_at_run(\n    cv_name: str | None, tune_name: str | None\n) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory to avoid cluttering the root directory.</p> <p>For cv it is expected that the cv function already nests the artifact directory Meaning for cv the artifact_dir of this function should be either {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If tune_name is specified, but cv_name is not, which is invalid.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_run(self, cv_name: str | None, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory to avoid cluttering the root directory.\n\n    For cv it is expected that the cv function already nests the artifact directory\n    Meaning for cv the artifact_dir of this function should be either\n    {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        cv_name (str | None): Name of the cross-validation.\n        tune_name (str | None): Name of the tuning.\n\n    Raises:\n        ValueError: If tune_name is specified, but cv_name is not, which is invalid.\n\n    Returns:\n        Path: The nested artifact directory path.\n\n    \"\"\"\n    # Run only\n    if cv_name is None and tune_name is None:\n        artifact_dir = self.artifact_dir / \"_runs\"\n    # Cross-validation only\n    elif cv_name is not None and tune_name is None:\n        artifact_dir = self.artifact_dir / \"_cross_validations\" / cv_name\n    # Cross-validation and tuning\n    elif cv_name is not None and tune_name is not None:\n        artifact_dir = self.artifact_dir / tune_name / cv_name\n    # Tuning only (invalid)\n    else:\n        raise ValueError(\n            \"Cannot parse artifact directory for cross-validation and tuning. \"\n            \"Please specify either cv_name or tune_name, but not both.\"\n        )\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig","title":"TrainRunConfig  <code>dataclass</code>","text":"<pre><code>TrainRunConfig(\n    name: str | None = None,\n    cv_name: str | None = None,\n    tune_name: str | None = None,\n    fold: int = 0,\n    random_seed: int = 42,\n)\n</code></pre> <p>Run related parameters for training.</p> <p>Defines the script inputs for the training script. Must be build by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str | None</code>)           \u2013            <p>Name of the run. If None is generated automatically. Defaults to None.</p> </li> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation. Should only be specified by a cross-validation script. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>fold</code>               (<code>int</code>)           \u2013            <p>Index of the current fold. Defaults to 0.</p> </li> <li> <code>random_seed</code>               (<code>int</code>)           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.cv_name","title":"cv_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cv_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.fold","title":"fold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: int = 42\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainRunConfig.tune_name","title":"tune_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<pre><code>TrainingConfig(\n    continue_from_checkpoint: pathlib.Path | None = None,\n    max_epochs: int = 100,\n    early_stopping_patience: int = 5,\n    num_workers: int = 0,\n)\n</code></pre> <p>Training related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Path to a checkpoint to continue training from. Defaults to None.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>)           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>)           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.continue_from_checkpoint","title":"continue_from_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>continue_from_checkpoint: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.early_stopping_patience","title":"early_stopping_patience  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>early_stopping_patience: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.max_epochs","title":"max_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_epochs: int = 100\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.TrainingConfig.num_workers","title":"num_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_workers: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs","title":"_ProcessInputs  <code>dataclass</code>","text":"<pre><code>_ProcessInputs(\n    current: int,\n    total: int,\n    seed: int,\n    fold: int,\n    cv: darts_segmentation.training.cv.CrossValidationConfig,\n    run: darts_segmentation.training.train.TrainRunConfig,\n    training_config: darts_segmentation.training.train.TrainingConfig,\n    logging_config: darts_segmentation.training.train.LoggingConfig,\n    data_config: darts_segmentation.training.train.DataConfig,\n    device_config: darts_segmentation.training.train.DeviceConfig,\n    hparams: darts_segmentation.training.hparams.Hyperparameters,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.current","title":"current  <code>instance-attribute</code>","text":"<pre><code>current: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.cv","title":"cv  <code>instance-attribute</code>","text":"<pre><code>cv: darts_segmentation.training.cv.CrossValidationConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.data_config","title":"data_config  <code>instance-attribute</code>","text":"<pre><code>data_config: darts_segmentation.training.train.DataConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.device_config","title":"device_config  <code>instance-attribute</code>","text":"<pre><code>device_config: (\n    darts_segmentation.training.train.DeviceConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.fold","title":"fold  <code>instance-attribute</code>","text":"<pre><code>fold: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.hparams","title":"hparams  <code>instance-attribute</code>","text":"<pre><code>hparams: darts_segmentation.training.hparams.Hyperparameters\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.logging_config","title":"logging_config  <code>instance-attribute</code>","text":"<pre><code>logging_config: (\n    darts_segmentation.training.train.LoggingConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.run","title":"run  <code>instance-attribute</code>","text":"<pre><code>run: darts_segmentation.training.train.TrainRunConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.seed","title":"seed  <code>instance-attribute</code>","text":"<pre><code>seed: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.total","title":"total  <code>instance-attribute</code>","text":"<pre><code>total: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessInputs.training_config","title":"training_config  <code>instance-attribute</code>","text":"<pre><code>training_config: (\n    darts_segmentation.training.train.TrainingConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessOutputs","title":"_ProcessOutputs  <code>dataclass</code>","text":"<pre><code>_ProcessOutputs(run_info: dict)\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._ProcessOutputs.run_info","title":"run_info  <code>instance-attribute</code>","text":"<pre><code>run_info: dict\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv._run_training","title":"_run_training","text":"<pre><code>_run_training(\n    inp: darts_segmentation.training.cv._ProcessInputs,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/cv.py</code> <pre><code>def _run_training(inp: _ProcessInputs):\n    # Wrapper function for handling parallel multiprocessing training runs.\n    import torch\n\n    from darts_segmentation.training.scoring import check_score_is_unstable\n    from darts_segmentation.training.train import train_smp\n\n    # Setup device configuration: If strategy is \"cv-parallel\" expect a mp scenario:\n    # Wait for a device to become available.\n    # Otherwise, expect a serial scenario, where the devices and strategy are set by the user.\n    is_parallel = inp.device_config.strategy == \"cv-parallel\"\n    if is_parallel:\n        device = available_devices.get()\n        device_config = inp.device_config.in_parallel(device)\n        logger.debug(f\"Starting run {inp.run.name} ({inp.current + 1}/{inp.total}) on device {device}.\")\n    else:\n        device = None\n        device_config = inp.device_config.in_parallel()\n        logger.debug(f\"Starting run {inp.run.name} ({inp.current + 1}/{inp.total}).\")\n\n    try:\n        tick_rstart = time.time()\n        trainer = train_smp(\n            run=inp.run,\n            training_config=inp.training_config,\n            data_config=inp.data_config,\n            device_config=device_config,\n            hparams=inp.hparams,\n            logging_config=inp.logging_config,\n        )\n        tick_rend = time.time()\n\n        run_info = {\n            \"run_name\": inp.run.name,\n            \"run_id\": trainer.lightning_module.hparams[\"run_id\"],\n            \"seed\": inp.seed,\n            \"fold\": inp.fold,\n            \"duration\": tick_rend - tick_rstart,\n        }\n        for metric, value in trainer.logged_metrics.items():\n            run_info[metric] = value.item() if isinstance(value, torch.Tensor) else value\n        if trainer.checkpoint_callback:\n            run_info[\"checkpoint\"] = trainer.checkpoint_callback.best_model_path\n        run_info[\"is_unstable\"] = check_score_is_unstable(run_info, inp.cv.scoring_metric)\n\n        logger.debug(f\"{run_info=}\")\n        output = _ProcessOutputs(run_info=run_info)\n    finally:\n        # If we are in parallel mode, we need to return the device to the queue.\n        if is_parallel:\n            logger.debug(f\"Free device {device} for cv {inp.run.name}\")\n            available_devices.put(device)\n    return output\n</code></pre>"},{"location":"reference/darts_segmentation/training/cv/#darts_segmentation.training.cv.cross_validation_smp","title":"cross_validation_smp","text":"<pre><code>cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    cv: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n)\n</code></pre> <p>Perform cross-validation for a model with given hyperparameters.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This cross-validation function is designed to evaluate the performance of a single model configuration. It can be used by a tuning script to tune hyperparameters. It calls the training function, hence most functionality is the same as the training function. In general, it does perform this:</p> <pre><code>for seed in seeds:\n    for fold in folds:\n        train_model(seed=seed, fold=fold, ...)\n</code></pre> <p>and calculates a score from the results.</p> <p>To specify on which metric(s) the score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code> for tunes (meaning if <code>tune_name</code> is not None) else <code>{artifact_dir}/_cross_validation</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation. If None, a name is generated automatically. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>cv</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for the training.</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for the data.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for the devices to use.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable, and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no runs were performed, meaning the configuration is invalid or no data was found.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/cv.py</code> <pre><code>def cross_validation_smp(\n    *,\n    name: str | None = None,\n    tune_name: str | None = None,\n    cv: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n    logging_config: LoggingConfig = LoggingConfig(),\n):\n    \"\"\"Perform cross-validation for a model with given hyperparameters.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This cross-validation function is designed to evaluate the performance of a single model configuration.\n    It can be used by a tuning script to tune hyperparameters.\n    It calls the training function, hence most functionality is the same as the training function.\n    In general, it does perform this:\n\n    ```py\n    for seed in seeds:\n        for fold in folds:\n            train_model(seed=seed, fold=fold, ...)\n    ```\n\n    and calculates a score from the results.\n\n    To specify on which metric(s) the score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}` for tunes (meaning if `tune_name` is not None)\n    else `{artifact_dir}/_cross_validation`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the cross-validation. If None, a name is generated automatically.\n            Defaults to None.\n        tune_name (str | None, optional): Name of the tuning. Should only be specified by a tuning script.\n            Defaults to None.\n        cv (CrossValidationConfig): Configuration for cross-validation.\n        training_config (TrainingConfig): Configuration for the training.\n        data_config (DataConfig): Configuration for the data.\n        device_config (DeviceConfig): Configuration for the devices to use.\n        hparams (Hyperparameters): Hyperparameters for the training.\n        logging_config (LoggingConfig): Logging configuration.\n\n    Returns:\n        tuple[float, bool, pd.DataFrame]: A single score, a boolean indicating if the score is unstable,\n            and a DataFrame containing run info (seed, fold, metrics, duration, checkpoint)\n\n    Raises:\n        ValueError: If no runs were performed, meaning the configuration is invalid or no data was found.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.scoring import score_from_runs\n\n    tick_fstart = time.perf_counter()\n\n    artifact_dir = logging_config.artifact_dir_at_cv(tune_name)\n    cv_name = name or generate_counted_name(artifact_dir)\n    artifact_dir = artifact_dir / cv_name\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    n_folds = cv.n_folds or data_config.total_folds\n\n    logger.info(\n        f\"Starting cross-validation '{cv_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_randoms*n_folds = {cv.n_randoms}*{n_folds} = {cv.n_randoms * n_folds} experiments.\"\n    )\n\n    seeds = cv.rng_seeds\n    logger.debug(f\"Using seeds: {seeds}\")\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs: list[_ProcessInputs] = []\n    for i, seed in enumerate(seeds):\n        for fold in range(n_folds):\n            current = i * len(seeds) + fold\n            total = n_folds * len(seeds)\n            run = TrainRunConfig(\n                name=f\"{cv_name}-run-f{fold}s{seed}\",\n                cv_name=cv_name,\n                tune_name=tune_name,\n                fold=fold,\n                random_seed=seed,\n            )\n            process_inputs.append(\n                _ProcessInputs(\n                    current=current,\n                    total=total,\n                    seed=seed,\n                    fold=fold,\n                    cv=cv,\n                    run=run,\n                    training_config=training_config,\n                    logging_config=logging_config,\n                    data_config=data_config,\n                    device_config=device_config,\n                    hparams=hparams,\n                )\n            )\n\n    run_infos = []\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"cv-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_training,\n    ):\n        run_infos.append(output.run_info)\n\n    if len(run_infos) == 0:\n        raise ValueError(\n            \"No runs were performed. Please check your configuration and data.\"\n            \" If you are using a tuning script, make sure to specify the correct parameters.\"\n        )\n\n    logger.debug(f\"{run_infos=}\")\n    score = score_from_runs(run_infos, cv.scoring_metric, cv.multi_score_strategy)\n\n    run_infos = pd.DataFrame(run_infos)\n    run_infos[\"score\"] = score\n    is_unstable = run_infos[\"is_unstable\"].any()\n    run_infos[\"score_is_unstable\"] = is_unstable\n    if is_unstable:\n        logger.warning(\"Score is unstable, meaning at least one of the metrics is NaN, Inf, -Inf or 0.\")\n    run_infos.to_parquet(artifact_dir / \"run_infos.parquet\")\n    logger.debug(f\"Saved run infos to {artifact_dir / 'run_infos.parquet'}\")\n\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Finished cross-validation '{cv_name}' in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, is_unstable, run_infos\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/","title":"darts_segmentation.training.data","text":""},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data","title":"darts_segmentation.training.data","text":"<p>Training script for DARTS segmentation.</p>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Augmentation","title":"Augmentation  <code>module-attribute</code>","text":"<pre><code>Augmentation = typing.Literal[\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\",\n    \"Posterize\",\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands","title":"Bands","text":"<p>               Bases: <code>collections.UserList[darts_segmentation.utils.Band]</code></p> <p>Wrapper for the list of bands.</p>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.factors","title":"factors  <code>property</code>","text":"<pre><code>factors: list[float]\n</code></pre> <p>Get the factors of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The factors of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.names","title":"names  <code>property</code>","text":"<pre><code>names: list[str]\n</code></pre> <p>Get the names of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: The names of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.offsets","title":"offsets  <code>property</code>","text":"<pre><code>offsets: list[float]\n</code></pre> <p>Get the offsets of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The offsets of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.__reduce__","title":"__reduce__","text":"<pre><code>__reduce__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __reduce__(self):  # noqa: D105\n    # This is needed to pickle (and unpickle) the Bands object as a dict\n    # This is needed, because this way we don't need to have this class present when unpickling\n    # a pytorch checkpoint\n    return (dict, (self.to_config(),))\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __repr__(self) -&gt; str:  # noqa: D105\n    band_info = \", \".join([f\"{band.name}(*{band.factor:.5f}+{band.offset:.5f})\" for band in self])\n    return f\"Bands({band_info})\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.filter","title":"filter","text":"<pre><code>filter(\n    band_names: list[str],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Filter the bands by name.</p> <p>Parameters:</p> <ul> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>The names of the bands to keep.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The filtered Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def filter(self, band_names: list[str]) -&gt; \"Bands\":\n    \"\"\"Filter the bands by name.\n\n    Args:\n        band_names (list[str]): The names of the bands to keep.\n\n    Returns:\n        Bands: The filtered Bands object.\n\n    \"\"\"\n    return Bands([band for band in self if band.name in band_names])\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: dict[\n        typing.Literal[\n            \"bands\", \"band_factors\", \"band_offsets\"\n        ],\n        list,\n    ]\n    | dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a config dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>The config dictionary containing the band information. Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\", with the values to be lists of the same length.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list] | dict[str, tuple[float, float]],\n) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a config dictionary.\n\n    Args:\n        config (dict): The config dictionary containing the band information.\n            Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\",\n            with the values to be lists of the same length.\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    assert \"bands\" in config and \"band_factors\" in config and \"band_offsets\" in config, (\n        f\"Config must contain keys 'bands', 'band_factors' and 'band_offsets'.Got {config} instead.\"\n    )\n    return cls(\n        [\n            Band(name=name, factor=factor, offset=offset)\n            for name, factor, offset in zip(config[\"bands\"], config[\"band_factors\"], config[\"band_offsets\"])\n        ]\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    config: dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, tuple[float, float]]</code>)           \u2013            <p>The dictionary containing the band information. Expects the keys to be the band names and the values to be tuples of (factor, offset). Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: dict[str, tuple[float, float]]) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a dictionary.\n\n    Args:\n        config (dict[str, tuple[float, float]]): The dictionary containing the band information.\n            Expects the keys to be the band names and the values to be tuples of (factor, offset).\n            Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    return cls([Band(name=name, factor=factor, offset=offset) for name, (factor, offset) in config.items()])\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.to_config","title":"to_config","text":"<pre><code>to_config() -&gt; dict[\n    typing.Literal[\"bands\", \"band_factors\", \"band_offsets\"],\n    list,\n]\n</code></pre> <p>Convert the Bands object to a config dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[typing.Literal['bands', 'band_factors', 'band_offsets'], list]</code> )          \u2013            <p>The config dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_config(self) -&gt; dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list]:\n    \"\"\"Convert the Bands object to a config dictionary.\n\n    Returns:\n        dict: The config dictionary containing the band information.\n\n    \"\"\"\n    return {\n        \"bands\": [band.name for band in self],\n        \"band_factors\": [band.factor for band in self],\n        \"band_offsets\": [band.offset for band in self],\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.Bands.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, tuple[float, float]]\n</code></pre> <p>Convert the Bands object to a dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict[str, tuple[float, float]]</code>           \u2013            <p>dict[str, tuple[float, float]]: The dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_dict(self) -&gt; dict[str, tuple[float, float]]:\n    \"\"\"Convert the Bands object to a dictionary.\n\n    Returns:\n        dict[str, tuple[float, float]]: The dictionary containing the band information.\n\n    \"\"\"\n    return {band.name: (band.factor, band.offset) for band in self}\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule","title":"DartsDataModule","text":"<pre><code>DartsDataModule(\n    data_dir: pathlib.Path,\n    batch_size: int,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n    ]\n    | None = \"kfold\",\n    total_folds: int = 5,\n    fold: int = 0,\n    subsample: int | None = None,\n    bands: darts_segmentation.utils.Bands\n    | list[str]\n    | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    num_workers: int = 0,\n    in_memory: bool = False,\n)\n</code></pre> <p>               Bases: <code>lightning.LightningDataModule</code></p> <p>Initialize the data module.</p> <p>Supports spliting the data into train and test set while also defining cv-folds. Folding only applies to the non-test set and splits this into a train and validation set.</p> Example <ol> <li> <p>Normal train-validate. (Can also be used for testing on the complete dataset) <pre><code>dm = DartsDataModule(data_dir, batch_size)\n</code></pre></p> </li> <li> <p>Specifying a test split by random (20% of the data will be used for testing) <pre><code>dm = DartsDataModule(data_dir, batch_size, data_split_method=\"random\")\n</code></pre></p> </li> <li> <p>Specific fold for cross-validation (On the complete dataset, because data_split_method is \"none\"). This will be take the third of a total of7 folds to determine the validation set. <pre><code>dm = DartsDataModule(data_dir, batch_size, fold_method=\"region-stratified\", fold=2, total_folds=7)\n</code></pre></p> </li> </ol> <p>In general this should be used in combination with a cross-validation loop. <pre><code>for fold in range(total_folds):\n    dm = DartsDataModule(\n        data_dir,\n        batch_size,\n        fold_method=\"region-stratified\",\n        fold=fold,\n        total_folds=total_folds)\n    ...\n</code></pre></p> <ol> <li>Don't split anything -&gt; only train <pre><code>dm = DartsDataModule(data_dir, batch_size, fold_method=None)\n</code></pre></li> </ol> <p>Parameters:</p> <ul> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified'] | None</code>, default:                   <code>'kfold'</code> )           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>fold</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Index of the current fold. Defaults to 0.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> <li> <code>bands</code>               (<code>darts_segmentation.utils.Bands | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of bands to use. Expects the data_dir to contain a config.toml with a \"darts.bands\" key, with which the indices of the bands will be mapped to. Defaults to None.</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>Whether to augment the data. Does nothing for testing. Defaults to True.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for data loading. See torch.utils.data.DataLoader. Defaults to 0.</p> </li> <li> <code>in_memory</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to load the data into memory. Defaults to False.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path,\n    batch_size: int,\n    # data_split is for the test split\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str | float] | None = None,\n    # fold is for cross-validation split (train/val)\n    fold_method: Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\"] | None = \"kfold\",\n    total_folds: int = 5,\n    fold: int = 0,\n    subsample: int | None = None,\n    bands: Bands | list[str] | None = None,\n    augment: list[Augmentation] | None = None,  # Not used for val or test\n    num_workers: int = 0,\n    in_memory: bool = False,\n):\n    \"\"\"Initialize the data module.\n\n    Supports spliting the data into train and test set while also defining cv-folds.\n    Folding only applies to the non-test set and splits this into a train and validation set.\n\n    Example:\n        1. Normal train-validate. (Can also be used for testing on the complete dataset)\n        ```py\n        dm = DartsDataModule(data_dir, batch_size)\n        ```\n\n        2. Specifying a test split by random (20% of the data will be used for testing)\n        ```py\n        dm = DartsDataModule(data_dir, batch_size, data_split_method=\"random\")\n        ```\n\n        3. Specific fold for cross-validation (On the complete dataset, because data_split_method is \"none\").\n        This will be take the third of a total of7 folds to determine the validation set.\n        ```py\n        dm = DartsDataModule(data_dir, batch_size, fold_method=\"region-stratified\", fold=2, total_folds=7)\n        ```\n\n        In general this should be used in combination with a cross-validation loop.\n        ```py\n        for fold in range(total_folds):\n            dm = DartsDataModule(\n                data_dir,\n                batch_size,\n                fold_method=\"region-stratified\",\n                fold=fold,\n                total_folds=total_folds)\n            ...\n        ```\n\n        4. Don't split anything -&gt; only train\n        ```py\n        dm = DartsDataModule(data_dir, batch_size, fold_method=None)\n        ```\n\n    Args:\n        data_dir (Path): The path to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        batch_size (int): Batch size for training and validation.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the test size can be specified\n            by providing a list with a single a float between 0 and 1 to data_split_by\n            This will be the fraction of the data to be used for testing.\n            E.g. [0.2] will use 20% of the data for testing.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str | float] | None, optional): Select by which regions/samples to split or\n            the size of test set. Defaults to None.\n        fold_method (Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\"] | None, optional):\n            Method for cross-validation split. Defaults to \"kfold\".\n        total_folds (int, optional): Total number of folds in cross-validation. Defaults to 5.\n        fold (int, optional): Index of the current fold. Defaults to 0.\n        subsample (int | None, optional): If set, will subsample the dataset to this number of samples.\n            This is useful for debugging and testing. Defaults to None.\n        bands (Bands | list[str] | None, optional): List of bands to use.\n            Expects the data_dir to contain a config.toml with a \"darts.bands\" key,\n            with which the indices of the bands will be mapped to.\n            Defaults to None.\n        augment (bool, optional): Whether to augment the data. Does nothing for testing. Defaults to True.\n        num_workers (int, optional): Number of workers for data loading. See torch.utils.data.DataLoader.\n            Defaults to 0.\n        in_memory (bool, optional): Whether to load the data into memory. Defaults to False.\n\n    \"\"\"\n    super().__init__()\n    self.save_hyperparameters(ignore=[\"num_workers\", \"in_memory\"])\n    self.data_dir = data_dir\n    self.batch_size = batch_size\n\n    self.fold = fold\n    self.data_split_method = data_split_method\n    self.data_split_by = data_split_by\n    self.fold_method = fold_method\n    self.total_folds = total_folds\n\n    self.subsample = subsample\n    self.augment = augment\n    self.num_workers = num_workers\n    self.in_memory = in_memory\n\n    data_dir = Path(data_dir)\n\n    metadata_file = data_dir / \"metadata.parquet\"\n    assert metadata_file.exists(), f\"Metadata file {metadata_file} not found!\"\n\n    config_file = data_dir / \"config.toml\"\n    assert config_file.exists(), f\"Config file {config_file} not found!\"\n    data_bands = toml.load(config_file)[\"darts\"][\"bands\"]\n    bands = bands.names if isinstance(bands, Bands) else bands\n    self.bands = [data_bands.index(b) for b in bands] if bands else None\n\n    zdir = data_dir / \"data.zarr\"\n    assert zdir.exists(), f\"Data directory {zdir} not found!\"\n    zroot = zarr.group(store=LocalStore(data_dir / \"data.zarr\"))\n    self.nsamples = zroot[\"x\"].shape[0]\n    logger.debug(f\"Data directory {zdir} found with {self.nsamples} samples.\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.augment","title":"augment  <code>instance-attribute</code>","text":"<pre><code>augment = darts_segmentation.training.data.DartsDataModule(\n    augment\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands = (\n    [\n        data_bands.index(b)\n        for b in darts_segmentation.training.data.DartsDataModule(\n            bands\n        )\n    ]\n    if darts_segmentation.training.data.DartsDataModule(\n        bands\n    )\n    else None\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = (\n    darts_segmentation.training.data.DartsDataModule(\n        batch_size\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.data_dir","title":"data_dir  <code>instance-attribute</code>","text":"<pre><code>data_dir = darts_segmentation.training.data.DartsDataModule(\n    data_dir\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.data_split_by","title":"data_split_by  <code>instance-attribute</code>","text":"<pre><code>data_split_by = (\n    darts_segmentation.training.data.DartsDataModule(\n        data_split_by\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.data_split_method","title":"data_split_method  <code>instance-attribute</code>","text":"<pre><code>data_split_method = (\n    darts_segmentation.training.data.DartsDataModule(\n        data_split_method\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.fold","title":"fold  <code>instance-attribute</code>","text":"<pre><code>fold = darts_segmentation.training.data.DartsDataModule(\n    fold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.fold_method","title":"fold_method  <code>instance-attribute</code>","text":"<pre><code>fold_method = (\n    darts_segmentation.training.data.DartsDataModule(\n        fold_method\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.in_memory","title":"in_memory  <code>instance-attribute</code>","text":"<pre><code>in_memory = (\n    darts_segmentation.training.data.DartsDataModule(\n        in_memory\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.nsamples","title":"nsamples  <code>instance-attribute</code>","text":"<pre><code>nsamples = zroot['x'].shape[0]\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.num_workers","title":"num_workers  <code>instance-attribute</code>","text":"<pre><code>num_workers = (\n    darts_segmentation.training.data.DartsDataModule(\n        num_workers\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.subsample","title":"subsample  <code>instance-attribute</code>","text":"<pre><code>subsample = (\n    darts_segmentation.training.data.DartsDataModule(\n        subsample\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.total_folds","title":"total_folds  <code>instance-attribute</code>","text":"<pre><code>total_folds = (\n    darts_segmentation.training.data.DartsDataModule(\n        total_folds\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.setup","title":"setup","text":"<pre><code>setup(\n    stage: typing.Literal[\n        \"fit\", \"validate\", \"test\", \"predict\"\n    ]\n    | None = None,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def setup(self, stage: Literal[\"fit\", \"validate\", \"test\", \"predict\"] | None = None):\n    if stage == \"predict\" or stage is None:\n        return\n\n    metadata = gpd.read_parquet(self.data_dir / \"metadata.parquet\")\n    if self.subsample is not None:\n        metadata = metadata.sample(n=self.subsample, random_state=42)\n    train_metadata, test_metadata = _split_metadata(metadata, self.data_split_method, self.data_split_by)\n\n    _log_stats(train_metadata, \"train-split\")\n    _log_stats(test_metadata, \"test-split\")\n\n    # Log stats about the data\n\n    if stage in [\"fit\", \"validate\"]:\n        train_index, val_index = _get_fold(train_metadata, self.fold_method, self.total_folds, self.fold)\n        _log_stats(metadata.loc[train_index], \"train-fold\")\n        _log_stats(metadata.loc[val_index], \"val-fold\")\n\n        dsclass = DartsDatasetInMemory if self.in_memory else DartsDatasetZarr\n        self.train = dsclass(self.data_dir / \"data.zarr\", self.augment, train_index, self.bands)\n        self.val = dsclass(self.data_dir / \"data.zarr\", None, val_index, self.bands)\n    if stage == \"test\":\n        test_index = test_metadata.index.tolist()\n        dsclass = DartsDatasetInMemory if self.in_memory else DartsDatasetZarr\n        self.test = dsclass(self.data_dir / \"data.zarr\", None, test_index, self.bands)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.test_dataloader","title":"test_dataloader","text":"<pre><code>test_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def test_dataloader(self):\n    return DataLoader(\n        self.test,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def train_dataloader(self):\n    return DataLoader(\n        self.train,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        shuffle=True,\n        drop_last=True,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDataModule.val_dataloader","title":"val_dataloader","text":"<pre><code>val_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def val_dataloader(self):\n    return DataLoader(\n        self.val,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        persistent_workers=True,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory","title":"DartsDatasetInMemory","text":"<pre><code>DartsDatasetInMemory(\n    data_dir: pathlib.Path | str,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    indices: list[int] | None = None,\n    bands: list[int] | None = None,\n)\n</code></pre> <p>               Bases: <code>torch.utils.data.Dataset</code></p> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path | str,\n    augment: list[Augmentation] | None = None,\n    indices: list[int] | None = None,\n    bands: list[int] | None = None,\n):\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    store = zarr.storage.LocalStore(data_dir)\n    self.zroot = zarr.group(store=store)\n\n    assert \"x\" in self.zroot and \"y\" in self.zroot, (\n        f\"Dataset corrupted! {self.zroot.info=} must contain 'x' or 'y' arrays!\"\n    )\n\n    self.x = []\n    self.y = []\n    indices = indices or list(range(self.zroot[\"x\"].shape[0]))\n    for i in indices:\n        x = self.zroot[\"x\"][i, bands] if bands else self.zroot[\"x\"][i]\n        y = self.zroot[\"y\"][i]\n        self.x.append(x)\n        self.y.append(y)\n\n    self.transform = get_augmentation(augment)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = darts_segmentation.training.augmentations.get_augmentation(\n    darts_segmentation.training.data.DartsDatasetInMemory(\n        augment\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x = []\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y = []\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.zroot","title":"zroot  <code>instance-attribute</code>","text":"<pre><code>zroot = zarr.group(store=store)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    x = self.x[idx]\n    y = self.y[idx]\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetInMemory.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.x)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr","title":"DartsDatasetZarr","text":"<pre><code>DartsDatasetZarr(\n    data_dir: pathlib.Path | str,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    indices: list[int] | None = None,\n    bands: list[int] | None = None,\n)\n</code></pre> <p>               Bases: <code>torch.utils.data.Dataset</code></p> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path | str,\n    augment: list[Augmentation] | None = None,\n    indices: list[int] | None = None,\n    bands: list[int] | None = None,\n):\n    data_dir = Path(data_dir) if isinstance(data_dir, str) else data_dir\n\n    store = zarr.storage.LocalStore(data_dir)\n    self.zroot = zarr.group(store=store)\n\n    assert \"x\" in self.zroot and \"y\" in self.zroot, (\n        f\"Dataset corrupted! {self.zroot.info=} must contain 'x' or 'y' arrays!\"\n    )\n\n    self.indices = indices if indices is not None else list(range(self.zroot[\"x\"].shape[0]))\n    self.bands = bands\n\n    self.transform = get_augmentation(augment)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands = darts_segmentation.training.data.DartsDatasetZarr(\n    bands\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.indices","title":"indices  <code>instance-attribute</code>","text":"<pre><code>indices = (\n    darts_segmentation.training.data.DartsDatasetZarr(\n        indices\n    )\n    if darts_segmentation.training.data.DartsDatasetZarr(\n        indices\n    )\n    is not None\n    else list(\n        range(\n            darts_segmentation.training.data.DartsDatasetZarr(\n                self\n            )\n            .zroot[\"x\"]\n            .shape[0]\n        )\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = darts_segmentation.training.augmentations.get_augmentation(\n    darts_segmentation.training.data.DartsDatasetZarr(\n        augment\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.zroot","title":"zroot  <code>instance-attribute</code>","text":"<pre><code>zroot = zarr.group(store=store)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    i = self.indices[idx]\n\n    x = self.zroot[\"x\"][i, self.bands] if self.bands else self.zroot[\"x\"][i]\n    y = self.zroot[\"y\"][i]\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.DartsDatasetZarr.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.indices)\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data._get_fold","title":"_get_fold","text":"<pre><code>_get_fold(\n    metadata: geopandas.GeoDataFrame,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n        \"none\",\n    ]\n    | None,\n    n_folds: int,\n    fold: int,\n) -&gt; tuple[list[int], list[int]]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def _get_fold(\n    metadata: gpd.GeoDataFrame,\n    fold_method: Literal[\"kfold\", \"shuffle\", \"stratified\", \"region\", \"region-stratified\", \"none\"] | None,\n    n_folds: int,\n    fold: int,\n) -&gt; tuple[list[int], list[int]]:\n    fold = fold if fold_method is not None else 0\n    fold_method = fold_method or \"none\"\n    match fold_method:\n        case \"none\":\n            foldgen = [(metadata.index.tolist(), metadata.index.tolist())]\n        case \"kfold\":\n            foldgen = KFold(n_folds).split(metadata)\n        case \"shuffle\":\n            foldgen = StratifiedShuffleSplit(n_splits=n_folds, random_state=42).split(metadata, ~metadata[\"empty\"])\n        case \"stratified\":\n            foldgen = StratifiedKFold(n_folds, random_state=42, shuffle=True).split(metadata, ~metadata[\"empty\"])\n        case \"region\":\n            foldgen = GroupShuffleSplit(n_folds).split(metadata, groups=metadata[\"region\"])\n        case \"region-stratified\":\n            foldgen = StratifiedGroupKFold(n_folds, random_state=42, shuffle=True).split(\n                metadata, ~metadata[\"empty\"], groups=metadata[\"region\"]\n            )\n        case _:\n            raise ValueError(f\"Unknown fold method: {fold_method}\")\n\n    for i, (train_index, val_index) in enumerate(foldgen):\n        if i != fold:\n            continue\n        # Turn index into metadata index\n        train_index = metadata.index[train_index].tolist()\n        val_index = metadata.index[val_index].tolist()\n        return train_index, val_index\n\n    raise ValueError(f\"Fold {fold} not found\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data._log_stats","title":"_log_stats","text":"<pre><code>_log_stats(metadata: geopandas.GeoDataFrame, mode: str)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def _log_stats(metadata: gpd.GeoDataFrame, mode: str):\n    n_pos = (~metadata[\"empty\"]).sum()\n    n_neg = metadata[\"empty\"].sum()\n    logger.debug(\n        f\"{mode} dataset: {n_pos} positive, {n_neg} negative ({len(metadata)} total)\"\n        f\" with {metadata['region'].nunique()} unique regions and {metadata['sample_id'].nunique()} unique sample ids\"\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data._split_metadata","title":"_split_metadata","text":"<pre><code>_split_metadata(\n    metadata: geopandas.GeoDataFrame,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\", \"none\"\n    ]\n    | None,\n    data_split_by: list[str | float] | None,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def _split_metadata(\n    metadata: gpd.GeoDataFrame,\n    data_split_method: Literal[\"random\", \"region\", \"sample\", \"none\"] | None,\n    data_split_by: list[str | float] | None,\n):\n    # Match statement doesn't like None\n    data_split_method = data_split_method or \"none\"\n\n    match data_split_method:\n        case \"none\":\n            return metadata, metadata\n        case \"random\":\n            assert isinstance(data_split_by, list) and len(data_split_by) == 1\n            data_split_by = data_split_by[0]\n            assert isinstance(data_split_by, float)\n            for seed in range(100):\n                train_metadata = metadata.sample(frac=data_split_by, random_state=seed)\n                test_metadata = metadata.drop(train_metadata.index)\n                if (~test_metadata[\"empty\"]).sum() == 0:\n                    logger.warning(\"Test set is empty, retrying with another random seed...\")\n                    continue\n                return train_metadata, test_metadata\n            else:\n                raise ValueError(\"Could not split data randomly, please check your data.\")\n        case \"region\":\n            assert isinstance(data_split_by, list) and len(data_split_by) &gt; 0\n            train_metadata = metadata[~metadata[\"region\"].isin(data_split_by)]\n            test_metadata = metadata[metadata[\"region\"].isin(data_split_by)]\n            return train_metadata, test_metadata\n        case \"sample\":\n            assert isinstance(data_split_by, list) and len(data_split_by) &gt; 0\n            train_metadata = metadata[~metadata[\"sample_id\"].isin(data_split_by)]\n            test_metadata = metadata[metadata[\"sample_id\"].isin(data_split_by)]\n            return train_metadata, test_metadata\n        case _:\n            raise ValueError(f\"Invalid data split method: {data_split_method}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/data/#darts_segmentation.training.data.get_augmentation","title":"get_augmentation","text":"<pre><code>get_augmentation(\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None,\n) -&gt; albumentations.Compose | None\n</code></pre> <p>Get augmentations for segmentation tasks.</p> <p>Parameters:</p> <ul> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply. If None or emtpy, no augmentations are applied. If not empty, augmentations are applied in the order they are listed. Available augmentations:     - HorizontalFlip     - VerticalFlip     - RandomRotate90     - Blur     - RandomBrightnessContrast     - MultiplicativeNoise</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown augmentation is provided.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>albumentations.Compose | None</code>           \u2013            <p>A.Compose | None: A Compose object containing the augmentations. If no augmentations are provided, returns None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/augmentations.py</code> <pre><code>def get_augmentation(augment: list[Augmentation] | None) -&gt; \"A.Compose | None\":\n    \"\"\"Get augmentations for segmentation tasks.\n\n    Args:\n        augment (list[Augmentation] | None): List of augmentations to apply.\n            If None or emtpy, no augmentations are applied.\n            If not empty, augmentations are applied in the order they are listed.\n            Available augmentations:\n                - HorizontalFlip\n                - VerticalFlip\n                - RandomRotate90\n                - Blur\n                - RandomBrightnessContrast\n                - MultiplicativeNoise\n\n    Raises:\n        ValueError: If an unknown augmentation is provided.\n\n    Returns:\n        A.Compose | None: A Compose object containing the augmentations.\n            If no augmentations are provided, returns None.\n\n    \"\"\"\n    import albumentations as A  # noqa: N812\n\n    if not isinstance(augment, list) or len(augment) == 0:\n        return None\n    transforms = []\n    for aug in augment:\n        match aug:\n            case \"HorizontalFlip\":\n                transforms.append(A.HorizontalFlip())\n            case \"VerticalFlip\":\n                transforms.append(A.VerticalFlip())\n            case \"RandomRotate90\":\n                transforms.append(A.RandomRotate90())\n            case \"Blur\":\n                transforms.append(A.Blur())\n            case \"RandomBrightnessContrast\":\n                transforms.append(A.RandomBrightnessContrast())\n            case \"MultiplicativeNoise\":\n                transforms.append(A.MultiplicativeNoise(per_channel=True, elementwise=True))\n            case \"Posterize\":\n                # First convert to uint8, then apply posterization, then convert back to float32\n                # * Note: This does only work for float32 images.\n                transforms += [\n                    A.FromFloat(dtype=\"uint8\"),\n                    A.Posterize(num_bits=6, p=1.0),\n                    A.ToFloat(),\n                ]\n            case _:\n                raise ValueError(f\"Unknown augmentation: {aug}\")\n    return A.Compose(transforms)\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/","title":"darts_segmentation.training.hparams","text":""},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams","title":"darts_segmentation.training.hparams","text":"<p>Hyperparameters for training.</p>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Augmentation","title":"Augmentation  <code>module-attribute</code>","text":"<pre><code>Augmentation = typing.Literal[\n    \"HorizontalFlip\",\n    \"VerticalFlip\",\n    \"RandomRotate90\",\n    \"Blur\",\n    \"RandomBrightnessContrast\",\n    \"MultiplicativeNoise\",\n    \"Posterize\",\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.HP_NAMES","title":"HP_NAMES  <code>module-attribute</code>","text":"<pre><code>HP_NAMES = [\n    field.name\n    for field in darts_segmentation.training.hparams.Hyperparameters.__dataclass_fields__.values()\n]\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters","title":"Hyperparameters  <code>dataclass</code>","text":"<pre><code>Hyperparameters(\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Hyperparameters for Cyclopts CLI.</p> <p>Attributes:</p> <ul> <li> <code>model_arch</code>               (<code>str</code>)           \u2013            <p>Architecture of the model to use.</p> </li> <li> <code>model_encoder</code>               (<code>str</code>)           \u2013            <p>Encoder type for the model.</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>)           \u2013            <p>Weights for the encoder, if any.</p> </li> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>)           \u2013            <p>Learning rate for training.</p> </li> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Decay factor for learning rate.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float | None</code>)           \u2013            <p>Alpha parameter for focal loss, if using.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>)           \u2013            <p>Gamma parameter for focal loss.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>)           \u2013            <p>List of bands to use. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.augment","title":"augment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>augment: (\n    list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.bands","title":"bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bands: list[str] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.focal_loss_alpha","title":"focal_loss_alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_alpha: float | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.focal_loss_gamma","title":"focal_loss_gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_gamma: float = 2.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.9\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.001\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.model_arch","title":"model_arch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_arch: str = 'Unet'\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.model_encoder","title":"model_encoder  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder: str = 'dpn107'\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.Hyperparameters.model_encoder_weights","title":"model_encoder_weights  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder_weights: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.parse_hyperparameters","title":"parse_hyperparameters","text":"<pre><code>parse_hyperparameters(\n    hpconfig_file: pathlib.Path,\n) -&gt; dict[\n    str,\n    list\n    | scipy.stats.rv_discrete\n    | scipy.stats.rv_continuous,\n]\n</code></pre> <p>Parse hyperparameter configuration file to a valid dictionary for sklearn parameter search.</p> <p>Can be YAML or TOML. Must contain a key called \"hyperparameters\" containing a list of hyperparameters distributions. These distributions can either be explicit defined by another dictionary containing a \"distribution\" key, or they can be implicit defined by a single value, a list or a dictionary containing a \"low\" and \"high\" key.</p> The following distributions are supported <ul> <li>\"uniform\": Uniform distribution - must have a \"low\" and \"high\" value</li> <li>\"loguniform\": Log-uniform distribution - must have a \"low\" and \"high\" value</li> <li>\"intuniform\": Integer uniform distribution - must have a \"low\" and \"high\" value (both are inclusive)</li> <li>\"choice\": Choice distribution - must have a list of \"choices\" for explicit case, else just pass a list</li> <li>\"value\": Fixed value distribution - must have a \"value\" key for explicit case, else just pass a value</li> </ul> <p>Examples:</p> <p>Explicit Toml:</p> <pre><code>[hyperparameters]\nlearning_rate = {distribution = \"loguniform\", low = 1.0e-5, high = 1.0e-2}\nbatch_size = {distribution = \"choice\", choices = [32, 64, 128]}\ngamma = {distribution = \"uniform\", low = 0.9, high = 2.5}\ndropout = {distribution = \"uniform\", low = 0.0, high = 0.5}\nlayers = {distribution = \"intuniform\", low = 1, high = 10}\narchitecture = {distribution = \"constant\", value = \"resnet\"}\n</code></pre> <p>Explicit YAML:</p> <pre><code>hyperparameters:\n    learning_rate:\n        distribution: loguniform\n        low: 1.0e-5\n        high: 1.0e-2\n    batch_size:\n        distribution: choice\n        choices: [32, 64, 128]\n    gamma:\n        distribution: uniform\n        low: 0.9\n        high: 2.5\n    dropout:\n        distribution: uniform\n        low: 0.0\n        high: 0.5\n    layers:\n        distribution: intuniform\n        low: 1\n        high: 10\n    architecture:\n        distribution: constant\n        value: \"resnet\"\n</code></pre> <p>Implicit YAML:</p> <pre><code>hyperparameters:\n    learning_rate:\n        distribution: loguniform\n        low: 1.0e-5\n        high: 1.0e-2\n    batch_size: [32, 64, 128]\n    gamma:\n        low: 0.9\n        high: 2.5\n    dropout:\n        low: 0.0\n        high: 0.5\n    layers:\n        low: 1\n        high: 10\n    architecture: \"resnet\"\n</code></pre> <p>Will all result in the following dictionary:</p> <pre><code>{\n    \"learning_rate\": scipy.stats.loguniform(1.0e-5, 1.0e-2),\n    \"batch_size\": [32, 64, 128],\n    \"gamma\": scipy.stats.uniform(0.9, 1.6),\n    \"dropout\": scipy.stats.uniform(0.0, 0.5),\n    \"layers\": scipy.stats.randint(1, 11),\n    \"architecture\": [\"resnet\"]\n}\n</code></pre> <p>Parameters:</p> <ul> <li> <code>hpconfig_file</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the hyperparameter configuration file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[str, list | scipy.stats.rv_discrete | scipy.stats.rv_continuous]</code> )          \u2013            <p>Dictionary of hyperparameters to tune and their distributions.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the hyperparameter configuration file is not a valid YAML or TOML file.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/hparams.py</code> <pre><code>def parse_hyperparameters(  # noqa: C901\n    hpconfig_file: Path,\n) -&gt; dict[str, \"list | scipy.stats.rv_discrete | scipy.stats.rv_continuous\"]:\n    \"\"\"Parse hyperparameter configuration file to a valid dictionary for sklearn parameter search.\n\n    Can be YAML or TOML.\n    Must contain a key called \"hyperparameters\" containing a list of hyperparameters distributions.\n    These distributions can either be explicit defined by another dictionary containing a \"distribution\" key,\n    or they can be implicit defined by a single value, a list or a dictionary containing a \"low\" and \"high\" key.\n\n    The following distributions are supported:\n        - \"uniform\": Uniform distribution - must have a \"low\" and \"high\" value\n        - \"loguniform\": Log-uniform distribution - must have a \"low\" and \"high\" value\n        - \"intuniform\": Integer uniform distribution - must have a \"low\" and \"high\" value (both are inclusive)\n        - \"choice\": Choice distribution - must have a list of \"choices\" for explicit case, else just pass a list\n        - \"value\": Fixed value distribution - must have a \"value\" key for explicit case, else just pass a value\n\n    Examples:\n        Explicit Toml:\n\n        ```toml\n        [hyperparameters]\n        learning_rate = {distribution = \"loguniform\", low = 1.0e-5, high = 1.0e-2}\n        batch_size = {distribution = \"choice\", choices = [32, 64, 128]}\n        gamma = {distribution = \"uniform\", low = 0.9, high = 2.5}\n        dropout = {distribution = \"uniform\", low = 0.0, high = 0.5}\n        layers = {distribution = \"intuniform\", low = 1, high = 10}\n        architecture = {distribution = \"constant\", value = \"resnet\"}\n        ```\n\n        Explicit YAML:\n\n        ```yaml\n        hyperparameters:\n            learning_rate:\n                distribution: loguniform\n                low: 1.0e-5\n                high: 1.0e-2\n            batch_size:\n                distribution: choice\n                choices: [32, 64, 128]\n            gamma:\n                distribution: uniform\n                low: 0.9\n                high: 2.5\n            dropout:\n                distribution: uniform\n                low: 0.0\n                high: 0.5\n            layers:\n                distribution: intuniform\n                low: 1\n                high: 10\n            architecture:\n                distribution: constant\n                value: \"resnet\"\n        ```\n\n        Implicit YAML:\n\n        ```yaml\n        hyperparameters:\n            learning_rate:\n                distribution: loguniform\n                low: 1.0e-5\n                high: 1.0e-2\n            batch_size: [32, 64, 128]\n            gamma:\n                low: 0.9\n                high: 2.5\n            dropout:\n                low: 0.0\n                high: 0.5\n            layers:\n                low: 1\n                high: 10\n            architecture: \"resnet\"\n        ```\n\n        Will all result in the following dictionary:\n\n        ```py\n        {\n            \"learning_rate\": scipy.stats.loguniform(1.0e-5, 1.0e-2),\n            \"batch_size\": [32, 64, 128],\n            \"gamma\": scipy.stats.uniform(0.9, 1.6),\n            \"dropout\": scipy.stats.uniform(0.0, 0.5),\n            \"layers\": scipy.stats.randint(1, 11),\n            \"architecture\": [\"resnet\"]\n        }\n        ```\n\n    Args:\n        hpconfig_file (Path): Path to the hyperparameter configuration file.\n\n    Returns:\n        dict: Dictionary of hyperparameters to tune and their distributions.\n\n    Raises:\n        ValueError: If the hyperparameter configuration file is not a valid YAML or TOML file.\n\n    \"\"\"\n    import scipy.stats\n\n    # Read yaml\n    if hpconfig_file.suffix == \".yaml\" or hpconfig_file.suffix == \".yml\":\n        with hpconfig_file.open() as f:\n            hpconfig = yaml.safe_load(f)[\"hyperparameters\"]\n    # Read toml\n    elif hpconfig_file.suffix == \".toml\":\n        with hpconfig_file.open() as f:\n            hpconfig = toml.load(f)[\"hyperparameters\"]\n    else:\n        raise ValueError(f\"Invalid hyperparameter configuration file format: {hpconfig.suffix}\")\n\n    hpdistributions = {}\n    for hparam, config in hpconfig.items():\n        if \"-\" in hparam:\n            logger.debug(f\"Hyphen in hyperparameter name {hparam} is not supported. Replacing with underscore.\")\n            hparam = hparam.replace(\"-\", \"_\")\n        # Assume implicit case\n        if isinstance(config, list):\n            # Choice\n            hpdistributions[hparam] = config\n            continue\n        elif not isinstance(config, dict):\n            # Constant\n            hpdistributions[hparam] = [config]\n            continue\n        else:\n            if \"low\" in config.keys() and \"high\" in config.keys():\n                if isinstance(config[\"low\"], int) and isinstance(config[\"high\"], int):\n                    # Randint\n                    hpdistributions[hparam] = scipy.stats.randint(config[\"low\"], config[\"high\"] + 1)\n                    continue\n                elif isinstance(config[\"low\"], float) and isinstance(config[\"high\"], float):\n                    # Randfloat\n                    hpdistributions[hparam] = scipy.stats.uniform(config[\"low\"], config[\"high\"] - config[\"low\"])\n                    continue\n                else:\n                    raise ValueError(\n                        f\"Invalid hyperparameter configuration for {hparam}: low and high must be of the same type.\"\n                        f\" Got {type(config['low'])=} and {type(config['high'])=}.\"\n                    )\n\n        # Now Explicit\n        assert isinstance(config, dict), f\"Invalid hyperparameter configuration for {hparam}\"\n        assert \"distribution\" in config, (\n            f\"Could not implicitly define distribution for {hparam}.\"\n            \" Please provide a distribution type via a 'distribution' key.\"\n        )\n\n        match config[\"distribution\"]:\n            case \"uniform\":\n                assert \"low\" in config, f\"Missing 'low' key in hyperparameter configuration for uniform {hparam}\"\n                assert \"high\" in config, f\"Missing 'high' key in hyperparameter configuration for uniform {hparam}\"\n                assert isinstance(config[\"low\"], (int, float)), (\n                    f\"Invalid 'low' value in hyperparameter configuration for uniform {hparam}:\"\n                    f\" got {config['low']} of type {type(config['low'])}\"\n                )\n                assert isinstance(config[\"high\"], (int, float)), (\n                    f\"Invalid 'high' value in hyperparameter configuration for uniform {hparam}:\"\n                    f\" got {config['high']} of type {type(config['high'])}\"\n                )\n                assert config[\"low\"] &lt; config[\"high\"], (\n                    f\"'low' value must be less than 'high' value in hyperparameter configuration for uniform {hparam}:\"\n                    f\" got low={config['low']}, high={config['high']}\"\n                )\n                hpdistributions[hparam] = scipy.stats.uniform(config[\"low\"], config[\"high\"] - config[\"low\"])\n            case \"loguniform\":\n                assert \"low\" in config, f\"Missing 'low' key in hyperparameter configuration for loguniform {hparam}\"\n                assert \"high\" in config, f\"Missing 'high' key in hyperparameter configuration for loguniform {hparam}\"\n                assert isinstance(config[\"low\"], (int, float)), (\n                    f\"Invalid 'low' value in hyperparameter configuration for loguniform {hparam}:\"\n                    f\" got {config['low']} of type {type(config['low'])}\"\n                )\n                assert isinstance(config[\"high\"], (int, float)), (\n                    f\"Invalid 'high' value in hyperparameter configuration for loguniform {hparam}:\"\n                    f\" got {config['high']} of type {type(config['high'])}\"\n                )\n                assert config[\"low\"] &lt; config[\"high\"], (\n                    f\"'low' must be less than 'high' in hyperparameter configuration for loguniform {hparam}:\"\n                    f\" got low={config['low']}, high={config['high']}\"\n                )\n                hpdistributions[hparam] = scipy.stats.loguniform(config[\"low\"], config[\"high\"])\n            case \"intuniform\":\n                assert \"low\" in config, f\"Missing 'low' key in hyperparameter configuration for int_uniform {hparam}\"\n                assert \"high\" in config, f\"Missing 'high' key in hyperparameter configuration for int_uniform {hparam}\"\n                assert isinstance(config[\"low\"], int), (\n                    f\"'low' must be an integer in hyperparameter configuration for intuniform {hparam}:\"\n                    f\" got {config['low']} of type {type(config['low'])}\"\n                )\n                assert isinstance(config[\"high\"], int), (\n                    f\"'high' must be an integer in hyperparameter configuration for intuniform {hparam}:\"\n                    f\" got {config['high']} of type {type(config['high'])}\"\n                )\n                assert config[\"low\"] &lt; config[\"high\"], (\n                    f\"'low' must be less than 'high' in hyperparameter configuration for intuniform {hparam}:\"\n                    f\" got low={config['low']}, high={config['high']}\"\n                )\n                hpdistributions[hparam] = scipy.stats.randint(config[\"low\"], config[\"high\"] + 1)\n            case \"choice\":\n                assert \"choices\" in config, f\"Missing 'choices' key in hyperparameter configuration for choice {hparam}\"\n                hpdistributions[hparam] = config[\"choices\"]\n            case \"constant\":\n                assert \"value\" in config, f\"Missing 'value' key in hyperparameter configuration for constant {hparam}\"\n                hpdistributions[hparam] = [config[\"value\"]]\n            case _:\n                raise ValueError(f\"Invalid hyperparameter type: {config['distribution']}\")\n\n    return hpdistributions\n</code></pre>"},{"location":"reference/darts_segmentation/training/hparams/#darts_segmentation.training.hparams.sample_hyperparameters","title":"sample_hyperparameters","text":"<pre><code>sample_hyperparameters(\n    param_grid: dict[\n        str,\n        list\n        | scipy.stats.rv_discrete\n        | scipy.stats.rv_continuous,\n    ],\n    n_trials: int | typing.Literal[\"grid\"] = 100,\n) -&gt; list[\n    darts_segmentation.training.hparams.Hyperparameters\n]\n</code></pre> <p>Sample hyperparameters from a parameter grid.</p> <p>This function samples a list of hyperparameter combinations from a parameter grid. It supports both random sampling and grid search.</p> <p>Parameters:</p> <ul> <li> <code>param_grid</code>               (<code>dict</code>)           \u2013            <p>Dictionary of hyperparameters to tune and their distributions. Values can be lists of values or scipy.stats distribution objects.</p> </li> <li> <code>n_trials</code>               (<code>int | typing.Literal['grid']</code>, default:                   <code>100</code> )           \u2013            <p>Number of hyperparameter combinations to sample. If set to \"grid\", will perform a grid search over all possible combinations. Defaults to 100.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (              <code>list[darts_segmentation.training.hparams.Hyperparameters]</code> )          \u2013            <p>List of dictionaries, where each dictionary represents a hyperparameter combination.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If n_trials is not an integer (saying a random search) or 'grid'.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/hparams.py</code> <pre><code>def sample_hyperparameters(\n    param_grid: dict[str, \"list | scipy.stats.rv_discrete | scipy.stats.rv_continuous\"],\n    n_trials: int | Literal[\"grid\"] = 100,\n) -&gt; list[Hyperparameters]:\n    \"\"\"Sample hyperparameters from a parameter grid.\n\n    This function samples a list of hyperparameter combinations from a parameter grid.\n    It supports both random sampling and grid search.\n\n    Args:\n        param_grid (dict): Dictionary of hyperparameters to tune and their distributions.\n            Values can be lists of values or scipy.stats distribution objects.\n        n_trials (int | Literal[\"grid\"], optional): Number of hyperparameter combinations to sample.\n            If set to \"grid\", will perform a grid search over all possible combinations.\n            Defaults to 100.\n\n    Returns:\n        list: List of dictionaries, where each dictionary represents a hyperparameter combination.\n\n    Raises:\n        ValueError: If n_trials is not an integer (saying a random search) or 'grid'.\n\n    \"\"\"\n    from sklearn.model_selection import ParameterSampler\n\n    # Check if the parameter of the grid are valid (part of Hyperparameters cclass)\n    for hparam in param_grid.keys():\n        assert hparam in HP_NAMES, f\"Invalid hyperparameter: {hparam} in config but not part of valid {HP_NAMES=}\"\n\n    # Random search\n    if isinstance(n_trials, int):\n        param_list = list(ParameterSampler(param_grid, n_iter=n_trials, random_state=42))\n    elif n_trials == \"grid\":\n        n_combinations = 1\n        for hparam, choices in param_grid.items():\n            assert isinstance(choices, list), (\n                f\"In a grid search, each parameter must be a list of choices. Got {type(choices)} for {hparam}.\"\n            )\n            n_combinations *= len(choices)\n        param_list = list(ParameterSampler(param_grid, n_iter=n_combinations, random_state=42))\n    else:\n        raise ValueError(\n            f\"Invalid value for n_trials: {n_trials}. Must be an integer to perform a random search or 'grid'.\"\n        )\n\n    # Convert to Hyperparameters objects\n    param_list = [Hyperparameters(**params) for params in param_list]\n\n    return param_list\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/","title":"darts_segmentation.training.module","text":""},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module","title":"darts_segmentation.training.module","text":"<p>Training script for DARTS segmentation.</p>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP","title":"LitSMP","text":"<pre><code>LitSMP(\n    config: darts_segmentation.segment.SMPSegmenterConfig,\n    learning_rate: float = 1e-05,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    **kwargs: dict[str, typing.Any],\n)\n</code></pre> <p>               Bases: <code>lightning.LightningModule</code></p> <p>Lightning module for training a segmentation model using the segmentation_models_pytorch library.</p> <p>Initialize the LitSMP.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>darts_segmentation.segment.SMPSegmenterConfig</code>)           \u2013            <p>Configuration for the segmentation model.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>1e-05</code> )           \u2013            <p>Initial learning rate. Defaults to 1e-5.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>kwargs</code>               (<code>dict[str, typing.Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments which should be saved to the hyperparameter file.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def __init__(\n    self,\n    config: SMPSegmenterConfig,\n    learning_rate: float = 1e-5,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Initialize the LitSMP.\n\n    Args:\n        config (SMPSegmenterConfig): Configuration for the segmentation model.\n        learning_rate (float, optional): Initial learning rate. Defaults to 1e-5.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        kwargs (dict[str, Any]): Additional keyword arguments which should be saved to the hyperparameter file.\n\n    \"\"\"\n    super().__init__()\n\n    # This saves config, learning_rate and gamma under self.hparams\n    self.save_hyperparameters(ignore=[\"test_set\", \"val_set\"])\n    self.model = smp.create_model(**config[\"model\"], activation=\"sigmoid\")\n\n    # Assumes that the training preparation was done with setting invalid pixels in the mask to 2\n    self.loss_fn = smp.losses.FocalLoss(\n        mode=\"binary\", alpha=focal_loss_alpha, gamma=focal_loss_gamma, ignore_index=2\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.loss_fn","title":"loss_fn  <code>instance-attribute</code>","text":"<pre><code>loss_fn = segmentation_models_pytorch.losses.FocalLoss(\n    mode=\"binary\",\n    alpha=darts_segmentation.training.module.LitSMP(\n        focal_loss_alpha\n    ),\n    gamma=darts_segmentation.training.module.LitSMP(\n        focal_loss_gamma\n    ),\n    ignore_index=2,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = segmentation_models_pytorch.create_model(\n    **darts_segmentation.training.module.LitSMP(config)[\n        \"model\"\n    ],\n    activation=\"sigmoid\",\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def __repr__(self):  # noqa: D105\n    return f\"LitSMP({self.hparams['config']['model']})\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def configure_optimizers(self):  # noqa: D102\n    optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.hparams.gamma)\n    return [optimizer], [scheduler]\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def on_train_epoch_end(self):  # noqa: D102\n    self.log(\"learning_rate\", self.lr_schedulers().get_last_lr()[0])\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def test_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def training_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.LitSMP.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def validation_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.SMPSegmenterConfig","title":"SMPSegmenterConfig","text":"<p>               Bases: <code>typing.TypedDict</code></p> <p>Configuration for the segmentor.</p>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.SMPSegmenterConfig.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands: darts_segmentation.utils.Bands\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.SMPSegmenterConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: dict[str, typing.Any]\n</code></pre>"},{"location":"reference/darts_segmentation/training/module/#darts_segmentation.training.module.SMPSegmenterConfig.from_ckpt","title":"from_ckpt  <code>classmethod</code>","text":"<pre><code>from_ckpt(\n    config: dict[str, typing.Any],\n) -&gt; darts_segmentation.segment.SMPSegmenterConfig\n</code></pre> <p>Validate the config for the segmentor.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, typing.Any]</code>)           \u2013            <p>The configuration to validate.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>darts_segmentation.segment.SMPSegmenterConfig</code>           \u2013            <p>The validated configuration.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@classmethod\ndef from_ckpt(cls, config: dict[str, Any]) -&gt; \"SMPSegmenterConfig\":\n    \"\"\"Validate the config for the segmentor.\n\n    Args:\n        config: The configuration to validate.\n\n    Returns:\n        The validated configuration.\n\n    \"\"\"\n    # Handling legacy case that the config contains the old keys\n    if \"input_combination\" in config and \"norm_factors\" in config:\n        # Check if all input_combination features are in norm_factors\n        config[\"bands\"] = Bands([Band(name, config[\"norm_factors\"][name]) for name in config[\"input_combination\"]])\n        config.pop(\"norm_factors\")\n        config.pop(\"input_combination\")\n\n    assert \"model\" in config, \"Model config is missing!\"\n    assert \"bands\" in config, \"Bands config is missing!\"\n    # The Bands object is always pickled as a dict for interoperability, so we need to convert it back\n    if not isinstance(config[\"bands\"], Bands):\n        config[\"bands\"] = Bands.from_config(config[\"bands\"])\n    return config\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/","title":"darts_segmentation.training.prepare_training","text":""},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training","title":"darts_segmentation.training.prepare_training","text":"<p>Functions to prepare the training data for the segmentation model training.</p>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands","title":"Bands","text":"<p>               Bases: <code>collections.UserList[darts_segmentation.utils.Band]</code></p> <p>Wrapper for the list of bands.</p>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.factors","title":"factors  <code>property</code>","text":"<pre><code>factors: list[float]\n</code></pre> <p>Get the factors of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The factors of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.names","title":"names  <code>property</code>","text":"<pre><code>names: list[str]\n</code></pre> <p>Get the names of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: The names of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.offsets","title":"offsets  <code>property</code>","text":"<pre><code>offsets: list[float]\n</code></pre> <p>Get the offsets of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The offsets of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.__reduce__","title":"__reduce__","text":"<pre><code>__reduce__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __reduce__(self):  # noqa: D105\n    # This is needed to pickle (and unpickle) the Bands object as a dict\n    # This is needed, because this way we don't need to have this class present when unpickling\n    # a pytorch checkpoint\n    return (dict, (self.to_config(),))\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __repr__(self) -&gt; str:  # noqa: D105\n    band_info = \", \".join([f\"{band.name}(*{band.factor:.5f}+{band.offset:.5f})\" for band in self])\n    return f\"Bands({band_info})\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.filter","title":"filter","text":"<pre><code>filter(\n    band_names: list[str],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Filter the bands by name.</p> <p>Parameters:</p> <ul> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>The names of the bands to keep.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The filtered Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def filter(self, band_names: list[str]) -&gt; \"Bands\":\n    \"\"\"Filter the bands by name.\n\n    Args:\n        band_names (list[str]): The names of the bands to keep.\n\n    Returns:\n        Bands: The filtered Bands object.\n\n    \"\"\"\n    return Bands([band for band in self if band.name in band_names])\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: dict[\n        typing.Literal[\n            \"bands\", \"band_factors\", \"band_offsets\"\n        ],\n        list,\n    ]\n    | dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a config dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>The config dictionary containing the band information. Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\", with the values to be lists of the same length.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list] | dict[str, tuple[float, float]],\n) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a config dictionary.\n\n    Args:\n        config (dict): The config dictionary containing the band information.\n            Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\",\n            with the values to be lists of the same length.\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    assert \"bands\" in config and \"band_factors\" in config and \"band_offsets\" in config, (\n        f\"Config must contain keys 'bands', 'band_factors' and 'band_offsets'.Got {config} instead.\"\n    )\n    return cls(\n        [\n            Band(name=name, factor=factor, offset=offset)\n            for name, factor, offset in zip(config[\"bands\"], config[\"band_factors\"], config[\"band_offsets\"])\n        ]\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    config: dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, tuple[float, float]]</code>)           \u2013            <p>The dictionary containing the band information. Expects the keys to be the band names and the values to be tuples of (factor, offset). Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: dict[str, tuple[float, float]]) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a dictionary.\n\n    Args:\n        config (dict[str, tuple[float, float]]): The dictionary containing the band information.\n            Expects the keys to be the band names and the values to be tuples of (factor, offset).\n            Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    return cls([Band(name=name, factor=factor, offset=offset) for name, (factor, offset) in config.items()])\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.to_config","title":"to_config","text":"<pre><code>to_config() -&gt; dict[\n    typing.Literal[\"bands\", \"band_factors\", \"band_offsets\"],\n    list,\n]\n</code></pre> <p>Convert the Bands object to a config dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[typing.Literal['bands', 'band_factors', 'band_offsets'], list]</code> )          \u2013            <p>The config dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_config(self) -&gt; dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list]:\n    \"\"\"Convert the Bands object to a config dictionary.\n\n    Returns:\n        dict: The config dictionary containing the band information.\n\n    \"\"\"\n    return {\n        \"bands\": [band.name for band in self],\n        \"band_factors\": [band.factor for band in self],\n        \"band_offsets\": [band.offset for band in self],\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.Bands.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, tuple[float, float]]\n</code></pre> <p>Convert the Bands object to a dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict[str, tuple[float, float]]</code>           \u2013            <p>dict[str, tuple[float, float]]: The dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_dict(self) -&gt; dict[str, tuple[float, float]]:\n    \"\"\"Convert the Bands object to a dictionary.\n\n    Returns:\n        dict[str, tuple[float, float]]: The dictionary containing the band information.\n\n    \"\"\"\n    return {band.name: (band.factor, band.offset) for band in self}\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords","title":"PatchCoords  <code>dataclass</code>","text":"<pre><code>PatchCoords(\n    i: int,\n    patch_idx_y: int,\n    patch_idx_x: int,\n    y: slice,\n    x: slice,\n)\n</code></pre> <p>Wrapper which stores the coordinate information of a patch in the original image.</p>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.i","title":"i  <code>instance-attribute</code>","text":"<pre><code>i: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.patch_idx_x","title":"patch_idx_x  <code>instance-attribute</code>","text":"<pre><code>patch_idx_x: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.patch_idx_y","title":"patch_idx_y  <code>instance-attribute</code>","text":"<pre><code>patch_idx_y: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: slice\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: slice\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.PatchCoords.from_tensor","title":"from_tensor  <code>classmethod</code>","text":"<pre><code>from_tensor(\n    coords: torch.Tensor, patch_size: int\n) -&gt; (\n    darts_segmentation.training.prepare_training.PatchCoords\n)\n</code></pre> <p>Create a PatchCoords object from the returned coord tensor of <code>create_patches</code>.</p> <p>Parameters:</p> <ul> <li> <code>coords</code>               (<code>torch.Tensor</code>)           \u2013            <p>The coordinates of the patch in the original image, from <code>create_patches</code>.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>PatchCoords</code> (              <code>darts_segmentation.training.prepare_training.PatchCoords</code> )          \u2013            <p>The coordinates of the patch in the original image.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>@classmethod\ndef from_tensor(cls, coords: torch.Tensor, patch_size: int) -&gt; \"PatchCoords\":\n    \"\"\"Create a PatchCoords object from the returned coord tensor of `create_patches`.\n\n    Args:\n        coords (torch.Tensor): The coordinates of the patch in the original image, from `create_patches`.\n        patch_size (int): The size of the patch.\n\n    Returns:\n        PatchCoords: The coordinates of the patch in the original image.\n\n    \"\"\"\n    i, y, x, h, w = coords.int().numpy()\n    return cls(\n        i=i,\n        patch_idx_y=h.item(),\n        patch_idx_x=w.item(),\n        y=slice(y.item(), y.item() + patch_size),\n        x=slice(x.item(), x.item() + patch_size),\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder","title":"TrainDatasetBuilder  <code>dataclass</code>","text":"<pre><code>TrainDatasetBuilder(\n    train_data_dir: pathlib.Path,\n    patch_size: int,\n    overlap: int,\n    bands: darts_segmentation.utils.Bands,\n    exclude_nopositive: bool,\n    exclude_nan: bool,\n    mask_erosion_size: int,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n    append: bool = False,\n)\n</code></pre> <p>Helper class to create all necessary files for a DARTS training dataset.</p>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.append","title":"append  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>append: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.bands","title":"bands  <code>instance-attribute</code>","text":"<pre><code>bands: darts_segmentation.utils.Bands\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: typing.Literal['cuda', 'cpu'] | int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.exclude_nan","title":"exclude_nan  <code>instance-attribute</code>","text":"<pre><code>exclude_nan: bool\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.exclude_nopositive","title":"exclude_nopositive  <code>instance-attribute</code>","text":"<pre><code>exclude_nopositive: bool\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.mask_erosion_size","title":"mask_erosion_size  <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.overlap","title":"overlap  <code>instance-attribute</code>","text":"<pre><code>overlap: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.train_data_dir","title":"train_data_dir  <code>instance-attribute</code>","text":"<pre><code>train_data_dir: pathlib.Path\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize the TrainDatasetBuilder class based on provided dataclass params.</p> <p>This will setup everything needed to add patches to the dataset:</p> <ul> <li>Create the train_data_dir if it does not exist</li> <li>Create an emtpy zarr store</li> <li>Initialize the metadata list</li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the TrainDatasetBuilder class based on provided dataclass params.\n\n    This will setup everything needed to add patches to the dataset:\n\n    - Create the train_data_dir if it does not exist\n    - Create an emtpy zarr store\n    - Initialize the metadata list\n    \"\"\"\n    lovely_tensors.monkey_patch()\n    lovely_tensors.set_config(color=False)\n    self._metadata = []\n    if self.append and (self.train_data_dir / \"metadata.parquet\").exists():\n        self._metadata = gpd.read_parquet(self.train_data_dir / \"metadata.parquet\").to_dict(orient=\"records\")\n\n    self.train_data_dir.mkdir(exist_ok=True, parents=True)\n\n    self._zroot = zarr.group(store=LocalStore(self.train_data_dir / \"data.zarr\"), overwrite=not self.append)\n    # We need do declare the number of patches to 0, because we can't know the final number of patches\n\n    if not self.append:\n        self._zroot.create(\n            name=\"x\",\n            shape=(0, len(self.bands), self.patch_size, self.patch_size),\n            # shards=(100, len(bands), patch_size, patch_size),\n            chunks=(1, 1, self.patch_size, self.patch_size),\n            dtype=\"float32\",\n            compressors=BloscCodec(cname=\"lz4\", clevel=9),\n        )\n        self._zroot.create(\n            name=\"y\",\n            shape=(0, self.patch_size, self.patch_size),\n            # shards=(100, patch_size, patch_size),\n            chunks=(1, self.patch_size, self.patch_size),\n            dtype=\"uint8\",\n            compressors=BloscCodec(cname=\"lz4\", clevel=9),\n        )\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.add_tile","title":"add_tile","text":"<pre><code>add_tile(\n    tile: xarray.Dataset,\n    labels: geopandas.GeoDataFrame,\n    region: str,\n    sample_id: str,\n    metadata: dict[str, str] | None = None,\n)\n</code></pre> <p>Add a tile to the dataset.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>labels</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The labels to be used for training.</p> </li> <li> <code>region</code>               (<code>str</code>)           \u2013            <p>The region of the tile.</p> </li> <li> <code>sample_id</code>               (<code>str</code>)           \u2013            <p>The sample id of the tile.</p> </li> <li> <code>metadata</code>               (<code>dict[str, str]</code>, default:                   <code>None</code> )           \u2013            <p>Any metadata to be added to the metadata file. Will not be used for the training, but can be used for better debugging or reproducibility.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def add_tile(\n    self,\n    tile: xr.Dataset,\n    labels: gpd.GeoDataFrame,\n    region: str,\n    sample_id: str,\n    metadata: dict[str, str] | None = None,\n):\n    \"\"\"Add a tile to the dataset.\n\n    Args:\n        tile (xr.Dataset): The input tile, containing preprocessed, harmonized data.\n        labels (gpd.GeoDataFrame): The labels to be used for training.\n        region (str): The region of the tile.\n        sample_id (str): The sample id of the tile.\n        metadata (dict[str, str], optional): Any metadata to be added to the metadata file.\n            Will not be used for the training, but can be used for better debugging or reproducibility.\n\n    \"\"\"\n    metadata = metadata or {}\n    # Convert all paths of metadata to strings\n    metadata = {k: str(v) if isinstance(v, Path) else v for k, v in metadata.items()}\n\n    x, y, stacked_coords = create_training_patches(\n        tile=tile,\n        labels=labels,\n        bands=self.bands,\n        patch_size=self.patch_size,\n        overlap=self.overlap,\n        exclude_nopositive=self.exclude_nopositive,\n        exclude_nan=self.exclude_nan,\n        device=self.device,\n        mask_erosion_size=self.mask_erosion_size,\n    )\n\n    self._zroot[\"x\"].append(x.numpy().astype(\"float32\"))\n    self._zroot[\"y\"].append(y.numpy().astype(\"uint8\"))\n\n    for patch_id, coords in enumerate(stacked_coords):\n        geometry = tile.isel(x=coords.x, y=coords.y).odc.geobox.geographic_extent.geom\n        self._metadata.append(\n            {\n                \"patch_id\": patch_id,\n                \"region\": region,\n                \"sample_id\": sample_id,\n                \"empty\": not (y == 1).any(),\n                \"x\": coords.x.start,\n                \"y\": coords.y.start,\n                \"patch_idx_x\": coords.patch_idx_x,\n                \"patch_idx_y\": coords.patch_idx_y,\n                \"geometry\": geometry,\n                **metadata,\n            }\n        )\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.TrainDatasetBuilder.finalize","title":"finalize","text":"<pre><code>finalize(data_config: dict[str, str] | None = None)\n</code></pre> <p>Finalize the dataset by saving the metadata and the config file.</p> <p>Parameters:</p> <ul> <li> <code>data_config</code>               (<code>dict[str, str]</code>, default:                   <code>None</code> )           \u2013            <p>The data config to be saved in the config file. This should contain all the information needed to recreate the dataset. It will be saved as a toml file, along with the configuration provided in this dataclass.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no patches were found in the dataset.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def finalize(self, data_config: dict[str, str] | None = None):\n    \"\"\"Finalize the dataset by saving the metadata and the config file.\n\n    Args:\n        data_config (dict[str, str], optional): The data config to be saved in the config file.\n            This should contain all the information needed to recreate the dataset.\n            It will be saved as a toml file, along with the configuration provided in this dataclass.\n\n    Raises:\n        ValueError: If no patches were found in the dataset.\n\n    \"\"\"\n    if len(self._metadata) == 0:\n        logger.error(\"No patches found in the dataset.\", exc_info=True)\n        raise ValueError(\"No patches found in the dataset.\")\n\n    # Save the metadata\n    metadata = gpd.GeoDataFrame(self._metadata, crs=\"EPSG:4326\")\n    metadata.to_parquet(self.train_data_dir / \"metadata.parquet\")\n\n    data_config = data_config or {}\n    # Convert the data_config paths to strings\n    data_config = {k: str(v) if isinstance(v, Path) else v for k, v in data_config.items()}\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"train_data_dir\": str(self.train_data_dir),\n            \"patch_size\": self.patch_size,\n            \"overlap\": self.overlap,\n            \"n_bands\": len(self.bands),\n            \"exclude_nopositive\": self.exclude_nopositive,\n            \"exclude_nan\": self.exclude_nan,\n            \"mask_erosion_size\": self.mask_erosion_size,\n            \"n_patches\": len(metadata),\n            \"device\": self.device,\n            **self.bands.to_config(),  # keys: bands, band_factors, band_offsets\n            **data_config,\n        }\n    }\n    with open(self.train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {len(metadata)} patches to {self.train_data_dir}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.create_patches","title":"create_patches","text":"<pre><code>create_patches(\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    return_coords: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Create patches from a tensor.</p> <p>Parameters:</p> <ul> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>return_coords</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"reference/darts_segmentation/training/prepare_training/#darts_segmentation.training.prepare_training.create_training_patches","title":"create_training_patches","text":"<pre><code>create_training_patches(\n    tile: xarray.Dataset,\n    labels: geopandas.GeoDataFrame,\n    bands: darts_segmentation.utils.Bands,\n    patch_size: int,\n    overlap: int,\n    exclude_nopositive: bool,\n    exclude_nan: bool,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n    mask_erosion_size: int,\n) -&gt; tuple[\n    torch.tensor,\n    torch.tensor,\n    list[\n        darts_segmentation.training.prepare_training.PatchCoords\n    ],\n]\n</code></pre> <p>Create training patches from a tile and labels.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>labels</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The labels to be used for training.</p> </li> <li> <code>bands</code>               (<code>darts_segmentation.utils.Bands</code>)           \u2013            <p>The bands to be used for training.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>)           \u2013            <p>Whether to exclude patches where the labels do not contain positives.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>)           \u2013            <p>Whether to exclude patches where the input data has nan values.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for the erosion.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>)           \u2013            <p>The size of the disk to use for erosion.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[torch.tensor, torch.tensor, list[darts_segmentation.training.prepare_training.PatchCoords]]</code>           \u2013            <p>tuple[torch.tensor, torch.tensor, list[PatchCoords]]: A tuple containing the input, the labels and the coords. The input has the format (C, H, W), the labels (H, W).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a band is not found in the preprocessed data.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def create_training_patches(\n    tile: xr.Dataset,\n    labels: gpd.GeoDataFrame,\n    bands: Bands,\n    patch_size: int,\n    overlap: int,\n    exclude_nopositive: bool,\n    exclude_nan: bool,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n    mask_erosion_size: int,\n) -&gt; tuple[torch.tensor, torch.tensor, list[PatchCoords]]:\n    \"\"\"Create training patches from a tile and labels.\n\n    Args:\n        tile (xr.Dataset): The input tile, containing preprocessed, harmonized data.\n        labels (gpd.GeoDataFrame): The labels to be used for training.\n        bands (Bands): The bands to be used for training.\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        exclude_nopositive (bool): Whether to exclude patches where the labels do not contain positives.\n        exclude_nan (bool): Whether to exclude patches where the input data has nan values.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for the erosion.\n        mask_erosion_size (int): The size of the disk to use for erosion.\n\n    Returns:\n        tuple[torch.tensor, torch.tensor, list[PatchCoords]]: A tuple containing the input, the labels and the coords.\n            The input has the format (C, H, W), the labels (H, W).\n\n    Raises:\n        ValueError: If a band is not found in the preprocessed data.\n\n    \"\"\"\n    if len(labels) == 0 and exclude_nopositive:\n        logger.warning(\"No labels found in the labels GeoDataFrame. Skipping.\")\n        return\n\n    # Rasterize the labels\n    if len(labels) &gt; 0:\n        labels_rasterized = 1 - make_geocube(labels, measurements=[\"id\"], like=tile).id.isnull()\n    else:\n        labels_rasterized = xr.zeros_like(tile[\"quality_data_mask\"])\n\n    # Filter out the nodata values (class 2 -&gt; invalid data)\n    # quality_mask = erode_mask(tile[\"quality_data_mask\"] == 2, mask_erosion_size, device)\n    # labels_rasterized = xr.where(quality_mask, labels_rasterized, 2)\n\n    # Transpose to (H, W)\n    tile = tile.transpose(\"y\", \"x\")\n    n_bands = len(bands)\n    tensor_labels = torch.tensor(labels_rasterized.values, device=device).float()\n    tensor_tile = torch.zeros((n_bands, tile.dims[\"y\"], tile.dims[\"x\"]), device=device)\n    invalid_mask = (tile[\"quality_data_mask\"] == 0).values\n    # This will also order the data into the correct order of bands\n    for i, band in enumerate(bands):\n        if band.name not in tile:\n            raise ValueError(f\"Band '{band.name}' not found in the preprocessed data.\")\n        band_data = torch.tensor(tile[band.name].values, device=device).float()\n        # Normalize the bands and clip the values\n        band_data = band_data * band.factor + band.offset\n        band_data = band_data.clip(0, 1)\n        # Apply the quality mask\n        band_data[invalid_mask] = float(\"nan\")\n        # Merge with the tile and move back to cpu\n        tensor_tile[i] = band_data\n\n    assert tensor_tile.dim() == 3, f\"Expects tensor_tile to has shape (C, H, W), got {tensor_tile.shape}\"\n    assert tensor_labels.dim() == 2, f\"Expects tensor_labels to has shape (H, W), got {tensor_labels.shape}\"\n\n    # Create patches\n    tensor_patches = create_patches(tensor_tile.unsqueeze(0), patch_size, overlap)\n    tensor_patches = tensor_patches.reshape(-1, n_bands, patch_size, patch_size)\n    tensor_labels, tensor_coords = create_patches(\n        tensor_labels.unsqueeze(0).unsqueeze(0), patch_size, overlap, return_coords=True\n    )\n    tensor_labels = tensor_labels.reshape(-1, patch_size, patch_size)\n    tensor_coords = tensor_coords.reshape(-1, 5).to(device=device)\n\n    # Filter out patches based on settings\n    few_visible = ((tensor_labels != 2).sum(dim=(1, 2)) / tensor_labels[0].numel()) &lt; 0.1\n    logger.debug(f\"Excluding {few_visible.sum().item()} patches with less than 10% visible pixels\")\n    all_nans = torch.isnan(tensor_patches).all(dim=(2, 3)).any(dim=1)\n    logger.debug(f\"Excluding {all_nans.sum().item()} patches where everything is nan\")\n    filter_mask = few_visible | all_nans\n    if exclude_nopositive:\n        nopositives = (tensor_labels == 1).any(dim=(1, 2))\n        logger.debug(f\"Excluding {nopositives.sum().item()} patches with no positive labels\")\n        filter_mask |= ~nopositives\n    if exclude_nan:\n        has_nans = torch.isnan(tensor_patches).any(dim=(1, 2, 3))\n        logger.debug(f\"Excluding {has_nans.sum().item()} patches with nan values\")\n        filter_mask |= has_nans\n\n    n_patches = tensor_patches.shape[0]\n    logger.debug(f\"Using {n_patches - filter_mask.sum().item()} patches out of {n_patches} total patches\")\n\n    tensor_patches = tensor_patches[~filter_mask].cpu()\n    tensor_labels = tensor_labels[~filter_mask].cpu()\n    tensor_coords = tensor_coords[~filter_mask].cpu()\n    coords = [PatchCoords.from_tensor(tensor_coords[i], patch_size) for i in range(tensor_coords.shape[0])]\n    return tensor_patches, tensor_labels, coords\n</code></pre>"},{"location":"reference/darts_segmentation/training/scoring/","title":"darts_segmentation.training.scoring","text":""},{"location":"reference/darts_segmentation/training/scoring/#darts_segmentation.training.scoring","title":"darts_segmentation.training.scoring","text":"<p>Scoring calculation.</p>"},{"location":"reference/darts_segmentation/training/scoring/#darts_segmentation.training.scoring.check_score_is_unstable","title":"check_score_is_unstable","text":"<pre><code>check_score_is_unstable(\n    run_info: dict, scoring_metric: list[str] | str\n) -&gt; bool\n</code></pre> <p>Check the stability of the scoring metric.</p> <p>If any metric value is not finite or equal to zero, the scoring metric is considered unstable.</p> <p>Parameters:</p> <ul> <li> <code>run_info</code>               (<code>dict</code>)           \u2013            <p>The run information.</p> </li> <li> <code>scoring_metric</code>               (<code>list[str] | str</code>)           \u2013            <p>The scoring metric.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the scoring metric is unstable, False otherwise.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown scoring metric type is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/scoring.py</code> <pre><code>def check_score_is_unstable(run_info: dict, scoring_metric: list[str] | str) -&gt; bool:\n    \"\"\"Check the stability of the scoring metric.\n\n    If any metric value is not finite or equal to zero, the scoring metric is considered unstable.\n\n    Args:\n        run_info (dict): The run information.\n        scoring_metric (list[str] | str): The scoring metric.\n\n    Returns:\n        bool: True if the scoring metric is unstable, False otherwise.\n\n    Raises:\n        ValueError: If an unknown scoring metric type is provided.\n\n    \"\"\"\n    # Single score in list\n    if isinstance(scoring_metric, list) and len(scoring_metric) == 1:\n        scoring_metric = scoring_metric[0]\n\n    if isinstance(scoring_metric, str):\n        metric_value = run_info[scoring_metric]\n        is_unstable = not isfinite(metric_value) or metric_value == 0\n        return is_unstable\n    elif isinstance(scoring_metric, list):\n        metric_values = [run_info[metric] for metric in scoring_metric]\n        is_unstable = any(not isfinite(val) or val == 0 for val in metric_values)\n        return is_unstable\n    else:\n        raise ValueError(\"Invalid scoring metric type\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/scoring/#darts_segmentation.training.scoring.score_from_runs","title":"score_from_runs","text":"<pre><code>score_from_runs(\n    run_infos: list[dict[str, float]],\n    scoring_metric: list[str] | str,\n    multi_score_strategy: typing.Literal[\n        \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n    ] = \"harmonic\",\n) -&gt; float\n</code></pre> <p>Calculate a score from run metrics.</p> <p>Each metric can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each run the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>Ignores unstable runs when multi_score_strategy is \"harmonic\" or \"geometric\" If no runs are left, return 0. An unstable run is where one of the metrics is not finite or zero.</p> <p>Parameters:</p> <ul> <li> <code>run_infos</code>               (<code>list[dict[str, float]]</code>)           \u2013            <p>List of dictionaries containing run information and metrics</p> </li> <li> <code>scoring_metric</code>               (<code>list[str] | str</code>)           \u2013            <p>Metric(s) to use for scoring.</p> </li> <li> <code>multi_score_strategy</code>               (<code>typing.Literal['harmonic', 'arithmetic', 'geometric', 'min']</code>, default:                   <code>'harmonic'</code> )           \u2013            <p>Strategy for combining multiple metrics. Defaults to \"harmonic\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>The calculated score</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown multi-score strategy is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/scoring.py</code> <pre><code>def score_from_runs(  # noqa: C901\n    run_infos: list[dict[str, float]],\n    scoring_metric: list[str] | str,\n    multi_score_strategy: Literal[\"harmonic\", \"arithmetic\", \"geometric\", \"min\"] = \"harmonic\",\n) -&gt; float:\n    \"\"\"Calculate a score from run metrics.\n\n    Each metric can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each run the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    Ignores unstable runs when multi_score_strategy is \"harmonic\" or \"geometric\"\n    If no runs are left, return 0.\n    An unstable run is where one of the metrics is not finite or zero.\n\n    Args:\n        run_infos (list[dict[str, float]]): List of dictionaries containing run information and metrics\n        scoring_metric (list[str] | str): Metric(s) to use for scoring.\n        multi_score_strategy (Literal[\"harmonic\", \"arithmetic\", \"geometric\", \"min\"], optional):\n            Strategy for combining multiple metrics. Defaults to \"harmonic\".\n\n    Returns:\n        float: The calculated score\n\n    Raises:\n        ValueError: If an unknown multi-score strategy is provided.\n\n    \"\"\"\n    # Single score in list\n    if isinstance(scoring_metric, list) and len(scoring_metric) == 1:\n        scoring_metric = scoring_metric[0]\n\n    # Case single score\n    if isinstance(scoring_metric, str):\n        # In case the use set a specific direction\n        scoring_metric = scoring_metric.removesuffix(\":higher\").removesuffix(\":lower\")\n        metric_values = [run_info[scoring_metric] for run_info in run_infos]\n        score = mean(metric_values)\n    # Case multiple scores\n    elif isinstance(scoring_metric, list):\n        scores = []\n        for run_info in run_infos:\n            # Check if we can calculate a score\n            is_unstable = check_score_is_unstable(run_info, scoring_metric)\n            if is_unstable and multi_score_strategy in [\"harmonic\", \"geometric\"]:\n                continue\n            metric_values = []\n            for metric in scoring_metric:\n                higher_is_better = False if metric.endswith(\":lower\") else True\n                metric = metric.removesuffix(\":higher\").removesuffix(\":lower\")\n                val = run_info[metric]\n                metric_values.append(val if higher_is_better else 1 / val)\n\n            match multi_score_strategy:\n                case \"harmonic\":\n                    run_score = harmonic_mean(metric_values)\n                case \"arithmetic\":\n                    run_score = mean(metric_values)\n                case \"min\":\n                    run_score = min(metric_values)\n                case \"geometric\":\n                    run_score = geometric_mean(metric_values)\n                case _:\n                    raise ValueError(\"If an unknown multi-score strategy is provided.\")\n            scores.append(run_score)\n        if len(scores) == 0:\n            score = 0.0\n        elif len(scores) == 1:\n            score = scores[0]\n        else:\n            score = mean(scores)\n\n    return score\n</code></pre>"},{"location":"reference/darts_segmentation/training/scoring/#darts_segmentation.training.scoring.score_from_single_run","title":"score_from_single_run","text":"<pre><code>score_from_single_run(\n    run_info: dict[str, float],\n    scoring_metric: list[str] | str,\n    multi_score_strategy: typing.Literal[\n        \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n    ] = \"harmonic\",\n) -&gt; float\n</code></pre> <p>Calculate a score from run metrics.</p> <p>Each metric can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combining the metrics through the specified strategy. Please refer to the documentation to understand the different multi-score strategies.</p> <p>Unstable runs when multi_score_strategy is \"harmonic\" or \"geometric\" will result in a score of 0. An unstable run is where one of the metrics is not finite or zero.</p> <p>Parameters:</p> <ul> <li> <code>run_info</code>               (<code>dict[str, float]</code>)           \u2013            <p>Dictionary containing run information and metrics</p> </li> <li> <code>scoring_metric</code>               (<code>list[str] | str</code>)           \u2013            <p>Metric(s) to use for scoring.</p> </li> <li> <code>multi_score_strategy</code>               (<code>typing.Literal['harmonic', 'arithmetic', 'geometric', 'min']</code>, default:                   <code>'harmonic'</code> )           \u2013            <p>Strategy for combining multiple metrics. Defaults to \"harmonic\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>The calculated score</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown multi-score strategy is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/scoring.py</code> <pre><code>def score_from_single_run(\n    run_info: dict[str, float],\n    scoring_metric: list[str] | str,\n    multi_score_strategy: Literal[\"harmonic\", \"arithmetic\", \"geometric\", \"min\"] = \"harmonic\",\n) -&gt; float:\n    \"\"\"Calculate a score from run metrics.\n\n    Each metric can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combining the metrics through the specified strategy.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    Unstable runs when multi_score_strategy is \"harmonic\" or \"geometric\" will result in a score of 0.\n    An unstable run is where one of the metrics is not finite or zero.\n\n    Args:\n        run_info (dict[str, float]): Dictionary containing run information and metrics\n        scoring_metric (list[str] | str): Metric(s) to use for scoring.\n        multi_score_strategy (Literal[\"harmonic\", \"arithmetic\", \"geometric\", \"min\"], optional):\n            Strategy for combining multiple metrics. Defaults to \"harmonic\".\n\n    Returns:\n        float: The calculated score\n\n    Raises:\n        ValueError: If an unknown multi-score strategy is provided.\n\n    \"\"\"\n    if isinstance(scoring_metric, str):\n        return run_info[scoring_metric]\n\n    scores = [run_info[metric] for metric in scoring_metric]\n    is_unstable = check_score_is_unstable(run_info, scoring_metric)\n    if is_unstable and multi_score_strategy in [\"harmonic\", \"geometric\"]:\n        return 0.0\n    match multi_score_strategy:\n        case \"harmonic\":\n            return harmonic_mean(scores)\n        case \"arithmetic\":\n            return mean(scores)\n        case \"geometric\":\n            return geometric_mean(scores)\n        case \"min\":\n            return min(scores)\n        case _:\n            raise ValueError(f\"Unknown multi-score strategy: {multi_score_strategy}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/","title":"darts_segmentation.training.train","text":""},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train","title":"darts_segmentation.training.train","text":"<p>Training scripts for DARTS.</p>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig","title":"DataConfig  <code>dataclass</code>","text":"<pre><code>DataConfig(\n    train_data_dir: pathlib.Path = pathlib.Path(\"train\"),\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n    ] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n)\n</code></pre> <p>Data related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script. Defaults to \"train\".</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>)           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>)           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified']</code>)           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>)           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>)           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.data_split_by","title":"data_split_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_by: list[str | float] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.data_split_method","title":"data_split_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_method: (\n    typing.Literal[\"random\", \"region\", \"sample\"] | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.fold_method","title":"fold_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold_method: typing.Literal[\n    \"kfold\",\n    \"shuffle\",\n    \"stratified\",\n    \"region\",\n    \"region-stratified\",\n] = \"kfold\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.subsample","title":"subsample  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subsample: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.total_folds","title":"total_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_folds: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DataConfig.train_data_dir","title":"train_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_data_dir: pathlib.Path = pathlib.Path('train')\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig","title":"DeviceConfig  <code>dataclass</code>","text":"<pre><code>DeviceConfig(\n    accelerator: typing.Literal[\n        \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n    ] = \"auto\",\n    strategy: typing.Literal[\n        \"auto\",\n        \"ddp\",\n        \"ddp_fork\",\n        \"ddp_notebook\",\n        \"fsdp\",\n        \"cv-parallel\",\n        \"tune-parallel\",\n    ] = \"auto\",\n    devices: list[int | str] = lambda: [\"auto\"](),\n    num_nodes: int = 1,\n)\n</code></pre> <p>Device and Distributed Strategy related parameters.</p> <p>Attributes:</p> <ul> <li> <code>accelerator</code>               (<code>typing.Literal['auto', 'cpu', 'gpu', 'mps', 'tpu']</code>)           \u2013            <p>Accelerator to use. Defaults to \"auto\".</p> </li> <li> <code>strategy</code>               (<code>typing.Literal['auto', 'ddp', 'ddp_fork', 'ddp_notebook', 'fsdp', 'cv-parallel', 'tune-parallel', 'cv-parallel', 'tune-parallel']</code>)           \u2013            <p>Distributed strategy to use. Defaults to \"auto\".</p> </li> <li> <code>devices</code>               (<code>list[int | str]</code>)           \u2013            <p>List of devices to use. Defaults to [\"auto\"].</p> </li> <li> <code>num_nodes</code>               (<code>int</code>)           \u2013            <p>Number of nodes to use for distributed training. Defaults to 1.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.accelerator","title":"accelerator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>accelerator: typing.Literal[\n    \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int | str] = dataclasses.field(\n    default_factory=lambda: [\"auto\"]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.lightning_strategy","title":"lightning_strategy  <code>property</code>","text":"<pre><code>lightning_strategy: str\n</code></pre> <p>Get the Lightning strategy for the current configuration.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The Lightning strategy to use.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.num_nodes","title":"num_nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_nodes: int = 1\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.strategy","title":"strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strategy: typing.Literal[\n    \"auto\",\n    \"ddp\",\n    \"ddp_fork\",\n    \"ddp_notebook\",\n    \"fsdp\",\n    \"cv-parallel\",\n    \"tune-parallel\",\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.DeviceConfig.in_parallel","title":"in_parallel","text":"<pre><code>in_parallel(\n    device: int | str | None = None,\n) -&gt; darts_segmentation.training.train.DeviceConfig\n</code></pre> <p>Turn the current configuration into a suitable configuration for parallel training.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to use for parallel training. If None, assumes non-multiprocessing parallel training and propagate all devices. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceConfig</code> (              <code>darts_segmentation.training.train.DeviceConfig</code> )          \u2013            <p>A new DeviceConfig instance that is suitable for parallel training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def in_parallel(self, device: int | str | None = None) -&gt; \"DeviceConfig\":\n    \"\"\"Turn the current configuration into a suitable configuration for parallel training.\n\n    Args:\n        device (int | str | None, optional): The device to use for parallel training.\n            If None, assumes non-multiprocessing parallel training and propagate all devices.\n            Defaults to None.\n\n    Returns:\n        DeviceConfig: A new DeviceConfig instance that is suitable for parallel training.\n\n    \"\"\"\n    # In case of parallel training via multiprocessing, only few strategies are allowed.\n    if self.strategy in [\"ddp\", \"ddp_fork\", \"ddp_notebook\", \"fsdp\"]:\n        logger.warning(\"Using 'ddp_fork' instead of 'ddp' for multiprocessing.\")\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=\"ddp_fork\",  # Fork is the only supported strategy for multiprocessing\n            devices=self.devices,\n            num_nodes=self.num_nodes,\n        )\n    elif device is not None:\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=self.strategy,\n            # If a device is specified, we assume that we want to run on a single device\n            devices=[device],\n            num_nodes=1,\n        )\n    else:\n        return self\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters","title":"Hyperparameters  <code>dataclass</code>","text":"<pre><code>Hyperparameters(\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Hyperparameters for Cyclopts CLI.</p> <p>Attributes:</p> <ul> <li> <code>model_arch</code>               (<code>str</code>)           \u2013            <p>Architecture of the model to use.</p> </li> <li> <code>model_encoder</code>               (<code>str</code>)           \u2013            <p>Encoder type for the model.</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>)           \u2013            <p>Weights for the encoder, if any.</p> </li> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>)           \u2013            <p>Learning rate for training.</p> </li> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Decay factor for learning rate.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float | None</code>)           \u2013            <p>Alpha parameter for focal loss, if using.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>)           \u2013            <p>Gamma parameter for focal loss.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>)           \u2013            <p>List of bands to use. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.augment","title":"augment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>augment: (\n    list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.bands","title":"bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bands: list[str] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.focal_loss_alpha","title":"focal_loss_alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_alpha: float | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.focal_loss_gamma","title":"focal_loss_gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_gamma: float = 2.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.9\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.001\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.model_arch","title":"model_arch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_arch: str = 'Unet'\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.model_encoder","title":"model_encoder  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder: str = 'dpn107'\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.Hyperparameters.model_encoder_weights","title":"model_encoder_weights  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder_weights: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig","title":"LoggingConfig  <code>dataclass</code>","text":"<pre><code>LoggingConfig(\n    artifact_dir: pathlib.Path = pathlib.Path(\"artifacts\"),\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Logging related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Top-level path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"artifacts\").</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>)           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>)           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>)           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.artifact_dir","title":"artifact_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>artifact_dir: pathlib.Path = pathlib.Path('artifacts')\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.check_val_every_n_epoch","title":"check_val_every_n_epoch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>check_val_every_n_epoch: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.log_every_n_steps","title":"log_every_n_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_every_n_steps: int = 10\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.wandb_entity","title":"wandb_entity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_entity: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.wandb_project","title":"wandb_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_project: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.artifact_dir_at_cv","title":"artifact_dir_at_cv","text":"<pre><code>artifact_dir_at_cv(tune_name: str | None) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory for cross-validation runs.</p> <p>Similar to <code>parse_artifact_dir_for_run</code>, but meant to be used by the cross-validation script.</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning, if applicable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path for cross-validation runs.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_cv(self, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory for cross-validation runs.\n\n    Similar to `parse_artifact_dir_for_run`, but meant to be used by the cross-validation script.\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        tune_name (str | None): Name of the tuning, if applicable.\n\n    Returns:\n        Path: The nested artifact directory path for cross-validation runs.\n\n    \"\"\"\n    artifact_dir = self.artifact_dir / tune_name if tune_name else self.artifact_dir / \"_cross_validations\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.LoggingConfig.artifact_dir_at_run","title":"artifact_dir_at_run","text":"<pre><code>artifact_dir_at_run(\n    cv_name: str | None, tune_name: str | None\n) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory to avoid cluttering the root directory.</p> <p>For cv it is expected that the cv function already nests the artifact directory Meaning for cv the artifact_dir of this function should be either {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If tune_name is specified, but cv_name is not, which is invalid.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_run(self, cv_name: str | None, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory to avoid cluttering the root directory.\n\n    For cv it is expected that the cv function already nests the artifact directory\n    Meaning for cv the artifact_dir of this function should be either\n    {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        cv_name (str | None): Name of the cross-validation.\n        tune_name (str | None): Name of the tuning.\n\n    Raises:\n        ValueError: If tune_name is specified, but cv_name is not, which is invalid.\n\n    Returns:\n        Path: The nested artifact directory path.\n\n    \"\"\"\n    # Run only\n    if cv_name is None and tune_name is None:\n        artifact_dir = self.artifact_dir / \"_runs\"\n    # Cross-validation only\n    elif cv_name is not None and tune_name is None:\n        artifact_dir = self.artifact_dir / \"_cross_validations\" / cv_name\n    # Cross-validation and tuning\n    elif cv_name is not None and tune_name is not None:\n        artifact_dir = self.artifact_dir / tune_name / cv_name\n    # Tuning only (invalid)\n    else:\n        raise ValueError(\n            \"Cannot parse artifact directory for cross-validation and tuning. \"\n            \"Please specify either cv_name or tune_name, but not both.\"\n        )\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig","title":"TrainRunConfig  <code>dataclass</code>","text":"<pre><code>TrainRunConfig(\n    name: str | None = None,\n    cv_name: str | None = None,\n    tune_name: str | None = None,\n    fold: int = 0,\n    random_seed: int = 42,\n)\n</code></pre> <p>Run related parameters for training.</p> <p>Defines the script inputs for the training script. Must be build by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str | None</code>)           \u2013            <p>Name of the run. If None is generated automatically. Defaults to None.</p> </li> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation. Should only be specified by a cross-validation script. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>fold</code>               (<code>int</code>)           \u2013            <p>Index of the current fold. Defaults to 0.</p> </li> <li> <code>random_seed</code>               (<code>int</code>)           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.cv_name","title":"cv_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cv_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.fold","title":"fold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: int = 42\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainRunConfig.tune_name","title":"tune_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<pre><code>TrainingConfig(\n    continue_from_checkpoint: pathlib.Path | None = None,\n    max_epochs: int = 100,\n    early_stopping_patience: int = 5,\n    num_workers: int = 0,\n)\n</code></pre> <p>Training related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Path to a checkpoint to continue training from. Defaults to None.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>)           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>)           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.continue_from_checkpoint","title":"continue_from_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>continue_from_checkpoint: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.early_stopping_patience","title":"early_stopping_patience  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>early_stopping_patience: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.max_epochs","title":"max_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_epochs: int = 100\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.TrainingConfig.num_workers","title":"num_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_workers: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.convert_lightning_checkpoint","title":"convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.test_smp","title":"test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    bands: list[str] | None = None,\n    artifact_dir: pathlib.Path = pathlib.Path(\"artifacts\"),\n    num_workers: int = 0,\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script.</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be specified by providing a float between 0 and 1 to data_split_by. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str] | str | float | None</code>, default:                   <code>None</code> )           \u2013            <p>Select by which seed/regions/samples split. Defaults to None.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of bands to use. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('artifacts')</code> )           \u2013            <p>Directory to save artifacts. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    data_split_method: Literal[\"random\", \"region\", \"sample\"] | None = None,\n    data_split_by: list[str] | str | float | None = None,\n    bands: list[str] | None = None,\n    artifact_dir: Path = Path(\"artifacts\"),\n    num_workers: int = 0,\n    device_config: DeviceConfig = DeviceConfig(),\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): The path (top-level) to the data to be used for training.\n            Expects a directory containing:\n            1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array\n            2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.\n                This metadata should contain at least the following columns:\n                - \"sample_id\": The id of the sample\n                - \"region\": The region the sample belongs to\n                - \"empty\": Whether the image is empty\n                The index should refer to the index of the sample in the zarr data.\n            This directory should be created by a preprocessing script.\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int): Batch size for training and validation.\n        data_split_method (Literal[\"random\", \"region\", \"sample\"] | None, optional):\n            The method to use for splitting the data into a train and a test set.\n            \"random\" will split the data randomly, the seed is always 42 and the size of the test set can be\n            specified by providing a float between 0 and 1 to data_split_by.\n            \"region\" will split the data by one or multiple regions,\n            which can be specified by providing a str or list of str to data_split_by.\n            \"sample\" will split the data by sample ids, which can also be specified similar to \"region\".\n            If None, no split is done and the complete dataset is used for both training and testing.\n            The train split will further be split in the cross validation process.\n            Defaults to None.\n        data_split_by (list[str] | str | float | None, optional): Select by which seed/regions/samples split.\n            Defaults to None.\n        bands (list[str] | None, optional): List of bands to use. Defaults to None.\n        artifact_dir (Path, optional): Directory to save artifacts. Defaults to Path(\"lightning_logs\").\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device_config (DeviceConfig, optional): Device and distributed strategy related parameters.\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar, ThroughputMonitor\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n    from darts_segmentation.utils import Bands\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Further nest the artifact directory to avoid cluttering the root directory\n    artifact_dir = artifact_dir / \"_runs\"\n\n    logger.info(\n        f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device_config}\")\n\n    lovely_tensors.set_config(color=False)\n    lovely_tensors.monkey_patch()\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    data_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    all_bands = Bands.from_config(data_config)\n    bands = all_bands.filter(bands) if bands else all_bands\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir,\n        batch_size=batch_size,\n        data_split_method=data_split_method,\n        data_split_by=data_split_by,\n        bands=bands,\n        num_workers=num_workers,\n    )\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    logger.debug(f\"Using model checkpoint at {model_ckp.resolve()}\")\n    model = LitSMP.load_from_checkpoint(model_ckp)\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=\"none\",\n            job_type=\"none\",\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            bands=bands,\n            batch_size=batch_size,\n            patch_size=data_config[\"patch_size\"],\n        ),\n        ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0)),\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=device_config.accelerator,\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        devices=device_config.devices,\n        deterministic=True,\n    )\n\n    trainer.test(model, datamodule, ckpt_path=model_ckp)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/train/#darts_segmentation.training.train.train_smp","title":"train_smp","text":"<pre><code>train_smp(\n    *,\n    run: darts_segmentation.training.train.TrainRunConfig = darts_segmentation.training.train.TrainRunConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    hparams: darts_segmentation.training.hparams.Hyperparameters = darts_segmentation.training.hparams.Hyperparameters(),\n)\n</code></pre> <p>Run the training of the SMP model, specifically binary segmentation.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This training function is meant for single training runs but is also used for cross-validation and hyperparameter tuning by cv.py and tune.py. This strongly affects where artifacts are stored:</p> <ul> <li>Run was created by a tune: <code>{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}</code></li> <li>Run was created by a cross-validation: <code>{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}</code></li> <li>Single runs: <code>{artifact_dir}/_runs/{run_name}-{run_id}</code></li> </ul> <p><code>run_name</code> can be specified by the user, else it is generated automatically. In case of cross-validation, the run name is generated automatically by the cross-validation. <code>run_id</code> is generated automatically by the training function. Both are saved to the final checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Data related parameters for training.</p> </li> <li> <code>run</code>               (<code>darts_segmentation.training.train.TrainRunConfig</code>, default:                   <code>darts_segmentation.training.train.TrainRunConfig()</code> )           \u2013            <p>Run related parameters for training.</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Logging related parameters for training.</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Device and distributed strategy related parameters.</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Training related parameters for training.</p> </li> <li> <code>hparams</code>               (<code>darts_segmentation.training.hparams.Hyperparameters</code>, default:                   <code>darts_segmentation.training.hparams.Hyperparameters()</code> )           \u2013            <p>Hyperparameters for the model.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>pl.Trainer: The trainer object used for training. Contains also metrics.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def train_smp(\n    *,\n    run: TrainRunConfig = TrainRunConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    hparams: Hyperparameters = Hyperparameters(),\n):\n    \"\"\"Run the training of the SMP model, specifically binary segmentation.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This training function is meant for single training runs but is also used for cross-validation and hyperparameter\n    tuning by cv.py and tune.py.\n    This strongly affects where artifacts are stored:\n\n    - Run was created by a tune: `{artifact_dir}/{tune_name}/{cv_name}/{run_name}-{run_id}`\n    - Run was created by a cross-validation: `{artifact_dir}/_cross_validations/{cv_name}/{run_name}-{run_id}`\n    - Single runs: `{artifact_dir}/_runs/{run_name}-{run_id}`\n\n    `run_name` can be specified by the user, else it is generated automatically.\n    In case of cross-validation, the run name is generated automatically by the cross-validation.\n    `run_id` is generated automatically by the training function.\n    Both are saved to the final checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        data_config (DataConfig): Data related parameters for training.\n        run (TrainRunConfig): Run related parameters for training.\n        logging_config (LoggingConfig): Logging related parameters for training.\n        device_config (DeviceConfig): Device and distributed strategy related parameters.\n        training_config (TrainingConfig): Training related parameters for training.\n        hparams (Hyperparameters): Hyperparameters for the model.\n\n    Returns:\n        pl.Trainer: The trainer object used for training. Contains also metrics.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts.utils.logging import LoggingManager\n    from darts_utils.namegen import generate_counted_name, generate_id\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics, BinarySegmentationPreview\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import LitSMP\n    from darts_segmentation.utils import Bands\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\", level=logging.INFO)\n\n    tick_fstart = time.perf_counter()\n\n    # Get the right nesting of the artifact directory\n    artifact_dir = logging_config.artifact_dir_at_run(run.cv_name, run.tune_name)\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run.name or generate_counted_name(artifact_dir)\n    run_id = generate_id()  # Needed for wandb\n\n    logger.info(\n        f\"Starting training '{run_name}' ('{run_id}') with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {(artifact_dir / f'{run_name}-{run_id}').resolve()}.\"\n    )\n    logger.debug(\n        f\"Using config:\\n\\t{run}\\n\\t{training_config}\\n\\t{data_config}\\n\\t{logging_config}\\n\\t\"\n        f\"{device_config}\\n\\t{hparams}\"\n    )\n    if training_config.continue_from_checkpoint:\n        logger.debug(f\"Continuing from checkpoint '{training_config.continue_from_checkpoint.resolve()}'\")\n\n    lovely_tensors.monkey_patch()\n    lovely_tensors.set_config(color=False)\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(run.random_seed, workers=True, verbose=False)\n\n    dataset_config = toml.load(data_config.train_data_dir / \"config.toml\")[\"darts\"]\n    all_bands = Bands.from_config(dataset_config)\n    bands = all_bands.filter(hparams.bands) if hparams.bands else all_bands\n    config = SMPSegmenterConfig(\n        bands=bands,\n        model={\n            \"arch\": hparams.model_arch,\n            \"encoder_name\": hparams.model_encoder,\n            \"encoder_weights\": hparams.model_encoder_weights,\n            \"in_channels\": len(all_bands) if bands is None else len(bands),\n            \"classes\": 1,\n        },\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=data_config.train_data_dir,\n        batch_size=hparams.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        fold_method=data_config.fold_method,\n        total_folds=data_config.total_folds,\n        fold=run.fold,\n        subsample=data_config.subsample,\n        bands=hparams.bands,\n        augment=hparams.augment,\n        num_workers=training_config.num_workers,\n    )\n    model = LitSMP(\n        config=config,\n        learning_rate=hparams.learning_rate,\n        gamma=hparams.gamma,\n        focal_loss_alpha=hparams.focal_loss_alpha,\n        focal_loss_gamma=hparams.focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        cv_name=run.cv_name or \"none\",\n        tune_name=run.tune_name or \"none\",\n        random_seed=run.random_seed,\n    )\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=None, version=f\"{run_name}-{run_id}\"),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        tags = [data_config.train_data_dir.stem]\n        if run.cv_name:\n            tags.append(run.cv_name)\n        if run.tune_name:\n            tags.append(run.tune_name)\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir.parent.parent if run.tune_name or run.cv_name else artifact_dir.parent,\n            name=run_name,\n            version=run_id,\n            project=logging_config.wandb_project,\n            entity=logging_config.wandb_entity,\n            resume=\"allow\",\n            # Using the group and job_type is a workaround for wandb's lack of support for manually sweeps\n            group=run.tune_name or \"none\",\n            job_type=run.cv_name or \"none\",\n            # Using tags to quickly identify the run\n            tags=tags,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{logging_config.wandb_entity}' and project '{logging_config.wandb_project}'\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks and profiler\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            bands=bands,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            is_crossval=bool(run.cv_name),\n            batch_size=hparams.batch_size,\n            patch_size=dataset_config[\"patch_size\"],\n        ),\n        BinarySegmentationPreview(\n            bands=bands,\n            val_set=f\"val{run.fold}\",\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n        ),\n        # Something does not work well here...\n        # ThroughputMonitor(batch_size_fn=lambda batch: batch[0].size(0), window_size=log_every_n_steps),\n    ]\n    if training_config.early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {training_config.early_stopping_patience}\")\n        early_stopping = EarlyStopping(\n            monitor=\"val/JaccardIndex\", mode=\"max\", patience=training_config.early_stopping_patience\n        )\n        callbacks.append(early_stopping)\n\n    # Unsupported: https://github.com/Lightning-AI/pytorch-lightning/issues/19983\n    # profiler_dir = artifact_dir / f\"{run_name}-{run_id}\" / \"profiler\"\n    # profiler_dir.mkdir(parents=True, exist_ok=True)\n    # profiler = AdvancedProfiler(dirpath=profiler_dir, filename=\"perf_logs\", dump_stats=True)\n    # logger.debug(f\"Using profiler with output to {profiler.dirpath.resolve()}\")\n\n    logger.debug(\n        f\"Creating lightning-trainer on {device_config.accelerator} with devices {device_config.devices}\"\n        f\" and strategy '{device_config.lightning_strategy}'\"\n    )\n    # Train\n    trainer = L.Trainer(\n        max_epochs=training_config.max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=logging_config.log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n        accelerator=device_config.accelerator,\n        devices=device_config.devices if device_config.devices[0] != \"auto\" else \"auto\",\n        strategy=device_config.lightning_strategy,\n        num_nodes=device_config.num_nodes,\n        deterministic=False,  # True does not work for some reason\n        # profiler=profiler,\n    )\n    trainer.fit(model, datamodule, ckpt_path=training_config.continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if logging_config.wandb_entity and logging_config.wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/","title":"darts_segmentation.training.tune","text":""},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune","title":"darts_segmentation.training.tune","text":"<p>More advanced hyper-parameter tuning.</p>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.available_devices","title":"available_devices  <code>module-attribute</code>","text":"<pre><code>available_devices = multiprocessing.Queue()\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig","title":"CrossValidationConfig  <code>dataclass</code>","text":"<pre><code>CrossValidationConfig(\n    n_folds: int | None = None,\n    n_randoms: int = 3,\n    scoring_metric: list[str] = lambda: [\n        \"val/JaccardIndex\",\n        \"val/Recall\",\n    ](),\n    multi_score_strategy: typing.Literal[\n        \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n    ] = \"harmonic\",\n)\n</code></pre> <p>Configuration for cross-validation.</p> <p>This is used to configure the cross-validation process. It is used by the <code>cross_validation_smp</code> function.</p> <p>Attributes:</p> <ul> <li> <code>n_folds</code>               (<code>int | None</code>)           \u2013            <p>Number of folds to perform in cross-validation. If None, all folds (total_folds) will be used. Defaults to None.</p> </li> <li> <code>n_randoms</code>               (<code>int</code>)           \u2013            <p>Number of random seeds to perform in cross-validation. First three seeds are always 42, 21, 69, further seeds are deterministic generated. Defaults to 3.</p> </li> <li> <code>scoring_metric</code>               (<code>list[str]</code>)           \u2013            <p>Metric(s) to use for scoring. Defaults to [\"val/JaccardIndex\", \"val/Recall\"].</p> </li> <li> <code>multi_score_strategy</code>               (<code>typing.Literal['harmonic', 'arithmetic', 'geometric', 'min']</code>)           \u2013            <p>Strategy for combining multiple metrics. Defaults to \"harmonic\".</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.multi_score_strategy","title":"multi_score_strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>multi_score_strategy: typing.Literal[\n    \"harmonic\", \"arithmetic\", \"geometric\", \"min\"\n] = \"harmonic\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.n_folds","title":"n_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_folds: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.n_randoms","title":"n_randoms  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_randoms: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.rng_seeds","title":"rng_seeds  <code>property</code>","text":"<pre><code>rng_seeds: list[int]\n</code></pre> <p>Generate a list of seeds for cross-validation.</p> <p>Returns:</p> <ul> <li> <code>list[int]</code>           \u2013            <p>list[int]: A list of seeds for cross-validation.</p> </li> <li> <code>list[int]</code>           \u2013            <p>The first three seeds are always 42, 21, 69, further seeds are deterministically generated.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.CrossValidationConfig.scoring_metric","title":"scoring_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scoring_metric: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"val/JaccardIndex\",\n        \"val/Recall\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig","title":"DataConfig  <code>dataclass</code>","text":"<pre><code>DataConfig(\n    train_data_dir: pathlib.Path = pathlib.Path(\"train\"),\n    data_split_method: typing.Literal[\n        \"random\", \"region\", \"sample\"\n    ]\n    | None = None,\n    data_split_by: list[str | float] | None = None,\n    fold_method: typing.Literal[\n        \"kfold\",\n        \"shuffle\",\n        \"stratified\",\n        \"region\",\n        \"region-stratified\",\n    ] = \"kfold\",\n    total_folds: int = 5,\n    subsample: int | None = None,\n)\n</code></pre> <p>Data related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path (top-level) to the data to be used for training. Expects a directory containing: 1. a zarr group called \"data.zarr\" containing a \"x\" and \"y\" array 2. a geoparquet file called \"metadata.parquet\" containing the metadata for the data.     This metadata should contain at least the following columns:     - \"sample_id\": The id of the sample     - \"region\": The region the sample belongs to     - \"empty\": Whether the image is empty     The index should refer to the index of the sample in the zarr data. This directory should be created by a preprocessing script. Defaults to \"train\".</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training and validation.</p> </li> <li> <code>data_split_method</code>               (<code>typing.Literal['random', 'region', 'sample'] | None</code>)           \u2013            <p>The method to use for splitting the data into a train and a test set. \"random\" will split the data randomly, the seed is always 42 and the test size can be specified by providing a list with a single a float between 0 and 1 to data_split_by This will be the fraction of the data to be used for testing. E.g. [0.2] will use 20% of the data for testing. \"region\" will split the data by one or multiple regions, which can be specified by providing a str or list of str to data_split_by. \"sample\" will split the data by sample ids, which can also be specified similar to \"region\". If None, no split is done and the complete dataset is used for both training and testing. The train split will further be split in the cross validation process. Defaults to None.</p> </li> <li> <code>data_split_by</code>               (<code>list[str | float] | None</code>)           \u2013            <p>Select by which regions/samples to split or the size of test set. Defaults to None.</p> </li> <li> <code>fold_method</code>               (<code>typing.Literal['kfold', 'shuffle', 'stratified', 'region', 'region-stratified']</code>)           \u2013            <p>Method for cross-validation split. Defaults to \"kfold\".</p> </li> <li> <code>total_folds</code>               (<code>int</code>)           \u2013            <p>Total number of folds in cross-validation. Defaults to 5.</p> </li> <li> <code>subsample</code>               (<code>int | None</code>)           \u2013            <p>If set, will subsample the dataset to this number of samples. This is useful for debugging and testing. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.data_split_by","title":"data_split_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_by: list[str | float] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.data_split_method","title":"data_split_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_split_method: (\n    typing.Literal[\"random\", \"region\", \"sample\"] | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.fold_method","title":"fold_method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold_method: typing.Literal[\n    \"kfold\",\n    \"shuffle\",\n    \"stratified\",\n    \"region\",\n    \"region-stratified\",\n] = \"kfold\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.subsample","title":"subsample  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subsample: int | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.total_folds","title":"total_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_folds: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DataConfig.train_data_dir","title":"train_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_data_dir: pathlib.Path = pathlib.Path('train')\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig","title":"DeviceConfig  <code>dataclass</code>","text":"<pre><code>DeviceConfig(\n    accelerator: typing.Literal[\n        \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n    ] = \"auto\",\n    strategy: typing.Literal[\n        \"auto\",\n        \"ddp\",\n        \"ddp_fork\",\n        \"ddp_notebook\",\n        \"fsdp\",\n        \"cv-parallel\",\n        \"tune-parallel\",\n    ] = \"auto\",\n    devices: list[int | str] = lambda: [\"auto\"](),\n    num_nodes: int = 1,\n)\n</code></pre> <p>Device and Distributed Strategy related parameters.</p> <p>Attributes:</p> <ul> <li> <code>accelerator</code>               (<code>typing.Literal['auto', 'cpu', 'gpu', 'mps', 'tpu']</code>)           \u2013            <p>Accelerator to use. Defaults to \"auto\".</p> </li> <li> <code>strategy</code>               (<code>typing.Literal['auto', 'ddp', 'ddp_fork', 'ddp_notebook', 'fsdp', 'cv-parallel', 'tune-parallel', 'cv-parallel', 'tune-parallel']</code>)           \u2013            <p>Distributed strategy to use. Defaults to \"auto\".</p> </li> <li> <code>devices</code>               (<code>list[int | str]</code>)           \u2013            <p>List of devices to use. Defaults to [\"auto\"].</p> </li> <li> <code>num_nodes</code>               (<code>int</code>)           \u2013            <p>Number of nodes to use for distributed training. Defaults to 1.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.accelerator","title":"accelerator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>accelerator: typing.Literal[\n    \"auto\", \"cpu\", \"gpu\", \"mps\", \"tpu\"\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.devices","title":"devices  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>devices: list[int | str] = dataclasses.field(\n    default_factory=lambda: [\"auto\"]\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.lightning_strategy","title":"lightning_strategy  <code>property</code>","text":"<pre><code>lightning_strategy: str\n</code></pre> <p>Get the Lightning strategy for the current configuration.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The Lightning strategy to use.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.num_nodes","title":"num_nodes  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_nodes: int = 1\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.strategy","title":"strategy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>strategy: typing.Literal[\n    \"auto\",\n    \"ddp\",\n    \"ddp_fork\",\n    \"ddp_notebook\",\n    \"fsdp\",\n    \"cv-parallel\",\n    \"tune-parallel\",\n] = \"auto\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.DeviceConfig.in_parallel","title":"in_parallel","text":"<pre><code>in_parallel(\n    device: int | str | None = None,\n) -&gt; darts_segmentation.training.train.DeviceConfig\n</code></pre> <p>Turn the current configuration into a suitable configuration for parallel training.</p> <p>Parameters:</p> <ul> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to use for parallel training. If None, assumes non-multiprocessing parallel training and propagate all devices. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DeviceConfig</code> (              <code>darts_segmentation.training.train.DeviceConfig</code> )          \u2013            <p>A new DeviceConfig instance that is suitable for parallel training.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def in_parallel(self, device: int | str | None = None) -&gt; \"DeviceConfig\":\n    \"\"\"Turn the current configuration into a suitable configuration for parallel training.\n\n    Args:\n        device (int | str | None, optional): The device to use for parallel training.\n            If None, assumes non-multiprocessing parallel training and propagate all devices.\n            Defaults to None.\n\n    Returns:\n        DeviceConfig: A new DeviceConfig instance that is suitable for parallel training.\n\n    \"\"\"\n    # In case of parallel training via multiprocessing, only few strategies are allowed.\n    if self.strategy in [\"ddp\", \"ddp_fork\", \"ddp_notebook\", \"fsdp\"]:\n        logger.warning(\"Using 'ddp_fork' instead of 'ddp' for multiprocessing.\")\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=\"ddp_fork\",  # Fork is the only supported strategy for multiprocessing\n            devices=self.devices,\n            num_nodes=self.num_nodes,\n        )\n    elif device is not None:\n        return DeviceConfig(\n            accelerator=self.accelerator,\n            strategy=self.strategy,\n            # If a device is specified, we assume that we want to run on a single device\n            devices=[device],\n            num_nodes=1,\n        )\n    else:\n        return self\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters","title":"Hyperparameters  <code>dataclass</code>","text":"<pre><code>Hyperparameters(\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None = None,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    bands: list[str] | None = None,\n)\n</code></pre> <p>Hyperparameters for Cyclopts CLI.</p> <p>Attributes:</p> <ul> <li> <code>model_arch</code>               (<code>str</code>)           \u2013            <p>Architecture of the model to use.</p> </li> <li> <code>model_encoder</code>               (<code>str</code>)           \u2013            <p>Encoder type for the model.</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>)           \u2013            <p>Weights for the encoder, if any.</p> </li> <li> <code>augment</code>               (<code>list[darts_segmentation.training.augmentations.Augmentation] | None</code>)           \u2013            <p>List of augmentations to apply.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>)           \u2013            <p>Learning rate for training.</p> </li> <li> <code>gamma</code>               (<code>float</code>)           \u2013            <p>Decay factor for learning rate.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float | None</code>)           \u2013            <p>Alpha parameter for focal loss, if using.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>)           \u2013            <p>Gamma parameter for focal loss.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Batch size for training.</p> </li> <li> <code>bands</code>               (<code>list[str] | None</code>)           \u2013            <p>List of bands to use. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.augment","title":"augment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>augment: (\n    list[\n        darts_segmentation.training.augmentations.Augmentation\n    ]\n    | None\n) = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.bands","title":"bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bands: list[str] | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.focal_loss_alpha","title":"focal_loss_alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_alpha: float | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.focal_loss_gamma","title":"focal_loss_gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>focal_loss_gamma: float = 2.0\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 0.9\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.001\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.model_arch","title":"model_arch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_arch: str = 'Unet'\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.model_encoder","title":"model_encoder  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder: str = 'dpn107'\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.Hyperparameters.model_encoder_weights","title":"model_encoder_weights  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_encoder_weights: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig","title":"LoggingConfig  <code>dataclass</code>","text":"<pre><code>LoggingConfig(\n    artifact_dir: pathlib.Path = pathlib.Path(\"artifacts\"),\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Logging related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Top-level path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"artifacts\").</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>)           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>)           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>)           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>)           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.artifact_dir","title":"artifact_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>artifact_dir: pathlib.Path = pathlib.Path('artifacts')\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.check_val_every_n_epoch","title":"check_val_every_n_epoch  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>check_val_every_n_epoch: int = 3\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.log_every_n_steps","title":"log_every_n_steps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_every_n_steps: int = 10\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.wandb_entity","title":"wandb_entity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_entity: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.wandb_project","title":"wandb_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>wandb_project: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.artifact_dir_at_cv","title":"artifact_dir_at_cv","text":"<pre><code>artifact_dir_at_cv(tune_name: str | None) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory for cross-validation runs.</p> <p>Similar to <code>parse_artifact_dir_for_run</code>, but meant to be used by the cross-validation script.</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning, if applicable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path for cross-validation runs.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_cv(self, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory for cross-validation runs.\n\n    Similar to `parse_artifact_dir_for_run`, but meant to be used by the cross-validation script.\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        tune_name (str | None): Name of the tuning, if applicable.\n\n    Returns:\n        Path: The nested artifact directory path for cross-validation runs.\n\n    \"\"\"\n    artifact_dir = self.artifact_dir / tune_name if tune_name else self.artifact_dir / \"_cross_validations\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.LoggingConfig.artifact_dir_at_run","title":"artifact_dir_at_run","text":"<pre><code>artifact_dir_at_run(\n    cv_name: str | None, tune_name: str | None\n) -&gt; pathlib.Path\n</code></pre> <p>Nest the artifact directory to avoid cluttering the root directory.</p> <p>For cv it is expected that the cv function already nests the artifact directory Meaning for cv the artifact_dir of this function should be either {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}</p> <p>Also creates the directory if it does not exist.</p> <p>Parameters:</p> <ul> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If tune_name is specified, but cv_name is not, which is invalid.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code> (              <code>pathlib.Path</code> )          \u2013            <p>The nested artifact directory path.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/train.py</code> <pre><code>def artifact_dir_at_run(self, cv_name: str | None, tune_name: str | None) -&gt; Path:\n    \"\"\"Nest the artifact directory to avoid cluttering the root directory.\n\n    For cv it is expected that the cv function already nests the artifact directory\n    Meaning for cv the artifact_dir of this function should be either\n    {artifact_dir}/_cross_validations/{cv_name} or {artifact_dir}/{tune_name}/{cv_name}\n\n    Also creates the directory if it does not exist.\n\n    Args:\n        cv_name (str | None): Name of the cross-validation.\n        tune_name (str | None): Name of the tuning.\n\n    Raises:\n        ValueError: If tune_name is specified, but cv_name is not, which is invalid.\n\n    Returns:\n        Path: The nested artifact directory path.\n\n    \"\"\"\n    # Run only\n    if cv_name is None and tune_name is None:\n        artifact_dir = self.artifact_dir / \"_runs\"\n    # Cross-validation only\n    elif cv_name is not None and tune_name is None:\n        artifact_dir = self.artifact_dir / \"_cross_validations\" / cv_name\n    # Cross-validation and tuning\n    elif cv_name is not None and tune_name is not None:\n        artifact_dir = self.artifact_dir / tune_name / cv_name\n    # Tuning only (invalid)\n    else:\n        raise ValueError(\n            \"Cannot parse artifact directory for cross-validation and tuning. \"\n            \"Please specify either cv_name or tune_name, but not both.\"\n        )\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n    return artifact_dir\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig","title":"TrainRunConfig  <code>dataclass</code>","text":"<pre><code>TrainRunConfig(\n    name: str | None = None,\n    cv_name: str | None = None,\n    tune_name: str | None = None,\n    fold: int = 0,\n    random_seed: int = 42,\n)\n</code></pre> <p>Run related parameters for training.</p> <p>Defines the script inputs for the training script. Must be build by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str | None</code>)           \u2013            <p>Name of the run. If None is generated automatically. Defaults to None.</p> </li> <li> <code>cv_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the cross-validation. Should only be specified by a cross-validation script. Defaults to None.</p> </li> <li> <code>tune_name</code>               (<code>str | None</code>)           \u2013            <p>Name of the tuning. Should only be specified by a tuning script. Defaults to None.</p> </li> <li> <code>fold</code>               (<code>int</code>)           \u2013            <p>Index of the current fold. Defaults to 0.</p> </li> <li> <code>random_seed</code>               (<code>int</code>)           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.cv_name","title":"cv_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cv_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.fold","title":"fold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fold: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.random_seed","title":"random_seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>random_seed: int = 42\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainRunConfig.tune_name","title":"tune_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_name: str | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<pre><code>TrainingConfig(\n    continue_from_checkpoint: pathlib.Path | None = None,\n    max_epochs: int = 100,\n    early_stopping_patience: int = 5,\n    num_workers: int = 0,\n)\n</code></pre> <p>Training related parameters for training.</p> <p>Defines the script inputs for the training script and can be propagated by the cross-validation and tuning scripts.</p> <p>Attributes:</p> <ul> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>)           \u2013            <p>Path to a checkpoint to continue training from. Defaults to None.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>)           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>)           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>)           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> </ul>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.continue_from_checkpoint","title":"continue_from_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>continue_from_checkpoint: pathlib.Path | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.early_stopping_patience","title":"early_stopping_patience  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>early_stopping_patience: int = 5\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.max_epochs","title":"max_epochs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_epochs: int = 100\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.TrainingConfig.num_workers","title":"num_workers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_workers: int = 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs","title":"_ProcessInputs  <code>dataclass</code>","text":"<pre><code>_ProcessInputs(\n    current: int,\n    total: int,\n    tune_name: str,\n    cv: darts_segmentation.training.cv.CrossValidationConfig,\n    training_config: darts_segmentation.training.train.TrainingConfig,\n    logging_config: darts_segmentation.training.train.LoggingConfig,\n    data_config: darts_segmentation.training.train.DataConfig,\n    device_config: darts_segmentation.training.train.DeviceConfig,\n    hparams: darts_segmentation.training.hparams.Hyperparameters,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.current","title":"current  <code>instance-attribute</code>","text":"<pre><code>current: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.cv","title":"cv  <code>instance-attribute</code>","text":"<pre><code>cv: darts_segmentation.training.cv.CrossValidationConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.data_config","title":"data_config  <code>instance-attribute</code>","text":"<pre><code>data_config: darts_segmentation.training.train.DataConfig\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.device_config","title":"device_config  <code>instance-attribute</code>","text":"<pre><code>device_config: (\n    darts_segmentation.training.train.DeviceConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.hparams","title":"hparams  <code>instance-attribute</code>","text":"<pre><code>hparams: darts_segmentation.training.hparams.Hyperparameters\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.logging_config","title":"logging_config  <code>instance-attribute</code>","text":"<pre><code>logging_config: (\n    darts_segmentation.training.train.LoggingConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.total","title":"total  <code>instance-attribute</code>","text":"<pre><code>total: int\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.training_config","title":"training_config  <code>instance-attribute</code>","text":"<pre><code>training_config: (\n    darts_segmentation.training.train.TrainingConfig\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessInputs.tune_name","title":"tune_name  <code>instance-attribute</code>","text":"<pre><code>tune_name: str\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessOutputs","title":"_ProcessOutputs  <code>dataclass</code>","text":"<pre><code>_ProcessOutputs(\n    run_infos: pandas.DataFrame,\n    score: float,\n    is_unstable: bool,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessOutputs.is_unstable","title":"is_unstable  <code>instance-attribute</code>","text":"<pre><code>is_unstable: bool\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessOutputs.run_infos","title":"run_infos  <code>instance-attribute</code>","text":"<pre><code>run_infos: pandas.DataFrame\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._ProcessOutputs.score","title":"score  <code>instance-attribute</code>","text":"<pre><code>score: float\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune._run_cv","title":"_run_cv","text":"<pre><code>_run_cv(\n    inp: darts_segmentation.training.tune._ProcessInputs,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/tune.py</code> <pre><code>def _run_cv(inp: _ProcessInputs):\n    # Wrapper function for handling parallel multiprocessing training runs.\n    import pandas as pd\n\n    from darts_segmentation.training.cv import cross_validation_smp\n\n    cv_name = f\"{inp.tune_name}-cv{inp.current}\"\n\n    # Setup device configuration: If strategy is \"tune-parallel\" expect a mp scenario:\n    # Wait for a device to become available.\n    # Otherwise, expect a serial scenario, where the devices and strategy are set by the user.\n    is_parallel = inp.device_config.strategy == \"tune-parallel\"\n    if is_parallel:\n        device = available_devices.get()\n        device_config = inp.device_config.in_parallel(device)\n        logger.info(f\"Starting cv '{cv_name}' ({inp.current + 1}/{inp.total}) on device {device}.\")\n    else:\n        device = None\n        device_config = inp.device_config.in_parallel()\n        logger.info(f\"Starting cv '{cv_name}' ({inp.current + 1}/{inp.total}).\")\n\n    try:\n        score, is_unstable, cv_run_infos = cross_validation_smp(\n            name=cv_name,\n            tune_name=inp.tune_name,\n            cv=inp.cv,\n            training_config=inp.training_config,\n            data_config=inp.data_config,\n            logging_config=inp.logging_config,\n            hparams=inp.hparams,\n            device_config=device_config,\n        )\n\n        for key, value in asdict(inp.hparams).items():\n            cv_run_infos[key] = value if not isinstance(value, list) else pd.Series([value] * len(cv_run_infos))\n\n        cv_run_infos[\"cv_name\"] = cv_name\n        output = _ProcessOutputs(\n            run_infos=cv_run_infos,\n            score=score,\n            is_unstable=is_unstable,\n        )\n    finally:\n        # If we are in parallel mode, we need to return the device to the queue.\n        if is_parallel:\n            logger.debug(f\"Free device {device} for cv {cv_name}\")\n            available_devices.put(device)\n    return output\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.check_score_is_unstable","title":"check_score_is_unstable","text":"<pre><code>check_score_is_unstable(\n    run_info: dict, scoring_metric: list[str] | str\n) -&gt; bool\n</code></pre> <p>Check the stability of the scoring metric.</p> <p>If any metric value is not finite or equal to zero, the scoring metric is considered unstable.</p> <p>Parameters:</p> <ul> <li> <code>run_info</code>               (<code>dict</code>)           \u2013            <p>The run information.</p> </li> <li> <code>scoring_metric</code>               (<code>list[str] | str</code>)           \u2013            <p>The scoring metric.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the scoring metric is unstable, False otherwise.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If an unknown scoring metric type is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/scoring.py</code> <pre><code>def check_score_is_unstable(run_info: dict, scoring_metric: list[str] | str) -&gt; bool:\n    \"\"\"Check the stability of the scoring metric.\n\n    If any metric value is not finite or equal to zero, the scoring metric is considered unstable.\n\n    Args:\n        run_info (dict): The run information.\n        scoring_metric (list[str] | str): The scoring metric.\n\n    Returns:\n        bool: True if the scoring metric is unstable, False otherwise.\n\n    Raises:\n        ValueError: If an unknown scoring metric type is provided.\n\n    \"\"\"\n    # Single score in list\n    if isinstance(scoring_metric, list) and len(scoring_metric) == 1:\n        scoring_metric = scoring_metric[0]\n\n    if isinstance(scoring_metric, str):\n        metric_value = run_info[scoring_metric]\n        is_unstable = not isfinite(metric_value) or metric_value == 0\n        return is_unstable\n    elif isinstance(scoring_metric, list):\n        metric_values = [run_info[metric] for metric in scoring_metric]\n        is_unstable = any(not isfinite(val) or val == 0 for val in metric_values)\n        return is_unstable\n    else:\n        raise ValueError(\"Invalid scoring metric type\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/tune/#darts_segmentation.training.tune.tune_smp","title":"tune_smp","text":"<pre><code>tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | typing.Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    cv_config: darts_segmentation.training.cv.CrossValidationConfig = darts_segmentation.training.cv.CrossValidationConfig(),\n    training_config: darts_segmentation.training.train.TrainingConfig = darts_segmentation.training.train.TrainingConfig(),\n    data_config: darts_segmentation.training.train.DataConfig = darts_segmentation.training.train.DataConfig(),\n    device_config: darts_segmentation.training.train.DeviceConfig = darts_segmentation.training.train.DeviceConfig(),\n    logging_config: darts_segmentation.training.train.LoggingConfig = darts_segmentation.training.train.LoggingConfig(),\n    hpconfig: pathlib.Path | None = None,\n    config_file: pathlib.Path | None = None,\n)\n</code></pre> <p>Tune the hyper-parameters of the model using cross-validation and random states.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.</p> <p>Please also consider reading our training guide (docs/guides/training.md).</p> <p>This tuning script is designed to sweep over hyperparameters with a cross-validation used to evaluate each hyperparameter configuration. Optionally, by setting <code>retrain_and_test</code> to True, the best hyperparameters are then selected based on the cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.</p> <p>Hyperparameters can be configured using a <code>hpconfig</code> file (YAML or Toml). Please consult the training guide or the documentation of <code>darts_segmentation.training.hparams.parse_hyperparameters</code> to learn how such a file should be structured. Per default, a random search is performed, where the number of samples can be specified by <code>n_trials</code>. If <code>n_trials</code> is set to \"grid\", a grid search is performed instead. However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.</p> <p>To specify on which metric(s) the cv score is calculated, the <code>scoring_metric</code> parameter can be specified. Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics. This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\". If no direction is provided, it is assumed to be \":higher\". Has no real effect on the single score calculation, since only the mean is calculated there.</p> <p>In a multi-score setting, the score is calculated by combine-then-reduce the metrics. Meaning that first for each fold the metrics are combined using the specified strategy, and then the results are reduced via mean. Please refer to the documentation to understand the different multi-score strategies.</p> <p>If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\". In such cases, the configuration is not considered for further evaluation.</p> <p>Artifacts are stored under <code>{artifact_dir}/{tune_name}</code>.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>. Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch. If <code>log_every_n_steps</code> is set to 50 then the training logs and metrics will be logged 4 times per epoch. If <code>check_val_every_n_epoch</code> is set to 5 then validation will be performed every 5 epochs. If <code>plot_every_n_val_epochs</code> is set to 2 then validation samples will be plotted every 10 epochs. If <code>early_stopping_patience</code> is set to 3 then early stopping will be performed after 15 epochs without improvement.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n\u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the tuning run. Will be generated based on the number of existing directories in the artifact directory if None. Defaults to None.</p> </li> <li> <code>n_trials</code>               (<code>int | typing.Literal['grid']</code>, default:                   <code>100</code> )           \u2013            <p>Number of trials to perform in hyperparameter tuning. If \"grid\", span a grid search over all configured hyperparameters. In a grid search, only constant or choice hyperparameters are allowed. Defaults to 100.</p> </li> <li> <code>retrain_and_test</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to retrain the model with the best hyperparameters and test it. Defaults to False.</p> </li> <li> <code>cv_config</code>               (<code>darts_segmentation.training.cv.CrossValidationConfig</code>, default:                   <code>darts_segmentation.training.cv.CrossValidationConfig()</code> )           \u2013            <p>Configuration for cross-validation. Defaults to CrossValidationConfig().</p> </li> <li> <code>training_config</code>               (<code>darts_segmentation.training.train.TrainingConfig</code>, default:                   <code>darts_segmentation.training.train.TrainingConfig()</code> )           \u2013            <p>Configuration for training. Defaults to TrainingConfig().</p> </li> <li> <code>data_config</code>               (<code>darts_segmentation.training.train.DataConfig</code>, default:                   <code>darts_segmentation.training.train.DataConfig()</code> )           \u2013            <p>Configuration for data. Defaults to DataConfig().</p> </li> <li> <code>device_config</code>               (<code>darts_segmentation.training.train.DeviceConfig</code>, default:                   <code>darts_segmentation.training.train.DeviceConfig()</code> )           \u2013            <p>Configuration for device. Defaults to DeviceConfig().</p> </li> <li> <code>logging_config</code>               (<code>darts_segmentation.training.train.LoggingConfig</code>, default:                   <code>darts_segmentation.training.train.LoggingConfig()</code> )           \u2013            <p>Configuration for logging. Defaults to LoggingConfig().</p> </li> <li> <code>hpconfig</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the hyperparameter configuration file. Please see the documentation of <code>hyperparameters</code> for more information. Defaults to None.</p> </li> <li> <code>config_file</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the configuration file. If provided, it will be used instead of <code>hpconfig</code> if <code>hpconfig</code> is None. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li>           \u2013            <p>tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If no hyperparameter configuration file is provided.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/tune.py</code> <pre><code>def tune_smp(\n    *,\n    name: str | None = None,\n    n_trials: int | Literal[\"grid\"] = 100,\n    retrain_and_test: bool = False,\n    cv_config: CrossValidationConfig = CrossValidationConfig(),\n    training_config: TrainingConfig = TrainingConfig(),\n    data_config: DataConfig = DataConfig(),\n    device_config: DeviceConfig = DeviceConfig(),\n    logging_config: LoggingConfig = LoggingConfig(),\n    hpconfig: Path | None = None,\n    config_file: Annotated[Path | None, cyclopts.Parameter(parse=False)] = None,\n):\n    \"\"\"Tune the hyper-parameters of the model using cross-validation and random states.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations of architecture and encoder.\n\n    Please also consider reading our training guide (docs/guides/training.md).\n\n    This tuning script is designed to sweep over hyperparameters with a cross-validation\n    used to evaluate each hyperparameter configuration.\n    Optionally, by setting `retrain_and_test` to True, the best hyperparameters are then selected based on the\n    cross-validation scores and a new model is trained on the entire train-split and tested on the test-split.\n\n    Hyperparameters can be configured using a `hpconfig` file (YAML or Toml).\n    Please consult the training guide or the documentation of\n    `darts_segmentation.training.hparams.parse_hyperparameters` to learn how such a file should be structured.\n    Per default, a random search is performed, where the number of samples can be specified by `n_trials`.\n    If `n_trials` is set to \"grid\", a grid search is performed instead.\n    However, this expects to be every hyperparameter to be configured as either constant value or a choice / list.\n\n    To specify on which metric(s) the cv score is calculated, the `scoring_metric` parameter can be specified.\n    Each score can be provided by either \":higher\" or \":lower\" to indicate the direction of the metrics.\n    This allows to correctly combine multiple metrics by doing 1/metric before calculation if a metric is \":lower\".\n    If no direction is provided, it is assumed to be \":higher\".\n    Has no real effect on the single score calculation, since only the mean is calculated there.\n\n    In a multi-score setting, the score is calculated by combine-then-reduce the metrics.\n    Meaning that first for each fold the metrics are combined using the specified strategy,\n    and then the results are reduced via mean.\n    Please refer to the documentation to understand the different multi-score strategies.\n\n    If one of the metrics of any of the runs contains NaN, Inf, -Inf or is 0 the score is reported to be \"unstable\".\n    In such cases, the configuration is not considered for further evaluation.\n\n    Artifacts are stored under `{artifact_dir}/{tune_name}`.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n    Example: There are 400 training samples and the batch size is 2, resulting in 200 training steps per epoch.\n    If `log_every_n_steps` is set to 50 then the training logs and metrics will be logged 4 times per epoch.\n    If `check_val_every_n_epoch` is set to 5 then validation will be performed every 5 epochs.\n    If `plot_every_n_val_epochs` is set to 2 then validation samples will be plotted every 10 epochs.\n    If `early_stopping_patience` is set to 3 then early stopping will be performed after 15 epochs without improvement.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 data.zarr/ # this zarr group contains the dataarrays x and y\n    \u251c\u2500\u2500 metadata.parquet # this contains information necessary to split the data into train, val, and test sets.\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        name (str | None, optional): Name of the tuning run.\n            Will be generated based on the number of existing directories in the artifact directory if None.\n            Defaults to None.\n        n_trials (int | Literal[\"grid\"], optional): Number of trials to perform in hyperparameter tuning.\n            If \"grid\", span a grid search over all configured hyperparameters.\n            In a grid search, only constant or choice hyperparameters are allowed.\n            Defaults to 100.\n        retrain_and_test (bool, optional): Whether to retrain the model with the best hyperparameters and test it.\n            Defaults to False.\n        cv_config (CrossValidationConfig, optional): Configuration for cross-validation.\n            Defaults to CrossValidationConfig().\n        training_config (TrainingConfig, optional): Configuration for training.\n            Defaults to TrainingConfig().\n        data_config (DataConfig, optional): Configuration for data.\n            Defaults to DataConfig().\n        device_config (DeviceConfig, optional): Configuration for device.\n            Defaults to DeviceConfig().\n        logging_config (LoggingConfig, optional): Configuration for logging.\n            Defaults to LoggingConfig().\n        hpconfig (Path | None, optional): Path to the hyperparameter configuration file.\n            Please see the documentation of `hyperparameters` for more information.\n            Defaults to None.\n        config_file (Path | None, optional): Path to the configuration file. If provided,\n            it will be used instead of `hpconfig` if `hpconfig` is None. Defaults to None.\n\n    Returns:\n        tuple[float, pd.DataFrame]: The best score (if retrained and tested) and the run infos of all runs.\n\n    Raises:\n        ValueError: If no hyperparameter configuration file is provided.\n\n    \"\"\"\n    import pandas as pd\n    from darts_utils.namegen import generate_counted_name\n\n    from darts_segmentation.training.adp import _adp\n    from darts_segmentation.training.hparams import parse_hyperparameters, sample_hyperparameters\n    from darts_segmentation.training.scoring import score_from_single_run\n    from darts_segmentation.training.train import test_smp, train_smp\n\n    tick_fstart = time.perf_counter()\n\n    tune_name = name or generate_counted_name(logging_config.artifact_dir)\n    artifact_dir = logging_config.artifact_dir / tune_name\n    run_infos_file = artifact_dir / f\"{tune_name}.parquet\"\n\n    # Check if the artifact directory is empty\n    assert not artifact_dir.exists(), f\"{artifact_dir} already exists.\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    hpconfig = hpconfig or config_file\n    if hpconfig is None:\n        raise ValueError(\n            \"No hyperparameter configuration file provided. Please provide a valid file via the `--hpconfig` flag.\"\n        )\n    param_grid = parse_hyperparameters(hpconfig)\n    logger.debug(f\"Parsed hyperparameter grid: {param_grid}\")\n    param_list = sample_hyperparameters(param_grid, n_trials)\n\n    logger.info(\n        f\"Starting tune '{tune_name}' with data from {data_config.train_data_dir.resolve()}.\"\n        f\" Artifacts will be saved to {artifact_dir.resolve()}.\"\n        f\" Will run n_trials*n_randoms*n_folds =\"\n        f\" {len(param_list)}*{cv_config.n_randoms}*{cv_config.n_folds} =\"\n        f\" {len(param_list) * cv_config.n_randoms * cv_config.n_folds} experiments.\"\n    )\n\n    # Plan which runs to perform. These are later consumed based on the parallelization strategy.\n    process_inputs = [\n        _ProcessInputs(\n            current=i,\n            total=len(param_list),\n            tune_name=tune_name,\n            cv=cv_config,\n            training_config=training_config,\n            logging_config=logging_config,\n            data_config=data_config,\n            device_config=device_config,\n            hparams=hparams,\n        )\n        for i, hparams in enumerate(param_list)\n    ]\n\n    run_infos: list[pd.DataFrame] = []\n    best_score = 0\n    best_hp = None\n\n    # This function abstracts away common logic for running multiprocessing\n    for inp, output in _adp(\n        process_inputs=process_inputs,\n        is_parallel=device_config.strategy == \"tune-parallel\",\n        devices=device_config.devices,\n        available_devices=available_devices,\n        _run=_run_cv,\n    ):\n        run_infos.append(output.run_infos)\n        if not output.is_unstable and output.score &gt; best_score:\n            best_score = output.score\n            best_hp = inp.hparams\n\n        # Save already here to prevent data loss if something goes wrong\n        pd.concat(run_infos).reset_index(drop=True).to_parquet(run_infos_file)\n        logger.debug(f\"Saved run infos to {run_infos_file}\")\n\n    if len(run_infos) == 0:\n        logger.error(\"No hyperparameters resulted in a valid score. Please check the logs for more information.\")\n        return 0, run_infos\n\n    run_infos = pd.concat(run_infos).reset_index(drop=True)\n\n    tick_fend = time.perf_counter()\n\n    if best_hp is None:\n        logger.warning(\n            f\"Tuning completed in {tick_fend - tick_fstart:.2f}s.\"\n            \" No hyperparameters resulted in a valid score. Please check the logs for more information.\"\n        )\n        return 0, run_infos\n    logger.info(\n        f\"Tuning completed in {tick_fend - tick_fstart:.2f}s. The best score was {best_score:.4f} with {best_hp}.\"\n    )\n\n    # =====================\n    # === End of tuning ===\n    # =====================\n\n    if not retrain_and_test:\n        return 0, run_infos\n\n    logger.info(\"Starting retraining with the best hyperparameters.\")\n\n    tick_fstart = time.perf_counter()\n    trainer = train_smp(\n        run=TrainRunConfig(name=f\"{tune_name}-retrain\"),\n        training_config=training_config,  # TODO: device and strategy\n        data_config=DataConfig(\n            train_data_dir=data_config.train_data_dir,\n            data_split_method=data_config.data_split_method,\n            data_split_by=data_config.data_split_by,\n            fold_method=None,  # No fold method for retraining\n            total_folds=None,  # No folds for retraining\n        ),\n        logging_config=LoggingConfig(\n            artifact_dir=artifact_dir,\n            log_every_n_steps=logging_config.log_every_n_steps,\n            check_val_every_n_epoch=logging_config.check_val_every_n_epoch,\n            plot_every_n_val_epochs=logging_config.plot_every_n_val_epochs,\n            wandb_entity=logging_config.wandb_entity,\n            wandb_project=logging_config.wandb_project,\n        ),\n        hparams=best_hp,\n    )\n    run_id = trainer.lightning_module.hparams[\"run_id\"]\n    trainer = test_smp(\n        train_data_dir=data_config.train_data_dir,\n        run_id=run_id,\n        run_name=f\"{tune_name}-retrain\",\n        model_ckp=trainer.checkpoint_callback.best_model_path,\n        batch_size=best_hp.batch_size,\n        data_split_method=data_config.data_split_method,\n        data_split_by=data_config.data_split_by,\n        artifact_dir=artifact_dir,\n        num_workers=training_config.num_workers,\n        device_config=device_config,\n        wandb_entity=logging_config.wandb_entity,\n        wandb_project=logging_config.wandb_project,\n    )\n\n    run_info = {k: v.item() for k, v in trainer.callback_metrics.items()}\n    test_scoring_metric = (\n        cv_config.scoring_metric.replace(\"val/\", \"test/\")\n        if isinstance(cv_config.scoring_metric, str)\n        else [sm.replace(\"val/\", \"test/\") for sm in cv_config.scoring_metric]\n    )\n    score = score_from_single_run(run_info, test_scoring_metric, cv_config.multi_score_strategy)\n    is_unstable = check_score_is_unstable(run_info, cv_config.scoring_metric)\n    tick_fend = time.perf_counter()\n    logger.info(\n        f\"Retraining and testing completed successfully in {tick_fend - tick_fstart:.2f}s\"\n        f\" with {score=:.4f} ({'stable' if not is_unstable else 'unstable'}).\"\n    )\n\n    return score, run_infos\n</code></pre>"},{"location":"reference/darts_segmentation/training/viz/","title":"darts_segmentation.training.viz","text":""},{"location":"reference/darts_segmentation/training/viz/#darts_segmentation.training.viz","title":"darts_segmentation.training.viz","text":"<p>Visualization utilities for the training module.</p>"},{"location":"reference/darts_segmentation/training/viz/#darts_segmentation.training.viz.plot_sample","title":"plot_sample","text":"<pre><code>plot_sample(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    y_pred: torch.Tensor,\n    band_names: list[str],\n) -&gt; tuple[\n    matplotlib.pyplot.Figure,\n    dict[str, matplotlib.pyplot.Axes],\n]\n</code></pre> <p>Plot a single sample with the input, the ground truth and the prediction.</p> <p>This function does a few expections on the input: - The input is expected to be normalized to 0-1. - The prediction is expected to be converted from logits to prediction. - The target is expected to be a int or long tensor with values of:     0 (negative class)     1 (positive class) and     2 (invalid pixels).</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor [C, H, W] (float).</p> </li> <li> <code>y</code>               (<code>torch.Tensor</code>)           \u2013            <p>The ground truth tensor [H, W] (int).</p> </li> <li> <code>y_pred</code>               (<code>torch.Tensor</code>)           \u2013            <p>The prediction tensor [H, W] (float).</p> </li> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>The combinations of the input bands.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[matplotlib.pyplot.Figure, dict[str, matplotlib.pyplot.Axes]]</code>           \u2013            <p>tuple[Figure, dict[str, Axes]]: The figure and the axes of the plot.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/viz.py</code> <pre><code>def plot_sample(\n    x: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor, band_names: list[str]\n) -&gt; tuple[plt.Figure, dict[str, plt.Axes]]:\n    \"\"\"Plot a single sample with the input, the ground truth and the prediction.\n\n    This function does a few expections on the input:\n    - The input is expected to be normalized to 0-1.\n    - The prediction is expected to be converted from logits to prediction.\n    - The target is expected to be a int or long tensor with values of:\n        0 (negative class)\n        1 (positive class) and\n        2 (invalid pixels).\n\n    Args:\n        x (torch.Tensor): The input tensor [C, H, W] (float).\n        y (torch.Tensor): The ground truth tensor [H, W] (int).\n        y_pred (torch.Tensor): The prediction tensor [H, W] (float).\n        band_names (list[str]): The combinations of the input bands.\n\n    Returns:\n        tuple[Figure, dict[str, Axes]]: The figure and the axes of the plot.\n\n    \"\"\"\n    x = x.cpu()\n    y = y.cpu()\n    y_pred = y_pred.detach().cpu()\n\n    # Make y class 2 invalids (replace 2 with nan)\n    x = x.where(y != 2, torch.nan)\n    y_pred = y_pred.where(y != 2, torch.nan)\n    y = y.where(y != 2, torch.nan)\n\n    # pred == 0, y == 0 -&gt; 0 (true negative)\n    # pred == 1, y == 0 -&gt; 1 (false positive)\n    # pred == 0, y == 1 -&gt; 2 (false negative)\n    # pred == 1, y == 1 -&gt; 3 (true positive)\n    classification_labels = (y_pred &gt; 0.5).int() + y * 2\n    classification_labels = classification_labels.where(classification_labels != 0, torch.nan)\n\n    # Calculate f1 and iou\n    true_positive = (classification_labels == 3).sum()\n    false_positive = (classification_labels == 1).sum()\n    false_negative = (classification_labels == 2).sum()\n    true_negative = (classification_labels == 0).sum()\n    acc = (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)\n    f1 = 2 * true_positive / (2 * true_positive + false_positive + false_negative)\n    iou = true_positive / (true_positive + false_positive + false_negative)\n\n    cmap = mcolors.ListedColormap([\"#cd43b2\", \"#3e0f2f\", \"#6cd875\"])\n    fig, axs = plt.subplot_mosaic(\n        # [[\"rgb\", \"rgb\", \"ndvi\", \"tcvis\", \"stats\"], [\"rgb\", \"rgb\", \"pred\", \"slope\", \"elev\"]],\n        [[\"rgb\", \"rgb\", \"pred\", \"tcvis\"], [\"rgb\", \"rgb\", \"ndvi\", \"slope\"], [\"none\", \"stats\", \"stats\", \"stats\"]],\n        # layout=\"constrained\",\n        figsize=(11, 8),\n    )\n\n    # Disable none plot\n    axs[\"none\"].axis(\"off\")\n\n    # RGB Plot\n    ax_rgb = axs[\"rgb\"]\n    # disable axis\n    ax_rgb.axis(\"off\")\n    is_rgb = \"red\" in band_names and \"green\" in band_names and \"blue\" in band_names\n    if is_rgb:\n        red_band = band_names.index(\"red\")\n        green_band = band_names.index(\"green\")\n        blue_band = band_names.index(\"blue\")\n        rgb = x[[red_band, green_band, blue_band]].transpose(0, 2).transpose(0, 1)\n        ax_rgb.imshow(rgb ** (1 / 1.4))\n        ax_rgb.set_title(f\"Acc: {acc:.1%} F1: {f1:.1%} IoU: {iou:.1%}\")\n    else:\n        # Plot empty with message that RGB is not provided\n        ax_rgb.set_title(\"No RGB values are provided!\")\n    ax_rgb.imshow(classification_labels, alpha=0.6, cmap=cmap, vmin=1, vmax=3)\n    # Add a legend\n    patches = [\n        mpatches.Patch(color=\"#6cd875\", label=\"True Positive\"),\n        mpatches.Patch(color=\"#3e0f2f\", label=\"False Negative\"),\n        mpatches.Patch(color=\"#cd43b2\", label=\"False Positive\"),\n    ]\n    ax_rgb.legend(handles=patches, loc=\"upper left\")\n\n    # NDVI Plot\n    ax_ndvi = axs[\"ndvi\"]\n    ax_ndvi.axis(\"off\")\n    is_ndvi = \"ndvi\" in band_names\n    if is_ndvi:\n        ndvi_band = band_names.index(\"ndvi\")\n        ndvi = x[ndvi_band]\n        ax_ndvi.imshow(ndvi, vmin=0, vmax=1, cmap=\"RdYlGn\")\n        ax_ndvi.set_title(\"NDVI\")\n    else:\n        # Plot empty with message that NDVI is not provided\n        ax_ndvi.set_title(\"No NDVI values are provided!\")\n\n    # TCVIS Plot\n    ax_tcv = axs[\"tcvis\"]\n    ax_tcv.axis(\"off\")\n    is_tcvis = \"tc_brightness\" in band_names and \"tc_greenness\" in band_names and \"tc_wetness\" in band_names\n    if is_tcvis:\n        tcb_band = band_names.index(\"tc_brightness\")\n        tcg_band = band_names.index(\"tc_greenness\")\n        tcw_band = band_names.index(\"tc_wetness\")\n        tcvis = x[[tcb_band, tcg_band, tcw_band]].transpose(0, 2).transpose(0, 1)\n        ax_tcv.imshow(tcvis)\n        ax_tcv.set_title(\"TCVIS\")\n    else:\n        ax_tcv.set_title(\"No TCVIS values are provided!\")\n\n    # Statistics Plot\n    ax_stat = axs[\"stats\"]\n    if (y == 1).sum() &gt; 0:\n        n_bands = x.shape[0]\n        n_pixel = x.shape[1] * x.shape[2]\n        x_flat = x.flatten().cpu()\n        y_flat = y.flatten().repeat(n_bands).cpu()\n        bands = list(itertools.chain.from_iterable([band_names[i]] * n_pixel for i in range(n_bands)))\n        plot_data = pd.DataFrame({\"x\": x_flat, \"y\": y_flat, \"band\": bands})\n        if len(plot_data) &gt; 50000:\n            plot_data = plot_data.sample(50000)\n        plot_data = plot_data.sort_values(\"band\")\n        sns.violinplot(\n            x=\"x\",\n            y=\"band\",\n            hue=\"y\",\n            data=plot_data,\n            split=True,\n            inner=\"quart\",\n            fill=False,\n            palette={1: \"g\", 0: \".35\"},\n            density_norm=\"width\",\n            ax=ax_stat,\n        )\n        ax_stat.set_title(\"Band Statistics\")\n    else:\n        ax_stat.set_title(\"No positive labels in this sample!\")\n        ax_stat.axis(\"off\")\n\n    # Prediction Plot\n    ax_mask = axs[\"pred\"]\n    ax_mask.imshow(y_pred, vmin=0, vmax=1)\n    ax_mask.axis(\"off\")\n    ax_mask.set_title(\"Model Output\")\n\n    # Slope Plot\n    ax_slope = axs[\"slope\"]\n    ax_slope.axis(\"off\")\n    is_slope = \"slope\" in band_names\n    if is_slope:\n        slope_band = band_names.index(\"slope\")\n        slope = x[slope_band]\n        ax_slope.imshow(slope, cmap=\"cividis\")\n        # Add TPI as contour lines\n        is_rel_elev = \"relative_elevation\" in band_names\n        if is_rel_elev:\n            rel_elev_band = band_names.index(\"relative_elevation\")\n            rel_elev = x[rel_elev_band]\n            cs = ax_slope.contour(rel_elev, [0], colors=\"red\", linewidths=0.3, alpha=0.6)\n            ax_slope.clabel(cs, inline=True, fontsize=5, fmt=\"%.1f\")\n\n        ax_slope.set_title(\"Slope\")\n    else:\n        # Plot empty with message that slope is not provided\n        ax_slope.set_title(\"No Slope values are provided!\")\n\n    # Relative Elevation Plot\n    # rel_elev_band = band_names.index(\"relative_elevation\")\n    # rel_elev = x[rel_elev_band]\n    # ax_rel_elev = axs[\"elev\"]\n    # ax_rel_elev.imshow(rel_elev, cmap=\"cividis\")\n    # ax_rel_elev.axis(\"off\")\n    # ax_rel_elev.set_title(\"Relative Elevation\")\n\n    return fig, axs\n</code></pre>"},{"location":"reference/darts_segmentation/utils/","title":"darts_segmentation.utils","text":""},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils","title":"darts_segmentation.utils","text":"<p>Shared utilities for the inference modules.</p>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Band","title":"Band  <code>dataclass</code>","text":"<pre><code>Band(name: str, factor: float = 1.0, offset: float = 0.0)\n</code></pre> <p>Wrapper for the band information.</p>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Band.factor","title":"factor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>factor: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Band.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Band.offset","title":"offset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offset: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands","title":"Bands","text":"<p>               Bases: <code>collections.UserList[darts_segmentation.utils.Band]</code></p> <p>Wrapper for the list of bands.</p>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.factors","title":"factors  <code>property</code>","text":"<pre><code>factors: list[float]\n</code></pre> <p>Get the factors of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The factors of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.names","title":"names  <code>property</code>","text":"<pre><code>names: list[str]\n</code></pre> <p>Get the names of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list[str]: The names of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.offsets","title":"offsets  <code>property</code>","text":"<pre><code>offsets: list[float]\n</code></pre> <p>Get the offsets of the bands.</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>list[float]: The offsets of the bands.</p> </li> </ul>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.__reduce__","title":"__reduce__","text":"<pre><code>__reduce__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __reduce__(self):  # noqa: D105\n    # This is needed to pickle (and unpickle) the Bands object as a dict\n    # This is needed, because this way we don't need to have this class present when unpickling\n    # a pytorch checkpoint\n    return (dict, (self.to_config(),))\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def __repr__(self) -&gt; str:  # noqa: D105\n    band_info = \", \".join([f\"{band.name}(*{band.factor:.5f}+{band.offset:.5f})\" for band in self])\n    return f\"Bands({band_info})\"\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.filter","title":"filter","text":"<pre><code>filter(\n    band_names: list[str],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Filter the bands by name.</p> <p>Parameters:</p> <ul> <li> <code>band_names</code>               (<code>list[str]</code>)           \u2013            <p>The names of the bands to keep.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The filtered Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def filter(self, band_names: list[str]) -&gt; \"Bands\":\n    \"\"\"Filter the bands by name.\n\n    Args:\n        band_names (list[str]): The names of the bands to keep.\n\n    Returns:\n        Bands: The filtered Bands object.\n\n    \"\"\"\n    return Bands([band for band in self if band.name in band_names])\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(\n    config: dict[\n        typing.Literal[\n            \"bands\", \"band_factors\", \"band_offsets\"\n        ],\n        list,\n    ]\n    | dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a config dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>The config dictionary containing the band information. Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\", with the values to be lists of the same length.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    config: dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list] | dict[str, tuple[float, float]],\n) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a config dictionary.\n\n    Args:\n        config (dict): The config dictionary containing the band information.\n            Expects config to be a dictionary with keys \"bands\", \"band_factors\" and \"band_offsets\",\n            with the values to be lists of the same length.\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    assert \"bands\" in config and \"band_factors\" in config and \"band_offsets\" in config, (\n        f\"Config must contain keys 'bands', 'band_factors' and 'band_offsets'.Got {config} instead.\"\n    )\n    return cls(\n        [\n            Band(name=name, factor=factor, offset=offset)\n            for name, factor, offset in zip(config[\"bands\"], config[\"band_factors\"], config[\"band_offsets\"])\n        ]\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    config: dict[str, tuple[float, float]],\n) -&gt; darts_segmentation.utils.Bands\n</code></pre> <p>Create a Bands object from a dictionary.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, tuple[float, float]]</code>)           \u2013            <p>The dictionary containing the band information. Expects the keys to be the band names and the values to be tuples of (factor, offset). Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Bands</code> (              <code>darts_segmentation.utils.Bands</code> )          \u2013            <p>The Bands object.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@classmethod\ndef from_dict(cls, config: dict[str, tuple[float, float]]) -&gt; \"Bands\":\n    \"\"\"Create a Bands object from a dictionary.\n\n    Args:\n        config (dict[str, tuple[float, float]]): The dictionary containing the band information.\n            Expects the keys to be the band names and the values to be tuples of (factor, offset).\n            Example: {\"band1\": (1.0, 0.0), \"band2\": (2.0, 1.0)}\n\n    Returns:\n        Bands: The Bands object.\n\n    \"\"\"\n    return cls([Band(name=name, factor=factor, offset=offset) for name, (factor, offset) in config.items()])\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.to_config","title":"to_config","text":"<pre><code>to_config() -&gt; dict[\n    typing.Literal[\"bands\", \"band_factors\", \"band_offsets\"],\n    list,\n]\n</code></pre> <p>Convert the Bands object to a config dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict[typing.Literal['bands', 'band_factors', 'band_offsets'], list]</code> )          \u2013            <p>The config dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_config(self) -&gt; dict[Literal[\"bands\", \"band_factors\", \"band_offsets\"], list]:\n    \"\"\"Convert the Bands object to a config dictionary.\n\n    Returns:\n        dict: The config dictionary containing the band information.\n\n    \"\"\"\n    return {\n        \"bands\": [band.name for band in self],\n        \"band_factors\": [band.factor for band in self],\n        \"band_offsets\": [band.offset for band in self],\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.Bands.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, tuple[float, float]]\n</code></pre> <p>Convert the Bands object to a dictionary.</p> <p>Returns:</p> <ul> <li> <code>dict[str, tuple[float, float]]</code>           \u2013            <p>dict[str, tuple[float, float]]: The dictionary containing the band information.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def to_dict(self) -&gt; dict[str, tuple[float, float]]:\n    \"\"\"Convert the Bands object to a dictionary.\n\n    Returns:\n        dict[str, tuple[float, float]]: The dictionary containing the band information.\n\n    \"\"\"\n    return {band.name: (band.factor, band.offset) for band in self}\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.create_patches","title":"create_patches","text":"<pre><code>create_patches(\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    return_coords: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Create patches from a tensor.</p> <p>Parameters:</p> <ul> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>return_coords</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.patch_coords","title":"patch_coords","text":"<pre><code>patch_coords(\n    h: int, w: int, patch_size: int, overlap: int\n) -&gt; collections.abc.Generator[\n    tuple[int, int, int, int], None, None\n]\n</code></pre> <p>Yield patch coordinates based on height, width, patch size and margin size.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>int</code>)           \u2013            <p>Height of the image.</p> </li> <li> <code>w</code>               (<code>int</code>)           \u2013            <p>Width of the image.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Patch size.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>Margin size.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[int, int, int, int]</code>           \u2013            <p>tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def patch_coords(h: int, w: int, patch_size: int, overlap: int) -&gt; Generator[tuple[int, int, int, int], None, None]:\n    \"\"\"Yield patch coordinates based on height, width, patch size and margin size.\n\n    Args:\n        h (int): Height of the image.\n        w (int): Width of the image.\n        patch_size (int): Patch size.\n        overlap (int): Margin size.\n\n    Yields:\n        tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.\n\n    \"\"\"\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            yield y, x, patch_idx_y, patch_idx_x\n</code></pre>"},{"location":"reference/darts_segmentation/utils/#darts_segmentation.utils.predict_in_patches","title":"predict_in_patches","text":"<pre><code>predict_in_patches(\n    model: torch.nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Predict on a tensor.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>torch.nn.Module</code>)           \u2013            <p>The model to use for prediction.</p> </li> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> </li> <li> <code>reflection</code>               (<code>int</code>)           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>torch.device</code> )           \u2013            <p>The device to use for the prediction.</p> </li> <li> <code>return_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>The predicted tensor.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    n_skipped = 0\n    for i, batch in enumerate(patches):\n        # If batch contains only nans, skip it\n        if torch.isnan(batch).all(axis=0).any():\n            patched_probabilities[i * batch_size : (i + 1) * batch_size] = 0\n            n_skipped += 1\n            continue\n        # If batch contains some nans, replace them with zeros\n        batch[torch.isnan(batch)] = 0\n\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    if n_skipped &gt; 0:\n        logger.debug(f\"Skipped {n_skipped} batches because they only contained NaNs\")\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"reference/darts_utils/","title":"darts_utils","text":""},{"location":"reference/darts_utils/#darts_utils","title":"darts_utils","text":"<p>Utility functions for the DARTS dataset.</p>"},{"location":"reference/darts_utils/#darts_utils.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_utils/cuda/","title":"darts_utils.cuda","text":""},{"location":"reference/darts_utils/cuda/#darts_utils.cuda","title":"darts_utils.cuda","text":"<p>Utility functions around cuda, e.g. memory management.</p>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.free_cupy","title":"free_cupy","text":"<pre><code>free_cupy()\n</code></pre> <p>Free the CUDA memory of cupy.</p> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def free_cupy():\n    \"\"\"Free the CUDA memory of cupy.\"\"\"\n    try:\n        import cupy as cp  # type: ignore\n    except ImportError:\n        cp = None\n\n    if cp is not None:\n        gc.collect()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.free_torch","title":"free_torch","text":"<pre><code>free_torch()\n</code></pre> <p>Free the CUDA memory of pytorch.</p> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def free_torch():\n    \"\"\"Free the CUDA memory of pytorch.\"\"\"\n    import torch\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/darts_utils/functools/","title":"darts_utils.functools","text":""},{"location":"reference/darts_utils/functools/#darts_utils.functools","title":"darts_utils.functools","text":"<p>Function helpers.</p>"},{"location":"reference/darts_utils/functools/#darts_utils.functools.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_utils\", \"darts.shared_utils\")\n)\n</code></pre>"},{"location":"reference/darts_utils/functools/#darts_utils.functools.write_function_args_to_config_file","title":"write_function_args_to_config_file","text":"<pre><code>write_function_args_to_config_file(\n    fpath: pathlib.Path, function: callable, locals_: dict\n)\n</code></pre> <p>Write the arguments of a function.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the config file</p> </li> <li> <code>function</code>               (<code>callable</code>)           \u2013            <p>function to get the arguments from</p> </li> <li> <code>locals_</code>               (<code>dict</code>)           \u2013            <p>locals() dictionary. Needs to be called in parent function</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/functools.py</code> <pre><code>@stopwatch.f(\"Save function arguments to config file\", printer=logger.debug, print_kwargs=[\"fpath\"])\ndef write_function_args_to_config_file(\n    fpath: Path,\n    function: callable,\n    locals_: dict,\n):\n    \"\"\"Write the arguments of a function.\n\n    Args:\n        fpath (Path): Path to the config file\n        function (callable): function to get the arguments from\n        locals_ (dict): locals() dictionary. Needs to be called in parent function\n\n    \"\"\"\n    nargs = function.__code__.co_argcount + function.__code__.co_kwonlyargcount\n    args_ = function.__code__.co_varnames[:nargs]\n    config = {k: locals_[k] for k in args_ if k in locals_}\n    # Convert everything to json serializable\n    for key, value in config.items():\n        if isinstance(value, Path):\n            config[key] = str(value.resolve())\n        elif isinstance(value, list):\n            config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n    with open(fpath, \"w\") as f:\n        json.dump(config, f)\n</code></pre>"},{"location":"reference/darts_utils/namegen/","title":"darts_utils.namegen","text":""},{"location":"reference/darts_utils/namegen/#darts_utils.namegen","title":"darts_utils.namegen","text":"<p>Random name generator.</p>"},{"location":"reference/darts_utils/namegen/#darts_utils.namegen.generate_counted_name","title":"generate_counted_name","text":"<pre><code>generate_counted_name(artifact_dir: pathlib.Path) -&gt; str\n</code></pre> <p>Generate a random name with a count attached.</p> <p>The count is calculated by the number of existing directories in the specified artifact directory. The final name is in the format '{somename}-{somesecondname}-{count+1}'.</p> <p>Parameters:</p> <ul> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory of existing runs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The final name.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/namegen.py</code> <pre><code>def generate_counted_name(artifact_dir: Path) -&gt; str:\n    \"\"\"Generate a random name with a count attached.\n\n    The count is calculated by the number of existing directories in the specified artifact directory.\n    The final name is in the format '{somename}-{somesecondname}-{count+1}'.\n\n    Args:\n        artifact_dir (Path): The directory of existing runs.\n\n    Returns:\n        str: The final name.\n\n    \"\"\"\n    from names_generator import generate_name as _generate_name\n\n    run_name = _generate_name(style=\"hyphen\")\n    # Count the number of existing runs in the artifact_dir, increase the number by one and append it to the name\n    run_count = sum(1 for p in artifact_dir.glob(\"*\") if p.is_dir())\n    run_name = f\"{run_name}-{run_count + 1}\"\n    return run_name\n</code></pre>"},{"location":"reference/darts_utils/namegen/#darts_utils.namegen.generate_id","title":"generate_id","text":"<pre><code>generate_id(length: int = 8) -&gt; str\n</code></pre> <p>Generate a random base-36 string of <code>length</code> digits.</p> <p>This method is taken from the wandb SDK.</p> <p>There are ~2.8T base-36 8-digit strings. Generating 210k ids will have a ~1% chance of collision.</p> <p>Parameters:</p> <ul> <li> <code>length</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The length of the string. Defaults to 8.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>A random base-36 string of <code>length</code> digits.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/namegen.py</code> <pre><code>def generate_id(length: int = 8) -&gt; str:\n    \"\"\"Generate a random base-36 string of `length` digits.\n\n    This method is taken from the wandb SDK.\n\n    There are ~2.8T base-36 8-digit strings. Generating 210k ids will have a ~1% chance of collision.\n\n    Args:\n        length (int, optional): The length of the string. Defaults to 8.\n\n    Returns:\n        str: A random base-36 string of `length` digits.\n\n    \"\"\"\n    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))\n</code></pre>"},{"location":"reference/darts_utils/namegen/#darts_utils.namegen.generate_name","title":"generate_name","text":"<pre><code>generate_name() -&gt; str\n</code></pre> <p>Generate a random name.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>The final name.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/namegen.py</code> <pre><code>def generate_name() -&gt; str:\n    \"\"\"Generate a random name.\n\n    Returns:\n        str: The final name.\n\n    \"\"\"\n    from names_generator import generate_name as _generate_name\n\n    return _generate_name(style=\"hyphen\")\n</code></pre>"},{"location":"reference/darts_utils/patcher/","title":"darts_utils.patcher","text":""},{"location":"reference/darts_utils/patcher/#darts_utils.patcher","title":"darts_utils.patcher","text":"<p>Patch a dataset into smaller patches.</p>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch","title":"Patch  <code>dataclass</code>","text":"<pre><code>Patch(\n    i: int,\n    patch_idx_y: int,\n    patch_idx_x: int,\n    y: slice,\n    x: slice,\n    data: xarray.Dataset | xarray.DataArray,\n)\n</code></pre> <p>Class representing a patch of a dataset.</p>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: xarray.Dataset | xarray.DataArray\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.i","title":"i  <code>instance-attribute</code>","text":"<pre><code>i: int\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.patch_idx","title":"patch_idx  <code>property</code>","text":"<pre><code>patch_idx: tuple[int, int]\n</code></pre> <p>Return the patch index as a tuple.</p>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.patch_idx_x","title":"patch_idx_x  <code>instance-attribute</code>","text":"<pre><code>patch_idx_x: int\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.patch_idx_y","title":"patch_idx_y  <code>instance-attribute</code>","text":"<pre><code>patch_idx_y: int\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x: slice\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y: slice\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.Patch.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __repr__(self) -&gt; str:  # noqa: D105\n    return f\"Patch {self.i} ({self.patch_idx_y}, {self.patch_idx_x})\"\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset","title":"PatchedDataset","text":"<pre><code>PatchedDataset(\n    ds: xarray.Dataset | xarray.DataArray,\n    patch_size: int,\n    overlap: int,\n)\n</code></pre> <p>Class representing a dataset that has been patched into smaller patches.</p> Example <p>Via getter/setter:</p> <pre><code>tile: xr.Dataset\npatches = PatchedDataset(tile, patch_size, overlap)\nprint(len(patches))\ngrey = (patches[\"blue\"] + patches[\"green\"] + patches[\"red\"]) / 3 # grey is a numpy array\npatches[None] = grey # Replace the data in the patches with the gray data\nnew_tile = patches.combine_patches()\nnew_tile # This is now a DataArray containing the gray data\n</code></pre> <p>Via loop:</p> <pre><code>tile: xr.Dataset\npatches = PatchedDataset(tile, patch_size, overlap)\n# Calculate gray area for each patch\nfor patch in patches:\n    patch.data = (patch.data.blue + patch.data.green + patch.data.red) / 3\n\nnew_tile = patches.combine_patches()\nnew_tile # This is now a DataArray containing the gray data\n</code></pre> <p>Initialize the PatchedDataset.</p> <p>Parameters:</p> <ul> <li> <code>ds</code>               (<code>xarray.Dataset | xarray.DataArray</code>)           \u2013            <p>The dataset to patch.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap between patches.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __init__(self, ds: xr.Dataset | xr.DataArray, patch_size: int, overlap: int) -&gt; list[\"Patch\"]:\n    \"\"\"Initialize the PatchedDataset.\n\n    Args:\n        ds (xr.Dataset | xr.DataArray): The dataset to patch.\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap between patches.\n\n    \"\"\"\n    self.patch_size = patch_size\n    self.overlap = overlap\n    self.coords = ds.coords\n    self._patches = []\n    h, w = ds.sizes[\"y\"], ds.sizes[\"x\"]\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            ys = slice(y, y + patch_size)\n            xs = slice(x, x + patch_size)\n            self._patches.append(\n                Patch(\n                    i=len(self._patches),\n                    patch_idx_y=patch_idx_y,\n                    patch_idx_x=patch_idx_x,\n                    y=ys,\n                    x=xs,\n                    data=ds.isel(y=ys, x=xs),\n                )\n            )\n\n    # Create a soft margin for the patches (NumPy version)\n    margin_ramp = np.concatenate(\n        [\n            np.linspace(0, 1, overlap),\n            np.ones(patch_size - 2 * overlap),\n            np.linspace(1, 0, overlap),\n        ]\n    )\n    self.soft_margin = margin_ramp.reshape(1, patch_size) * margin_ramp.reshape(patch_size, 1)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.coords","title":"coords  <code>instance-attribute</code>","text":"<pre><code>coords = darts_utils.patcher.PatchedDataset(ds).coords\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.overlap","title":"overlap  <code>instance-attribute</code>","text":"<pre><code>overlap = darts_utils.patcher.PatchedDataset(overlap)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.patch_size","title":"patch_size  <code>instance-attribute</code>","text":"<pre><code>patch_size = darts_utils.patcher.PatchedDataset(patch_size)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.soft_margin","title":"soft_margin  <code>instance-attribute</code>","text":"<pre><code>soft_margin = margin_ramp.reshape(\n    1, darts_utils.patcher.PatchedDataset(patch_size)\n) * margin_ramp.reshape(\n    darts_utils.patcher.PatchedDataset(patch_size), 1\n)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str | None) -&gt; numpy.ndarray\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __getitem__(self, key: str | None) -&gt; np.ndarray:  # noqa: D105\n    is_dataarray = all(isinstance(patch.data, xr.DataArray) for patch in self._patches)\n    is_dataset = all(isinstance(patch.data, xr.Dataset) for patch in self._patches)\n    if is_dataset:\n        assert key is not None, \"Key must be provided for Dataset\"\n        return np.array([patch.data[key].data for patch in self._patches])\n    elif is_dataarray:\n        assert key is None, \"Key must be None for DataArray\"\n        return np.array([patch.data.data for patch in self._patches])\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; collections.abc.Iterator[\n    darts_utils.patcher.Patch\n]\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Patch]:  # noqa: D105\n    return iter(self._patches)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __len__(self) -&gt; int:  # noqa: D105\n    return len(self._patches)\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key: str | None, a: numpy.array)\n</code></pre> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def __setitem__(self, key: str | None, a: np.array):  # noqa: D105\n    for i, patch in enumerate(self._patches):\n        if key is None:\n            patch.data = xr.DataArray(a[i], dims=(\"y\", \"x\"))\n        else:\n            patch.data[key] = xr.DataArray(a[i], dims=(\"y\", \"x\"))\n    return self\n</code></pre>"},{"location":"reference/darts_utils/patcher/#darts_utils.patcher.PatchedDataset.combine_patches","title":"combine_patches","text":"<pre><code>combine_patches() -&gt; xarray.DataArray | xarray.Dataset\n</code></pre> <p>Combine patches into a single dataarray.</p> <p>Returns:</p> <ul> <li> <code>xarray.DataArray | xarray.Dataset</code>           \u2013            <p>xr.DataArray | xr.Dataset: The combined dataarray or dataset.</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/patcher.py</code> <pre><code>def combine_patches(self) -&gt; xr.DataArray | xr.Dataset:\n    \"\"\"Combine patches into a single dataarray.\n\n    Returns:\n        xr.DataArray | xr.Dataset: The combined dataarray or dataset.\n\n    \"\"\"\n    is_dataarray = all(isinstance(patch.data, xr.DataArray) for patch in self._patches)\n    is_dataset = all(isinstance(patch.data, xr.Dataset) for patch in self._patches)\n\n    if is_dataarray:\n        combined = xr.DataArray(0.0, dims=(\"y\", \"x\"), coords=self.coords)\n    elif is_dataset:\n        combined = xr.Dataset(coords=self.coords)\n        for var in self._patches[0].data.data_vars:\n            combined[var] = xr.DataArray(0.0, dims=(\"y\", \"x\"), coords=self.coords)\n\n    weights = xr.DataArray(0.0, dims=(\"y\", \"x\"), coords=self.coords)\n    for patch in self._patches:\n        weights[patch.y, patch.x] += self.soft_margin\n        if is_dataarray:\n            combined[patch.y, patch.x] += patch.data * self.soft_margin\n        elif is_dataset:\n            for var in patch.data.data_vars:\n                combined[var][patch.y, patch.x] += patch.data[var] * self.soft_margin\n    # Normalize the combined data by the weights\n    combined /= weights\n    return combined\n</code></pre>"},{"location":"reference/darts_utils/tilecache/","title":"darts_utils.tilecache","text":""},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache","title":"darts_utils.tilecache","text":"<p>Caching functionality for xarray datasets.</p>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = logging.getLogger(\n    __name__.replace(\"darts_\", \"darts.\")\n)\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager","title":"XarrayCacheManager","text":"<pre><code>XarrayCacheManager(\n    cache_dir: str | pathlib.Path | None = None,\n)\n</code></pre> <p>Manager for caching xarray datasets.</p> Example <pre><code>    def process_tile(tile_id: str):\n        # Initialize cache manager\n        preprocess_cache = Path(\"preprocess_cache\")\n        cache_manager = XarrayCacheManager(preprocess_cache)\n\n        def create_tile():\n            # Your existing tile creation logic goes here\n            return create_tile(...)  # Replace with actual implementation\n\n        # Get cached tile or create and cache it\n        tile = cache_manager.get_or_create(\n            identifier=tile_id,\n            creation_func=create_tile\n        )\n\n        return tile\n</code></pre> <p>Initialize the cache manager.</p> <p>Parameters:</p> <ul> <li> <code>cache_dir</code>               (<code>str | pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Directory path for caching files</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def __init__(self, cache_dir: str | Path | None = None):\n    \"\"\"Initialize the cache manager.\n\n    Args:\n        cache_dir (str | Path | None): Directory path for caching files\n\n    \"\"\"\n    self.cache_dir = Path(cache_dir) if isinstance(cache_dir, str) else cache_dir\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.cache_dir","title":"cache_dir  <code>instance-attribute</code>","text":"<pre><code>cache_dir = (\n    pathlib.Path(\n        darts_utils.tilecache.XarrayCacheManager(cache_dir)\n    )\n    if isinstance(\n        darts_utils.tilecache.XarrayCacheManager(cache_dir),\n        str,\n    )\n    else darts_utils.tilecache.XarrayCacheManager(cache_dir)\n)\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.exists","title":"exists","text":"<pre><code>exists(identifier: str) -&gt; bool\n</code></pre> <p>Check if a cached Dataset exists.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the cached file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the Dataset exists in cache, False otherwise</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def exists(self, identifier: str) -&gt; bool:\n    \"\"\"Check if a cached Dataset exists.\n\n    Args:\n        identifier (str): Unique identifier for the cached file\n\n    Returns:\n        bool: True if the Dataset exists in cache, False otherwise\n\n    \"\"\"\n    if not self.cache_dir:\n        return False\n\n    cache_path = self.cache_dir / f\"{identifier}.nc\"\n    return cache_path.exists()\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.get_or_create","title":"get_or_create","text":"<pre><code>get_or_create(\n    identifier: str,\n    creation_func: callable,\n    force: bool,\n    *args: tuple[typing.Any, ...],\n    **kwargs: dict[str, typing.Any],\n) -&gt; xarray.Dataset\n</code></pre> <p>Get cached Dataset or create and cache it if it doesn't exist.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the cached file</p> </li> <li> <code>creation_func</code>               (<code>callable</code>)           \u2013            <p>Function to create the Dataset if not cached</p> </li> <li> <code>force</code>               (<code>bool</code>)           \u2013            <p>If True, forces reprocessing even if cached</p> </li> <li> <code>*args</code>               (<code>tuple[typing.Any, ...]</code>, default:                   <code>()</code> )           \u2013            <p>Arguments to pass to creation_func</p> </li> <li> <code>**kwargs</code>               (<code>dict[str, typing.Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to creation_func</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The Dataset (either loaded from cache or newly created)</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def get_or_create(\n    self,\n    identifier: str,\n    creation_func: callable,\n    force: bool,\n    *args: tuple[Any, ...],\n    **kwargs: dict[str, Any],\n) -&gt; xr.Dataset:\n    \"\"\"Get cached Dataset or create and cache it if it doesn't exist.\n\n    Args:\n        identifier (str): Unique identifier for the cached file\n        creation_func (callable): Function to create the Dataset if not cached\n        force (bool): If True, forces reprocessing even if cached\n        *args: Arguments to pass to creation_func\n        **kwargs: Keyword arguments to pass to creation_func\n\n    Returns:\n        xr.Dataset: The Dataset (either loaded from cache or newly created)\n\n    \"\"\"\n    logger.debug(f\"Checking cache for {identifier} ({force=})\")\n    cached_dataset = None if force else self.load_from_cache(identifier)\n    logger.debug(f\"Cache hit: {cached_dataset is not None}\")\n    if cached_dataset is not None:\n        return cached_dataset\n\n    dataset = creation_func(*args, **kwargs)\n    if cached_dataset is None:\n        self.save_to_cache(dataset, identifier)\n    return dataset\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.load_from_cache","title":"load_from_cache","text":"<pre><code>load_from_cache(identifier: str) -&gt; xarray.Dataset | None\n</code></pre> <p>Load a Dataset from cache if it exists.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the cached file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset | None</code>           \u2013            <p>xr.Dataset | None: Dataset if found in cache, otherwise None</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def load_from_cache(self, identifier: str) -&gt; xr.Dataset | None:\n    \"\"\"Load a Dataset from cache if it exists.\n\n    Args:\n        identifier (str): Unique identifier for the cached file\n\n    Returns:\n        xr.Dataset | None: Dataset if found in cache, otherwise None\n\n    \"\"\"\n    if not self.cache_dir:\n        return None\n\n    cache_path = self.cache_dir / f\"{identifier}.nc\"\n    if not cache_path.exists():\n        return None\n    dataset = xr.open_dataset(cache_path, engine=\"h5netcdf\").set_coords(\"spatial_ref\")\n    return dataset\n</code></pre>"},{"location":"reference/darts_utils/tilecache/#darts_utils.tilecache.XarrayCacheManager.save_to_cache","title":"save_to_cache","text":"<pre><code>save_to_cache(\n    dataset: xarray.Dataset, identifier: str\n) -&gt; bool\n</code></pre> <p>Save a Dataset to cache.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Dataset to cache</p> </li> <li> <code>identifier</code>               (<code>str</code>)           \u2013            <p>Unique identifier for the cached file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>Success of operation</p> </li> </ul> Source code in <code>darts-utils/src/darts_utils/tilecache.py</code> <pre><code>def save_to_cache(self, dataset: xr.Dataset, identifier: str) -&gt; bool:\n    \"\"\"Save a Dataset to cache.\n\n    Args:\n        dataset (xr.Dataset): Dataset to cache\n        identifier (str): Unique identifier for the cached file\n\n    Returns:\n        bool: Success of operation\n\n    \"\"\"\n    if not self.cache_dir:\n        return False\n\n    self.cache_dir.mkdir(exist_ok=True, parents=True)\n    cache_path = self.cache_dir / f\"{identifier}.nc\"\n    logger.debug(f\"Caching {identifier=} to {cache_path}\")\n    dataset.to_netcdf(cache_path, engine=\"h5netcdf\")\n    return True\n</code></pre>"}]}