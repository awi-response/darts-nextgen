{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DARTS nextgen","text":"<p>Panarctic Database of Active Layer Detachment Slides and Retrogressive Thaw Slumps from Deep Learning on High Resolution Satellite Imagery. This is te successor of the thaw-slump-segmentation (pipeline), with which the first version of the DARTS dataset was created.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li> <p>Download source code from the GitHub repository:</p> <pre><code>git clone git@github.com:awi-response/darts-nextgen.git\ncd darts-nextgen\n</code></pre> </li> <li> <p>Install the required dependencies:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> </li> <li> <p>Run the Sentinel 2 based pipeline on an area of interest:</p> <pre><code>uv run darts run-sequential-aoi-sentinel2-pipeline \\\n  --aoi-shapefile path/to/your/aoi.geojson \\\n  --model-files path/to/your/model/checkpoint \\\n  --start-date 2024-07 \\\n  --end-date 2024-09\n</code></pre> </li> </ol> <ul> <li> <p> Overview</p> <p>Get an overview on how this project works and how to run different pipelines.</p> <p> Get Started</p> </li> <li> <p> Install</p> <p>View detailed instructions on how to install the project for different environments and setup, e.g. with CUDA.</p> <p> Install</p> </li> <li> <p> Pipeline Components</p> <p>Learn about the different components of the pipeline and how they work together.</p> <p> Components</p> </li> <li> <p> API Reference</p> <p>View the API reference of the components.</p> <p> Reference</p> </li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>Before contributing please contact one of the authors and make sure to read the Contribution Guidelines.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>This page is also meant for internal documentation.</p>"},{"location":"contribute/#editor-setup","title":"Editor setup","text":"<p>There is only setup files provided for VSCode and no other editor (yet). A list of extensions and some settings can be found in the <code>.vscode</code>. At the first start, VSCode should ask you if you want to install the recommended extension. The settings should be automaticly used by VSCode. Both should provide the developers with a better experience and enforce code-style.</p>"},{"location":"contribute/#environment-setup","title":"Environment setup","text":"<p>Please read and follow the installation guide to setup the environment.</p>"},{"location":"contribute/#writing-docs","title":"Writing docs","text":"<p>The documentation is managed with Material for MkDocs. The documentation related dependencies are separated from the main dependencies and can be installed with:</p> <pre><code>uv sync --group docs\n</code></pre> <p>Note</p> <p>You should combine the <code>--group docs</code> with the extras you previously used, e.g. <code>uv sync --extra training --extra cuda126 --group docs</code>.</p> <p>To start the documentation server for live-update, run:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>In general all mkdocs commands can be run with <code>uv run mkdocs ...</code>.</p>"},{"location":"contribute/#recommended-notebook-header","title":"Recommended Notebook header","text":"<p>The following code snipped can be put in the very first cell of a notebook to already to add logging and initialize earth engine.</p> <pre><code>import logging\n\nfrom rich.logging import RichHandler\nfrom rich import traceback\n\nfrom darts.utils.earthengine import init_ee\nfrom darts.utils.logging import LoggingManager\n\nLoggingManager.setup_logging()\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\ntraceback.install(show_locals=True)  # Change to False if you encounter too large tracebacks\ninit_ee(\"ee-project\")  # Replace with your project\n</code></pre>"},{"location":"overview/","title":"Overview","text":"<p>This is a guide to help you, as a user / data engineer, get started with the project.</p> Table of Contents<ul> <li>Overview<ul> <li>Installation</li> <li>Running stuff via the CLI<ul> <li>Device selection</li> <li>Config files</li> <li>Log files</li> </ul> </li> <li>Running a pipeline based on Sentinel 2 data</li> <li>Running a pipeline based on PLANET data<ul> <li>Create a config file</li> <li>Run a the pipeline</li> </ul> </li> <li>Creating your own pipeline</li> </ul> </li> </ul>"},{"location":"overview/#installation","title":"Installation","text":"<p>To setup the environment for the project, you need to install uv and run the following command, assuming CUDA 12.6 is installed:</p> <pre><code>uv sync --extra cuda126\n</code></pre> <p>For other CUDA versions, see the installation guide.</p> <p>Training specific dependencies are optional and therefore not installed by default. To install them, add <code>--extra training</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> <p>To see if the installation was successful, you can run the following command:</p> <pre><code>uv run darts env-info\n</code></pre>"},{"location":"overview/#running-stuff-via-the-cli","title":"Running stuff via the CLI","text":"<p>The project provides a CLI to run different pipelines, training and other utility functions. Because the environment is setup with <code>uv</code>, you can run the CLI commands with <code>uv run darts ...</code>. If you manually active the environment with <code>source .venv/bin/activate</code>, you can run the CLI commands just via <code>darts ...</code> without <code>uv run</code>.</p> <p>To see a list of all available commands, run:</p> <pre><code>uv run darts --help\n</code></pre> <p>To get help for a specific command, run:</p> <pre><code>uv run darts the-specific-command --help\n</code></pre>"},{"location":"overview/#device-selection","title":"Device selection","text":"<p>By default, the CLI will automatic select the device for functions that support it. To force a specific device, you can use the <code>--device</code> parameter. Read more about the device selection in the device guide.</p>"},{"location":"overview/#config-files","title":"Config files","text":"<p>The CLI supports config files in TOML format to reduce the amount of parameters you need to pass or to safe different configurations. By default the CLI tries to load a <code>config.toml</code> file from the current directory. However, you can specify a different file with the <code>--config-file</code> parameter.</p> <p>As of right now, the CLI tries to match all parameters under the <code>darts</code> key of the config file, skipping not needed ones. For more information about the config file,  see the config guide..</p>"},{"location":"overview/#log-files","title":"Log files","text":"<p>By default the CLI sets up a logging handler at <code>INFO</code> level for the <code>darts</code> specific packages found in this workspace. The log-level can be changed via the <code>--verbose</code> flag of the CLI to set it to <code>DEBUG</code>. Running any command will output a logging file at the logging directory, which can be specified via the <code>--log-dir</code> parameter. The logging file will be named after the command and the current timestamp. If you want to change the logging behavior in python code, you can check out the logging guide.</p>"},{"location":"overview/#running-a-pipeline-based-on-sentinel-2-data","title":"Running a pipeline based on Sentinel 2 data","text":"<p>The <code>run-sequential-aoi-sentinel2-pipeline</code> automatically downloads and processes Sentinel 2 data based on an Area of Interest (AOI) in GeoJSON format. Before running you need access to a trained model. Note, that only special checkpoints can be used, as described in the architecture guide. In future versions, downloading of the model via huggingface will be supported, but for now you need to ask the developers for a valid model checkpoint.</p> <p>To run the pipeline run:</p> <pre><code>uv run darts run-sequential-aoi-sentinel2-pipeline --aoi-shapefile path/to/your/aoi.geojson --model-files path/to/your/model/checkpoint --start-date 2024-07 --end-date 2024-09\n</code></pre> <p>Run <code>uv run darts run-sequential-aoi-sentinel2-pipeline --help</code> for more configuration options.</p> <p>Pipeline v2</p> <p>The Pipeline v2 Guide provides a more in-depth explanation of the pipeline and its components.</p>"},{"location":"overview/#running-a-pipeline-based-on-planet-data","title":"Running a pipeline based on PLANET data","text":"<p>PLANET data cannot be downloaded automatically. Hence, you need to download the data manually and place it a directory of you choice.</p> <p>Example directory structure of a PLANET Orthotile:</p> <pre><code>    data/input/planet/PSOrthoTile/\n    \u251c\u2500\u2500 4372514/\n    \u2502  \u2514\u2500\u2500 5790392_4372514_2022-07-16_2459/\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_Analytic_metadata.xml\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_DN_udm.tif\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_BGRN_SR.tif\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_metadata.json\n    \u2502      \u251c\u2500\u2500 5790392_4372514_2022-07-16_2459_udm2.tif\n    \u2502      \u2514\u2500\u2500 Thumbs.db\n    \u2514\u2500\u2500 4974017/\n        \u2514\u2500\u2500 5854937_4974017_2022-08-14_2475/\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_Analytic_metadata.xml\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_DN_udm.tif\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_BGRN_SR.tif\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_metadata.json\n            \u251c\u2500\u2500 5854937_4974017_2022-08-14_2475_udm2.tif\n            \u2514\u2500\u2500 Thumbs.db\n</code></pre> <p>Example directory structure of a PLANET Scene:</p> <pre><code>    data/input/planet/PSScene/\n    \u251c\u2500\u2500 20230703_194241_43_2427/\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427.json\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_metadata.xml\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_AnalyticMS_SR.tif\n    \u2502  \u251c\u2500\u2500 20230703_194241_43_2427_3B_udm2.tif\n    \u2502  \u2514\u2500\u2500 20230703_194241_43_2427_metadata.json\n    \u2514\u2500\u2500 20230703_194243_54_2427/\n       \u251c\u2500\u2500 20230703_194243_54_2427.json\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_AnalyticMS_metadata.xml\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_AnalyticMS_SR.tif\n       \u251c\u2500\u2500 20230703_194243_54_2427_3B_udm2.tif\n       \u2514\u2500\u2500 20230703_194243_54_2427_metadata.json\n</code></pre> <p>Backcompatability of Sentinel 2 data</p> <p>For historical reasons, it is possible to run similar pipelines with Sentinel 2 data. For this, the Sentinel 2 data is expected to be in the same directory structure as the PLANET data. Hence, data from Google EarthEngine or from the Copernicus Cloud needs to be adjusted and scaled by the factor of <code>0.0001</code>.</p> <pre><code>data/input/sentinel2/\n\u251c\u2500\u2500 20210818T223529_20210818T223531_T03WXP/\n\u2502  \u251c\u2500\u2500 20210818T223529_20210818T223531_T03WXP_SCL_clip.tif\n\u2502  \u2514\u2500\u2500 20210818T223529_20210818T223531_T03WXP_SR_clip.tif\n\u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ/\n\u251c\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SCL_clip.tif\n\u2514\u2500\u2500 20220826T200911_20220826T200905_T17XMJ_SR_clip.tif\n</code></pre>"},{"location":"overview/#create-a-config-file","title":"Create a config file","text":"<p>Because the minimal amount of parameters to pass for the PLANET pipeline, it is recommended to use a config file.</p> <p>An example config file can be found in the root of this repository called <code>config.toml.example</code>. You can copy this file to either <code>configs/</code> or copy and rename it to <code>config.toml</code>, so that you personal config will be ignored by git.</p> <p>Please change  <code>orthotiles-dir</code> and <code>scenes-dir</code> according to your PLANET download directory.</p> <p>You also need to specify the paths the model checkpoints (<code>model-dir</code>, <code>tcvis-model-name</code> and <code>notcvis-model-name</code>) you want to use. Note, that only special checkpoints can be used, as described in the architecture guide By setting <code>notcvis-model-name</code> to <code>None</code>, the pipeline will only use the TCVIS model.</p> <p>Auxiliary data (TCVIS and ArcticDEM) will be downloaded on demand into a datacube, which paths needs to be specified as well (<code>arcticdem-dir</code> and <code>tcvis-dir</code>).</p> <p>Finally, specify an output directory (<code>output-dir</code>), where you want to save the results of the pipeline.</p> <p>Of course you can tweak all other options aswell, also via the CLI. A list of all options can be found in the config guide or by running a command with the <code>--help</code> parameter.</p> Example config file <p>This is how an example config file could look like for the automated Sentinel 2 pipeline:</p> config.toml<pre><code>[darts]\nee-project = \"ee-tobias-hoelzer\"\ndask-worker = 4\n\n[darts.aoi]\naoi-shapefile = \"./data/banks_island.shp\"\nstart-date = \"2024-07\"\nend-date = \"2024-10\"\nmax-cloud-cover = 1 # %\n\n[darts.paths]\ninput-cache = \"./data/cache/s2gee\"\noutput-data-dir = \"./data/out\"\narcticdem-dir = \"./data/download/arcticdem\"\ntcvis-dir = \"./data/download/tcvis\"\nmodel-file = \"./models/s2-tcvis-final-large_2025-02-12.ckpt\"\n\n[darts.tiling]\nbatch-size = 8  # Reduce incase of memory issues\npatch-size = 512  # Reduce incase of memory issues\noverlap = 128  # Recommended to be 1/4 of patch-size\n</code></pre>"},{"location":"overview/#run-a-the-pipeline","title":"Run a the pipeline","text":"<p>Finally run the pipeline with the following command. Additional parameters can be passed via the CLI, which will overwrite the config file.</p> <pre><code>rye run darts run-sequential-planet-pipeline-fast --config-file path/to/your/config.toml\n</code></pre>"},{"location":"overview/#creating-your-own-pipeline","title":"Creating your own pipeline","text":"<p>The project was build with the idea in mind, that it is easy to create a new pipeline, with e.g. different parallelisation techniques. The architecture guide provides an overview of the project structure and the key components. A good starting point to understand the components is the intro to components. The build-in pipelines are a good example how the components can be used and put together to create a new pipeline.</p>"},{"location":"dev/arch/","title":"Architecture describtion","text":"<p>This repository is a workspace repository, managed by uv. Read more about workspaces at the uv docs. Each workspace-member starts with <code>darts-*</code> and can be seen as an own package or module, except the <code>darts</code> directory which is the top-level package. Each package has it's own internal functions and it's public facing API. The public facing API of each package MUST follow the API paradigms.</p> Table of Contents<ul> <li>Architecture describtion<ul> <li>Package overview<ul> <li>Conceptual migration from thaw-slump-segmentation</li> <li>Create a new package</li> <li>Versioning</li> </ul> </li> <li>PyTorch Model checkpoints</li> <li>API paradigms<ul> <li>Examples</li> <li>About the Xarray overhead with Ray</li> </ul> </li> </ul> </li> </ul>"},{"location":"dev/arch/#package-overview","title":"Package overview","text":"<p>Main design priciple</p> <p>Each package should provide components - stateless functions or stateful classes - which should be then combined either by the top-level <code>darts</code> package or by the user. Each component should take a Xarray Dataset as input and return a Xarray Dataset as output, with the exception of components of the <code>darts-aquisition</code> and <code>darts-export</code> packages. This way it should be easy to combine different components to build a custom pipeline for different parallelization frameworks and workflows.</p> Package Name Type Description (Major) Dependencies - all need Xarray <code>darts-acquisition</code> Data Fetches data from the data sources and created Xarray Datasets GEE, rasterio, ODC-Geo <code>darts-preprocessing</code> Data Combines Xarray Datasets from different acquisition sources and do some preprocessing on the data Cupy, Xarray-Spatial <code>darts-superresolution</code> Train Run a super-resolution model to scale Sentinel 2 images from 10m to 3m resolution PyTorch <code>darts-segmentation</code> Train Run the segmentation model PyTorch, segmentation_models_pytorch <code>darts-ensemble</code> Ensemble Ensembles the different models and run the multi-stage inference pipeline. PyTorch <code>darts-postprocessing</code> Data Further refines the output from an ensemble or segmentaion and binarizes the probs Scipy, Cucim <code>darts-export</code> Data Saves the results from inference and combines the result to the final DARTS dataset GeoPandas, rasterio <code>darts-utils</code> Data Shared utilities for data processing <p>The packages are currently designed around the v2 Pipeline.</p>"},{"location":"dev/arch/#conceptual-migration-from-thaw-slump-segmentation","title":"Conceptual migration from thaw-slump-segmentation","text":"<ul> <li>The <code>darts-ensemble</code> and <code>darts-postprocessing</code> packages is the successor of the <code>process-02-inference</code> and <code>process-03-ensemble</code> scripts.</li> <li>The <code>darts-preprocessing</code> and <code>darts-acquisition</code> packages are the successors of the <code>setup-raw-data</code> script and manual work of obtaining data.</li> <li>The <code>darts-export</code> package is splitted from the  <code>inference</code> script, should include the previous manual works of combining everything into the final dataset.</li> <li>The <code>darts-superresolution</code> package is the successor of the <code>superresolution</code> repository.</li> <li>The <code>darts-segmentation</code> package is the successor of the <code>train</code> and <code>prepare_data</code> script.</li> </ul> <p>The following diagram visualizes how the new packages are meant to work together. </p> <p>This is a mock</p> <p>This diagram is not realised in any form. It just exists for demonstrational purposes. To see an example of a realized pipeline based on this architecture please see the Pipeline v2 Guide</p>"},{"location":"dev/arch/#create-a-new-package","title":"Create a new package","text":"<p>A new package can easily created with:</p> <pre><code>uv init darts-packagename\n</code></pre> <p>uv creates a minimal project structure for us.</p> <p>The following things needs to be done updated and created:</p> <ol> <li>The <code>pyproject.toml</code> file inside the new package:</li> </ol> <p>Add to the <code>pyproject.toml</code> file inside the new package is the following to enable Ruff:</p> <pre><code>```toml\n[tool.ruff]\n# Extend the `pyproject.toml` file in the parent directory...\nextend = \"../pyproject.toml\"\n```\n\nPlease also provide a description and a list of authors to the file.\n</code></pre> <ol> <li> <p>The docs:     By updating the <code>notebooks/create_api_docs.ipynb</code>, running it and updating the <code>nav</code> section of the <code>mkdocs.yml</code> with the generated text.     To enable code detection, also add the package directory under <code>plugins</code> in the <code>mkdocs.yml</code>.</p> </li> <li> <p>The Readme of the package</p> </li> </ol>"},{"location":"dev/arch/#versioning","title":"Versioning","text":"<p>All packages have at all time the same version. The versioning is done via git-tags and the uv dynamic versioning tool. Hence, the version of the <code>pyproject.toml</code> of each subpackage is ignored and has no meaning.</p>"},{"location":"dev/arch/#pytorch-model-checkpoints","title":"PyTorch Model checkpoints","text":"<p>Each checkpoint is stored as a torch <code>.pt</code> tensor file. The checkpoint MUST have the following structure:</p> <pre><code>{\n    \"config\": {\n        \"model_framework\": \"smp\", # Identifier which framework or model was used\n        \"model\": { ... }, # Model specific hyperparameter which are needed to create the model\n        \"input_combination\": [ ... ], # List of strings of the names with which the model was trained, order is important\n        \"patch_size\": 1024, # Patch size on which the model was trained\n        ... # More model-framework specific parameter, e.g. normalization method and factors\n    },\n    \"statedict\": model.module.state_dict(),\n}\n</code></pre> <p>Pre-Deprecation warning</p> <p>It is planned to switch from our custom structure to huggingface model accessors.</p>"},{"location":"dev/arch/#api-paradigms","title":"API paradigms","text":"<p>The packages should pass the data as Xarray Datasets between each other. Datasets can hold coordinate information aswell as other metadata (like CRS) in a single self-describing object. Since different <code>tiles</code> do not share the same coordinates or metadata, each <code>tile</code> should be represented by a single Xarray <code>Dataset</code>.</p> <ul> <li>Each public facing API function which in some way transforms data should accept a Xarray Dataset as input and return an Xarray Dataset.</li> <li>Data can also be accepted as a list of Xarray Dataset as input and returned as a list of Xarray Datasets for batched processing.     In this case, concattenation should happend internally and on <code>numpy</code> or <code>pytorch</code> level, NOT on <code>xarray</code> abstraction level.     The reason behind this it that the tiles don't share their coordinates, resulting in a lot of empty spaces between the tiles and high memory usage.     The name of the function should then be <code>function_batched</code>.</li> <li>Each public facing API function which loads data should return a single Xarray Dataset for each <code>tile</code>.</li> <li>Data should NOT be saved to file internally, with <code>darts-export</code> as the only exception. Instead, data should returned in-memory as a Xarray Dataset, so the user / pipeline can decide what to save and when.</li> <li>Function names should be verbs, e.g. <code>process</code>, <code>ensemble</code>, <code>do_inference</code>.</li> <li>If a function is stateless it should NOT be part of a class or wrapper</li> <li>If a function is stateful it should be part of a class or wrapper, this is important for Ray</li> <li>Each Tile should be represented as a single <code>xr.Dataset</code> with each feature / band as <code>DataVariable</code>.</li> <li>Each DataVariable should have their <code>data_source</code> documented in the <code>attrs</code>, aswell as <code>long_name</code> and <code>units</code> if any for plotting.</li> <li>A <code>_FillValue</code> should also be set for no-data with <code>.rio.write_nodata(\"no-data-value\")</code>.</li> </ul> <p>Components</p> <p>The goal of these paradigms is to write functions which work as Components. Potential users can then later pick their components and put them together in their custom pipeline, utilizing their own parallelization framework.</p>"},{"location":"dev/arch/#examples","title":"Examples","text":"<p>Here are some examples, how these API paradigms should look like.</p> <p>This is a mock</p> <p>Even if some real packages are shown, these examples use mock-functions / non-existing functions and will not work .</p> <ol> <li> <p>Single transformation</p> <pre><code>import darts-package\nimport xarray as xr\n\n# User loads / creates the dataset (a single tile) by themself\nds = xr.open_dataset(\"...\")\n\n# User calls the function to transform the dataset\nds = darts-package.transform(ds, **kwargs)\n\n# User can decide by themself what to do next, e.g. save\nds.to_netcdf(\"...\")\n</code></pre> </li> <li> <p>Batched transformation</p> <pre><code>import darts_package\nimport xarray as xr\n\n# User loads / creates multiple datasets (hence, multiple tiles) by themself\ndata = [xr.open_dataset(\"...\"), xr.open_dataset(\"...\"), ...]\n\n# User calls the function to transform the dataset\ndata = darts_package.transform_batched(data, **kwargs)\n\n# User can decide by themself what to do next\ndata[0].whatever()\n</code></pre> </li> <li> <p>Load &amp; preprocess some data</p> <pre><code>import darts_package\n\n# User calls the function to transform the dataset\nds = darts_package.load(\"path/to/data\", **kwargs)\n\n# User can decide by themself what to do next\nds.whatever()\n</code></pre> </li> <li> <p>Custom pipeline example</p> <pre><code>from pathlib import Path\nimport darts_preprocessing\nimport darts_ensemble\n\nDATA_DIR = Path(\"./data/\")\nMODEL_FILE = Path(\"./models/model.pt\")\nOUT_DIR = Path(\"./out/\")\n\n# Inference is a stateful transformation, because it needs to load the model\n# Hence, the \nensemble = darts_ensemble.EnsembleV1(MODEL_FILE)\n\n# The data directory contains subfolders which then hold the input data\nfor dir in DATA_DIR:\n    name = dir.name\n\n    # Load the files from the processing directory\n    ds = darts_preprocessing.load_and_preprocess(dir)\n\n    # Do the inferencce\n    ds = ensemble.inference(ds)\n\n    # Save the results\n    ds.to_netcdf(OUT_DIR / f\"{name}-result.nc\")\n</code></pre> </li> <li> <p>Pipeline with Ray</p> <pre><code>from dataclasses import dataclass\nfrom pathlib import Path\nimport ray\nimport darts_preprocess\nimport darts_inference\nimport darts_export\n\nDATA_DIR = Path(\"./data/\")\nMODEL_DIR = Path(\"./models/\")\nOUT_DIR = Path(\"./out/\")\n\nray.init()\n\n# We need to wrap the Xarray dataset in a class, so that Ray can serialize it\n@dataclass\nclass Tile:\n    ds: xr.Dataset\n\n# Wrapper for ray\ndef open_dataset_ray(row: dict[str, Any]) -&gt; dict[str, Any]:\n    data = xr.open_dataset(row[\"path\"])\n    tile = Tile(data)\n    return {\n        \"input\": tile,\n    }\n\n# Wrapper for the preprocessing -&gt; Stateless\ndef preprocess_tile_ray(row: dict[str, Tile]) -&gt; dict[str, Tile]:\n    ds = darts_preprocess.preprocess(row[\"input\"].ds)\n    return {\n        \"preprocessed\": Tile(ds),\n        \"input\": row[\"input\"]\n    }\n\n# Wrapper for the inference -&gt; Statefull\nclass EnsembleRay:\n    def __init__(self):\n        self.ensemble = darts_inference.Ensemble.load(MODEL_DIR)\n\n    def __call__(self, row: dict[str, Tile]) -&gt; dict[str, Tile]:\n        ds = self.ensemble.inference(row[\"preprocessed\"].ds)\n        return {\n            \"output\": Tile(ds),\n            \"preprocessed\": row[\"preprocessed\"],\n            \"input\": row[\"input\"],\n        }\n\n# We need to add 'local:///' to tell ray that we want to use the local filesystem\nfiles = data.glob(\"*.nc\")\nfile_list = [f\"local:////{file.resolve().absolute()}\" for file in files]\n\nds = ray.data.read_binary_files(file_list, include_paths=True)\nds = ds.map(open_dataset_ray) # Lazy open\nds = ds.map(preprocess_tile_ray) # Lazy preprocess\nds = ds.map(EnsembleRay) # Lazy inference\n\n# Save the results\nfor row in ds.iter_rows():\n    darts_export.save(row[\"output\"].ds, OUT_DIR / f\"{row['input'].ds.name}-result.nc\")\n</code></pre> </li> </ol>"},{"location":"dev/arch/#about-the-xarray-overhead-with-ray","title":"About the Xarray overhead with Ray","text":"<p>Ray expects batched data to be in either numpy or pandas format and can't work with Xarray datasets directly. Hence, a wrapper with custom stacking functions is needed. This tradeoff is not small, however, the benefits in terms of maintainability and readability are worth it.</p> <p></p>"},{"location":"dev/auxiliary/","title":"Auxiliary Data and Datacubes","text":"<p>DARTS uses several auxiliary data - data which does not change between different scenes and / or time steps. Raster auxiliary data is stored in Zarr Datacubes.</p> <p>Currently, the following auxiliary data is used:</p> <ul> <li>ArcticDEM</li> <li>Tasseled Cap indices (Brightness, Greenness, Wetness)</li> </ul> <p>with more to come.</p>"},{"location":"dev/auxiliary/#arcticdem","title":"ArcticDEM","text":"<p>The ArcticDEM is downloaded via their STAC server using these extend files.</p> <p>The user can specify the download directory, where the ArcticDEM will be procedurally stored in a Zarr Datacube. The user can also specify the resolution of the ArcticDEM, which is either 2m, 10m or 32m. Each resolution is stored in their own Zarr Datacube.</p>"},{"location":"dev/auxiliary/#darts_acquisition.load_arcticdem","title":"darts_acquisition.load_arcticdem","text":"<pre><code>load_arcticdem(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which the tile should be loaded.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory where the ArcticDEM data is stored.</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in m.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The ArcticDEM tile, with a buffer applied. Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different. Final dataset is NOT matched to the reference CRS and resolution.</p> </li> </ul> Warning <p>Geobox must be in a meter based CRS.</p> Usage <p>Since the API of the <code>load_arcticdem</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_arcticdem\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\narcticdem = load_arcticdem(\n    optical.odc.geobox,\n    \"/path/to/arcticdem-parent-directory\",\n    resolution=2,\n    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n)\n\n# Now we can for example match the resolution and extent of the optical data:\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> <p>The <code>buffer</code> parameter is used to extend the region of interest by a certain amount of pixels. This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the resolution is not supported.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopuhr.funkuhr(\"Loading ArcticDEM\", printer=logger.debug, print_kwargs=True)\ndef load_arcticdem(\n    geobox: GeoBox, data_dir: Path | str, resolution: RESOLUTIONS, buffer: int = 0, persist: bool = True\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded.\n        data_dir (Path | str): The directory where the ArcticDEM data is stored.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in m.\n        buffer (int, optional): The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The ArcticDEM tile, with a buffer applied.\n            Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different.\n            Final dataset is NOT matched to the reference CRS and resolution.\n\n    Warning:\n        Geobox must be in a meter based CRS.\n\n    Usage:\n        Since the API of the `load_arcticdem` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_arcticdem\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        arcticdem = load_arcticdem(\n            optical.odc.geobox,\n            \"/path/to/arcticdem-parent-directory\",\n            resolution=2,\n            buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n        The `buffer` parameter is used to extend the region of interest by a certain amount of pixels.\n        This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.\n\n    Raises:\n        ValueError: If the resolution is not supported.\n\n    \"\"\"  # noqa: E501\n    odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n\n    match resolution:\n        case 2:\n            accessor = smart_geocubes.ArcticDEM2m(data_dir)\n        case 10:\n            accessor = smart_geocubes.ArcticDEM10m(data_dir)\n        case 32:\n            accessor = smart_geocubes.ArcticDEM32m(data_dir)\n        case _:\n            raise ValueError(f\"Resolution {resolution} not supported, only 2m, 10m and 32m are supported\")\n\n    accessor.assert_created()\n\n    arcticdem = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem[\"datamask\"] = arcticdem.datamask.astype(\"uint8\")\n\n    return arcticdem\n</code></pre>"},{"location":"dev/auxiliary/#tasseled-cap-indices-tcvis","title":"Tasseled Cap indices (TCVIS)","text":"<p>The TCVIS data is downloaded from Google Earth-Engine (GEE) using the TCVIS collection from Ingmar Nitze: <code>\"users/ingmarnitze/TCTrend_SR_2000-2019_TCVIS\"</code>.</p>"},{"location":"dev/auxiliary/#darts_acquisition.load_tcvis","title":"darts_acquisition.load_tcvis","text":"<pre><code>load_tcvis(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox to load the data for.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory to store the downloaded data for faster access for consecutive calls.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The TCVIS dataset.</p> </li> </ul> Usage <p>Since the API of the <code>load_tcvis</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_tcvis\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\ntcvis = load_tcvis(\n    optical.odc.geobox,\n    \"/path/to/tcvis-parent-directory\",\n)\n\n# Now we can for example match the resolution and extent of the optical data:\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopuhr.funkuhr(\"Loading TCVIS\", printer=logger.debug, print_kwargs=True)\ndef load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the TCVIS for the given geobox, fetch new data from GEE if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox to load the data for.\n        data_dir (Path | str): The directory to store the downloaded data for faster access for consecutive calls.\n        buffer (int, optional): The buffer around the geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset.\n\n    Usage:\n        Since the API of the `load_tcvis` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_tcvis\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        tcvis = load_tcvis(\n            optical.odc.geobox,\n            \"/path/to/tcvis-parent-directory\",\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"  # noqa: E501\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n\n    # We want to assume that the datacube is already created to be save in a multi-process environment\n    accessor.assert_created()\n\n    tcvis = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Rename to follow our conventions\n    tcvis = tcvis.rename_vars(\n        {\n            \"TCB_slope\": \"tc_brightness\",\n            \"TCG_slope\": \"tc_greenness\",\n            \"TCW_slope\": \"tc_wetness\",\n        }\n    )\n\n    return tcvis\n</code></pre>"},{"location":"dev/auxiliary/#why-zarr-datacubes","title":"Why Zarr Datacubes?","text":"<p>Zarr is a file format for storing chunked, compressed, N-dimensional arrays. It is designed to store large arrays of data, and to facilitate fast and efficient IO. Zarr works well integrated with Dask and Xarray.</p> <p>By storing the auxiliary data in Zarr Datacubes, it is much easier and faster to access the data of interest. If we would use GeoTiffs, we would have to first create a Cloud-Optimized GeoTiff (COG), which is basically an ensemble (mosaic) of multiple GeoTiffs. Then we would have to read from the COG, which behind the scenes would open multiple GeoTiffs and crops them to fit the region of interest. E.g. Opening a specific region of interest 10km x 10km from a 2m resolution COG would take up to 2 minutes, if the COGs extend is panarctic. Opening the same region from a Zarr Datacube takes less than 1 second.</p> <p>Inspiration</p> <p>This implementation and concept is heavily inspired by EarthMovers implementation of serverless datacube generation.</p>"},{"location":"dev/auxiliary/#procedural-download","title":"Procedural download","text":"<p>Info</p> <p>The currently used auxiliary data is downloaded on demand, only data actually used is downloaded and stored on your local machine. Hence, the stored datacubes can be thought of as a cache, which is filled with data as needed.</p> <p>There are currently two implementations of the procedural download used: a cloud based STAC download and a download via Google Earth-Engine.</p> <p>Because the single tiles of the STAC mosaic can be overlapping and intersect with multiple Zarr chunks, the STAC download is slightly more complicated. Since Google Earth-Engine allows for exact geoboxes, download of the exact chunks is possible. This reduces the complexity of the download.</p> STAC GEE 1. ROI 2. ROI <p>The above graphics shows the difference between loading data from STAC (left) and Google Earth-Engine (right). With the STAC download, the data is downloaded from a mosaic of tiles, which can be overlapping with each other and cover multiple Zarr chunks. It may occur that a chunk is not fully covered by the STAC mosaic, which results in only partial loaded chunks. In such cases, the missing data in these chunks will be updated if the other intersecting tile is downloaded, which may occur to a later time if a connected ROI is requested. The download process is much easier for GEE, since one can request the exact geoboxes of the Zarr chunks and GEE will handle the rest. Hence, chunks will always be fully covered by the downloaded data.</p> <p>Regarding the open ROI process, both implementations follow the same principle:</p> <ol> <li>Check which Tiles / Chunks intersect with the region of interest</li> <li>Dowload all new Tiles / Chunks</li> <li>Store the new Tiles / Chunks in their specific Zarr chunks</li> <li>Return the region of interest of the Zarr Datacube</li> </ol>"},{"location":"dev/auxiliary/#stac-download","title":"STAC download","text":""},{"location":"dev/auxiliary/#google-earth-engine-download","title":"Google Earth-Engine download","text":""},{"location":"dev/docs/","title":"Documentation organization","text":"<p>Guides should help users to get familar on how to use the pipelines or how they could implement certain parts of our project / components.</p> <p>The dev section should help us developers to understand the codebase and decision making.</p>"},{"location":"dev/docs/#todos","title":"ToDos","text":"<ul> <li> Add components guide</li> <li> Further document them</li> <li> Document the required inputs and output data_vars and attrs<ul> <li> In API ref</li> <li> In Components Guide</li> </ul> </li> </ul>"},{"location":"guides/components/","title":"Introduction to the darts components","text":"<p>Components</p> <p>The idea behind the Architecture of <code>darts-nextgen</code> is to provide <code>Components</code> to the user. Users should pick their components and put them together in their custom pipeline, utilizing their own parallelization framework. Pipeline v2 shows how this could look like for a simple sequential pipeline - hence without parallelization framework.</p> <p>Currently, the implemented components are:</p> Component Statefull? and why? Bound darts_acquisition.load_arcticdem Stateless Network-IO + Disk-IO darts_acquisition.load_tcvis Stateless Network-IO + Disk-IO darts_acquisition.load_planet_scene Stateless Disk-IO darts_acquisition.load_planet_masks Stateless Disk-IO darts_acquisition.load_s2_scene Stateless Disk-IO darts_acquisition.load_s2_from_gee Stateless Network-IO (+ Disk-IO) + CPU darts_acquisition.load_s2_from_stac Stateless Network-IO (+ Disk-IO) darts_acquisition.load_s2_masks Stateless Disk-IO + CPU darts_preprocessing.preprocess_legacy_fast Stateless CPU + GPU darts_segmentation.SMPSegmenter.segment_tile Statefull: Model-Weights GPU darts_ensemble.EnsembleV1.segment_tile Statefull: Model-Weights GPU darts_postprocessing.prepare_export Stateless CPU + GPU darts_export.export_tile Stateless Disk-IO"},{"location":"guides/components/#component-outputs","title":"Component Outputs","text":"<p>Incomplete</p> <p>This section is incomplete and will be updated in the future.</p> <p>All component-tiles are <code>xr.Datasets</code> which have geospatial coordinates <code>x</code>, <code>y</code> and a spatial reference  <code>spatial_ref</code> (from rioxarray / odc-geo) as coordinates.. The following documents the input and output of each component.</p>"},{"location":"guides/components/#acquisition-load-arcticdem","title":"Acquisition: Load ArcticDEM","text":""},{"location":"guides/components/#input","title":"Input","text":"<p>(Acquisition components do not have an input in form of a <code>xr.Dataset</code>)</p>"},{"location":"guides/components/#output","title":"Output","text":"DataVariable shape dtype no-data attrs note <code>dem</code> (x, y) float32 nan data_source, long_name, units <code>dem_datamask</code> (x, y) bool False data_source, long_name, units"},{"location":"guides/components/#preprocessing-legacy-fast","title":"Preprocessing: Legacy Fast","text":""},{"location":"guides/components/#input_1","title":"Input","text":"DataVariable shape dtype no-data attrs note <code>blue</code> (x, y) uint16 0 data_source, long_name, units <code>green</code> (x, y) uint16 0 data_source, long_name, units <code>red</code> (x, y) uint16 0 data_source, long_name, units <code>nir</code> (x, y) uint16 0 data_source, long_name, units <code>dem</code> (x, y) float32 nan data_source, long_name, units <code>dem_datamask</code> (x, y) bool False data_source, long_name, units <code>tc_brightness</code> (x, y) uint8 - data_source, long_name <code>tc_greenness</code> (x, y) uint8 - data_source, long_name <code>tc_wetness</code> (x, y) uint8 - data_source, long_name"},{"location":"guides/components/#output_1","title":"Output","text":"DataVariable shape dtype no-data attrs note [Input] <code>ndvi</code> (x, y) uint16 0 data_source, long_name Values between 0-20.000 (+1, *1e4) <code>relative_elevation</code> (x, y) int16 0 data_source, long_name, units <code>slope</code> (x, y) float32 nan data_source, long_name"},{"location":"guides/components/#segmentation-ensemble-output","title":"Segmentation / Ensemble Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs [Output from Preprocessing] <code>probabilities</code> (x, y) float32 nan long_name <code>probabilities-model-X*</code> (x, y) float32 nan long_name <p>*: optional intermedia probabilities in an ensemble</p>"},{"location":"guides/components/#postprocessing-output","title":"Postprocessing Output","text":"<p>Coordinates: <code>x</code>, <code>y</code> and <code>spatial_ref</code> (from rioxarray)</p> DataVariable shape dtype no-data attrs note [Output from Preprocessing] <code>probabilities_percent</code> (x, y) uint8 255 long_name, units Values between 0-100 <code>binarized_segmentation</code> (x, y) uint8 - long_name"},{"location":"guides/config/","title":"Config Files","text":"<p>The <code>darts</code> CLI support passing parameters via a config file in TOML format. This can be useful to reduce the amount of parameters you need to pass or to safe different configurations. In general, the CLI tries to match all parameters under the <code>darts</code> key of the config file, skipping not needed ones.</p>"},{"location":"guides/config/#example-usage","title":"Example usage","text":"<p>Let's take a closer look with the example command <code>darts hello</code>. This command has the following function signature:</p> <pre><code>def hello(name: str, n: int = 1):\n    \"\"\"Say hello to someone.\n\n    Args:\n        name (str): The name of the person to say hello to\n        n (int, optional): The number of times to say hello. Defaults to 1.\n\n    Raises:\n        ValueError: If n is 3.\n\n    \"\"\"\n    for i in range(n):\n        logger.debug(f\"Currently at {i=}\")\n        if n == 3:\n            raise ValueError(\"I don't like 3\")\n        logger.info(f\"Hello {name}\")\n</code></pre> <p>Let's run the command without making a config file:</p> <pre><code>$ uv run darts hello Alice\nDEBUG Currently at i=0\nINFO Hello Alice\n</code></pre> <p>Now specify a config file <code>config.toml</code>:</p> <pre><code>[darts]\nname = \"Not Alice\"\nn = 2\n</code></pre> <p>And run the same command:</p> <pre><code>$ uv run darts hello Alice\nDEBUG Currently at i=0\nINFO Hello Alice\nDEBUG Currently at i=1\nINFO Hello Alice\n</code></pre> <p>The <code>name</code> parameter is still taken from the CLI, while the <code>n</code> parameter is taken from the config file.</p> <p>Because the CLI utilized a custom TOML parser to parse the config file and pass it to the CLI tool cyclopts, only parameters under the <code>darts</code> key are considered. Subheading keys are not considered, but can be used to structure the config file:</p> <pre><code>[darts]\nname = \"Not Alice\"\n\n[darts.numbers]\nn = 2\n</code></pre> <p>The <code>numbers</code> key is ignored by the CLI, hence <code>n</code> will be add to the command as before.</p> <p>Warning</p> <p>The only parameters not passed from the config file are the <code>--config-file</code> and <code>--log-dir</code> parameters. The <code>--log-dir</code> parameter is evaluated before the config file is parsed, hence it is not possible to specify the logging directory via the config file.</p>"},{"location":"guides/config/#real-world-example-with-sentinel-2-processing","title":"Real world example with Sentinel 2 processing","text":"<p>Sentinel 2 processing via. Area of Interest file:</p> <pre><code>[darts]\nee-project = \"your-ee-project\"\ndask-worker = 4\n\n[darts.paths]\ninput-cache = \"./data/cache/s2gee\"\noutput-data-dir = \"./data/out\"\narcticdem-dir = \"./data/datacubes/arcticdem\"\ntcvis-dir = \"./data/datacubes/tcvis\"\nmodel-file = \"./models/s2-tcvis-final-large_2025-02-12.ckpt\"\n</code></pre> <p>Running the command:</p> <pre><code>uv run darts run-sequential-aoi-sentinel2-pipeline --aoi-shapefile path/to/your/aoi.geojson --start-date 2024-07 --end-date 2024-09\n</code></pre>"},{"location":"guides/devices/","title":"Devices","text":"<p>Supported devices</p> <p>As of right now, only CUDA and CPU devices are supported. How to install a working Python environment for either case please refer to the installation guide.</p> <p>Some functions can be run on the GPU if a CUDA device is available and the python environment is properly installed with CUDA enabled. These functions will automatically detect if a CUDA device is available and will use it if so. It is possible to also force the use of a specific device through the <code>device</code> parameter of the respective function. For most GPU-capable functions it is possible to pass either <code>cpu</code> or <code>cuda</code> as a string to the <code>device</code> parameter. In a multi-GPU setup, the device can be specified by passing the device index as an integer (e.g. <code>0</code> for the first GPU, <code>1</code> for the second GPU, etc.). However, functions which use PyTorch expect the device to be a PyTorch device object, so you need to pass <code>torch.device(\"cuda:0\")</code> instead of just <code>0</code>. Which type of device is expected is documented in the respective function documentation.</p> <p>As of now, the following functions can be run on the GPU:</p> <ul> <li>darts_preprocessing.preprocess_legacy_fast - <code>device</code>: <code>\"cpu\" | \"cuda\" | int</code></li> <li>darts_postprocessing.prepare_export - <code>device</code>: <code>\"cpu\" | \"cuda\" | int</code></li> <li>darts_segmentation.SMPSegmenter - <code>device</code>: <code>torch.device</code></li> <li>darts_ensemble.EnsembleV1 - <code>device</code>: <code>torch.device</code></li> </ul>"},{"location":"guides/installation/","title":"Advanced Installation","text":"<p>Prereq:</p> <ul> <li>uv: <code>curl -LsSf https://astral.sh/uv/install.sh | sh</code></li> <li>postgresql (optional for training-only)</li> <li>cuda (optional for GPU support)</li> </ul> <p>This project uses <code>uv</code> to manage the python environment. If you are not familiar with <code>uv</code> yet, please read their documentation first. Please don't use <code>pip</code> or <code>conda</code> to install the dependencies, as this often leads to problems. We have spend a lot of time making sure that the install process is easy and quick, but this is only possible with <code>uv</code>. So please use it.</p> <p>In general the environment can be installed with <code>uv sync</code>. However, this project depends on some libraries (torch and torchvision) which don't get installed per default. Therefore you need to specify an extra flag to install the correct dependencies, e.g.</p> <pre><code>uv sync --extra cuda126\n</code></pre> <p>This will install the environment with the correct dependencies for CUDA 12.6. The following sections will explain the different extra flags and groups which can be used to install the environment for different purposes and systems.</p>"},{"location":"guides/installation/#cuda-and-cpu-only-installations","title":"CUDA and CPU-only installations","text":"<p>Several CUDA versions can be used, but it may happen that some problems occur on different systems. Currently CUDA 11.8, 12.1, and 12.6 are supported, but sometimes other versions work as well. We use python extra dependencies, so it is possible to specify the CUDA version via an <code>--extra</code> flag in the <code>uv sync</code> command.</p> <p>You can check the currently installed CUDA version via:</p> <pre><code>nvidia-smi\n# Look at the top right corner for the CUDA version\n</code></pre> <p>Warning</p> <p>If the <code>nvidia-smi</code> command is not found, you might need to install the nvidia drivers. Be very cautious with the installation of the driver, rather read the documentation with care.</p> <p>To install the python environment for a specific CUDA version use one of the following commands respectively:</p> <pre><code>uv sync --extra cuda118\nuv sync --extra cuda121\nuv sync --extra cuda126\n</code></pre> <p>CUDA version missmatch</p> <p>Sometimes it is possible to use a different CUDA version for the python packages than the one installed. E.g. we tested our code on a system with CUDA 12.2 installed, but used the python packages for CUDA 12.1. This is not recommended, but sometimes it works.</p> <p>Install the python environment for CPU-only use:</p> <pre><code>uv sync --extra cpu\n</code></pre> <p>Danger</p> <p>Either <code>--extra cpu</code> or <code>--extra cudaXXX</code> must be specified. Without important libraries like PyTorch will not be installed and the environment will not work.</p>"},{"location":"guides/installation/#workaround-for-cuda-related-errors","title":"Workaround for CUDA related errors","text":"<p>If CUDA is not installed correctly, some CUDA optional packages are missing or the wrong version of CUDA is installed, conda / mamba can be used as a workaround.</p> <p>First create a new conda environment and activate it:</p> <pre><code>mamba create -n darts-nextgen-cuda-env\nmamba activate darts-nextgen-cuda-env\n</code></pre> <p>Then install CUDA toolkit and required system packages via conda / mamba:</p> <pre><code>mamba install cuda-toolkit nvidia::cuda-nvrtc\n...\n</code></pre> <p>Now you can (while the conda / mamba environment is active) sync your uv environment.</p> <pre><code>uv sync --...\n</code></pre>"},{"location":"guides/installation/#training-specific-dependencies","title":"Training specific dependencies","text":"<p>Training specific dependencies are optional and therefore not installed by default. To install them, add <code>--extra training</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training\n</code></pre> <p>psycopg2</p> <p>The training dependencies depend on psycopg2, which requires postgresql installed on your system.</p>"},{"location":"guides/installation/#packages-for-the-documentation","title":"Packages for the documentation","text":"<p>Packages which are used to create this documentation are not installed by default and are not available via as an extra. Instead they are installed as part of an optional <code>dependency-group</code>, or <code>group</code> for short. To install the documentation dependencies, add <code>--group docs</code> to the <code>uv sync</code> command, e.g.:</p> <pre><code>uv sync --extra cuda126 --extra training --group docs\n</code></pre>"},{"location":"guides/logging/","title":"Logging Guide","text":"<p>We want to use the python logging module as much as possible to traceback errors and document the pipeline processes. Furthermore, we want to configure each logger with the <code>RichHandler</code>, which prettyfies the output with rich.</p>"},{"location":"guides/logging/#setup-guide","title":"Setup Guide","text":"<p>Currently, all setup related to logging is found in the <code>darts.utils.logging.py</code> file. It contains two functions:</p> <ol> <li>A setup function which sets the log-level for all <code>darts.*</code> logger and add default options to xarray and pytorch to supress arrays. See how to supress arrays.</li> <li>A function which adds a file and a rich log handler.</li> </ol> <p>Both functions are used in the CLI setup but can also be called from e.g. a notebook. The recommended approach for handling logging within a notebook is the following:</p> <pre><code>import logging\nfrom rich.logging import RichHandler\nfrom darts.utils.logging import LoggingManager\n\nLoggingManager.setup_logging(verbose=True)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(message)s\",\n    datefmt=\"[%X]\",\n    handlers=[RichHandler(rich_tracebacks=True)],\n)\n</code></pre> <p>This way the notebook won't spam logfiles everywhere and we still have control over our rich handler.</p>"},{"location":"guides/logging/#usage-guide","title":"Usage Guide","text":"<p>For logging inside a darts-package should be done without any further configuration:</p> <pre><code>import logging\n\nlogger = logging.getLogger(__name__.replace(\"darts_\", \"darts.\")) # don't replace __name__\n</code></pre> <p>Logging at the top-level <code>darts</code> package can just use a <code>__name__</code> logger:</p> <pre><code>import loggin\n\nlogger = logging.getLogger(__name__) # don't replace __name__\n</code></pre>"},{"location":"guides/logging/#supressing-arrays","title":"Supressing Arrays","text":"<p>When printing or logging large numpy arrays a lot of numbers get truncated, however the array still takes a lot of space. Using <code>lovely_numpy</code> and <code>lovely_tensor</code> can help here:</p> <pre><code>import numyp as np\nimport torch\nimport xarray as xr\nfrom lovely_numpy import lo\nfrom lovely_tensors import monkey_patch\n\nmonkey_patch()\nxr.set_options(display_expand_data=False)\n\na = np.zeros((8, 1024, 1024))\nla = lo(a)\nda = xr.DataArray(a)\nt = torch.tensor(a)\n\nlogger.warning(la)\nlogger.warning(da)\nlogger.warning(t)\n</code></pre>"},{"location":"guides/pipeline-v2/","title":"Pipeline v2","text":"<p>The following document describes the pipeline which lead to the DARTS v1 dataset and will potentially lead to the DARTS v2 dataset. The orginial v1 dataset was not created with this repository, however, a newer, faster version of this pipeline is implemented here, which still uses the exact same pipeline-steps. Hence, it should be possible to re-create the DARTS v1 dataset with this repository. The implemented pipeline in this repository could potentially be used for future iterations and releases of the DARTS dataset.</p> <p>In addition to the PLANET version of the DARTS dataset, the pipeline also supports Sentinel 2 imagery as optical input, resulting in a lower spatial resolution (10m instead of 3m).</p> <p>Note</p> <p>The v1 / v2 pipeline is also aliased by <code>legacy</code> pipeline somewhere deep in the code.</p> <p>As of right now, three basic realisation of the v1 pipeline are implemented:</p> <ul> <li><code>run-sequential-planet-pipeline-fast</code></li> <li><code>run-sequential-sentinel2-pipeline-fast</code></li> <li><code>run-sequential-aoi-sentinel2-pipeline</code></li> </ul> <p>The naming convention has changed a lot and probably will further change with more pipeline realisations becoming implemented. <code>sequential</code> indicates that the pipeline runs without any parallelization framework.</p> <p>The pipeline currently consists of the following steps:</p> <ol> <li>Load the optical and auxiliary data     This step depends on the realisation of the pipeline.     Either darts_acquisition.load_planet_scene, darts_acquisition.load_s2_scene darts_acquisition.load_s2_from_gee or darts_acquisition.load_s2_from_stac.     Also loads the masks if not loaded from gee or stac: darts_acquisition.load_planet_masks or darts_acquisition.load_s2_masks, for the gee and stac versions the masks are already included.     For the auxiliary data: darts_acquisition.load_arcticdem and darts_acquisition.load_tcvis</li> <li>Preprocess the optical data: darts_preprocessing.preprocess_legacy_fast</li> <li>Segment the optical data: darts_segmentation.SMPSegmenter.segment_tile or darts_ensemble.segment_tile.</li> <li>Postprocess the segmentation and make it ready for export: darts_postprocessing.prepare_export</li> <li>Export the data: darts_export.export_tile.</li> </ol> <p></p> <p>A very simplified version of this implementation looks like this:</p> <pre><code>from darts_acquisition import load_arcticdem, load_tcvis\nfrom darts_segmentation import SMPSegmenter\nfrom darts_export import export_tile, missing_outputs\nfrom darts_postprocessing import prepare_export\nfrom darts_preprocessing import preprocess_legacy_fast\nfrom darts_acquisition.s2 import load_s2_from_gee\n\ns2id = \"20230701T194909_20230701T195350_T11XNA\"\narcticdem_dir = \"/path/to/arcticdem\"\ntcvis_dir = \"/path/to/tcvis\"\nmodel_file = \"/path/to/model.pt\"\noutpath = \"/path/to/output\"\n\nsegmenter = SMPSegmenter(model_file)\n\ntile = load_s2_from_gee(s2id)\n\narcticdem = load_arcticdem(\n    tile.odc.geobox,\n    arcticdem_dir,\n    resolution=10,\n    buffer=ceil(100 / 2 * sqrt(2)),\n)\n\ntcvis = load_tcvis(tile.odc.geobox, tcvis_dir)\n\ntile = preprocess_legacy_fast(tile, arcticdem, tcvis)\n\ntile = segmenter.segment_tile(tile)\n\ntile = prepare_export(tile)\n\nexport_tile(tile, outpath)\n</code></pre> <p>Further reading</p> <p>To learn more about how the pipeline and their components steps work, please refer to the following materials:</p> <ul> <li>The Paper about the DARTS Dataset (No link yet)</li> <li>The Components Guide</li> <li>The API Reference</li> </ul> <p>There are further features implemented, which do not come from the components:</p> <ul> <li>Time tracking of processing steps</li> <li>Skipping of already processed tiles</li> <li>Environment debugging info</li> </ul>"},{"location":"guides/pipeline-v2/#minimal-configuration-example","title":"Minimal configuration example","text":"<pre><code>[darts]\nee-project = \"ee-tobias-hoelzer\"\nmodel-files = \"./models/s2-tcvis-final-large_2025-02-12.ckpt\"\naoi-shapefile = \"./data/myaoi.gpkg\"\nstart-date = \"2024-07\"\nend-date = \"2024-09\"\n</code></pre>"},{"location":"guides/pipeline-v2/#full-configuration-explaination","title":"Full configuration explaination","text":"<p>TODO</p>"},{"location":"guides/training/","title":"Training","text":"Table of Contents<ul> <li>Training<ul> <li>Preprocess the data<ul> <li>\u00a0preprocess_s2_train_data</li> <li>\u00a0preprocess_planet_train_data</li> </ul> </li> <li>Simple SMP train and test<ul> <li>\u00a0train_smp</li> <li>\u00a0test_smp</li> <li>\u00a0convert_lightning_checkpoint</li> </ul> </li> <li>Run a cross-validation hyperparameter sweep</li> <li>Example config and sweep-config files</li> </ul> </li> </ul> Preprocessed data <p>All training and sweeps expect data to be present in preprocessed form. This means that the <code>train_data_dir</code> should look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>With each zarr group containing a <code>x</code> and <code>y</code> dataarray.</p> <p>Ideally, use the preprocessing functions explained below to create this structure.</p>"},{"location":"guides/training/#preprocess-the-data","title":"Preprocess the data","text":"<p>The train, validation and test flow ist best descriped in the following image: </p> <p>To split your sentinel 2 data into the three different datasets and preprocess it, you can use the following command:</p> <pre><code>[uv run] darts preprocess-s2-train-data --your-args-here ... \n</code></pre> <p>PLANET data</p> <p>If you are using PLANET data, you can use the following command instead:</p> <pre><code>[uv run] darts preprocess-planet-train-data --your-args-here ...\n</code></pre> <p>This will create three data splits:</p> <ul> <li><code>cross-val</code>, used for train and validation</li> <li><code>val-test</code> 5% random leave-out for testing the randomness distribution shift of the data</li> <li><code>test</code> leave-out region for testing the spatial distribution shift of the data</li> </ul> <p>The final train data is saved to disk in form of zarr arrays with dimensions <code>[n, c, h, w]</code> and <code>[n, h, w]</code> for the labels respectivly, with chunksizes of <code>n=1</code>. Hence, every sample is saved in a separate chunk and therefore in a seperate file on disk, but all managed by zarr.</p> <p>The preprocessing is done with the same components used in the segmentation pipeline. Hence, the same configuration options are available. In addition, this preprocessing splits larger images into smaller patches of a fixed size. Size and overlap can be configured in the configuration file or via the arguments of the CLI.</p> You can also use the underlying functions directly:"},{"location":"guides/training/#darts.legacy_training.preprocess_s2_train_data","title":"darts.legacy_training.preprocess_s2_train_data","text":"<pre><code>preprocess_s2_train_data(\n    *,\n    bands: list[str],\n    sentinel2_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    dask_worker: int = min(\n        16, multiprocessing.cpu_count() - 1\n    ),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n)\n</code></pre> <p>Preprocess Sentinel 2 data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Sentinel 2 scenes.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/s2.py</code> <pre><code>def preprocess_s2_train_data(\n    *,\n    bands: list[str],\n    sentinel2_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Sentinel 2 data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        sentinel2_dir (Path): The directory containing the Sentinel 2 scenes.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_s2_masks, load_s2_scene, load_tcvis\n    from darts_acquisition.s2 import parse_s2_tile_id\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        s2_paths = sorted(sentinel2_dir.glob(\"*/\"))\n        logger.info(f\"Found {len(s2_paths)} Sentinel 2 scenes in {sentinel2_dir}\")\n        path_gen = split_dataset_paths(s2_paths, train_data_dir, test_val_split, test_regions, admin_dir)\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(s2_paths), console=console\n        ):\n            try:\n                _, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}' ({tile_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{tile_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{tile_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_s2_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 10\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_s2_masks(fpath, optical.odc.geobox)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                labels = gpd.read_file(fpath / f\"{s2_tile_id}.shp\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile,\n                    labels,\n                    bands,\n                    norm_factors,\n                    patch_size,\n                    overlap,\n                    exclude_nopositive,\n                    exclude_nan,\n                    device,\n                    mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}'\"\n                    f\"({tile_id=}) with {patch_id} patches.\"\n                )\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"sentinel2_dir\": sentinel2_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"guides/training/#darts.legacy_training.preprocess_planet_train_data","title":"darts.legacy_training.preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    bands: list[str],\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    dask_worker: int = min(\n        16, multiprocessing.cpu_count() - 1\n    ),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/planet.py</code> <pre><code>def preprocess_planet_train_data(\n    *,\n    bands: list[str],\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n        labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n        footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n        footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        planet_paths = sorted(_legacy_path_gen(data_dir))\n        logger.info(f\"Found {len(planet_paths)} PLANET scenes and orthotiles in {data_dir}\")\n        path_gen = split_dataset_paths(\n            planet_paths, footprints, train_data_dir, test_val_split, test_regions, admin_dir\n        )\n\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(planet_paths), console=console\n        ):\n            try:\n                planet_id = fpath.stem\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(planet_paths)}\"\n                    f\" '{fpath.resolve()}' ({planet_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{planet_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{planet_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_planet_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 2\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_planet_masks(fpath)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile=tile,\n                    labels=labels[labels.image_id == planet_id],\n                    bands=bands,\n                    norm_factors=norm_factors,\n                    patch_size=patch_size,\n                    overlap=overlap,\n                    exclude_nopositive=exclude_nopositive,\n                    exclude_nan=exclude_nan,\n                    device=device,\n                    mask_erosion_size=mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(planet_paths)} '{fpath.resolve()}'\"\n                    f\"({planet_id=}) with {patch_id} patches.\"\n                )\n\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"guides/training/#simple-smp-train-and-test","title":"Simple SMP train and test","text":"<p>To train a simple SMP (Segmentation Model Pytorch) model you can use the command:</p> <pre><code>[uv run] darts train-smp --your-args-here ...\n</code></pre> <p>Configurations for the architecture and encoder can be found in the SMP documentation for model configurations.</p> <p>Change defaults</p> <p>Even though the defaults from the CLI are somewhat useful, it is recommended to create a config file and change the behavior of the training there.</p> <p>This will train a model with the <code>cross-val</code> data and save the model to disk. You don't need to specify the concrete path to the <code>cross-val</code> split, the training script expects that the <code>--train-data-dir</code> points to the root directory of the splits, hence, the same path used in the preprocessing should be specified. The training relies on PyTorch Lightning, which is a high-level interface for PyTorch. It is recommended to use Weights and Biases (wandb) for the logging, because the training script is heavily influenced by how the organization of wandb works.</p> <p>Each training run is assigned a unique name and id pair and optionally a trial name. The name, which the user can provide, should be used as a grouping mechanism of equal hyperparameter and code. Hence, different versions of the same name should only differ by random state or run settings parameter, like logs. Each version is assigned a unique id. Artifacts (metrics &amp; checkpoints) are then stored under <code>{artifact_dir}/{run_name}/{run_id}</code> in no-crossval runs. If <code>trial_name</code> is specified, the artifacts are stored under <code>{artifact_dir}/{trial_name}/{run_name}-{run_id}</code>. Wandb logs are always stored under <code>{wandb_entity}/{wandb_project}/{run_name}</code>, regardless of <code>trial_name</code>. However, they are further grouped by the <code>trial_name</code> (via job_type), if specified. Both <code>run_name</code> and <code>run_id</code> are also stored in the hparams of each checkpoint.</p> <p>You can now test the model on the other two splits (<code>val-test</code> and <code>test</code>) with the following command:</p> <pre><code>[uv run] darts test-smp --your-args-here ...\n</code></pre> <p>The checkpoint stored is not usable for the pipeline yet, since it is stored in a different format. To convert the model to a format, you need to convert is first:</p> <pre><code>[uv run] darts convert-lightning-checkpoint --your-args-here ...\n</code></pre> You can also use the underlying functions directly:"},{"location":"guides/training/#darts.legacy_training.train_smp","title":"darts.legacy_training.train_smp","text":"<pre><code>train_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    fold: int = 0,\n    continue_from_checkpoint: pathlib.Path | None = None,\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the training of the SMP model.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.</p> <p>Each training run is assigned a unique name and id pair and optionally a trial name. The name, which the user can provide, should be used as a grouping mechanism of equal hyperparameter and code. Hence, different versions of the same name should only differ by random state or run settings parameter, like logs. Each version is assigned a unique id. Artifacts (metrics &amp; checkpoints) are then stored under <code>{artifact_dir}/{run_name}/{run_id}</code> in no-crossval runs. If <code>trial_name</code> is specified, the artifacts are stored under <code>{artifact_dir}/{trial_name}/{run_name}-{run_id}</code>. Wandb logs are always stored under <code>{wandb_entity}/{wandb_project}/{run_name}</code>, regardless of <code>trial_name</code>. However, they are further grouped by the <code>trial_name</code> (via job_type), if specified. Both <code>run_name</code> and <code>run_id</code> are also stored in the hparams of each checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>fold</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The current fold to train on. Must be in [0, 4]. Defaults to 0.</p> </li> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to a checkpoint to continue training from. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the encoder weights. Defaults to None.</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>random_seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The device to run the model on. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>wandb_group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Wandb group. Usefull for CV-Sweeps. Defaults to None.</p> </li> <li> <code>run_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of this run, as a further grouping method for logs etc. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>run_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ID of the run. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>trial_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation run / trial. This effects primary logging and artifact storage. If None, will do nothing. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def train_smp(\n    *,\n    # Data config\n    train_data_dir: Path,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    fold: int = 0,\n    continue_from_checkpoint: Path | None = None,\n    # Hyperparameters\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the training of the SMP model.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.\n\n    Each training run is assigned a unique **name** and **id** pair and optionally a trial name.\n    The name, which the user _can_ provide, should be used as a grouping mechanism of equal hyperparameter and code.\n    Hence, different versions of the same name should only differ by random state or run settings parameter, like logs.\n    Each version is assigned a unique id.\n    Artifacts (metrics &amp; checkpoints) are then stored under `{artifact_dir}/{run_name}/{run_id}` in no-crossval runs.\n    If `trial_name` is specified, the artifacts are stored under `{artifact_dir}/{trial_name}/{run_name}-{run_id}`.\n    Wandb logs are always stored under `{wandb_entity}/{wandb_project}/{run_name}`, regardless of `trial_name`.\n    However, they are further grouped by the `trial_name` (via job_type), if specified.\n    Both `run_name` and `run_id` are also stored in the hparams of each checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        fold (int, optional): The current fold to train on. Must be in [0, 4]. Defaults to 0.\n        continue_from_checkpoint (Path | None, optional): Path to a checkpoint to continue training from.\n            Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        model_encoder_weights (str | None, optional): Path to the encoder weights. Defaults to None.\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        early_stopping_patience (int, optional): Number of epochs to wait for improvement before stopping.\n            Defaults to 5.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        random_seed (int, optional): Random seed for deterministic training. Defaults to 42.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str, optional): The device to run the model on. Defaults to \"auto\".\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        wandb_group (str | None, optional): Wandb group. Usefull for CV-Sweeps. Defaults to None.\n        run_name (str | None, optional): Name of this run, as a further grouping method for logs etc.\n            If None, will generate a random one. Defaults to None.\n        run_id (str | None, optional): ID of the run. If None, will generate a random one. Defaults to None.\n        trial_name (str | None, optional): Name of the cross-validation run / trial.\n            This effects primary logging and artifact storage.\n            If None, will do nothing. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.legacy_training.util import generate_id, get_generated_name\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run_name or get_generated_name(artifact_dir)\n    run_id = run_id or generate_id()\n\n    logger.info(f\"Starting training '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(\n        f\"Using config:\\n\\t{model_arch=}\\n\\t{model_encoder=}\\n\\t{model_encoder_weights=}\\n\\t{augment=}\\n\\t\"\n        f\"{learning_rate=}\\n\\t{gamma=}\\n\\t{batch_size=}\\n\\t{max_epochs=}\\n\\t{log_every_n_steps=}\\n\\t\"\n        f\"{check_val_every_n_epoch=}\\n\\t{early_stopping_patience=}\\n\\t{plot_every_n_val_epochs=}\\n\\t{num_workers=}\"\n        f\"\\n\\t{device=}\\n\\t{random_seed=}\"\n    )\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(random_seed, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    config = SMPSegmenterConfig(\n        input_combination=preprocess_config[\"bands\"],\n        model={\n            \"arch\": model_arch,\n            \"encoder_name\": model_encoder,\n            \"encoder_weights\": model_encoder_weights,\n            \"in_channels\": len(preprocess_config[\"bands\"]),\n            \"classes\": 1,\n        },\n        norm_factors=preprocess_config[\"norm_factors\"],\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir / \"cross-val.zarr\",\n        batch_size=batch_size,\n        fold=fold,\n        augment=augment,\n        num_workers=num_workers,\n    )\n    model = SMPSegmenter(\n        config=config,\n        learning_rate=learning_rate,\n        gamma=gamma,\n        focal_loss_alpha=focal_loss_alpha,\n        focal_loss_gamma=focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        trial_name=trial_name,\n        random_seed=random_seed,\n    )\n\n    # Loggers\n    is_crossval = bool(trial_name)\n    trainer_loggers = [\n        CSVLogger(\n            save_dir=artifact_dir,\n            name=run_name if not is_crossval else trial_name,\n            version=run_id if not is_crossval else f\"{run_name}-{run_id}\",\n        ),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            group=wandb_group,\n            job_type=trial_name,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            input_combination=config[\"input_combination\"],\n            val_set=f\"val{fold}\",\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            is_crossval=is_crossval,\n        ),\n    ]\n    if early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {early_stopping_patience}\")\n        early_stopping = EarlyStopping(monitor=\"val/JaccardIndex\", mode=\"max\", patience=early_stopping_patience)\n        callbacks.append(early_stopping)\n\n    # Train\n    trainer = L.Trainer(\n        max_epochs=max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=check_val_every_n_epoch,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=False,\n    )\n    trainer.fit(model, datamodule, ckpt_path=continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"guides/training/#darts.legacy_training.test_smp","title":"darts.legacy_training.test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size. Defaults to 8.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Directory to save artifacts. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device to use. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/test.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int, optional): Batch size. Defaults to 8.\n        artifact_dir (Path, optional): Directory to save artifacts. Defaults to Path(\"lightning_logs\").\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device (int | str, optional): Device to use. Defaults to \"auto\".\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n    logger.info(f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device=}\")\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    # Data and model\n    datamodule_val_test = DartsDataModule(\n        data_dir=train_data_dir / \"val-test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    datamodule_test = DartsDataModule(\n        data_dir=train_data_dir / \"test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / run_name / run_id / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    model = SMPSegmenter.load_from_checkpoint(model_ckp)\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=run_name, version=run_id),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            id=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    metrics_cb = BinarySegmentationMetrics(\n        input_combination=preprocess_config[\"bands\"],\n    )\n    callbacks = [\n        RichProgressBar(),\n        metrics_cb,\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=True,\n    )\n    # Overwrite the names of the test sets to test agains two separate sets\n    metrics_cb.test_set = \"val-test\"\n    model.test_set = \"val-test\"\n    trainer.test(model, datamodule_val_test, ckpt_path=model_ckp)\n    metrics_cb.test_set = \"test\"\n    model.test_set = \"test\"\n    trainer.test(model, datamodule_test)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"guides/training/#darts.legacy_training.convert_lightning_checkpoint","title":"darts.legacy_training.convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/util.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"guides/training/#run-a-cross-validation-hyperparameter-sweep","title":"Run a cross-validation hyperparameter sweep","text":"<p>Terminal Multiplexers</p> <p>It is recommended to use a terminal multiplexer like <code>tmux</code>, <code>screen</code> or <code>zellij</code> to run multiple training runs in parallel. This way there is no need to have multiple terminal open over the span of multiple days.</p> <p>To sweep over a certrain set of hyperparameters, some preparations are necessary:</p> <ol> <li>Create a sweep configuration file in YAML format. This file should contain the hyperparameters to sweep over and the search space for each hyperparameter.</li> <li>Setup a PostgreSQL database to store the results of the sweep, so we can run multiple runs in parallel with Optuna.</li> </ol> <p>The sweep configuration file should look like a wandb sweep configuration. All values will be parsed and transformed to fit to an optuna sweep.</p> <p>To setup the PostgreSQL database, search for an appropriate guide on how to setup a PostgreSQL database. There are many ways to do this, depending on your environment. The only important thing is that the database is reachable from the machine you are running the sweep on.</p> <p>Now you can setup the sweep with the following command:</p> <pre><code>uv run darts optuna-sweep-smp --your-args-here ... --device 0\n</code></pre> <p>This will output some information about the sweep, especially the sweep id. In addition, it will start running trials on the CUDA:0 device.</p> <p>Starting and continuing sweeps</p> <p>Starting and continuing sweeps is done via the same <code>optuna-sweep-smp</code> command. Depending on the two arguments <code>-sweep-id</code> and <code>device</code>, the command will decide what to do. If the <code>sweep-id</code> is not specified, a new sweep will be started. If the <code>sweep-id</code> is specified, the sweep will continue from the last run. If the <code>device</code> is specified, <code>n-trials</code> will be started on the specified device (sequentially). If the <code>device</code> is not specified, but <code>sweep-id</code> is, then an error will be raised. If neither <code>device</code> nor <code>sweep-id</code> is specified, then a new sweep will be created without starting trials.</p> <p>To start a second runner, you must open a new terminal (or panel/window in a terminal multiplexer) and run the following command:</p> <pre><code>uv run darts optuna-sweep-smp --your-args-here ... --device 1 --sweep-id &lt;sweep-id&gt;\n</code></pre> <p>Multiple runners</p> <p>You can run as many runners as you have devices available. Each runner will start n trials sequentially, specified by <code>n-trials</code>, which each request a new hyperparameter-combination from optuna. Each trial further creates multiple runs, depending on the <code>n_folds</code> and <code>n_randoms</code> parameters. This is the cross-validation part: Each trial, hence same hyperparameter-combination, is run <code>n_folds</code> times with <code>n_randoms</code> different random seeds. Therefore, the total number of runs done by a runner is <code>n-trials * n_folds * n_randoms</code>. This should ensure that a single random good (or bad) run does not influence the overall result of a hyperparameter-combination.</p>"},{"location":"guides/training/#example-config-and-sweep-config-files","title":"Example config and sweep-config files","text":"<p>For better readability, the example config file uses different sub-headings which are not necessary and could be named differently or even removed. The only important heading is the <code>[darts]</code> heading, which is the root of the configuration file. Every value which is not under a <code>darts</code> top-level heading is ignored, as descriped in the Configuration Guide.</p> <p>The following <code>config.toml</code> expects that the labels are cloned from the ML_training_labels repository and that PLANET scenes and tiles are downloaded into the <code>/large-storage/planet_data</code> directory. The resulting file structure would look like this:</p> File structure under cd .<pre><code>./\n\u251c\u2500\u2500 ../ML_training_labels/retrogressive_thaw_slumps/\n\u251c\u2500\u2500 darts/\n\u251c\u2500\u2500 logs/\n\u2514\u2500\u2500 configs/\n    \u251c\u2500\u2500 planet-sweep-config.toml\n    \u2514\u2500\u2500 planet-tcvis-sweep.yaml\n</code></pre> File structure under /large-storage/<pre><code>/large-storage/\n\u251c\u2500\u2500 planet_data/\n\u2514\u2500\u2500 darts-nextgen/\n    \u251c\u2500\u2500 artifacts/\n    \u2514\u2500\u2500 data/\n        \u251c\u2500\u2500 training/\n        \u2502   \u2514\u2500\u2500 planet_native_tcvis_896_partial/\n        \u251c\u2500\u2500 cache/\n        \u251c\u2500\u2500 datacubes/\n        \u2502   \u251c\u2500\u2500 arcticdem/\n        \u2502   \u2514\u2500\u2500 tcvis/\n        \u2514\u2500\u2500 aux/admin/\n</code></pre> File structure under /fast-storage/<pre><code>/fast-storage/\n\u2514\u2500\u2500 darts-nextgen/\n    \u2514\u2500\u2500 data/\n        \u2514\u2500\u2500 training/\n            \u2514\u2500\u2500 planet_native_tcvis_896_partial/\n</code></pre> configs/planet-sweep-config.toml<pre><code>[darts.wandb]\nwandb-project = \"darts\"\nwandb-entity = \"your-wandb-username\"\n\n[darts.sweep]\nn-trials = 100\nsweep-db = \"postgresql://pguser@localhost:5432/sweeps\"\nn_folds = 3\nn_randoms = 3\nsweep-id = \"sweep-cv-large-planet\"\n\n[darts.training]\nnum-workers = 16\nmax-epochs = 60\nlog-every-n-steps = 100\ncheck-val-every-n-epoch = 5\nplot-every-n-val-epochs = 4 # == 20 epochs\nearly-stopping-patience = 0\n\n# These are the default one, if not specified in the sweep-config\n[darts.hyperparameters]\nbatch-size = 6\naugment = true\n\n[darts.training_preprocess]\nee-project = \"your-ee-project\"\ntpi-outer-radius = 100\ntpi-inner-radius = 0\nbands = [\n    'blue',\n    'green',\n    'red',\n    'nir',\n    'ndvi',\n    'tc_brightness',\n    'tc_greenness',\n    'tc_wetness',\n    'relative_elevation',\n    'slope',\n]\npatch-size = 896\noverlap = 0 # increase to 64 if exclude-nan = True\nexclude-nopositive = false\nexclude-nan = false\ntest-val-split = 0.05\ntest-regions = ['Taymyrsky Dolgano-Nenetsky District']\n\n[darts.paths]\ndata-dir = \"/large-storage/planet_data\"\nlabels-dir = \"../ML_training_labels/retrogressive_thaw_slumps\" # (1)\narcticdem-dir = \"/large-storage/darts-nextgen/data/datacubes/arcticdem\"\ntcvis-dir = \"/large-storage/darts-nextgen/data/datacubes/tcvis\"\nadmin-dir = \"/large-storage/darts-nextgen/data/aux/admin\"\ntrain-data-dir = \"/fast-storage/darts-nextgen/data/training/planet_native_tcvis_896_partial\" # (2)\npreprocess-cache = \"/large-storage/darts-nextgen/data/cache\"\nsweep-config = \"configs/planet-tcvis-sweep.yaml\"\nartifact-dir = \"/large-storage/darts-nextgen/artifacts\"\n</code></pre> <ol> <li>Clone this repository to obtain the labels for the training data.</li> <li>The <code>train-data-dir</code> should point to a fast read-access storage, like a local mounted SSD to speed up the training process.</li> </ol> configs/planet-tcvis-sweep.yaml<pre><code>name: planet-tcvis-large\nmethod: random\nmetric:\n  goal: maximize\n  name: val0/JaccardIndex\nparameters:\n  learning_rate:\n    max: !!float 1e-3\n    min: !!float 1e-5\n    distribution: log_uniform_values\n  gamma: # How fast the learning rate will decrease\n    value: 0.997\n  focal_loss_alpha: # How much the positive class is weighted\n    min: 0.8\n    max: 0.99\n  focal_loss_gamma: # How much focus should be given to \"bad\" predictions\n    min: 0.0\n    max: 2.0\n  model_arch:\n    values:\n      - Unet\n      - MAnet\n      - UPerNet\n      - Segformer\n  model_encoder:\n    values:\n      - resnet50\n      - resnext50_32x4d\n      - mit_b2\n      - tu-convnextv2_tiny\n      - tu-maxvit_tiny_rw_224\n</code></pre>"},{"location":"reference/","title":"DARTS API Reference","text":"<ul> <li>DARTS</li> <li>Pipelines</li> <li>Legacy Training</li> <li>DARTS Segmentation</li> <li>Training</li> <li>Metrics</li> <li>DARTS Acquisition</li> <li>DARTS Ensemble</li> <li>DARTS Export</li> <li>DARTS Postprocessing</li> <li>DARTS Preprocessing</li> <li>DARTS Utils</li> <li>Cuda</li> <li>Rich</li> </ul>"},{"location":"reference/darts/","title":"darts","text":"<p>DARTS processing pipeline.</p> <p>Attributes:</p> <ul> <li> <code>__version__</code>           \u2013            </li> </ul>"},{"location":"reference/darts/#darts.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts/legacy_training/","title":"darts.legacy_training","text":"<p>Legacy training module for DARTS.</p> <p>Functions:</p> <ul> <li> <code>convert_lightning_checkpoint</code>             \u2013              <p>Convert a lightning checkpoint to our own format.</p> </li> <li> <code>optuna_sweep_smp</code>             \u2013              <p>Create an optuna sweep and run it on the specified cuda device, or continue an existing sweep.</p> </li> <li> <code>preprocess_planet_train_data</code>             \u2013              <p>Preprocess Planet data for training.</p> </li> <li> <code>preprocess_s2_train_data</code>             \u2013              <p>Preprocess Sentinel 2 data for training.</p> </li> <li> <code>test_smp</code>             \u2013              <p>Run the testing of the SMP model.</p> </li> <li> <code>train_smp</code>             \u2013              <p>Run the training of the SMP model.</p> </li> <li> <code>wandb_sweep_smp</code>             \u2013              <p>Create a sweep with wandb and run it on the specified cuda device, or continue an existing sweep.</p> </li> </ul> <ul> <li>convert_lightning_checkpoint</li> <li>optuna_sweep_smp</li> <li>preprocess_planet_train_data</li> <li>preprocess_s2_train_data</li> <li>test_smp</li> <li>train_smp</li> <li>wandb_sweep_smp</li> </ul>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.convert_lightning_checkpoint","title":"convert_lightning_checkpoint","text":"<pre><code>convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: pathlib.Path,\n    out_directory: pathlib.Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n)\n</code></pre> <p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/util.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.optuna_sweep_smp","title":"optuna_sweep_smp","text":"<pre><code>optuna_sweep_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    sweep_config: pathlib.Path,\n    n_trials: int = 10,\n    sweep_db: str | None = None,\n    sweep_id: str | None = None,\n    n_folds: int = 5,\n    n_randoms: int = 3,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    augment: bool = True,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n)\n</code></pre> <p>Create an optuna sweep and run it on the specified cuda device, or continue an existing sweep.</p> <p>If <code>sweep_id</code> already exists in <code>sweep_db</code>, the sweep will be continued. Otherwise, a new sweep will be created.</p> <p>If a <code>cuda_device</code> is specified, run an agent on this device. If None, do nothing.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>This will use cross-validation.</p> Example <p>In one terminal, start a sweep: <pre><code>    $ rye run darts sweep-smp --config-file /path/to/sweep-config.toml\n    ...  # Many logs\n    Created sweep with ID 123456789\n    ... # More logs from spawned agent\n</code></pre></p> <p>In another terminal, start an a second agent: <pre><code>    $ rye run darts sweep-smp --sweep-id 123456789\n    ...\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory.</p> </li> <li> <code>sweep_config</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration. Hyperparameters must contain the following fields: <code>model_arch</code>, <code>model_encoder</code>, <code>augment</code>, <code>gamma</code>, <code>batch_size</code>. Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.</p> </li> <li> <code>n_trials</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of runs to execute. Defaults to 10.</p> </li> <li> <code>sweep_db</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the optuna database. If None, a new database will be created.</p> </li> <li> <code>sweep_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The ID of the sweep. If None, a new sweep will be created. Defaults to None.</p> </li> <li> <code>n_folds</code>               (<code>(int, optinoal)</code>, default:                   <code>5</code> )           \u2013            <p>Number of folds in cross-validation. Max 5. Defaults to 5.</p> </li> <li> <code>n_randoms</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of repetitions with different random-seeds. First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline. Rest are pseudo-random generated beforehand, hence always equal. Defaults to 5.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. Defaults to None.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/sweep.py</code> <pre><code>def optuna_sweep_smp(\n    *,\n    # Data and sweep config\n    train_data_dir: Path,\n    sweep_config: Path,\n    n_trials: int = 10,\n    sweep_db: str | None = None,\n    sweep_id: str | None = None,\n    n_folds: int = 5,\n    n_randoms: int = 3,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    # Hyperparameters (default values if not provided by sweep-config)\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n):\n    \"\"\"Create an optuna sweep and run it on the specified cuda device, or continue an existing sweep.\n\n    If `sweep_id` already exists in `sweep_db`, the sweep will be continued. Otherwise, a new sweep will be created.\n\n    If a `cuda_device` is specified, run an agent on this device. If None, do nothing.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    This will use cross-validation.\n\n    Example:\n        In one terminal, start a sweep:\n        ```sh\n            $ rye run darts sweep-smp --config-file /path/to/sweep-config.toml\n            ...  # Many logs\n            Created sweep with ID 123456789\n            ... # More logs from spawned agent\n        ```\n\n        In another terminal, start an a second agent:\n        ```sh\n            $ rye run darts sweep-smp --sweep-id 123456789\n            ...\n        ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory.\n        sweep_config (Path): Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration.\n            Hyperparameters must contain the following fields: `model_arch`, `model_encoder`, `augment`, `gamma`,\n            `batch_size`.\n            Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.\n        n_trials (int, optional): Number of runs to execute. Defaults to 10.\n        sweep_db (str | None, optional): Path to the optuna database. If None, a new database will be created.\n        sweep_id (str | None, optional): The ID of the sweep. If None, a new sweep will be created. Defaults to None.\n        n_folds (int, optinoal): Number of folds in cross-validation. Max 5. Defaults to 5.\n        n_randoms (int, optional): Number of repetitions with different random-seeds.\n            First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline.\n            Rest are pseudo-random generated beforehand, hence always equal.\n            Defaults to 5.\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str | None, optional): The device to run the model on. Defaults to None.\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n\n    \"\"\"\n    import optuna\n    from names_generator import generate_name\n\n    from darts.legacy_training.util import suggest_optuna_params_from_wandb_config\n\n    with sweep_config.open(\"r\") as f:\n        sweep_configuration = yaml.safe_load(f)\n\n    logger.debug(f\"Loaded sweep configuration from {sweep_config.resolve()}:\\n{sweep_configuration}\")\n\n    # Create a new study-id if none is given\n    if sweep_id is None:\n        sweep_id = f\"sweep-{generate_name('hyphen')}\"\n        logger.info(f\"Generated new sweep ID: {sweep_id}\")\n        logger.info(\"To start a sweep agents, use the following command:\")\n        logger.info(f\"$ rye run darts optuna-sweep-smp --sweep-id {sweep_id}\")\n\n    artifact_dir = artifact_dir / sweep_id\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    def objective(trial):\n        hparams = suggest_optuna_params_from_wandb_config(trial, sweep_configuration)\n        logger.info(f\"Running trial with parameters: {hparams}\")\n\n        # Get the trial a more readable name\n        trial_name = f\"{generate_name(style='hyphen')}-{trial.number}\"\n\n        # We set the default weights to None, to be able to use different architectures\n        model_encoder_weights = None\n        # We set early stopping to None, because wandb will handle the early stopping\n        early_stopping_patience = None\n\n        # Overwrite the default values with the suggested ones, if they are present\n        learning_rate_trial = hparams.get(\"learning_rate\", learning_rate)\n        gamma_trial = hparams.get(\"gamma\", gamma)\n        focal_loss_alpha_trial = hparams.get(\"focal_loss_alpha\", focal_loss_alpha)\n        focal_loss_gamma_trial = hparams.get(\"focal_loss_gamma\", focal_loss_gamma)\n        batch_size_trial = hparams.get(\"batch_size\", batch_size)\n        model_arch_trial = hparams.get(\"model_arch\", model_arch)\n        model_encoder_trial = hparams.get(\"model_encoder\", model_encoder)\n        augment_trial = hparams.get(\"augment\", augment)\n\n        crossval_scores = defaultdict(list)\n\n        folds = list(range(n_folds))\n        rng = random.Random(42)\n        seeds = [42, 21, 69]\n        if n_randoms &gt; 3:\n            seeds += rng.sample(range(9999), n_randoms - 3)\n        elif n_randoms &lt; 3:\n            seeds = seeds[:n_randoms]\n\n        for random_seed in seeds:\n            for fold in folds:\n                logger.info(f\"Running cross-validation fold {fold}\")\n                _gather_and_reset_wandb_env()\n                trainer = train_smp(\n                    # Data config\n                    train_data_dir=train_data_dir,\n                    artifact_dir=artifact_dir,\n                    fold=fold,\n                    random_seed=random_seed,\n                    # Hyperparameters\n                    model_arch=model_arch_trial,\n                    model_encoder=model_encoder_trial,\n                    model_encoder_weights=model_encoder_weights,\n                    augment=augment_trial,\n                    learning_rate=learning_rate_trial,\n                    gamma=gamma_trial,\n                    focal_loss_alpha=focal_loss_alpha_trial,\n                    focal_loss_gamma=focal_loss_gamma_trial,\n                    batch_size=batch_size_trial,\n                    # Epoch and Logging config\n                    early_stopping_patience=early_stopping_patience,\n                    max_epochs=max_epochs,\n                    log_every_n_steps=log_every_n_steps,\n                    check_val_every_n_epoch=check_val_every_n_epoch,\n                    plot_every_n_val_epochs=plot_every_n_val_epochs,\n                    # Device and Manager config\n                    num_workers=num_workers,\n                    device=device,\n                    wandb_entity=wandb_entity,\n                    wandb_project=wandb_project,\n                    wandb_group=sweep_id,\n                    trial_name=trial_name,\n                    run_name=f\"{trial_name}-f{fold}r{random_seed}\",\n                )\n                for metric, value in trainer.callback_metrics.items():\n                    crossval_scores[metric].append(value.item())\n\n        logger.debug(f\"Cross-validation scores: {crossval_scores}\")\n        crossval_jaccard = mean(crossval_scores[\"val/JaccardIndex\"])\n        crossval_recall = mean(crossval_scores[\"val/Recall\"])\n\n        return crossval_jaccard, crossval_recall\n\n    study = optuna.create_study(\n        storage=sweep_db,\n        study_name=sweep_id,\n        directions=[\"maximize\", \"maximize\"],\n        load_if_exists=True,\n    )\n\n    if device is None:\n        logger.info(\"No device specified, closing script...\")\n        return\n\n    logger.info(\"Starting optimizing\")\n    study.optimize(objective, n_trials=n_trials)\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.preprocess_planet_train_data","title":"preprocess_planet_train_data","text":"<pre><code>preprocess_planet_train_data(\n    *,\n    bands: list[str],\n    data_dir: pathlib.Path,\n    labels_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    dask_worker: int = min(\n        16, multiprocessing.cpu_count() - 1\n    ),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n)\n</code></pre> <p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/planet.py</code> <pre><code>def preprocess_planet_train_data(\n    *,\n    bands: list[str],\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n        labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n        footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n        footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        planet_paths = sorted(_legacy_path_gen(data_dir))\n        logger.info(f\"Found {len(planet_paths)} PLANET scenes and orthotiles in {data_dir}\")\n        path_gen = split_dataset_paths(\n            planet_paths, footprints, train_data_dir, test_val_split, test_regions, admin_dir\n        )\n\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(planet_paths), console=console\n        ):\n            try:\n                planet_id = fpath.stem\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(planet_paths)}\"\n                    f\" '{fpath.resolve()}' ({planet_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{planet_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{planet_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_planet_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 2\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_planet_masks(fpath)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile=tile,\n                    labels=labels[labels.image_id == planet_id],\n                    bands=bands,\n                    norm_factors=norm_factors,\n                    patch_size=patch_size,\n                    overlap=overlap,\n                    exclude_nopositive=exclude_nopositive,\n                    exclude_nan=exclude_nan,\n                    device=device,\n                    mask_erosion_size=mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(planet_paths)} '{fpath.resolve()}'\"\n                    f\"({planet_id=}) with {patch_id} patches.\"\n                )\n\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.preprocess_s2_train_data","title":"preprocess_s2_train_data","text":"<pre><code>preprocess_s2_train_data(\n    *,\n    bands: list[str],\n    sentinel2_dir: pathlib.Path,\n    train_data_dir: pathlib.Path,\n    arcticdem_dir: pathlib.Path,\n    tcvis_dir: pathlib.Path,\n    admin_dir: pathlib.Path,\n    preprocess_cache: pathlib.Path | None = None,\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    dask_worker: int = min(\n        16, multiprocessing.cpu_count() - 1\n    ),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n)\n</code></pre> <p>Preprocess Sentinel 2 data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Sentinel 2 scenes.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/s2.py</code> <pre><code>def preprocess_s2_train_data(\n    *,\n    bands: list[str],\n    sentinel2_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Sentinel 2 data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        sentinel2_dir (Path): The directory containing the Sentinel 2 scenes.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_s2_masks, load_s2_scene, load_tcvis\n    from darts_acquisition.s2 import parse_s2_tile_id\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        s2_paths = sorted(sentinel2_dir.glob(\"*/\"))\n        logger.info(f\"Found {len(s2_paths)} Sentinel 2 scenes in {sentinel2_dir}\")\n        path_gen = split_dataset_paths(s2_paths, train_data_dir, test_val_split, test_regions, admin_dir)\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(s2_paths), console=console\n        ):\n            try:\n                _, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}' ({tile_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{tile_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{tile_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_s2_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 10\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_s2_masks(fpath, optical.odc.geobox)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                labels = gpd.read_file(fpath / f\"{s2_tile_id}.shp\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile,\n                    labels,\n                    bands,\n                    norm_factors,\n                    patch_size,\n                    overlap,\n                    exclude_nopositive,\n                    exclude_nan,\n                    device,\n                    mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}'\"\n                    f\"({tile_id=}) with {patch_id} patches.\"\n                )\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"sentinel2_dir\": sentinel2_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.test_smp","title":"test_smp","text":"<pre><code>test_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: pathlib.Path | None = None,\n    batch_size: int = 8,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size. Defaults to 8.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Directory to save artifacts. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device to use. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/test.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int, optional): Batch size. Defaults to 8.\n        artifact_dir (Path, optional): Directory to save artifacts. Defaults to Path(\"lightning_logs\").\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device (int | str, optional): Device to use. Defaults to \"auto\".\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n    logger.info(f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device=}\")\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    # Data and model\n    datamodule_val_test = DartsDataModule(\n        data_dir=train_data_dir / \"val-test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    datamodule_test = DartsDataModule(\n        data_dir=train_data_dir / \"test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / run_name / run_id / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    model = SMPSegmenter.load_from_checkpoint(model_ckp)\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=run_name, version=run_id),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            id=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    metrics_cb = BinarySegmentationMetrics(\n        input_combination=preprocess_config[\"bands\"],\n    )\n    callbacks = [\n        RichProgressBar(),\n        metrics_cb,\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=True,\n    )\n    # Overwrite the names of the test sets to test agains two separate sets\n    metrics_cb.test_set = \"val-test\"\n    model.test_set = \"val-test\"\n    trainer.test(model, datamodule_val_test, ckpt_path=model_ckp)\n    metrics_cb.test_set = \"test\"\n    model.test_set = \"test\"\n    trainer.test(model, datamodule_test)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.train_smp","title":"train_smp","text":"<pre><code>train_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    fold: int = 0,\n    continue_from_checkpoint: pathlib.Path | None = None,\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 0.001,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; pytorch_lightning.Trainer\n</code></pre> <p>Run the training of the SMP model.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.</p> <p>Each training run is assigned a unique name and id pair and optionally a trial name. The name, which the user can provide, should be used as a grouping mechanism of equal hyperparameter and code. Hence, different versions of the same name should only differ by random state or run settings parameter, like logs. Each version is assigned a unique id. Artifacts (metrics &amp; checkpoints) are then stored under <code>{artifact_dir}/{run_name}/{run_id}</code> in no-crossval runs. If <code>trial_name</code> is specified, the artifacts are stored under <code>{artifact_dir}/{trial_name}/{run_name}-{run_id}</code>. Wandb logs are always stored under <code>{wandb_entity}/{wandb_project}/{run_name}</code>, regardless of <code>trial_name</code>. However, they are further grouped by the <code>trial_name</code> (via job_type), if specified. Both <code>run_name</code> and <code>run_id</code> are also stored in the hparams of each checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>fold</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The current fold to train on. Must be in [0, 4]. Defaults to 0.</p> </li> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to a checkpoint to continue training from. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the encoder weights. Defaults to None.</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>random_seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The device to run the model on. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>wandb_group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Wandb group. Usefull for CV-Sweeps. Defaults to None.</p> </li> <li> <code>run_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of this run, as a further grouping method for logs etc. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>run_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ID of the run. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>trial_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation run / trial. This effects primary logging and artifact storage. If None, will do nothing. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def train_smp(\n    *,\n    # Data config\n    train_data_dir: Path,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    fold: int = 0,\n    continue_from_checkpoint: Path | None = None,\n    # Hyperparameters\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the training of the SMP model.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.\n\n    Each training run is assigned a unique **name** and **id** pair and optionally a trial name.\n    The name, which the user _can_ provide, should be used as a grouping mechanism of equal hyperparameter and code.\n    Hence, different versions of the same name should only differ by random state or run settings parameter, like logs.\n    Each version is assigned a unique id.\n    Artifacts (metrics &amp; checkpoints) are then stored under `{artifact_dir}/{run_name}/{run_id}` in no-crossval runs.\n    If `trial_name` is specified, the artifacts are stored under `{artifact_dir}/{trial_name}/{run_name}-{run_id}`.\n    Wandb logs are always stored under `{wandb_entity}/{wandb_project}/{run_name}`, regardless of `trial_name`.\n    However, they are further grouped by the `trial_name` (via job_type), if specified.\n    Both `run_name` and `run_id` are also stored in the hparams of each checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        fold (int, optional): The current fold to train on. Must be in [0, 4]. Defaults to 0.\n        continue_from_checkpoint (Path | None, optional): Path to a checkpoint to continue training from.\n            Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        model_encoder_weights (str | None, optional): Path to the encoder weights. Defaults to None.\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        early_stopping_patience (int, optional): Number of epochs to wait for improvement before stopping.\n            Defaults to 5.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        random_seed (int, optional): Random seed for deterministic training. Defaults to 42.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str, optional): The device to run the model on. Defaults to \"auto\".\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        wandb_group (str | None, optional): Wandb group. Usefull for CV-Sweeps. Defaults to None.\n        run_name (str | None, optional): Name of this run, as a further grouping method for logs etc.\n            If None, will generate a random one. Defaults to None.\n        run_id (str | None, optional): ID of the run. If None, will generate a random one. Defaults to None.\n        trial_name (str | None, optional): Name of the cross-validation run / trial.\n            This effects primary logging and artifact storage.\n            If None, will do nothing. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.legacy_training.util import generate_id, get_generated_name\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run_name or get_generated_name(artifact_dir)\n    run_id = run_id or generate_id()\n\n    logger.info(f\"Starting training '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(\n        f\"Using config:\\n\\t{model_arch=}\\n\\t{model_encoder=}\\n\\t{model_encoder_weights=}\\n\\t{augment=}\\n\\t\"\n        f\"{learning_rate=}\\n\\t{gamma=}\\n\\t{batch_size=}\\n\\t{max_epochs=}\\n\\t{log_every_n_steps=}\\n\\t\"\n        f\"{check_val_every_n_epoch=}\\n\\t{early_stopping_patience=}\\n\\t{plot_every_n_val_epochs=}\\n\\t{num_workers=}\"\n        f\"\\n\\t{device=}\\n\\t{random_seed=}\"\n    )\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(random_seed, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    config = SMPSegmenterConfig(\n        input_combination=preprocess_config[\"bands\"],\n        model={\n            \"arch\": model_arch,\n            \"encoder_name\": model_encoder,\n            \"encoder_weights\": model_encoder_weights,\n            \"in_channels\": len(preprocess_config[\"bands\"]),\n            \"classes\": 1,\n        },\n        norm_factors=preprocess_config[\"norm_factors\"],\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir / \"cross-val.zarr\",\n        batch_size=batch_size,\n        fold=fold,\n        augment=augment,\n        num_workers=num_workers,\n    )\n    model = SMPSegmenter(\n        config=config,\n        learning_rate=learning_rate,\n        gamma=gamma,\n        focal_loss_alpha=focal_loss_alpha,\n        focal_loss_gamma=focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        trial_name=trial_name,\n        random_seed=random_seed,\n    )\n\n    # Loggers\n    is_crossval = bool(trial_name)\n    trainer_loggers = [\n        CSVLogger(\n            save_dir=artifact_dir,\n            name=run_name if not is_crossval else trial_name,\n            version=run_id if not is_crossval else f\"{run_name}-{run_id}\",\n        ),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            group=wandb_group,\n            job_type=trial_name,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            input_combination=config[\"input_combination\"],\n            val_set=f\"val{fold}\",\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            is_crossval=is_crossval,\n        ),\n    ]\n    if early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {early_stopping_patience}\")\n        early_stopping = EarlyStopping(monitor=\"val/JaccardIndex\", mode=\"max\", patience=early_stopping_patience)\n        callbacks.append(early_stopping)\n\n    # Train\n    trainer = L.Trainer(\n        max_epochs=max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=check_val_every_n_epoch,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=False,\n    )\n    trainer.fit(model, datamodule, ckpt_path=continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/legacy_training/#darts.legacy_training.wandb_sweep_smp","title":"wandb_sweep_smp","text":"<pre><code>wandb_sweep_smp(\n    *,\n    train_data_dir: pathlib.Path,\n    sweep_config: pathlib.Path,\n    n_trials: int = 10,\n    sweep_id: str | None = None,\n    artifact_dir: pathlib.Path = pathlib.Path(\n        \"lightning_logs\"\n    ),\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n)\n</code></pre> <p>Create a sweep with wandb and run it on the specified cuda device, or continue an existing sweep.</p> <p>If <code>sweep_id</code> is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued. All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}. Since each sweep-configuration has (currently) an own name and id, a single run can be found under: {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.</p> <p>If a <code>cuda_device</code> is specified, run an agent on this device. If None, do nothing.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>This will NOT use cross-validation. For cross-validation, use <code>optuna_sweep_smp</code>.</p> Example <p>In one terminal, start a sweep: <pre><code>    $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n    ...  # Many logs\n    Created sweep with ID 123456789\n    ... # More logs from spawned agent\n</code></pre></p> <p>In another terminal, start an a second agent: <pre><code>    $ rye run darts wandb-sweep-smp --sweep-id 123456789\n    ...\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory.</p> </li> <li> <code>sweep_config</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration. Hyperparameters must contain the following fields: <code>model_arch</code>, <code>model_encoder</code>, <code>augment</code>, <code>gamma</code>, <code>batch_size</code>. Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.</p> </li> <li> <code>n_trials</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of runs to execute. Defaults to 10.</p> </li> <li> <code>sweep_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The ID of the sweep. If None, a new sweep will be created. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. Defaults to None.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def wandb_sweep_smp(\n    *,\n    # Data and sweep config\n    train_data_dir: Path,\n    sweep_config: Path,\n    n_trials: int = 10,\n    sweep_id: str | None = None,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n):\n    \"\"\"Create a sweep with wandb and run it on the specified cuda device, or continue an existing sweep.\n\n    If `sweep_id` is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued.\n    All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}.\n    Since each sweep-configuration has (currently) an own name and id, a single run can be found under:\n    {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.\n\n    If a `cuda_device` is specified, run an agent on this device. If None, do nothing.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    This will NOT use cross-validation. For cross-validation, use `optuna_sweep_smp`.\n\n    Example:\n        In one terminal, start a sweep:\n        ```sh\n            $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n            ...  # Many logs\n            Created sweep with ID 123456789\n            ... # More logs from spawned agent\n        ```\n\n        In another terminal, start an a second agent:\n        ```sh\n            $ rye run darts wandb-sweep-smp --sweep-id 123456789\n            ...\n        ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory.\n        sweep_config (Path): Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration.\n            Hyperparameters must contain the following fields: `model_arch`, `model_encoder`, `augment`, `gamma`,\n            `batch_size`.\n            Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.\n        n_trials (int, optional): Number of runs to execute. Defaults to 10.\n        sweep_id (str | None, optional): The ID of the sweep. If None, a new sweep will be created. Defaults to None.\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str | None, optional): The device to run the model on. Defaults to None.\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n\n    \"\"\"\n    import wandb\n\n    # Wandb has a stupid way of logging (they log per default with click.echo to stdout)\n    # We need to silence this and redirect all possible logs to our logger\n    # wl = wandb.setup({\"silent\": True})\n    # wandb.termsetup(wl.settings, logging.getLogger(\"wandb\"))\n    # LoggingManager.apply_logging_handlers(\"wandb\")\n\n    if sweep_id is not None and device is None:\n        logger.warning(\"Continuing a sweep without specifying a device will not do anything.\")\n\n    with sweep_config.open(\"r\") as f:\n        sweep_configuration = yaml.safe_load(f)\n\n    logger.debug(f\"Loaded sweep configuration from {sweep_config.resolve()}:\\n{sweep_configuration}\")\n\n    if sweep_id is None:\n        sweep_id = wandb.sweep(sweep=sweep_configuration, project=wandb_project, entity=wandb_entity)\n        logger.info(f\"Created sweep with ID {sweep_id}\")\n        logger.info(\"To start a sweep agents, use the following command:\")\n        logger.info(f\"$ rye run darts sweep_smp --sweep-id {sweep_id}\")\n\n    artifact_dir = artifact_dir / f\"sweep-{sweep_id}\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    def run():\n        run = wandb.init(config=sweep_configuration)\n        # We need to manually log the run data since the wandb logger only logs to its own logs and click\n        logger.info(f\"Starting sweep run '{run.settings.run_name}'\")\n        logger.debug(f\"Run data is saved locally in {Path(run.settings.sync_dir).resolve()}\")\n        logger.debug(f\"View project at {run.settings.project_url}\")\n        logger.debug(f\"View sweep at {run.settings.sweep_url}\")\n        logger.debug(f\"View run at {run.settings.run_url}\")\n\n        # We set the default weights to None, to be able to use different architectures\n        model_encoder_weights = None\n        # We set early stopping to None, because wandb will handle the early stopping\n        early_stopping_patience = None\n        learning_rate = wandb.config[\"learning_rate\"]\n        gamma = wandb.config[\"gamma\"]\n        batch_size = wandb.config[\"batch_size\"]\n        model_arch = wandb.config[\"model_arch\"]\n        model_encoder = wandb.config[\"model_encoder\"]\n        augment = wandb.config[\"augment\"]\n        focal_loss_alpha = wandb.config[\"focal_loss_alpha\"]\n        focal_loss_gamma = wandb.config[\"focal_loss_gamma\"]\n        fold = wandb.config.get(\"fold\", 0)\n        random_seed = wandb.config.get(\"random_seed\", 42)\n\n        train_smp(\n            # Data config\n            train_data_dir=train_data_dir,\n            artifact_dir=artifact_dir,\n            fold=fold,\n            # Hyperparameters\n            model_arch=model_arch,\n            model_encoder=model_encoder,\n            model_encoder_weights=model_encoder_weights,\n            augment=augment,\n            learning_rate=learning_rate,\n            gamma=gamma,\n            focal_loss_alpha=focal_loss_alpha,\n            focal_loss_gamma=focal_loss_gamma,\n            batch_size=batch_size,\n            # Epoch and Logging config\n            early_stopping_patience=early_stopping_patience,\n            max_epochs=max_epochs,\n            log_every_n_steps=log_every_n_steps,\n            check_val_every_n_epoch=check_val_every_n_epoch,\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            # Device and Manager config\n            random_seed=random_seed,\n            num_workers=num_workers,\n            device=device,\n            wandb_entity=wandb_entity,\n            wandb_project=wandb_project,\n            run_name=wandb.run.name,\n            run_id=wandb.run.id,\n        )\n\n    if device is None:\n        logger.info(\"No device specified, closing script...\")\n        return\n\n    logger.info(\"Starting a default sweep agent\")\n    wandb.agent(sweep_id, function=run, count=n_trials, project=wandb_project, entity=wandb_entity)\n</code></pre>"},{"location":"reference/darts/legacy_training/convert_lightning_checkpoint/","title":"darts.legacy_training.convert_lightning_checkpoint","text":"<p>Convert a lightning checkpoint to our own format.</p> <p>The final checkpoint will contain the model configuration and the state dict. It will be saved to:</p> <pre><code>    out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n</code></pre> <p>Parameters:</p> <ul> <li> <code>lightning_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the lightning checkpoint.</p> </li> <li> <code>out_directory</code>               (<code>pathlib.Path</code>)           \u2013            <p>Output directory for the converted checkpoint.</p> </li> <li> <code>checkpoint_name</code>               (<code>str</code>)           \u2013            <p>A unique name of the new checkpoint.</p> </li> <li> <code>framework</code>               (<code>str</code>, default:                   <code>'smp'</code> )           \u2013            <p>The framework used for the model. Defaults to \"smp\".</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/util.py</code> <pre><code>def convert_lightning_checkpoint(\n    *,\n    lightning_checkpoint: Path,\n    out_directory: Path,\n    checkpoint_name: str,\n    framework: str = \"smp\",\n):\n    \"\"\"Convert a lightning checkpoint to our own format.\n\n    The final checkpoint will contain the model configuration and the state dict.\n    It will be saved to:\n\n    ```python\n        out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n    ```\n\n    Args:\n        lightning_checkpoint (Path): Path to the lightning checkpoint.\n        out_directory (Path): Output directory for the converted checkpoint.\n        checkpoint_name (str): A unique name of the new checkpoint.\n        framework (str, optional): The framework used for the model. Defaults to \"smp\".\n\n    \"\"\"\n    import torch\n\n    logger.debug(f\"Loading checkpoint from {lightning_checkpoint.resolve()}\")\n    lckpt = torch.load(lightning_checkpoint, weights_only=False, map_location=torch.device(\"cpu\"))\n\n    now = datetime.now()\n    formatted_date = now.strftime(\"%Y-%m-%d\")\n    config = lckpt[\"hyper_parameters\"][\"config\"]\n    del config[\"model\"][\"encoder_weights\"]\n    config[\"time\"] = formatted_date\n    config[\"name\"] = checkpoint_name\n    config[\"model_framework\"] = framework\n\n    statedict = lckpt[\"state_dict\"]\n    # Statedict has model. prefix before every weight. We need to remove them. This is an in-place function\n    torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(statedict, \"model.\")\n\n    own_ckpt = {\n        \"config\": config,\n        \"statedict\": lckpt[\"state_dict\"],\n    }\n\n    out_directory.mkdir(exist_ok=True, parents=True)\n\n    out_checkpoint = out_directory / f\"{checkpoint_name}_{formatted_date}.ckpt\"\n\n    torch.save(own_ckpt, out_checkpoint)\n\n    logger.info(f\"Saved converted checkpoint to {out_checkpoint.resolve()}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/optuna_sweep_smp/","title":"darts.legacy_training.optuna_sweep_smp","text":"<p>Create an optuna sweep and run it on the specified cuda device, or continue an existing sweep.</p> <p>If <code>sweep_id</code> already exists in <code>sweep_db</code>, the sweep will be continued. Otherwise, a new sweep will be created.</p> <p>If a <code>cuda_device</code> is specified, run an agent on this device. If None, do nothing.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>This will use cross-validation.</p> Example <p>In one terminal, start a sweep: <pre><code>    $ rye run darts sweep-smp --config-file /path/to/sweep-config.toml\n    ...  # Many logs\n    Created sweep with ID 123456789\n    ... # More logs from spawned agent\n</code></pre></p> <p>In another terminal, start an a second agent: <pre><code>    $ rye run darts sweep-smp --sweep-id 123456789\n    ...\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory.</p> </li> <li> <code>sweep_config</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration. Hyperparameters must contain the following fields: <code>model_arch</code>, <code>model_encoder</code>, <code>augment</code>, <code>gamma</code>, <code>batch_size</code>. Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.</p> </li> <li> <code>n_trials</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of runs to execute. Defaults to 10.</p> </li> <li> <code>sweep_db</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the optuna database. If None, a new database will be created.</p> </li> <li> <code>sweep_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The ID of the sweep. If None, a new sweep will be created. Defaults to None.</p> </li> <li> <code>n_folds</code>               (<code>(int, optinoal)</code>, default:                   <code>5</code> )           \u2013            <p>Number of folds in cross-validation. Max 5. Defaults to 5.</p> </li> <li> <code>n_randoms</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Number of repetitions with different random-seeds. First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline. Rest are pseudo-random generated beforehand, hence always equal. Defaults to 5.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. Defaults to None.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/sweep.py</code> <pre><code>def optuna_sweep_smp(\n    *,\n    # Data and sweep config\n    train_data_dir: Path,\n    sweep_config: Path,\n    n_trials: int = 10,\n    sweep_db: str | None = None,\n    sweep_id: str | None = None,\n    n_folds: int = 5,\n    n_randoms: int = 3,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    # Hyperparameters (default values if not provided by sweep-config)\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n):\n    \"\"\"Create an optuna sweep and run it on the specified cuda device, or continue an existing sweep.\n\n    If `sweep_id` already exists in `sweep_db`, the sweep will be continued. Otherwise, a new sweep will be created.\n\n    If a `cuda_device` is specified, run an agent on this device. If None, do nothing.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    This will use cross-validation.\n\n    Example:\n        In one terminal, start a sweep:\n        ```sh\n            $ rye run darts sweep-smp --config-file /path/to/sweep-config.toml\n            ...  # Many logs\n            Created sweep with ID 123456789\n            ... # More logs from spawned agent\n        ```\n\n        In another terminal, start an a second agent:\n        ```sh\n            $ rye run darts sweep-smp --sweep-id 123456789\n            ...\n        ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory.\n        sweep_config (Path): Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration.\n            Hyperparameters must contain the following fields: `model_arch`, `model_encoder`, `augment`, `gamma`,\n            `batch_size`.\n            Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.\n        n_trials (int, optional): Number of runs to execute. Defaults to 10.\n        sweep_db (str | None, optional): Path to the optuna database. If None, a new database will be created.\n        sweep_id (str | None, optional): The ID of the sweep. If None, a new sweep will be created. Defaults to None.\n        n_folds (int, optinoal): Number of folds in cross-validation. Max 5. Defaults to 5.\n        n_randoms (int, optional): Number of repetitions with different random-seeds.\n            First 3 are always \"42\", \"21\" and \"69\" for better default comparibility with rest of this pipeline.\n            Rest are pseudo-random generated beforehand, hence always equal.\n            Defaults to 5.\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str | None, optional): The device to run the model on. Defaults to None.\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n\n    \"\"\"\n    import optuna\n    from names_generator import generate_name\n\n    from darts.legacy_training.util import suggest_optuna_params_from_wandb_config\n\n    with sweep_config.open(\"r\") as f:\n        sweep_configuration = yaml.safe_load(f)\n\n    logger.debug(f\"Loaded sweep configuration from {sweep_config.resolve()}:\\n{sweep_configuration}\")\n\n    # Create a new study-id if none is given\n    if sweep_id is None:\n        sweep_id = f\"sweep-{generate_name('hyphen')}\"\n        logger.info(f\"Generated new sweep ID: {sweep_id}\")\n        logger.info(\"To start a sweep agents, use the following command:\")\n        logger.info(f\"$ rye run darts optuna-sweep-smp --sweep-id {sweep_id}\")\n\n    artifact_dir = artifact_dir / sweep_id\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    def objective(trial):\n        hparams = suggest_optuna_params_from_wandb_config(trial, sweep_configuration)\n        logger.info(f\"Running trial with parameters: {hparams}\")\n\n        # Get the trial a more readable name\n        trial_name = f\"{generate_name(style='hyphen')}-{trial.number}\"\n\n        # We set the default weights to None, to be able to use different architectures\n        model_encoder_weights = None\n        # We set early stopping to None, because wandb will handle the early stopping\n        early_stopping_patience = None\n\n        # Overwrite the default values with the suggested ones, if they are present\n        learning_rate_trial = hparams.get(\"learning_rate\", learning_rate)\n        gamma_trial = hparams.get(\"gamma\", gamma)\n        focal_loss_alpha_trial = hparams.get(\"focal_loss_alpha\", focal_loss_alpha)\n        focal_loss_gamma_trial = hparams.get(\"focal_loss_gamma\", focal_loss_gamma)\n        batch_size_trial = hparams.get(\"batch_size\", batch_size)\n        model_arch_trial = hparams.get(\"model_arch\", model_arch)\n        model_encoder_trial = hparams.get(\"model_encoder\", model_encoder)\n        augment_trial = hparams.get(\"augment\", augment)\n\n        crossval_scores = defaultdict(list)\n\n        folds = list(range(n_folds))\n        rng = random.Random(42)\n        seeds = [42, 21, 69]\n        if n_randoms &gt; 3:\n            seeds += rng.sample(range(9999), n_randoms - 3)\n        elif n_randoms &lt; 3:\n            seeds = seeds[:n_randoms]\n\n        for random_seed in seeds:\n            for fold in folds:\n                logger.info(f\"Running cross-validation fold {fold}\")\n                _gather_and_reset_wandb_env()\n                trainer = train_smp(\n                    # Data config\n                    train_data_dir=train_data_dir,\n                    artifact_dir=artifact_dir,\n                    fold=fold,\n                    random_seed=random_seed,\n                    # Hyperparameters\n                    model_arch=model_arch_trial,\n                    model_encoder=model_encoder_trial,\n                    model_encoder_weights=model_encoder_weights,\n                    augment=augment_trial,\n                    learning_rate=learning_rate_trial,\n                    gamma=gamma_trial,\n                    focal_loss_alpha=focal_loss_alpha_trial,\n                    focal_loss_gamma=focal_loss_gamma_trial,\n                    batch_size=batch_size_trial,\n                    # Epoch and Logging config\n                    early_stopping_patience=early_stopping_patience,\n                    max_epochs=max_epochs,\n                    log_every_n_steps=log_every_n_steps,\n                    check_val_every_n_epoch=check_val_every_n_epoch,\n                    plot_every_n_val_epochs=plot_every_n_val_epochs,\n                    # Device and Manager config\n                    num_workers=num_workers,\n                    device=device,\n                    wandb_entity=wandb_entity,\n                    wandb_project=wandb_project,\n                    wandb_group=sweep_id,\n                    trial_name=trial_name,\n                    run_name=f\"{trial_name}-f{fold}r{random_seed}\",\n                )\n                for metric, value in trainer.callback_metrics.items():\n                    crossval_scores[metric].append(value.item())\n\n        logger.debug(f\"Cross-validation scores: {crossval_scores}\")\n        crossval_jaccard = mean(crossval_scores[\"val/JaccardIndex\"])\n        crossval_recall = mean(crossval_scores[\"val/Recall\"])\n\n        return crossval_jaccard, crossval_recall\n\n    study = optuna.create_study(\n        storage=sweep_db,\n        study_name=sweep_id,\n        directions=[\"maximize\", \"maximize\"],\n        load_if_exists=True,\n    )\n\n    if device is None:\n        logger.info(\"No device specified, closing script...\")\n        return\n\n    logger.info(\"Starting optimizing\")\n    study.optimize(objective, n_trials=n_trials)\n</code></pre>"},{"location":"reference/darts/legacy_training/preprocess_planet_train_data/","title":"darts.legacy_training.preprocess_planet_train_data","text":"<p>Preprocess Planet data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Planet scenes and orthotiles.</p> </li> <li> <code>labels_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the labels.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/planet.py</code> <pre><code>def preprocess_planet_train_data(\n    *,\n    bands: list[str],\n    data_dir: Path,\n    labels_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Planet data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        data_dir (Path): The directory containing the Planet scenes and orthotiles.\n        labels_dir (Path): The directory containing the labels.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_planet_masks, load_planet_scene, load_tcvis\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        labels = (gpd.read_file(labels_file) for labels_file in labels_dir.glob(\"*/TrainingLabel*.gpkg\"))\n        labels = gpd.GeoDataFrame(pd.concat(labels, ignore_index=True))\n\n        footprints = (gpd.read_file(footprints_file) for footprints_file in labels_dir.glob(\"*/ImageFootprints*.gpkg\"))\n        footprints = gpd.GeoDataFrame(pd.concat(footprints, ignore_index=True))\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressor=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        planet_paths = sorted(_legacy_path_gen(data_dir))\n        logger.info(f\"Found {len(planet_paths)} PLANET scenes and orthotiles in {data_dir}\")\n        path_gen = split_dataset_paths(\n            planet_paths, footprints, train_data_dir, test_val_split, test_regions, admin_dir\n        )\n\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(planet_paths), console=console\n        ):\n            try:\n                planet_id = fpath.stem\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(planet_paths)}\"\n                    f\" '{fpath.resolve()}' ({planet_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{planet_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{planet_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_planet_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 2\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_planet_masks(fpath)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{planet_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile=tile,\n                    labels=labels[labels.image_id == planet_id],\n                    bands=bands,\n                    norm_factors=norm_factors,\n                    patch_size=patch_size,\n                    overlap=overlap,\n                    exclude_nopositive=exclude_nopositive,\n                    exclude_nan=exclude_nan,\n                    device=device,\n                    mask_erosion_size=mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(planet_paths)} '{fpath.resolve()}'\"\n                    f\"({planet_id=}) with {patch_id} patches.\"\n                )\n\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"data_dir\": data_dir,\n            \"labels_dir\": labels_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/preprocess_s2_train_data/","title":"darts.legacy_training.preprocess_s2_train_data","text":"<p>Preprocess Sentinel 2 data for training.</p> <p>The data is split into a cross-validation, a validation-test and a test set:</p> <pre><code>- `cross-val` is meant to be used for train and validation\n- `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n- `test` leave-out region for testing the spatial distribution shift of the data\n</code></pre> <p>Each split is stored as a zarr group, containing a x and a y dataarray. The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size). The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size). Both dataarrays are chunked along the n_patches dimension. This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and therefore in a separate file.</p> <p>Through the parameters <code>test_val_split</code> and <code>test_regions</code>, the test and validation split can be controlled. To <code>test_regions</code> can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and put them in the test-split. With the <code>test_val_split</code> parameter, the ratio between further splitting of a test-validation set can be controlled.</p> <p>Through <code>exclude_nopositve</code> and <code>exclude_nan</code>, respective patches can be excluded from the final data.</p> <p>Further, a <code>config.toml</code> file is saved in the <code>train_data_dir</code> containing the configuration used for the preprocessing. Addionally, a <code>labels.geojson</code> file is saved in the <code>train_data_dir</code> containing the joined labels geometries used for the creation of the binarized label-masks, containing also information about the split via the <code>mode</code> column.</p> <p>The final directory structure of <code>train_data_dir</code> will look like this:</p> <pre><code>train_data_dir/\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/\n\u251c\u2500\u2500 test.zarr/\n\u251c\u2500\u2500 val-test.zarr/\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the preprocessing.</p> </li> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the Sentinel 2 scenes.</p> </li> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The \"output\" directory where the tensors are written to.</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist.</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the TCVis data.</p> </li> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory containing the admin files.</p> </li> <li> <code>preprocess_cache</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The directory to store the preprocessed data. Defaults to None.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>dask_worker</code>               (<code>int</code>, default:                   <code>min(16, multiprocessing.cpu_count() - 1)</code> )           \u2013            <p>The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to exclude patches where the labels do not contain positives. Defaults to False.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to exclude patches where the input data has nan values. Defaults to True.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>test_val_split</code>               (<code>float</code>, default:                   <code>0.05</code> )           \u2013            <p>The split ratio for the test and validation set. Defaults to 0.05.</p> </li> <li> <code>test_regions</code>               (<code>list[str] | str</code>, default:                   <code>None</code> )           \u2013            <p>The region to use for the test set. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/preprocess/s2.py</code> <pre><code>def preprocess_s2_train_data(\n    *,\n    bands: list[str],\n    sentinel2_dir: Path,\n    train_data_dir: Path,\n    arcticdem_dir: Path,\n    tcvis_dir: Path,\n    admin_dir: Path,\n    preprocess_cache: Path | None = None,\n    device: Literal[\"cuda\", \"cpu\", \"auto\"] | int | None = None,\n    dask_worker: int = min(16, mp.cpu_count() - 1),\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    exclude_nopositive: bool = False,\n    exclude_nan: bool = True,\n    mask_erosion_size: int = 10,\n    test_val_split: float = 0.05,\n    test_regions: list[str] | None = None,\n):\n    \"\"\"Preprocess Sentinel 2 data for training.\n\n    The data is split into a cross-validation, a validation-test and a test set:\n\n        - `cross-val` is meant to be used for train and validation\n        - `val-test` (5%) random leave-out for testing the randomness distribution shift of the data\n        - `test` leave-out region for testing the spatial distribution shift of the data\n\n    Each split is stored as a zarr group, containing a x and a y dataarray.\n    The x dataarray contains the input data with the shape (n_patches, n_bands, patch_size, patch_size).\n    The y dataarray contains the labels with the shape (n_patches, patch_size, patch_size).\n    Both dataarrays are chunked along the n_patches dimension.\n    This results in super fast random access to the data, because each sample / patch is stored in a separate chunk and\n    therefore in a separate file.\n\n    Through the parameters `test_val_split` and `test_regions`, the test and validation split can be controlled.\n    To `test_regions` can a list of admin 1 or admin 2 region names, based on the region shapefile maintained by\n    https://github.com/wmgeolab/geoBoundaries, be supplied to remove intersecting scenes from the dataset and\n    put them in the test-split.\n    With the `test_val_split` parameter, the ratio between further splitting of a test-validation set can be controlled.\n\n    Through `exclude_nopositve` and `exclude_nan`, respective patches can be excluded from the final data.\n\n    Further, a `config.toml` file is saved in the `train_data_dir` containing the configuration used for the\n    preprocessing.\n    Addionally, a `labels.geojson` file is saved in the `train_data_dir` containing the joined labels geometries used\n    for the creation of the binarized label-masks, containing also information about the split via the `mode` column.\n\n    The final directory structure of `train_data_dir` will look like this:\n\n    ```sh\n    train_data_dir/\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/\n    \u251c\u2500\u2500 test.zarr/\n    \u251c\u2500\u2500 val-test.zarr/\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        bands (list[str]): The bands to be used for training. Must be present in the preprocessing.\n        sentinel2_dir (Path): The directory containing the Sentinel 2 scenes.\n        train_data_dir (Path): The \"output\" directory where the tensors are written to.\n        arcticdem_dir (Path): The directory containing the ArcticDEM data (the datacube and the extent files).\n            Will be created and downloaded if it does not exist.\n        tcvis_dir (Path): The directory containing the TCVis data.\n        admin_dir (Path): The directory containing the admin files.\n        preprocess_cache (Path, optional): The directory to store the preprocessed data. Defaults to None.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the model on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            If \"auto\" try to automatically select a free GPU (&lt;50% memory usage).\n            Defaults to \"cuda\" if available, else \"cpu\".\n        dask_worker (int, optional): The number of Dask workers to use. Defaults to min(16, mp.cpu_count() - 1).\n        ee_project (str, optional): The Earth Engine project ID or number to use. May be omitted if\n            project is defined within persistent API credentials obtained via `earthengine authenticate`.\n        ee_use_highvolume (bool, optional): Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        patch_size (int, optional): The patch size to use for inference. Defaults to 1024.\n        overlap (int, optional): The overlap to use for inference. Defaults to 16.\n        exclude_nopositive (bool, optional): Whether to exclude patches where the labels do not contain positives.\n            Defaults to False.\n        exclude_nan (bool, optional): Whether to exclude patches where the input data has nan values.\n            Defaults to True.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        test_val_split (float, optional): The split ratio for the test and validation set. Defaults to 0.05.\n        test_regions (list[str] | str, optional): The region to use for the test set. Defaults to None.\n\n    \"\"\"\n    # Import here to avoid long loading times when running other commands\n    import geopandas as gpd\n    import pandas as pd\n    import toml\n    import xarray as xr\n    import zarr\n    from darts_acquisition import load_arcticdem, load_s2_masks, load_s2_scene, load_tcvis\n    from darts_acquisition.s2 import parse_s2_tile_id\n    from darts_preprocessing import preprocess_legacy_fast\n    from darts_segmentation.training.prepare_training import create_training_patches\n    from dask.distributed import Client, LocalCluster\n    from lovely_tensors import monkey_patch\n    from odc.stac import configure_rio\n    from rich.progress import track\n    from zarr.codecs import BloscCodec\n    from zarr.storage import LocalStore\n\n    from darts.utils.cuda import debug_info, decide_device\n    from darts.utils.earthengine import init_ee\n    from darts.utils.logging import console\n\n    monkey_patch()\n    debug_info()\n    device = decide_device(device)\n    init_ee(ee_project, ee_use_highvolume)\n\n    with LocalCluster(n_workers=dask_worker) as cluster, Client(cluster) as client:\n        logger.info(f\"Using Dask client: {client} on cluster {cluster}\")\n        logger.info(f\"Dashboard available at: {client.dashboard_link}\")\n        configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True}, client=client)\n        logger.info(\"Configured Rasterio with Dask\")\n\n        # We hardcode these because they depend on the preprocessing used\n        norm_factors = {\n            \"red\": 1 / 3000,\n            \"green\": 1 / 3000,\n            \"blue\": 1 / 3000,\n            \"nir\": 1 / 3000,\n            \"ndvi\": 1 / 20000,\n            \"relative_elevation\": 1 / 30000,\n            \"slope\": 1 / 90,\n            \"tc_brightness\": 1 / 255,\n            \"tc_greenness\": 1 / 255,\n            \"tc_wetness\": 1 / 255,\n        }\n        # Filter out bands that are not in the specified bands\n        norm_factors = {k: v for k, v in norm_factors.items() if k in bands}\n\n        train_data_dir.mkdir(exist_ok=True, parents=True)\n\n        zgroups = {\n            \"cross-val\": zarr.group(store=LocalStore(train_data_dir / \"cross-val.zarr\"), overwrite=True),\n            \"val-test\": zarr.group(store=LocalStore(train_data_dir / \"val-test.zarr\"), overwrite=True),\n            \"test\": zarr.group(store=LocalStore(train_data_dir / \"test.zarr\"), overwrite=True),\n        }\n        # We need do declare the number of patches to 0, because we can't know the final number of patches\n        for root in zgroups.values():\n            root.create(\n                name=\"x\",\n                shape=(0, len(bands), patch_size, patch_size),\n                # shards=(100, len(bands), patch_size, patch_size),\n                chunks=(1, len(bands), patch_size, patch_size),\n                dtype=\"float32\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n            root.create(\n                name=\"y\",\n                shape=(0, patch_size, patch_size),\n                # shards=(100, patch_size, patch_size),\n                chunks=(1, patch_size, patch_size),\n                dtype=\"uint8\",\n                compressors=BloscCodec(cname=\"lz4\", clevel=9),\n            )\n\n        # Find all Sentinel 2 scenes and split into train+val (cross-val), val-test (variance) and test (region)\n        n_patches = 0\n        n_patches_by_mode = {\"cross-val\": 0, \"val-test\": 0, \"test\": 0}\n        joint_lables = []\n        s2_paths = sorted(sentinel2_dir.glob(\"*/\"))\n        logger.info(f\"Found {len(s2_paths)} Sentinel 2 scenes in {sentinel2_dir}\")\n        path_gen = split_dataset_paths(s2_paths, train_data_dir, test_val_split, test_regions, admin_dir)\n        for i, (fpath, mode) in track(\n            enumerate(path_gen), description=\"Processing samples\", total=len(s2_paths), console=console\n        ):\n            try:\n                _, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n\n                logger.debug(\n                    f\"Processing sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}' ({tile_id=}) to split '{mode}'\"\n                )\n\n                # Check for a cached preprocessed file\n                if preprocess_cache and (preprocess_cache / f\"{tile_id}.nc\").exists():\n                    cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                    logger.info(f\"Loading preprocessed data from {cache_file.resolve()}\")\n                    tile = xr.open_dataset(preprocess_cache / f\"{tile_id}.nc\", engine=\"h5netcdf\").set_coords(\n                        \"spatial_ref\"\n                    )\n                else:\n                    optical = load_s2_scene(fpath)\n                    logger.info(f\"Found optical tile with size {optical.sizes}\")\n                    arctidem_res = 10\n                    arcticdem_buffer = ceil(tpi_outer_radius / arctidem_res * sqrt(2))\n                    arcticdem = load_arcticdem(\n                        optical.odc.geobox, arcticdem_dir, resolution=arctidem_res, buffer=arcticdem_buffer\n                    )\n                    tcvis = load_tcvis(optical.odc.geobox, tcvis_dir)\n                    data_masks = load_s2_masks(fpath, optical.odc.geobox)\n\n                    tile: xr.Dataset = preprocess_legacy_fast(\n                        optical,\n                        arcticdem,\n                        tcvis,\n                        data_masks,\n                        tpi_outer_radius,\n                        tpi_inner_radius,\n                        device,\n                    )\n                    # Only cache if we have a cache directory\n                    if preprocess_cache:\n                        preprocess_cache.mkdir(exist_ok=True, parents=True)\n                        cache_file = preprocess_cache / f\"{tile_id}.nc\"\n                        logger.info(f\"Caching preprocessed data to {cache_file.resolve()}\")\n                        tile.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n                labels = gpd.read_file(fpath / f\"{s2_tile_id}.shp\")\n\n                # Save the patches\n                gen = create_training_patches(\n                    tile,\n                    labels,\n                    bands,\n                    norm_factors,\n                    patch_size,\n                    overlap,\n                    exclude_nopositive,\n                    exclude_nan,\n                    device,\n                    mask_erosion_size,\n                )\n\n                zx = zgroups[mode][\"x\"]\n                zy = zgroups[mode][\"y\"]\n                patch_id = None\n                for patch_id, (x, y) in enumerate(gen):\n                    zx.append(x.unsqueeze(0).numpy().astype(\"float32\"))\n                    zy.append(y.unsqueeze(0).numpy().astype(\"uint8\"))\n                    n_patches += 1\n                    n_patches_by_mode[mode] += 1\n                if n_patches &gt; 0 and len(labels) &gt; 0:\n                    labels[\"mode\"] = mode\n                    joint_lables.append(labels.to_crs(\"EPSG:3413\"))\n\n                logger.info(\n                    f\"Processed sample {i + 1} of {len(s2_paths)} '{fpath.resolve()}'\"\n                    f\"({tile_id=}) with {patch_id} patches.\"\n                )\n            except KeyboardInterrupt:\n                logger.info(\"Interrupted by user.\")\n                break\n\n            except Exception as e:\n                logger.warning(f\"Could not process folder sample {i} '{fpath.resolve()}'.\\nSkipping...\")\n                logger.exception(e)\n\n    # Save the used labels\n    joint_lables = pd.concat(joint_lables)\n    joint_lables.to_file(train_data_dir / \"labels.geojson\", driver=\"GeoJSON\")\n\n    # Save a config file as toml\n    config = {\n        \"darts\": {\n            \"sentinel2_dir\": sentinel2_dir,\n            \"train_data_dir\": train_data_dir,\n            \"arcticdem_dir\": arcticdem_dir,\n            \"tcvis_dir\": tcvis_dir,\n            \"bands\": bands,\n            \"norm_factors\": norm_factors,\n            \"device\": device,\n            \"ee_project\": ee_project,\n            \"ee_use_highvolume\": ee_use_highvolume,\n            \"tpi_outer_radius\": tpi_outer_radius,\n            \"tpi_inner_radius\": tpi_inner_radius,\n            \"patch_size\": patch_size,\n            \"overlap\": overlap,\n            \"exclude_nopositive\": exclude_nopositive,\n            \"exclude_nan\": exclude_nan,\n            \"n_patches\": n_patches,\n        }\n    }\n    with open(train_data_dir / \"config.toml\", \"w\") as f:\n        toml.dump(config, f)\n\n    logger.info(f\"Saved {n_patches} ({n_patches_by_mode}) patches to {train_data_dir}\")\n</code></pre>"},{"location":"reference/darts/legacy_training/test_smp/","title":"darts.legacy_training.test_smp","text":"<p>Run the testing of the SMP model.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>run_id</code>               (<code>str</code>)           \u2013            <p>ID of the run.</p> </li> <li> <code>run_name</code>               (<code>str</code>)           \u2013            <p>Name of the run.</p> </li> <li> <code>model_ckp</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the model checkpoint. If None, try to find the latest checkpoint in <code>artifact_dir / run_name / run_id / checkpoints</code>. Defaults to None.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch size. Defaults to 8.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Directory to save artifacts. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of workers for the DataLoader. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>Device to use. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>WandB project. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/test.py</code> <pre><code>def test_smp(\n    *,\n    train_data_dir: Path,\n    run_id: str,\n    run_name: str,\n    model_ckp: Path | None = None,\n    batch_size: int = 8,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the testing of the SMP model.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        run_id (str): ID of the run.\n        run_name (str): Name of the run.\n        model_ckp (Path | None): Path to the model checkpoint.\n            If None, try to find the latest checkpoint in `artifact_dir / run_name / run_id / checkpoints`.\n            Defaults to None.\n        batch_size (int, optional): Batch size. Defaults to 8.\n        artifact_dir (Path, optional): Directory to save artifacts. Defaults to Path(\"lightning_logs\").\n        num_workers (int, optional): Number of workers for the DataLoader. Defaults to 0.\n        device (int | str, optional): Device to use. Defaults to \"auto\".\n        wandb_entity (str | None, optional): WandB entity. Defaults to None.\n        wandb_project (str | None, optional): WandB project. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n    logger.info(f\"Starting testing '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(f\"Using config:\\n\\t{batch_size=}\\n\\t{device=}\")\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(42, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    # Data and model\n    datamodule_val_test = DartsDataModule(\n        data_dir=train_data_dir / \"val-test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    datamodule_test = DartsDataModule(\n        data_dir=train_data_dir / \"test.zarr\",\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    # Try to infer model checkpoint if not given\n    if model_ckp is None:\n        checkpoint_dir = artifact_dir / run_name / run_id / \"checkpoints\"\n        logger.debug(f\"No checkpoint provided. Looking for model checkpoint in {checkpoint_dir.resolve()}\")\n        model_ckp = max(checkpoint_dir.glob(\"*.ckpt\"), key=lambda x: x.stat().st_mtime)\n    model = SMPSegmenter.load_from_checkpoint(model_ckp)\n\n    # Loggers\n    trainer_loggers = [\n        CSVLogger(save_dir=artifact_dir, name=run_name, version=run_id),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            id=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    metrics_cb = BinarySegmentationMetrics(\n        input_combination=preprocess_config[\"bands\"],\n    )\n    callbacks = [\n        RichProgressBar(),\n        metrics_cb,\n    ]\n\n    # Test\n    trainer = L.Trainer(\n        callbacks=callbacks,\n        logger=trainer_loggers,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=True,\n    )\n    # Overwrite the names of the test sets to test agains two separate sets\n    metrics_cb.test_set = \"val-test\"\n    model.test_set = \"val-test\"\n    trainer.test(model, datamodule_val_test, ckpt_path=model_ckp)\n    metrics_cb.test_set = \"test\"\n    model.test_set = \"test\"\n    trainer.test(model, datamodule_test)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished testing '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/legacy_training/train_smp/","title":"darts.legacy_training.train_smp","text":"<p>Run the training of the SMP model.</p> <p>Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.</p> <p>Each training run is assigned a unique name and id pair and optionally a trial name. The name, which the user can provide, should be used as a grouping mechanism of equal hyperparameter and code. Hence, different versions of the same name should only differ by random state or run settings parameter, like logs. Each version is assigned a unique id. Artifacts (metrics &amp; checkpoints) are then stored under <code>{artifact_dir}/{run_name}/{run_id}</code> in no-crossval runs. If <code>trial_name</code> is specified, the artifacts are stored under <code>{artifact_dir}/{trial_name}/{run_name}-{run_id}</code>. Wandb logs are always stored under <code>{wandb_entity}/{wandb_project}/{run_name}</code>, regardless of <code>trial_name</code>. However, they are further grouped by the <code>trial_name</code> (via job_type), if specified. Both <code>run_name</code> and <code>run_id</code> are also stored in the hparams of each checkpoint.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>early_stopping_patience</code> specifies how many epochs to wait for improvement before stopping.         In epochs, this would be <code>check_val_every_n_epoch * early_stopping_patience</code>.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>The data structure of the training data expects the \"preprocessing\" step to be done beforehand, which results in the following data structure:</p> <pre><code>preprocessed-data/ # the top-level directory\n\u251c\u2500\u2500 config.toml\n\u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n\u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n\u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n\u2514\u2500\u2500 labels.geojson\n</code></pre> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory (top-level).</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>fold</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The current fold to train on. Must be in [0, 4]. Defaults to 0.</p> </li> <li> <code>continue_from_checkpoint</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to a checkpoint to continue training from. Defaults to None.</p> </li> <li> <code>model_arch</code>               (<code>str</code>, default:                   <code>'Unet'</code> )           \u2013            <p>Model architecture to use. Defaults to \"Unet\".</p> </li> <li> <code>model_encoder</code>               (<code>str</code>, default:                   <code>'dpn107'</code> )           \u2013            <p>Encoder to use. Defaults to \"dpn107\".</p> </li> <li> <code>model_encoder_weights</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the encoder weights. Defaults to None.</p> </li> <li> <code>augment</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to apply augments or not. Defaults to True.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Learning Rate. Defaults to 1e-3.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>Batch Size. Defaults to 8.</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>early_stopping_patience</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of epochs to wait for improvement before stopping. Defaults to 5.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>random_seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for deterministic training. Defaults to 42.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The device to run the model on. Defaults to \"auto\".</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> <li> <code>wandb_group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Wandb group. Usefull for CV-Sweeps. Defaults to None.</p> </li> <li> <code>run_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of this run, as a further grouping method for logs etc. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>run_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>ID of the run. If None, will generate a random one. Defaults to None.</p> </li> <li> <code>trial_name</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Name of the cross-validation run / trial. This effects primary logging and artifact storage. If None, will do nothing. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Trainer</code> (              <code>pytorch_lightning.Trainer</code> )          \u2013            <p>The trainer object used for training.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def train_smp(\n    *,\n    # Data config\n    train_data_dir: Path,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    fold: int = 0,\n    continue_from_checkpoint: Path | None = None,\n    # Hyperparameters\n    model_arch: str = \"Unet\",\n    model_encoder: str = \"dpn107\",\n    model_encoder_weights: str | None = None,\n    augment: bool = True,\n    learning_rate: float = 1e-3,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    batch_size: int = 8,\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    early_stopping_patience: int = 5,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    random_seed: int = 42,\n    num_workers: int = 0,\n    device: int | str = \"auto\",\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n    wandb_group: str | None = None,\n    run_name: str | None = None,\n    run_id: str | None = None,\n    trial_name: str | None = None,\n) -&gt; \"pl.Trainer\":\n    \"\"\"Run the training of the SMP model.\n\n    Please see https://smp.readthedocs.io/en/latest/index.html for model configurations.\n\n    Each training run is assigned a unique **name** and **id** pair and optionally a trial name.\n    The name, which the user _can_ provide, should be used as a grouping mechanism of equal hyperparameter and code.\n    Hence, different versions of the same name should only differ by random state or run settings parameter, like logs.\n    Each version is assigned a unique id.\n    Artifacts (metrics &amp; checkpoints) are then stored under `{artifact_dir}/{run_name}/{run_id}` in no-crossval runs.\n    If `trial_name` is specified, the artifacts are stored under `{artifact_dir}/{trial_name}/{run_name}-{run_id}`.\n    Wandb logs are always stored under `{wandb_entity}/{wandb_project}/{run_name}`, regardless of `trial_name`.\n    However, they are further grouped by the `trial_name` (via job_type), if specified.\n    Both `run_name` and `run_id` are also stored in the hparams of each checkpoint.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `early_stopping_patience` specifies how many epochs to wait for improvement before stopping.\n            In epochs, this would be `check_val_every_n_epoch * early_stopping_patience`.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    The data structure of the training data expects the \"preprocessing\" step to be done beforehand,\n    which results in the following data structure:\n\n    ```sh\n    preprocessed-data/ # the top-level directory\n    \u251c\u2500\u2500 config.toml\n    \u251c\u2500\u2500 cross-val.zarr/ # this zarr group contains the dataarrays x and y for the training and validation\n    \u251c\u2500\u2500 test.zarr/ # this zarr group contains the dataarrays x and y for the left-out-region test set\n    \u251c\u2500\u2500 val-test.zarr/ # this zarr group contains the dataarrays x and y for the random selected validation set\n    \u2514\u2500\u2500 labels.geojson\n    ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory (top-level).\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        fold (int, optional): The current fold to train on. Must be in [0, 4]. Defaults to 0.\n        continue_from_checkpoint (Path | None, optional): Path to a checkpoint to continue training from.\n            Defaults to None.\n        model_arch (str, optional): Model architecture to use. Defaults to \"Unet\".\n        model_encoder (str, optional): Encoder to use. Defaults to \"dpn107\".\n        model_encoder_weights (str | None, optional): Path to the encoder weights. Defaults to None.\n        augment (bool, optional): Weather to apply augments or not. Defaults to True.\n        learning_rate (float, optional): Learning Rate. Defaults to 1e-3.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        batch_size (int, optional): Batch Size. Defaults to 8.\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        early_stopping_patience (int, optional): Number of epochs to wait for improvement before stopping.\n            Defaults to 5.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        random_seed (int, optional): Random seed for deterministic training. Defaults to 42.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str, optional): The device to run the model on. Defaults to \"auto\".\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n        wandb_group (str | None, optional): Wandb group. Usefull for CV-Sweeps. Defaults to None.\n        run_name (str | None, optional): Name of this run, as a further grouping method for logs etc.\n            If None, will generate a random one. Defaults to None.\n        run_id (str | None, optional): ID of the run. If None, will generate a random one. Defaults to None.\n        trial_name (str | None, optional): Name of the cross-validation run / trial.\n            This effects primary logging and artifact storage.\n            If None, will do nothing. Defaults to None.\n\n    Returns:\n        Trainer: The trainer object used for training.\n\n    \"\"\"\n    import lightning as L  # noqa: N812\n    import lovely_tensors\n    import torch\n    from darts_segmentation.segment import SMPSegmenterConfig\n    from darts_segmentation.training.callbacks import BinarySegmentationMetrics\n    from darts_segmentation.training.data import DartsDataModule\n    from darts_segmentation.training.module import SMPSegmenter\n    from lightning.pytorch import seed_everything\n    from lightning.pytorch.callbacks import EarlyStopping, RichProgressBar\n    from lightning.pytorch.loggers import CSVLogger, WandbLogger\n\n    from darts.legacy_training.util import generate_id, get_generated_name\n    from darts.utils.logging import LoggingManager\n\n    LoggingManager.apply_logging_handlers(\"lightning.pytorch\")\n\n    tick_fstart = time.perf_counter()\n\n    # Create unique run identification (name can be specified by user, id can be interpreded as a 'version')\n    run_name = run_name or get_generated_name(artifact_dir)\n    run_id = run_id or generate_id()\n\n    logger.info(f\"Starting training '{run_name}' ('{run_id}') with data from {train_data_dir.resolve()}.\")\n    logger.debug(\n        f\"Using config:\\n\\t{model_arch=}\\n\\t{model_encoder=}\\n\\t{model_encoder_weights=}\\n\\t{augment=}\\n\\t\"\n        f\"{learning_rate=}\\n\\t{gamma=}\\n\\t{batch_size=}\\n\\t{max_epochs=}\\n\\t{log_every_n_steps=}\\n\\t\"\n        f\"{check_val_every_n_epoch=}\\n\\t{early_stopping_patience=}\\n\\t{plot_every_n_val_epochs=}\\n\\t{num_workers=}\"\n        f\"\\n\\t{device=}\\n\\t{random_seed=}\"\n    )\n\n    lovely_tensors.monkey_patch()\n\n    torch.set_float32_matmul_precision(\"medium\")\n    seed_everything(random_seed, workers=True)\n\n    preprocess_config = toml.load(train_data_dir / \"config.toml\")[\"darts\"]\n\n    config = SMPSegmenterConfig(\n        input_combination=preprocess_config[\"bands\"],\n        model={\n            \"arch\": model_arch,\n            \"encoder_name\": model_encoder,\n            \"encoder_weights\": model_encoder_weights,\n            \"in_channels\": len(preprocess_config[\"bands\"]),\n            \"classes\": 1,\n        },\n        norm_factors=preprocess_config[\"norm_factors\"],\n    )\n\n    # Data and model\n    datamodule = DartsDataModule(\n        data_dir=train_data_dir / \"cross-val.zarr\",\n        batch_size=batch_size,\n        fold=fold,\n        augment=augment,\n        num_workers=num_workers,\n    )\n    model = SMPSegmenter(\n        config=config,\n        learning_rate=learning_rate,\n        gamma=gamma,\n        focal_loss_alpha=focal_loss_alpha,\n        focal_loss_gamma=focal_loss_gamma,\n        # These are only stored in the hparams and are not used\n        run_id=run_id,\n        run_name=run_name,\n        trial_name=trial_name,\n        random_seed=random_seed,\n    )\n\n    # Loggers\n    is_crossval = bool(trial_name)\n    trainer_loggers = [\n        CSVLogger(\n            save_dir=artifact_dir,\n            name=run_name if not is_crossval else trial_name,\n            version=run_id if not is_crossval else f\"{run_name}-{run_id}\",\n        ),\n    ]\n    logger.debug(f\"Logging CSV to {Path(trainer_loggers[0].log_dir).resolve()}\")\n    if wandb_entity and wandb_project:\n        wandb_logger = WandbLogger(\n            save_dir=artifact_dir,\n            name=run_name,\n            version=run_id,\n            project=wandb_project,\n            entity=wandb_entity,\n            resume=\"allow\",\n            group=wandb_group,\n            job_type=trial_name,\n        )\n        trainer_loggers.append(wandb_logger)\n        logger.debug(\n            f\"Logging to WandB with entity '{wandb_entity}' and project '{wandb_project}'.\"\n            f\"Artifacts are logged to {(Path(wandb_logger.save_dir) / 'wandb').resolve()}\"\n        )\n\n    # Callbacks\n    callbacks = [\n        RichProgressBar(),\n        BinarySegmentationMetrics(\n            input_combination=config[\"input_combination\"],\n            val_set=f\"val{fold}\",\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            is_crossval=is_crossval,\n        ),\n    ]\n    if early_stopping_patience:\n        logger.debug(f\"Using EarlyStopping with patience {early_stopping_patience}\")\n        early_stopping = EarlyStopping(monitor=\"val/JaccardIndex\", mode=\"max\", patience=early_stopping_patience)\n        callbacks.append(early_stopping)\n\n    # Train\n    trainer = L.Trainer(\n        max_epochs=max_epochs,\n        callbacks=callbacks,\n        log_every_n_steps=log_every_n_steps,\n        logger=trainer_loggers,\n        check_val_every_n_epoch=check_val_every_n_epoch,\n        accelerator=\"gpu\" if isinstance(device, int) else device,\n        devices=[device] if isinstance(device, int) else device,\n        deterministic=False,\n    )\n    trainer.fit(model, datamodule, ckpt_path=continue_from_checkpoint)\n\n    tick_fend = time.perf_counter()\n    logger.info(f\"Finished training '{run_name}' in {tick_fend - tick_fstart:.2f}s.\")\n\n    if wandb_entity and wandb_project:\n        wandb_logger.finalize(\"success\")\n        wandb_logger.experiment.finish(exit_code=0)\n        logger.debug(f\"Finalized WandB logging for '{run_name}'\")\n\n    return trainer\n</code></pre>"},{"location":"reference/darts/legacy_training/wandb_sweep_smp/","title":"darts.legacy_training.wandb_sweep_smp","text":"<p>Create a sweep with wandb and run it on the specified cuda device, or continue an existing sweep.</p> <p>If <code>sweep_id</code> is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued. All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}. Since each sweep-configuration has (currently) an own name and id, a single run can be found under: {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.</p> <p>If a <code>cuda_device</code> is specified, run an agent on this device. If None, do nothing.</p> <p>You can specify the frequency on how often logs will be written and validation will be performed.     - <code>log_every_n_steps</code> specifies how often train-logs will be written. This does not affect validation.     - <code>check_val_every_n_epoch</code> specifies how often validation will be performed.         This will also affect early stopping.     - <code>plot_every_n_val_epochs</code> specifies how often validation samples will be plotted.         Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.         In epochs, this would be <code>check_val_every_n_epoch * plot_every_n_val_epochs</code>.</p> <p>This will NOT use cross-validation. For cross-validation, use <code>optuna_sweep_smp</code>.</p> Example <p>In one terminal, start a sweep: <pre><code>    $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n    ...  # Many logs\n    Created sweep with ID 123456789\n    ... # More logs from spawned agent\n</code></pre></p> <p>In another terminal, start an a second agent: <pre><code>    $ rye run darts wandb-sweep-smp --sweep-id 123456789\n    ...\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>train_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the training data directory.</p> </li> <li> <code>sweep_config</code>               (<code>pathlib.Path</code>)           \u2013            <p>Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration. Hyperparameters must contain the following fields: <code>model_arch</code>, <code>model_encoder</code>, <code>augment</code>, <code>gamma</code>, <code>batch_size</code>. Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.</p> </li> <li> <code>n_trials</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Number of runs to execute. Defaults to 10.</p> </li> <li> <code>sweep_id</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The ID of the sweep. If None, a new sweep will be created. Defaults to None.</p> </li> <li> <code>artifact_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('lightning_logs')</code> )           \u2013            <p>Path to the training output directory. Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").</p> </li> <li> <code>max_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Maximum number of epochs to train. Defaults to 100.</p> </li> <li> <code>log_every_n_steps</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Log every n steps. Defaults to 10.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int</code>, default:                   <code>3</code> )           \u2013            <p>Check validation every n epochs. Defaults to 3.</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of Dataloader workers. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>int | str | None</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. Defaults to None.</p> </li> <li> <code>wandb_entity</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Entity. Defaults to None.</p> </li> <li> <code>wandb_project</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Weights and Biases Project. Defaults to None.</p> </li> </ul> Source code in <code>darts/src/darts/legacy_training/train.py</code> <pre><code>def wandb_sweep_smp(\n    *,\n    # Data and sweep config\n    train_data_dir: Path,\n    sweep_config: Path,\n    n_trials: int = 10,\n    sweep_id: str | None = None,\n    artifact_dir: Path = Path(\"lightning_logs\"),\n    # Epoch and Logging config\n    max_epochs: int = 100,\n    log_every_n_steps: int = 10,\n    check_val_every_n_epoch: int = 3,\n    plot_every_n_val_epochs: int = 5,\n    # Device and Manager config\n    num_workers: int = 0,\n    device: int | str | None = None,\n    wandb_entity: str | None = None,\n    wandb_project: str | None = None,\n):\n    \"\"\"Create a sweep with wandb and run it on the specified cuda device, or continue an existing sweep.\n\n    If `sweep_id` is None, a new sweep will be created. Otherwise, the sweep with the given ID will be continued.\n    All artifacts are gathered under nested directory based on the sweep id: {artifact_dir}/sweep-{sweep_id}.\n    Since each sweep-configuration has (currently) an own name and id, a single run can be found under:\n    {artifact_dir}/sweep-{sweep_id}/{run_name}/{run_id}. Read the training-docs for more info.\n\n    If a `cuda_device` is specified, run an agent on this device. If None, do nothing.\n\n    You can specify the frequency on how often logs will be written and validation will be performed.\n        - `log_every_n_steps` specifies how often train-logs will be written. This does not affect validation.\n        - `check_val_every_n_epoch` specifies how often validation will be performed.\n            This will also affect early stopping.\n        - `plot_every_n_val_epochs` specifies how often validation samples will be plotted.\n            Since plotting is quite costly, you can reduce the frequency. Works similar like early stopping.\n            In epochs, this would be `check_val_every_n_epoch * plot_every_n_val_epochs`.\n\n    This will NOT use cross-validation. For cross-validation, use `optuna_sweep_smp`.\n\n    Example:\n        In one terminal, start a sweep:\n        ```sh\n            $ rye run darts wandb-sweep-smp --config-file /path/to/sweep-config.toml\n            ...  # Many logs\n            Created sweep with ID 123456789\n            ... # More logs from spawned agent\n        ```\n\n        In another terminal, start an a second agent:\n        ```sh\n            $ rye run darts wandb-sweep-smp --sweep-id 123456789\n            ...\n        ```\n\n    Args:\n        train_data_dir (Path): Path to the training data directory.\n        sweep_config (Path): Path to the sweep yaml configuration file. Must contain a valid wandb sweep configuration.\n            Hyperparameters must contain the following fields: `model_arch`, `model_encoder`, `augment`, `gamma`,\n            `batch_size`.\n            Please read https://docs.wandb.ai/guides/sweeps/sweep-config-keys for more information.\n        n_trials (int, optional): Number of runs to execute. Defaults to 10.\n        sweep_id (str | None, optional): The ID of the sweep. If None, a new sweep will be created. Defaults to None.\n        artifact_dir (Path, optional): Path to the training output directory.\n            Will contain checkpoints and metrics. Defaults to Path(\"lightning_logs\").\n        max_epochs (int, optional): Maximum number of epochs to train. Defaults to 100.\n        log_every_n_steps (int, optional): Log every n steps. Defaults to 10.\n        check_val_every_n_epoch (int, optional): Check validation every n epochs. Defaults to 3.\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        num_workers (int, optional): Number of Dataloader workers. Defaults to 0.\n        device (int | str | None, optional): The device to run the model on. Defaults to None.\n        wandb_entity (str | None, optional): Weights and Biases Entity. Defaults to None.\n        wandb_project (str | None, optional): Weights and Biases Project. Defaults to None.\n\n    \"\"\"\n    import wandb\n\n    # Wandb has a stupid way of logging (they log per default with click.echo to stdout)\n    # We need to silence this and redirect all possible logs to our logger\n    # wl = wandb.setup({\"silent\": True})\n    # wandb.termsetup(wl.settings, logging.getLogger(\"wandb\"))\n    # LoggingManager.apply_logging_handlers(\"wandb\")\n\n    if sweep_id is not None and device is None:\n        logger.warning(\"Continuing a sweep without specifying a device will not do anything.\")\n\n    with sweep_config.open(\"r\") as f:\n        sweep_configuration = yaml.safe_load(f)\n\n    logger.debug(f\"Loaded sweep configuration from {sweep_config.resolve()}:\\n{sweep_configuration}\")\n\n    if sweep_id is None:\n        sweep_id = wandb.sweep(sweep=sweep_configuration, project=wandb_project, entity=wandb_entity)\n        logger.info(f\"Created sweep with ID {sweep_id}\")\n        logger.info(\"To start a sweep agents, use the following command:\")\n        logger.info(f\"$ rye run darts sweep_smp --sweep-id {sweep_id}\")\n\n    artifact_dir = artifact_dir / f\"sweep-{sweep_id}\"\n    artifact_dir.mkdir(parents=True, exist_ok=True)\n\n    def run():\n        run = wandb.init(config=sweep_configuration)\n        # We need to manually log the run data since the wandb logger only logs to its own logs and click\n        logger.info(f\"Starting sweep run '{run.settings.run_name}'\")\n        logger.debug(f\"Run data is saved locally in {Path(run.settings.sync_dir).resolve()}\")\n        logger.debug(f\"View project at {run.settings.project_url}\")\n        logger.debug(f\"View sweep at {run.settings.sweep_url}\")\n        logger.debug(f\"View run at {run.settings.run_url}\")\n\n        # We set the default weights to None, to be able to use different architectures\n        model_encoder_weights = None\n        # We set early stopping to None, because wandb will handle the early stopping\n        early_stopping_patience = None\n        learning_rate = wandb.config[\"learning_rate\"]\n        gamma = wandb.config[\"gamma\"]\n        batch_size = wandb.config[\"batch_size\"]\n        model_arch = wandb.config[\"model_arch\"]\n        model_encoder = wandb.config[\"model_encoder\"]\n        augment = wandb.config[\"augment\"]\n        focal_loss_alpha = wandb.config[\"focal_loss_alpha\"]\n        focal_loss_gamma = wandb.config[\"focal_loss_gamma\"]\n        fold = wandb.config.get(\"fold\", 0)\n        random_seed = wandb.config.get(\"random_seed\", 42)\n\n        train_smp(\n            # Data config\n            train_data_dir=train_data_dir,\n            artifact_dir=artifact_dir,\n            fold=fold,\n            # Hyperparameters\n            model_arch=model_arch,\n            model_encoder=model_encoder,\n            model_encoder_weights=model_encoder_weights,\n            augment=augment,\n            learning_rate=learning_rate,\n            gamma=gamma,\n            focal_loss_alpha=focal_loss_alpha,\n            focal_loss_gamma=focal_loss_gamma,\n            batch_size=batch_size,\n            # Epoch and Logging config\n            early_stopping_patience=early_stopping_patience,\n            max_epochs=max_epochs,\n            log_every_n_steps=log_every_n_steps,\n            check_val_every_n_epoch=check_val_every_n_epoch,\n            plot_every_n_val_epochs=plot_every_n_val_epochs,\n            # Device and Manager config\n            random_seed=random_seed,\n            num_workers=num_workers,\n            device=device,\n            wandb_entity=wandb_entity,\n            wandb_project=wandb_project,\n            run_name=wandb.run.name,\n            run_id=wandb.run.id,\n        )\n\n    if device is None:\n        logger.info(\"No device specified, closing script...\")\n        return\n\n    logger.info(\"Starting a default sweep agent\")\n    wandb.agent(sweep_id, function=run, count=n_trials, project=wandb_project, entity=wandb_entity)\n</code></pre>"},{"location":"reference/darts/pipelines/","title":"darts.pipelines","text":"<p>Predefined pipelines for DARTS.</p> <p>Classes:</p> <ul> <li> <code>AOISentinel2Pipeline</code>           \u2013            <p>Pipeline for Sentinel 2 data based on an area of interest.</p> </li> <li> <code>PlanetPipeline</code>           \u2013            <p>Pipeline for PlanetScope data.</p> </li> <li> <code>Sentinel2Pipeline</code>           \u2013            <p>Pipeline for Sentinel 2 data.</p> </li> </ul> <ul> <li>AOISentinel2Pipeline</li> <li>PlanetPipeline</li> <li>Sentinel2Pipeline</li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline","title":"AOISentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>AOISentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    aoi_shapefile: pathlib.Path = None,\n    start_date: str = None,\n    end_date: str = None,\n    max_cloud_cover: int = 10,\n    input_cache: pathlib.Path = pathlib.Path(\n        \"data/cache/input\"\n    ),\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data based on an area of interest.</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The shapefile containing the area of interest.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for filtering the Sentinel 2 scenes. Defaults to 10.</p> </li> <li> <code>input_cache</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/cache/input')</code> )           \u2013            <p>The directory to use for caching the input data. Defaults to Path(\"data/cache/input\").</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>cli</code>             \u2013              <p>Run the sequential pipeline for AOI Sentinel 2 data.</p> </li> <li> <code>run</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>binarization_threshold</code>               (<code>float</code>)           \u2013            </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>)           \u2013            </li> <li> <code>ee_project</code>               (<code>str | None</code>)           \u2013            </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>)           \u2013            </li> <li> <code>end_date</code>               (<code>str</code>)           \u2013            </li> <li> <code>export_bands</code>               (<code>list[str]</code>)           \u2013            </li> <li> <code>input_cache</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>mask_erosion_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>max_cloud_cover</code>               (<code>int</code>)           \u2013            </li> <li> <code>min_object_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>model_files</code>               (<code>list[pathlib.Path]</code>)           \u2013            </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            </li> <li> <code>overwrite</code>               (<code>bool</code>)           \u2013            </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>)           \u2013            </li> <li> <code>reflection</code>               (<code>int</code>)           \u2013            </li> <li> <code>start_date</code>               (<code>str</code>)           \u2013            </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>)           \u2013            </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>)           \u2013            </li> <li> <code>write_model_outputs</code>               (<code>bool</code>)           \u2013            </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.aoi_shapefile","title":"aoi_shapefile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_shapefile: pathlib.Path = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.input_cache","title":"input_cache  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_cache: pathlib.Path = pathlib.Path(\"data/cache/input\")\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.AOISentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for AOI Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"AOISentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for AOI Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.AOISentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import StopUhr\n\n    stopuhr = StopUhr(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with stopuhr(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with stopuhr(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2)),\n                )\n            with stopuhr(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with stopuhr(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with stopuhr(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with stopuhr(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with stopuhr(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            stopuhr.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        stopuhr.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline","title":"PlanetPipeline  <code>dataclass</code>","text":"<pre><code>PlanetPipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    orthotiles_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSOrthoTile\"\n    ),\n    scenes_dir: pathlib.Path = pathlib.Path(\n        \"data/input/planet/PSScene\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSOrthoTile')</code> )           \u2013            <p>The directory containing the PlanetScope orthotiles.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSScene')</code> )           \u2013            <p>The directory containing the PlanetScope scenes.</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>cli</code>             \u2013              <p>Run the sequential pipeline for Planet data.</p> </li> <li> <code>run</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>binarization_threshold</code>               (<code>float</code>)           \u2013            </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>)           \u2013            </li> <li> <code>ee_project</code>               (<code>str | None</code>)           \u2013            </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>)           \u2013            </li> <li> <code>export_bands</code>               (<code>list[str]</code>)           \u2013            </li> <li> <code>image_ids</code>               (<code>list</code>)           \u2013            </li> <li> <code>mask_erosion_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>min_object_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>model_files</code>               (<code>list[pathlib.Path]</code>)           \u2013            </li> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            </li> <li> <code>overwrite</code>               (<code>bool</code>)           \u2013            </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>)           \u2013            </li> <li> <code>reflection</code>               (<code>int</code>)           \u2013            </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>)           \u2013            </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>)           \u2013            </li> <li> <code>write_model_outputs</code>               (<code>bool</code>)           \u2013            </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSOrthoTile\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSScene\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Planet data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetPipeline\"):\n    \"\"\"Run the sequential pipeline for Planet data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.PlanetPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import StopUhr\n\n    stopuhr = StopUhr(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with stopuhr(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with stopuhr(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2)),\n                )\n            with stopuhr(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with stopuhr(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with stopuhr(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with stopuhr(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with stopuhr(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            stopuhr.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        stopuhr.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline","title":"Sentinel2Pipeline  <code>dataclass</code>","text":"<pre><code>Sentinel2Pipeline(\n    model_files: list[pathlib.Path] = None,\n    output_data_dir: pathlib.Path = pathlib.Path(\n        \"data/output\"\n    ),\n    arcticdem_dir: pathlib.Path = pathlib.Path(\n        \"data/download/arcticdem\"\n    ),\n    tcvis_dir: pathlib.Path = pathlib.Path(\n        \"data/download/tcvis\"\n    ),\n    device: typing.Literal[\"cuda\", \"cpu\", \"auto\"]\n    | int\n    | None = None,\n    ee_project: str | None = None,\n    ee_use_highvolume: bool = True,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    patch_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 8,\n    reflection: int = 0,\n    binarization_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 1,\n    export_bands: list[str] = lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ](),\n    write_model_outputs: bool = False,\n    overwrite: bool = False,\n    sentinel2_dir: pathlib.Path = pathlib.Path(\n        \"data/input/sentinel2\"\n    ),\n    image_ids: list = None,\n)\n</code></pre> <p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data.</p> <p>Parameters:</p> <ul> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/sentinel2')</code> )           \u2013            <p>The directory containing the Sentinel 2 scenes. Defaults to Path(\"data/input/sentinel2\").</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed. Defaults to None.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>cli</code>             \u2013              <p>Run the sequential pipeline for Sentinel 2 data.</p> </li> <li> <code>run</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>binarization_threshold</code>               (<code>float</code>)           \u2013            </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu', 'auto'] | int | None</code>)           \u2013            </li> <li> <code>ee_project</code>               (<code>str | None</code>)           \u2013            </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>)           \u2013            </li> <li> <code>export_bands</code>               (<code>list[str]</code>)           \u2013            </li> <li> <code>image_ids</code>               (<code>list</code>)           \u2013            </li> <li> <code>mask_erosion_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>min_object_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>model_files</code>               (<code>list[pathlib.Path]</code>)           \u2013            </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            </li> <li> <code>overwrite</code>               (<code>bool</code>)           \u2013            </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>)           \u2013            </li> <li> <code>reflection</code>               (<code>int</code>)           \u2013            </li> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>)           \u2013            </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>)           \u2013            </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>)           \u2013            </li> <li> <code>write_model_outputs</code>               (<code>bool</code>)           \u2013            </li> </ul>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.sentinel2_dir","title":"sentinel2_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_dir: pathlib.Path = pathlib.Path(\n    \"data/input/sentinel2\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/#darts.pipelines.Sentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import StopUhr\n\n    stopuhr = StopUhr(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with stopuhr(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with stopuhr(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2)),\n                )\n            with stopuhr(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with stopuhr(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with stopuhr(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with stopuhr(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with stopuhr(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            stopuhr.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        stopuhr.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/","title":"darts.pipelines.AOISentinel2Pipeline","text":"<p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data based on an area of interest.</p> <p>Parameters:</p> <ul> <li> <code>aoi_shapefile</code>               (<code>pathlib.Path</code>, default:                   <code>None</code> )           \u2013            <p>The shapefile containing the area of interest.</p> </li> <li> <code>start_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The start date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>end_date</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The end date of the time series in YYYY-MM-DD format.</p> </li> <li> <code>max_cloud_cover</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The maximum cloud cover percentage to use for filtering the Sentinel 2 scenes. Defaults to 10.</p> </li> <li> <code>input_cache</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/cache/input')</code> )           \u2013            <p>The directory to use for caching the input data. Defaults to Path(\"data/cache/input\").</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline._s2ids","title":"_s2ids  <code>cached</code> <code>property</code>","text":"<pre><code>_s2ids: list[str]\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.aoi_shapefile","title":"aoi_shapefile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aoi_shapefile: pathlib.Path = None\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.end_date","title":"end_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>end_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.input_cache","title":"input_cache  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_cache: pathlib.Path = pathlib.Path(\"data/cache/input\")\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.max_cloud_cover","title":"max_cloud_cover  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_cloud_cover: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.start_date","title":"start_date  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>start_date: str = None\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline._arcticdem_resolution","title":"_arcticdem_resolution","text":"<pre><code>_arcticdem_resolution() -&gt; typing.Literal[10]\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _arcticdem_resolution(self) -&gt; Literal[10]:\n    return 10\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline._get_tile_id","title":"_get_tile_id","text":"<pre><code>_get_tile_id(tilekey)\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _get_tile_id(self, tilekey):\n    # In case of the GEE tilekey is also the s2id\n    return tilekey\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline._load_tile","title":"_load_tile","text":"<pre><code>_load_tile(s2id: str) -&gt; xarray.Dataset\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _load_tile(self, s2id: str) -&gt; \"xr.Dataset\":\n    from darts_acquisition.s2 import load_s2_from_gee\n\n    tile = load_s2_from_gee(s2id, cache=self.input_cache)\n    return tile\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline._tileinfos","title":"_tileinfos","text":"<pre><code>_tileinfos() -&gt; list[tuple[str, pathlib.Path]]\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _tileinfos(self) -&gt; list[tuple[str, Path]]:\n    out = []\n    for s2id in self._s2ids:\n        outpath = self.output_data_dir / s2id\n        out.append((s2id, outpath))\n    out.sort()\n    return out\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.AOISentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for AOI Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"AOISentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for AOI Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/AOISentinel2Pipeline/#darts.pipelines.AOISentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import StopUhr\n\n    stopuhr = StopUhr(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with stopuhr(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with stopuhr(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2)),\n                )\n            with stopuhr(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with stopuhr(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with stopuhr(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with stopuhr(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with stopuhr(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            stopuhr.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        stopuhr.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/","title":"darts.pipelines.PlanetPipeline","text":"<p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for PlanetScope data.</p> <p>Parameters:</p> <ul> <li> <code>orthotiles_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSOrthoTile')</code> )           \u2013            <p>The directory containing the PlanetScope orthotiles.</p> </li> <li> <code>scenes_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/planet/PSScene')</code> )           \u2013            <p>The directory containing the PlanetScope scenes.</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.orthotiles_dir","title":"orthotiles_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>orthotiles_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSOrthoTile\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.scenes_dir","title":"scenes_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>scenes_dir: pathlib.Path = pathlib.Path(\n    \"data/input/planet/PSScene\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline._arcticdem_resolution","title":"_arcticdem_resolution","text":"<pre><code>_arcticdem_resolution() -&gt; typing.Literal[2]\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _arcticdem_resolution(self) -&gt; Literal[2]:\n    return 2\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline._get_tile_id","title":"_get_tile_id","text":"<pre><code>_get_tile_id(tilekey: pathlib.Path) -&gt; str\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _get_tile_id(self, tilekey: Path) -&gt; str:\n    from darts_acquisition import parse_planet_type\n\n    try:\n        fpath = tilekey\n        planet_type = parse_planet_type(fpath)\n        tile_id = fpath.parent.stem if planet_type == \"orthotile\" else fpath.stem\n        return tile_id\n    except Exception as e:\n        logger.error(\"Could not parse Planet tile-id. Please check the input data.\")\n        logger.exception(e)\n        raise e\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline._load_tile","title":"_load_tile","text":"<pre><code>_load_tile(fpath: pathlib.Path) -&gt; xarray.Dataset\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _load_tile(self, fpath: Path) -&gt; \"xr.Dataset\":\n    import xarray as xr\n    from darts_acquisition import load_planet_masks, load_planet_scene\n\n    optical = load_planet_scene(fpath)\n    data_masks = load_planet_masks(fpath)\n    tile = xr.merge([optical, data_masks])\n    return tile\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline._tileinfos","title":"_tileinfos","text":"<pre><code>_tileinfos() -&gt; list[tuple[pathlib.Path, pathlib.Path]]\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _tileinfos(self) -&gt; list[tuple[Path, Path]]:\n    out = []\n    # Find all PlanetScope orthotiles\n    for fpath in self.orthotiles_dir.glob(\"*/*/\"):\n        tile_id = fpath.parent.name\n        scene_id = fpath.name\n        if self.image_ids is not None:\n            if scene_id not in self.image_ids:\n                continue\n        outpath = self.output_data_dir / tile_id / scene_id\n        out.append((fpath.resolve(), outpath))\n\n    # Find all PlanetScope scenes\n    for fpath in self.scenes_dir.glob(\"*/\"):\n        scene_id = fpath.name\n        if self.image_ids is not None:\n            if scene_id not in self.image_ids:\n                continue\n        outpath = self.output_data_dir / scene_id\n        out.append((fpath.resolve(), outpath))\n    out.sort()\n    return out\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.PlanetPipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Planet data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"PlanetPipeline\"):\n    \"\"\"Run the sequential pipeline for Planet data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/PlanetPipeline/#darts.pipelines.PlanetPipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import StopUhr\n\n    stopuhr = StopUhr(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with stopuhr(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with stopuhr(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2)),\n                )\n            with stopuhr(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with stopuhr(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with stopuhr(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with stopuhr(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with stopuhr(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            stopuhr.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        stopuhr.summary()\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/","title":"darts.pipelines.Sentinel2Pipeline","text":"<p>               Bases: <code>darts.pipelines.sequential_v2._BasePipeline</code></p> <p>Pipeline for Sentinel 2 data.</p> <p>Parameters:</p> <ul> <li> <code>sentinel2_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/input/sentinel2')</code> )           \u2013            <p>The directory containing the Sentinel 2 scenes. Defaults to Path(\"data/input/sentinel2\").</p> </li> <li> <code>image_ids</code>               (<code>list</code>, default:                   <code>None</code> )           \u2013            <p>The list of image ids to process. If None, all images in the directory will be processed. Defaults to None.</p> </li> <li> <code>model_files</code>               (<code>pathlib.Path | list[pathlib.Path]</code>, default:                   <code>None</code> )           \u2013            <p>The path to the models to use for segmentation. Can also be a single Path to only use one model. This implies <code>write_model_outputs=False</code> If a list is provided, will use an ensemble of the models.</p> </li> <li> <code>output_data_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/output')</code> )           \u2013            <p>The \"output\" directory. Defaults to Path(\"data/output\").</p> </li> <li> <code>arcticdem_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/arcticdem')</code> )           \u2013            <p>The directory containing the ArcticDEM data (the datacube and the extent files). Will be created and downloaded if it does not exist. Defaults to Path(\"data/download/arcticdem\").</p> </li> <li> <code>tcvis_dir</code>               (<code>pathlib.Path</code>, default:                   <code>pathlib.Path('data/download/tcvis')</code> )           \u2013            <p>The directory containing the TCVis data. Defaults to Path(\"data/download/tcvis\").</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>None</code> )           \u2013            <p>The device to run the model on. If \"cuda\" take the first device (0), if int take the specified device. If \"auto\" try to automatically select a free GPU (&lt;50% memory usage). Defaults to \"cuda\" if available, else \"cpu\".</p> </li> <li> <code>ee_project</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The Earth Engine project ID or number to use. May be omitted if project is defined within persistent API credentials obtained via <code>earthengine authenticate</code>.</p> </li> <li> <code>ee_use_highvolume</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to use the high volume server (https://earthengine-highvolume.googleapis.com).</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The patch size to use for inference. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>256</code> )           \u2013            <p>The overlap to use for inference. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size to use for inference. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The reflection padding to use for inference. Defaults to 0.</p> </li> <li> <code>binarization_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | typing.Literal['high_quality', 'low_quality', 'none']</code>, default:                   <code>1</code> )           \u2013            <p>The quality level to use for the segmentation. Can also be an int. In this case 0=\"none\" 1=\"low_quality\" 2=\"high_quality\". Defaults to 1.</p> </li> <li> <code>export_bands</code>               (<code>list[str]</code>, default:                   <code>lambda: ['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']()</code> )           \u2013            <p>The bands to export. Can be a list of \"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\", \"optical\", \"dem\", \"tcvis\" or concrete band-names. Defaults to [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"].</p> </li> <li> <code>write_model_outputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Also save the model outputs, not only the ensemble result. Defaults to False.</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to overwrite existing files. Defaults to False.</p> </li> </ul>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.arcticdem_dir","title":"arcticdem_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>arcticdem_dir: pathlib.Path = pathlib.Path(\n    \"data/download/arcticdem\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 8\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.binarization_threshold","title":"binarization_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binarization_threshold: float = 0.5\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.device","title":"device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>device: (\n    typing.Literal[\"cuda\", \"cpu\", \"auto\"] | int | None\n) = None\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.ee_project","title":"ee_project  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_project: str | None = None\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.ee_use_highvolume","title":"ee_use_highvolume  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ee_use_highvolume: bool = True\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.export_bands","title":"export_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>export_bands: list[str] = dataclasses.field(\n    default_factory=lambda: [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ]\n)\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.image_ids","title":"image_ids  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>image_ids: list = None\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.mask_erosion_size","title":"mask_erosion_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_erosion_size: int = 10\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.min_object_size","title":"min_object_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_object_size: int = 32\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.model_files","title":"model_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_files: list[pathlib.Path] = None\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.output_data_dir","title":"output_data_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_data_dir: pathlib.Path = pathlib.Path('data/output')\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.overlap","title":"overlap  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap: int = 256\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.patch_size","title":"patch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>patch_size: int = 1024\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.quality_level","title":"quality_level  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quality_level: (\n    int\n    | typing.Literal[\"high_quality\", \"low_quality\", \"none\"]\n) = 1\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.reflection","title":"reflection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reflection: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.sentinel2_dir","title":"sentinel2_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sentinel2_dir: pathlib.Path = pathlib.Path(\n    \"data/input/sentinel2\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.tcvis_dir","title":"tcvis_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tcvis_dir: pathlib.Path = pathlib.Path(\n    \"data/download/tcvis\"\n)\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.tpi_inner_radius","title":"tpi_inner_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_inner_radius: int = 0\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.tpi_outer_radius","title":"tpi_outer_radius  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tpi_outer_radius: int = 100\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.write_model_outputs","title":"write_model_outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_model_outputs: bool = False\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline._arcticdem_resolution","title":"_arcticdem_resolution","text":"<pre><code>_arcticdem_resolution() -&gt; typing.Literal[10]\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _arcticdem_resolution(self) -&gt; Literal[10]:\n    return 10\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline._get_tile_id","title":"_get_tile_id","text":"<pre><code>_get_tile_id(tilekey: pathlib.Path)\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _get_tile_id(self, tilekey: Path):\n    from darts_acquisition import parse_s2_tile_id\n\n    try:\n        fpath = tilekey\n        _, _, tile_id = parse_s2_tile_id(fpath)\n        return tile_id\n    except Exception as e:\n        logger.error(\"Could not parse Sentinel 2 tile-id. Please check the input data.\")\n        logger.exception(e)\n        raise e\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline._load_tile","title":"_load_tile","text":"<pre><code>_load_tile(fpath: pathlib.Path) -&gt; xarray.Dataset\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _load_tile(self, fpath: Path) -&gt; \"xr.Dataset\":  # Here: fpath == 'tid'\n    import xarray as xr\n    from darts_acquisition import load_s2_masks, load_s2_scene\n\n    optical = load_s2_scene(fpath)\n    data_masks = load_s2_masks(fpath, optical.odc.geobox)\n    tile = xr.merge([optical, data_masks])\n    return tile\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline._tileinfos","title":"_tileinfos","text":"<pre><code>_tileinfos() -&gt; list[tuple[pathlib.Path, pathlib.Path]]\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def _tileinfos(self) -&gt; list[tuple[Path, Path]]:\n    out = []\n    for fpath in self.sentinel2_dir.glob(\"*/\"):\n        scene_id = fpath.name\n        if self.image_ids is not None:\n            if scene_id not in self.image_ids:\n                continue\n        outpath = self.output_data_dir / scene_id\n        out.append((fpath.resolve(), outpath))\n    out.sort()\n    return out\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.cli","title":"cli  <code>staticmethod</code>","text":"<pre><code>cli(\n    *,\n    pipeline: darts.pipelines.sequential_v2.Sentinel2Pipeline,\n)\n</code></pre> <p>Run the sequential pipeline for Sentinel 2 data.</p> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>@staticmethod\ndef cli(*, pipeline: \"Sentinel2Pipeline\"):\n    \"\"\"Run the sequential pipeline for Sentinel 2 data.\"\"\"\n    pipeline.run()\n</code></pre>"},{"location":"reference/darts/pipelines/Sentinel2Pipeline/#darts.pipelines.Sentinel2Pipeline.run","title":"run","text":"<pre><code>run()\n</code></pre> Source code in <code>darts/src/darts/pipelines/sequential_v2.py</code> <pre><code>def run(self):  # noqa: C901\n    if self.model_files is None or len(self.model_files) == 0:\n        raise ValueError(\"No model files provided. Please provide a list of model files.\")\n    if len(self.export_bands) == 0:\n        raise ValueError(\"No export bands provided. Please provide a list of export bands.\")\n\n    current_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    logger.info(f\"Starting pipeline at {current_time}.\")\n\n    # Storing the configuration as JSON file\n    self.output_data_dir.mkdir(parents=True, exist_ok=True)\n    with open(self.output_data_dir / f\"{current_time}.config.json\", \"w\") as f:\n        config = asdict(self)\n        # Convert everything to json serializable\n        for key, value in config.items():\n            if isinstance(value, Path):\n                config[key] = str(value.resolve())\n            elif isinstance(value, list):\n                config[key] = [str(v.resolve()) if isinstance(v, Path) else v for v in value]\n        json.dump(config, f)\n\n    from stopuhr import StopUhr\n\n    stopuhr = StopUhr(printer=logger.debug)\n\n    from darts.utils.cuda import debug_info\n\n    debug_info()\n\n    from darts.utils.earthengine import init_ee\n\n    init_ee(self.ee_project, self.ee_use_highvolume)\n\n    import pandas as pd\n    import smart_geocubes\n    import torch\n    from darts_acquisition import load_arcticdem, load_tcvis\n    from darts_ensemble import EnsembleV1\n    from darts_export import export_tile, missing_outputs\n    from darts_postprocessing import prepare_export\n    from darts_preprocessing import preprocess_legacy_fast\n\n    from darts.utils.cuda import decide_device\n    from darts.utils.logging import LoggingManager\n\n    self.device = decide_device(self.device)\n\n    # determine models to use\n    if isinstance(self.model_files, Path):\n        self.model_files = [self.model_files]\n        self.write_model_outputs = False\n    models = {model_file.stem: model_file for model_file in self.model_files}\n    ensemble = EnsembleV1(models, device=torch.device(self.device))\n\n    # Create the datacubes if they do not exist\n    LoggingManager.apply_logging_handlers(\"smart_geocubes\")\n    arcticdem_resolution = self._arcticdem_resolution()\n    if arcticdem_resolution == 2:\n        accessor = smart_geocubes.ArcticDEM2m(self.arcticdem_dir)\n    elif arcticdem_resolution == 10:\n        accessor = smart_geocubes.ArcticDEM10m(self.arcticdem_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n    accessor = smart_geocubes.TCTrend(self.tcvis_dir)\n    if not accessor.created:\n        accessor.create(overwrite=False)\n\n    # Iterate over all the data\n    tileinfo = self._tileinfos()\n    n_tiles = 0\n    logger.info(f\"Found {len(tileinfo)} tiles to process.\")\n    results = []\n    for i, (tilekey, outpath) in enumerate(tileinfo):\n        tile_id = self._get_tile_id(tilekey)\n        try:\n            if not self.overwrite:\n                mo = missing_outputs(outpath, bands=self.export_bands, ensemble_subsets=models.keys())\n                if mo == \"none\":\n                    logger.info(f\"Tile {tile_id} already processed. Skipping...\")\n                    continue\n                if mo == \"some\":\n                    logger.warning(\n                        f\"Tile {tile_id} already processed. Some outputs are missing.\"\n                        \" Skipping because overwrite=False...\"\n                    )\n                    continue\n\n            with stopuhr(\"Loading optical data\", log=False):\n                tile = self._load_tile(tilekey)\n            with stopuhr(\"Loading ArcticDEM\", log=False):\n                arcticdem = load_arcticdem(\n                    tile.odc.geobox,\n                    self.arcticdem_dir,\n                    resolution=arcticdem_resolution,\n                    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2)),\n                )\n            with stopuhr(\"Loading TCVis\", log=False):\n                tcvis = load_tcvis(tile.odc.geobox, self.tcvis_dir)\n            with stopuhr(\"Preprocessing tile\", log=False):\n                tile = preprocess_legacy_fast(\n                    tile,\n                    arcticdem,\n                    tcvis,\n                    self.tpi_outer_radius,\n                    self.tpi_inner_radius,\n                    self.device,\n                )\n            with stopuhr(\"Segmenting\", log=False):\n                tile = ensemble.segment_tile(\n                    tile,\n                    patch_size=self.patch_size,\n                    overlap=self.overlap,\n                    batch_size=self.batch_size,\n                    reflection=self.reflection,\n                    keep_inputs=self.write_model_outputs,\n                )\n            with stopuhr(\"Postprosessing\", log=False):\n                tile = prepare_export(\n                    tile,\n                    bin_threshold=self.binarization_threshold,\n                    mask_erosion_size=self.mask_erosion_size,\n                    min_object_size=self.min_object_size,\n                    quality_level=self.quality_level,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                    device=self.device,\n                )\n\n            with stopuhr(\"Exporting\", log=False):\n                export_tile(\n                    tile,\n                    outpath,\n                    bands=self.export_bands,\n                    ensemble_subsets=models.keys() if self.write_model_outputs else [],\n                )\n\n            n_tiles += 1\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"success\",\n                    \"error\": None,\n                }\n            )\n            logger.info(f\"Processed sample {i + 1} of {len(tileinfo)} '{tilekey}' ({tile_id=}).\")\n        except KeyboardInterrupt:\n            logger.warning(\"Keyboard interrupt detected.\\nExiting...\")\n            raise KeyboardInterrupt\n        except Exception as e:\n            logger.warning(f\"Could not process '{tilekey}' ({tile_id=}).\\nSkipping...\")\n            logger.exception(e)\n            results.append(\n                {\n                    \"tile_id\": tile_id,\n                    \"output_path\": str(outpath.resolve()),\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                }\n            )\n        finally:\n            if len(results) &gt; 0:\n                pd.DataFrame(results).to_parquet(self.output_data_dir / f\"{current_time}.results.parquet\")\n            stopuhr.export().to_parquet(self.output_data_dir / f\"{current_time}.stopuhr.parquet\")\n    else:\n        logger.info(f\"Processed {n_tiles} tiles to {self.output_data_dir.resolve()}.\")\n        stopuhr.summary()\n</code></pre>"},{"location":"reference/darts_acquisition/","title":"darts_acquisition","text":"<p>Acquisition of data from various sources for the DARTS dataset.</p> <p>Functions:</p> <ul> <li> <code>download_admin_files</code>             \u2013              <p>Download the admin files for the regions.</p> </li> <li> <code>load_arcticdem</code>             \u2013              <p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> </li> <li> <code>load_planet_masks</code>             \u2013              <p>Load the valid and quality data masks from a Planet scene.</p> </li> <li> <code>load_planet_scene</code>             \u2013              <p>Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.</p> </li> <li> <code>load_s2_from_gee</code>             \u2013              <p>Load a Sentinel 2 scene from Google Earth Engine and return it as an xarray dataset.</p> </li> <li> <code>load_s2_from_stac</code>             \u2013              <p>Load a Sentinel 2 scene from the Copernicus STAC API and return it as an xarray dataset.</p> </li> <li> <code>load_s2_masks</code>             \u2013              <p>Load the valid and quality data masks from a Sentinel 2 scene.</p> </li> <li> <code>load_s2_scene</code>             \u2013              <p>Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.</p> </li> <li> <code>load_tcvis</code>             \u2013              <p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> </li> <li> <code>parse_planet_type</code>             \u2013              <p>Parse the type of Planet data from the directory path.</p> </li> <li> <code>parse_s2_tile_id</code>             \u2013              <p>Parse the Sentinel 2 tile ID from a file path.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>__version__</code>           \u2013            </li> </ul> <ul> <li>download_admin_files</li> <li>load_arcticdem</li> <li>load_planet_masks</li> <li>load_planet_scene</li> <li>load_s2_from_gee</li> <li>load_s2_from_stac</li> <li>load_s2_masks</li> <li>load_s2_scene</li> <li>load_tcvis</li> <li>parse_planet_type</li> <li>parse_s2_tile_id</li> </ul>"},{"location":"reference/darts_acquisition/#darts_acquisition.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.download_admin_files","title":"download_admin_files","text":"<pre><code>download_admin_files(admin_dir: pathlib.Path)\n</code></pre> <p>Download the admin files for the regions.</p> <p>Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.</p> <p>Parameters:</p> <ul> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the admin files.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>@stopuhr.funkuhr(\"Downloading admin files\", printer=logger.debug, print_kwargs=True)\ndef download_admin_files(admin_dir: Path):\n    \"\"\"Download the admin files for the regions.\n\n    Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.\n\n    Args:\n        admin_dir (Path): The path to the admin files.\n\n    \"\"\"\n    # Download the admin files\n    admin_1_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.zip\"\n    admin_2_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.zip\"\n\n    admin_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.debug(f\"Downloading {admin_1_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_1_url, admin_dir)\n\n    logger.debug(f\"Downloading {admin_2_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_2_url, admin_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_arcticdem","title":"load_arcticdem","text":"<pre><code>load_arcticdem(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    resolution: darts_acquisition.arcticdem.RESOLUTIONS,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which the tile should be loaded.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory where the ArcticDEM data is stored.</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in m.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The ArcticDEM tile, with a buffer applied. Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different. Final dataset is NOT matched to the reference CRS and resolution.</p> </li> </ul> Warning <p>Geobox must be in a meter based CRS.</p> Usage <p>Since the API of the <code>load_arcticdem</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_arcticdem\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\narcticdem = load_arcticdem(\n    optical.odc.geobox,\n    \"/path/to/arcticdem-parent-directory\",\n    resolution=2,\n    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n)\n\n# Now we can for example match the resolution and extent of the optical data:\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> <p>The <code>buffer</code> parameter is used to extend the region of interest by a certain amount of pixels. This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the resolution is not supported.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopuhr.funkuhr(\"Loading ArcticDEM\", printer=logger.debug, print_kwargs=True)\ndef load_arcticdem(\n    geobox: GeoBox, data_dir: Path | str, resolution: RESOLUTIONS, buffer: int = 0, persist: bool = True\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded.\n        data_dir (Path | str): The directory where the ArcticDEM data is stored.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in m.\n        buffer (int, optional): The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The ArcticDEM tile, with a buffer applied.\n            Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different.\n            Final dataset is NOT matched to the reference CRS and resolution.\n\n    Warning:\n        Geobox must be in a meter based CRS.\n\n    Usage:\n        Since the API of the `load_arcticdem` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_arcticdem\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        arcticdem = load_arcticdem(\n            optical.odc.geobox,\n            \"/path/to/arcticdem-parent-directory\",\n            resolution=2,\n            buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n        The `buffer` parameter is used to extend the region of interest by a certain amount of pixels.\n        This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.\n\n    Raises:\n        ValueError: If the resolution is not supported.\n\n    \"\"\"  # noqa: E501\n    odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n\n    match resolution:\n        case 2:\n            accessor = smart_geocubes.ArcticDEM2m(data_dir)\n        case 10:\n            accessor = smart_geocubes.ArcticDEM10m(data_dir)\n        case 32:\n            accessor = smart_geocubes.ArcticDEM32m(data_dir)\n        case _:\n            raise ValueError(f\"Resolution {resolution} not supported, only 2m, 10m and 32m are supported\")\n\n    accessor.assert_created()\n\n    arcticdem = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem[\"datamask\"] = arcticdem.datamask.astype(\"uint8\")\n\n    return arcticdem\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_planet_masks","title":"load_planet_masks","text":"<pre><code>load_planet_masks(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the valid and quality data masks from a Planet scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The file path to the Planet scene from which to derive the masks.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching UDM-2 TIFF file is found in the specified path.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Planet masks\", printer=logger.debug, print_kwargs=True)\ndef load_planet_masks(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Planet scene.\n\n    Args:\n        fpath (str | Path): The file path to the Planet scene from which to derive the masks.\n\n    Raises:\n        FileNotFoundError: If no matching UDM-2 TIFF file is found in the specified path.\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # Get imagepath\n    udm_path = next(fpath.glob(\"*_udm2.tif\"), None)\n    if not udm_path:\n        udm_path = next(fpath.glob(\"*_udm2_clip.tif\"), None)\n    if not udm_path:\n        raise FileNotFoundError(f\"No matching UDM-2 TIFF files found in {fpath.resolve()} (.glob('*_udm2.tif'))\")\n\n    # See udm classes here: https://developers.planet.com/docs/data/udm-2/\n    da_udm = xr.open_dataarray(udm_path)\n\n    invalids = da_udm.sel(band=8).fillna(0) != 0\n    low_quality = da_udm.sel(band=[2, 3, 4, 5, 6]).max(axis=0) == 1\n    high_quality = ~low_quality &amp; ~invalids\n    qa_ds = xr.Dataset(coords={c: da_udm.coords[c] for c in da_udm.coords})\n    qa_ds[\"quality_data_mask\"] = (\n        xr.zeros_like(da_udm.sel(band=8)).where(invalids, 0).where(low_quality, 1).where(high_quality, 2)\n    )\n    qa_ds[\"quality_data_mask\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Quality data mask\",\n        \"description\": \"0 = Invalid, 1 = Low Quality, 2 = High Quality\",\n    }\n    return qa_ds\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_planet_scene","title":"load_planet_scene","text":"<pre><code>load_planet_scene(\n    fpath: str | pathlib.Path,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files or a specific path to the TIFF file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Planet scene\", printer=logger.debug, print_kwargs=True)\ndef load_planet_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files or a specific path to the TIFF file.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    # Check if the directory contains a PSOrthoTile or PSScene\n    planet_type = parse_planet_type(fpath)\n    logger.debug(f\"Loading Planet PS {planet_type.capitalize()} from {fpath.resolve()}\")\n\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    # Define band names and corresponding indices\n    planet_da = xr.open_dataarray(ps_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_planet = (\n        planet_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n    )\n    for var in ds_planet.variables:\n        ds_planet[var].assign_attrs(\n            {\n                \"long_name\": f\"PLANET {var.capitalize()}\",\n                \"data_source\": \"planet\",\n                \"planet_type\": planet_type,\n                \"units\": \"Reflectance\",\n            }\n        )\n    ds_planet.attrs = {\"tile_id\": fpath.parent.stem if planet_type == \"orthotile\" else fpath.stem}\n    return ds_planet\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_s2_from_gee","title":"load_s2_from_gee","text":"<pre><code>load_s2_from_gee(\n    img: str | ee.Image,\n    bands_mapping: dict = {\n        \"B2\": \"blue\",\n        \"B3\": \"green\",\n        \"B4\": \"red\",\n        \"B8\": \"nir\",\n    },\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: pathlib.Path | None = None,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel 2 scene from Google Earth Engine and return it as an xarray dataset.</p> <p>Parameters:</p> <ul> <li> <code>img</code>               (<code>str | ee.Image</code>)           \u2013            <p>The Sentinel 2 image ID or the ee image object.</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>, default:                   <code>{'B2': 'blue', 'B3': 'green', 'B4': 'red', 'B8': 'nir'}</code> )           \u2013            <p>A mapping from bands to obtain. Will be renamed to the corresponding band names. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>scale_and_offset</code>               (<code>bool | tuple[float, float]</code>, default:                   <code>True</code> )           \u2013            <p>Whether to apply the scale and offset to the bands. If a tuple is provided, it will be used as the (<code>scale</code>, <code>offset</code>) values with <code>band * scale + offset</code>. If True, use the default values of <code>scale</code> = 0.0001 and <code>offset</code> = 0, taken from ee_extra. Defaults to True.</p> </li> <li> <code>cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cache directory. If None, no caching will be done. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Sentinel 2 scene from GEE\", printer=logger.debug, print_kwargs=[\"img\"])\ndef load_s2_from_gee(\n    img: str | ee.Image,\n    bands_mapping: dict = {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"},\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: Path | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 scene from Google Earth Engine and return it as an xarray dataset.\n\n    Args:\n        img (str | ee.Image): The Sentinel 2 image ID or the ee image object.\n        bands_mapping (dict[str, str], optional): A mapping from bands to obtain.\n            Will be renamed to the corresponding band names.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        scale_and_offset (bool | tuple[float, float], optional): Whether to apply the scale and offset to the bands.\n            If a tuple is provided, it will be used as the (`scale`, `offset`) values with `band * scale + offset`.\n            If True, use the default values of `scale` = 0.0001 and `offset` = 0, taken from ee_extra.\n            Defaults to True.\n        cache (Path | None, optional): The path to the cache directory. If None, no caching will be done.\n            Defaults to None.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    \"\"\"\n    if isinstance(img, str):\n        s2id = img\n        img = ee.Image(f\"COPERNICUS/S2_SR_HARMONIZED/{s2id}\")\n    else:\n        s2id = img.id().getInfo().split(\"/\")[-1]\n    logger.debug(f\"Loading Sentinel 2 tile {s2id=} from GEE\")\n\n    if \"SCL\" not in bands_mapping.keys():\n        bands_mapping[\"SCL\"] = \"scl\"\n\n    cache_file = None if cache is None else cache / f\"gee-s2srh-{s2id}-{''.join(bands_mapping.keys())}.nc\"\n    if cache_file is not None and cache_file.exists():\n        ds_s2 = xr.open_dataset(cache_file, engine=\"h5netcdf\").set_coords(\"spatial_ref\")\n        ds_s2.load()\n        logger.debug(f\"Loaded {s2id=} from cache.\")\n    else:\n        img = img.select(list(bands_mapping.keys()))\n        ds_s2 = xr.open_dataset(\n            img,\n            engine=\"ee\",\n            geometry=img.geometry(),\n            crs=img.select(0).projection().crs().getInfo(),\n            scale=10,\n        )\n        ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n        ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\").rename({\"X\": \"x\", \"Y\": \"y\"}).transpose(\"y\", \"x\")\n        ds_s2 = ds_s2.odc.assign_crs(ds_s2.attrs[\"crs\"])\n        logger.debug(\n            f\"Found dataset with shape {ds_s2.sizes} for tile {s2id=}.\"\n            \"Start downloading data from GEE. This may take a while.\"\n        )\n\n        with stopuhr.stopuhr(f\"Downloading data from GEE for {s2id=}\", printer=logger.debug):\n            ds_s2.load()\n            if cache_file is not None:\n                ds_s2.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2-gee\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    ds_s2 = convert_masks(ds_s2)\n\n    # For some reason, there are some spatially random nan values in the data, not only at the borders\n    # To workaround this, set all nan values to 0 and add this information to the quality_data_mask\n    # This workaround is quite computational expensive, but it works for now\n    # TODO: Find other solutions for this problem!\n    with stopuhr.stopuhr(f\"Fixing nan values in {s2id=}\", printer=logger.debug):\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[\"quality_data_mask\"] = xr.where(ds_s2[band].isnull(), 0, ds_s2[\"quality_data_mask\"])\n            ds_s2[band] = ds_s2[band].fillna(0)\n            # Turn real nan values (scl is nan) into invalid data\n            ds_s2[band] = ds_s2[band].where(~ds_s2[\"scl\"].isnull())\n\n    if scale_and_offset:\n        if isinstance(scale_and_offset, tuple):\n            scale, offset = scale_and_offset\n        else:\n            scale, offset = 0.0001, 0\n        logger.debug(f\"Applying {scale=} and {offset=} to {s2id=} optical data\")\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[band] = ds_s2[band] * scale + offset\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_s2_from_stac","title":"load_s2_from_stac","text":"<pre><code>load_s2_from_stac(\n    s2id: str,\n    bands_mapping: dict = {\n        \"B02_10m\": \"blue\",\n        \"B03_10m\": \"green\",\n        \"B04_10m\": \"red\",\n        \"B08_10m\": \"nir\",\n    },\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: pathlib.Path | None = None,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel 2 scene from the Copernicus STAC API and return it as an xarray dataset.</p> <p>Parameters:</p> <ul> <li> <code>s2id</code>               (<code>str</code>)           \u2013            <p>The Sentinel 2 image ID.</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>, default:                   <code>{'B02_10m': 'blue', 'B03_10m': 'green', 'B04_10m': 'red', 'B08_10m': 'nir'}</code> )           \u2013            <p>A mapping from bands to obtain. Will be renamed to the corresponding band names. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>scale_and_offset</code>               (<code>bool | tuple[float, float]</code>, default:                   <code>True</code> )           \u2013            <p>Whether to apply the scale and offset to the bands. If a tuple is provided, it will be used as the (<code>scale</code>, <code>offset</code>) values with <code>band * scale + offset</code>. If True, use the default values of <code>scale</code> = 0.0001 and <code>offset</code> = 0, taken from ee_extra. Defaults to True.</p> </li> <li> <code>cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cache directory. If None, no caching will be done. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Sentinel 2 scene from STAC\", printer=logger.debug, print_kwargs=[\"s2id\"])\ndef load_s2_from_stac(\n    s2id: str,\n    bands_mapping: dict = {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"},\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: Path | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 scene from the Copernicus STAC API and return it as an xarray dataset.\n\n    Args:\n        s2id (str): The Sentinel 2 image ID.\n        bands_mapping (dict[str, str], optional): A mapping from bands to obtain.\n            Will be renamed to the corresponding band names.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        scale_and_offset (bool | tuple[float, float], optional): Whether to apply the scale and offset to the bands.\n            If a tuple is provided, it will be used as the (`scale`, `offset`) values with `band * scale + offset`.\n            If True, use the default values of `scale` = 0.0001 and `offset` = 0, taken from ee_extra.\n            Defaults to True.\n        cache (Path | None, optional): The path to the cache directory. If None, no caching will be done.\n            Defaults to None.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    \"\"\"\n    if \"SCL_20m\" not in bands_mapping.keys():\n        bands_mapping[\"SCL_20m\"] = \"scl\"\n\n    catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n    search = catalog.search(\n        collections=[\"sentinel-2-l2a\"],\n        ids=[s2id],\n    )\n\n    cache_file = None if cache is None else cache / f\"gee-s2srh-{s2id}-{''.join(bands_mapping.keys())}.nc\"\n    if cache_file is not None and cache_file.exists():\n        ds_s2 = xr.open_dataset(cache_file, engine=\"h5netcdf\").set_coords(\"spatial_ref\")\n        ds_s2.load()\n        logger.debug(f\"Loaded {s2id=} from cache.\")\n    else:\n        ds_s2 = xr.open_dataset(\n            search,\n            engine=\"stac\",\n            backend_kwargs={\"crs\": \"utm\", \"resolution\": 10, \"bands\": list(bands_mapping.keys())},\n        )\n        ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n        ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\")\n        logger.debug(\n            f\"Found a dataset with shape {ds_s2.sizes} for tile {s2id=}.\"\n            \"Start downloading data from STAC. This may take a while.\"\n        )\n\n        with stopuhr.stopuhr(f\"Downloading data from STAC for {s2id=}\", printer=logger.debug):\n            # Need double loading since the first load transforms lazy-stac to dask and second actually downloads\n            ds_s2.load().load()\n            if cache_file is not None:\n                ds_s2.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2-stac\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    ds_s2 = convert_masks(ds_s2)\n\n    if scale_and_offset:\n        if isinstance(scale_and_offset, tuple):\n            scale, offset = scale_and_offset\n        else:\n            scale, offset = 0.0001, 0\n        logger.debug(f\"Applying {scale=} and {offset=} to {s2id=} optical data\")\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[band] = ds_s2[band] * scale + offset\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_s2_masks","title":"load_s2_masks","text":"<pre><code>load_s2_masks(\n    fpath: str | pathlib.Path,\n    reference_geobox: odc.geo.geobox.GeoBox,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the valid and quality data masks from a Sentinel 2 scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> <li> <code>reference_geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The reference geobox to reproject, resample and crop the masks data to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Sentinel 2 masks\", printer=logger.debug, print_kwargs=[\"fpath\"])\ndef load_s2_masks(fpath: str | Path, reference_geobox: GeoBox) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Sentinel 2 scene.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n        reference_geobox (GeoBox): The reference geobox to reproject, resample and crop the masks data to.\n\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # TODO: SCL band in SR file\n    try:\n        scl_path = next(fpath.glob(\"*_SCL*.tif\"))\n    except StopIteration:\n        logger.warning(\"Found no data quality mask (SCL). No masking will occur.\")\n        valid_data_mask = (odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\") + 1).to_dataset(name=\"valid_data_mask\")\n        valid_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Valid Data Mask\"}\n        quality_data_mask = odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\").to_dataset(name=\"quality_data_mask\")\n        quality_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Quality Data Mask\"}\n        qa_ds = xr.merge([valid_data_mask, quality_data_mask])\n        return qa_ds\n\n    # See scene classes here: https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/scene-classification/\n    da_scl = xr.open_dataarray(scl_path)\n\n    da_scl = da_scl.odc.reproject(reference_geobox, sampling=\"nearest\")\n\n    # Match crs\n    da_scl = da_scl.rio.write_crs(reference_geobox.crs)\n\n    da_scl = xr.Dataset({\"scl\": da_scl.sel(band=1).fillna(0).drop_vars(\"band\").astype(\"uint8\")})\n    da_scl = convert_masks(da_scl)\n\n    return da_scl\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_s2_scene","title":"load_s2_scene","text":"<pre><code>load_s2_scene(fpath: str | pathlib.Path) -&gt; xarray.Dataset\n</code></pre> <p>Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Sentinel 2 scene from file\", printer=logger.debug, print_kwargs=True)\ndef load_s2_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading Sentinel 2 scene from {fpath.resolve()}\")\n\n    # Get imagepath\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n\n    # Define band names and corresponding indices\n    s2_da = xr.open_dataarray(s2_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_s2 = s2_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    planet_crop_id, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n    ds_s2.attrs[\"planet_crop_id\"] = planet_crop_id\n    ds_s2.attrs[\"s2_tile_id\"] = s2_tile_id\n    ds_s2.attrs[\"tile_id\"] = tile_id\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.load_tcvis","title":"load_tcvis","text":"<pre><code>load_tcvis(\n    geobox: odc.geo.geobox.GeoBox,\n    data_dir: pathlib.Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xarray.Dataset\n</code></pre> <p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox to load the data for.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory to store the downloaded data for faster access for consecutive calls.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The TCVIS dataset.</p> </li> </ul> Usage <p>Since the API of the <code>load_tcvis</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_tcvis\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\ntcvis = load_tcvis(\n    optical.odc.geobox,\n    \"/path/to/tcvis-parent-directory\",\n)\n\n# Now we can for example match the resolution and extent of the optical data:\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopuhr.funkuhr(\"Loading TCVIS\", printer=logger.debug, print_kwargs=True)\ndef load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the TCVIS for the given geobox, fetch new data from GEE if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox to load the data for.\n        data_dir (Path | str): The directory to store the downloaded data for faster access for consecutive calls.\n        buffer (int, optional): The buffer around the geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset.\n\n    Usage:\n        Since the API of the `load_tcvis` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_tcvis\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        tcvis = load_tcvis(\n            optical.odc.geobox,\n            \"/path/to/tcvis-parent-directory\",\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"  # noqa: E501\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n\n    # We want to assume that the datacube is already created to be save in a multi-process environment\n    accessor.assert_created()\n\n    tcvis = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Rename to follow our conventions\n    tcvis = tcvis.rename_vars(\n        {\n            \"TCB_slope\": \"tc_brightness\",\n            \"TCG_slope\": \"tc_greenness\",\n            \"TCW_slope\": \"tc_wetness\",\n        }\n    )\n\n    return tcvis\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.parse_planet_type","title":"parse_planet_type","text":"<pre><code>parse_planet_type(\n    fpath: pathlib.Path,\n) -&gt; typing.Literal[\"orthotile\", \"scene\"]\n</code></pre> <p>Parse the type of Planet data from the directory path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory path to the Planet data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['orthotile', 'scene']</code>           \u2013            <p>Literal[\"orthotile\", \"scene\"]: The type of Planet data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the Planet data type cannot be parsed from the file path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def parse_planet_type(fpath: Path) -&gt; Literal[\"orthotile\", \"scene\"]:\n    \"\"\"Parse the type of Planet data from the directory path.\n\n    Args:\n        fpath (Path): The directory path to the Planet data.\n\n    Returns:\n        Literal[\"orthotile\", \"scene\"]: The type of Planet data.\n\n    Raises:\n        ValueError: If the Planet data type cannot be parsed from the file path.\n\n    \"\"\"\n    # Cases for Scenes:\n    # - YYYYMMDD_HHMMSS_NN_XXXX\n    # - YYYYMMDD_HHMMSS_XXXX\n\n    # Cases for Orthotiles:\n    # NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n    # NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n\n    assert fpath.is_dir(), \"fpath must be the parent directory!\"\n\n    ps_name_parts = fpath.stem.split(\"_\")\n\n    if len(ps_name_parts) == 3:\n        # Must be scene or invalid\n        date, time, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and len(ident) == 4:\n            return \"scene\"\n\n    if len(ps_name_parts) == 4:\n        # Assume scene\n        date, time, n, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and n.isdigit() and len(ident) == 4:\n            return \"scene\"\n        # Is not scene, assume orthotile\n        chunkid, tileid, date, ident = ps_name_parts\n        if chunkid.isdigit() and tileid.isdigit() and _is_valid_date(date, \"%Y-%m-%d\") and len(ident) == 4:\n            return \"orthotile\"\n\n    raise ValueError(\n        f\"Could not parse Planet data type from {fpath}.\"\n        f\"Expected a format of YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX for scene, \"\n        \"or NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX or NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX for orthotile.\"\n        f\"Got {fpath.stem} instead.\"\n        \"Please ensure that the parent directory of the file is used, instead of the file itself.\"\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/#darts_acquisition.parse_s2_tile_id","title":"parse_s2_tile_id","text":"<pre><code>parse_s2_tile_id(\n    fpath: str | pathlib.Path,\n) -&gt; tuple[str, str, str]\n</code></pre> <p>Parse the Sentinel 2 tile ID from a file path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, str, str]</code>           \u2013            <p>tuple[str, str, str]: A tuple containing the Planet crop ID, the Sentinel 2 tile ID and the combined tile ID.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>def parse_s2_tile_id(fpath: str | Path) -&gt; tuple[str, str, str]:\n    \"\"\"Parse the Sentinel 2 tile ID from a file path.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        tuple[str, str, str]: A tuple containing the Planet crop ID, the Sentinel 2 tile ID and the combined tile ID.\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n    planet_crop_id = fpath.stem\n    s2_tile_id = \"_\".join(s2_image.stem.split(\"_\")[:3])\n    tile_id = f\"{planet_crop_id}_{s2_tile_id}\"\n    return planet_crop_id, s2_tile_id, tile_id\n</code></pre>"},{"location":"reference/darts_acquisition/download_admin_files/","title":"darts_acquisition.download_admin_files","text":"<p>Download the admin files for the regions.</p> <p>Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.</p> <p>Parameters:</p> <ul> <li> <code>admin_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the admin files.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/admin.py</code> <pre><code>@stopuhr.funkuhr(\"Downloading admin files\", printer=logger.debug, print_kwargs=True)\ndef download_admin_files(admin_dir: Path):\n    \"\"\"Download the admin files for the regions.\n\n    Files will be stored under [admin_dir]/adm1.shp and [admin_dir]/adm2.shp.\n\n    Args:\n        admin_dir (Path): The path to the admin files.\n\n    \"\"\"\n    # Download the admin files\n    admin_1_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM1.zip\"\n    admin_2_url = \"https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM2.zip\"\n\n    admin_dir.mkdir(exist_ok=True, parents=True)\n\n    logger.debug(f\"Downloading {admin_1_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_1_url, admin_dir)\n\n    logger.debug(f\"Downloading {admin_2_url} to {admin_dir.resolve()}\")\n    _download_zip(admin_2_url, admin_dir)\n</code></pre>"},{"location":"reference/darts_acquisition/load_arcticdem/","title":"darts_acquisition.load_arcticdem","text":"<p>Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox for which the tile should be loaded.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory where the ArcticDEM data is stored.</p> </li> <li> <code>resolution</code>               (<code>typing.Literal[2, 10, 32]</code>)           \u2013            <p>The resolution of the ArcticDEM data in m.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The ArcticDEM tile, with a buffer applied. Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different. Final dataset is NOT matched to the reference CRS and resolution.</p> </li> </ul> Warning <p>Geobox must be in a meter based CRS.</p> Usage <p>Since the API of the <code>load_arcticdem</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_arcticdem\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\narcticdem = load_arcticdem(\n    optical.odc.geobox,\n    \"/path/to/arcticdem-parent-directory\",\n    resolution=2,\n    buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n)\n\n# Now we can for example match the resolution and extent of the optical data:\narcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> <p>The <code>buffer</code> parameter is used to extend the region of interest by a certain amount of pixels. This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the resolution is not supported.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/arcticdem.py</code> <pre><code>@stopuhr.funkuhr(\"Loading ArcticDEM\", printer=logger.debug, print_kwargs=True)\ndef load_arcticdem(\n    geobox: GeoBox, data_dir: Path | str, resolution: RESOLUTIONS, buffer: int = 0, persist: bool = True\n) -&gt; xr.Dataset:\n    \"\"\"Load the ArcticDEM for the given geobox, fetch new data from the STAC server if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox for which the tile should be loaded.\n        data_dir (Path | str): The directory where the ArcticDEM data is stored.\n        resolution (Literal[2, 10, 32]): The resolution of the ArcticDEM data in m.\n        buffer (int, optional): The buffer around the projected (epsg:3413) geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The ArcticDEM tile, with a buffer applied.\n            Note: The buffer is applied in the arcticdem dataset's CRS, hence the orientation might be different.\n            Final dataset is NOT matched to the reference CRS and resolution.\n\n    Warning:\n        Geobox must be in a meter based CRS.\n\n    Usage:\n        Since the API of the `load_arcticdem` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_arcticdem\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        arcticdem = load_arcticdem(\n            optical.odc.geobox,\n            \"/path/to/arcticdem-parent-directory\",\n            resolution=2,\n            buffer=ceil(self.tpi_outer_radius / 2 * sqrt(2))\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        arcticdem = arcticdem.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n        The `buffer` parameter is used to extend the region of interest by a certain amount of pixels.\n        This comes handy when calculating e.g. the Topographic Position Index (TPI), which requires a buffer around the region of interest to remove edge effects.\n\n    Raises:\n        ValueError: If the resolution is not supported.\n\n    \"\"\"  # noqa: E501\n    odc.stac.configure_rio(cloud_defaults=True, aws={\"aws_unsigned\": True})\n\n    match resolution:\n        case 2:\n            accessor = smart_geocubes.ArcticDEM2m(data_dir)\n        case 10:\n            accessor = smart_geocubes.ArcticDEM10m(data_dir)\n        case 32:\n            accessor = smart_geocubes.ArcticDEM32m(data_dir)\n        case _:\n            raise ValueError(f\"Resolution {resolution} not supported, only 2m, 10m and 32m are supported\")\n\n    accessor.assert_created()\n\n    arcticdem = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Change dtype of the datamask to uint8 for later reproject_match\n    arcticdem[\"datamask\"] = arcticdem.datamask.astype(\"uint8\")\n\n    return arcticdem\n</code></pre>"},{"location":"reference/darts_acquisition/load_planet_masks/","title":"darts_acquisition.load_planet_masks","text":"<p>Load the valid and quality data masks from a Planet scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The file path to the Planet scene from which to derive the masks.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching UDM-2 TIFF file is found in the specified path.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Planet masks\", printer=logger.debug, print_kwargs=True)\ndef load_planet_masks(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Planet scene.\n\n    Args:\n        fpath (str | Path): The file path to the Planet scene from which to derive the masks.\n\n    Raises:\n        FileNotFoundError: If no matching UDM-2 TIFF file is found in the specified path.\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # Get imagepath\n    udm_path = next(fpath.glob(\"*_udm2.tif\"), None)\n    if not udm_path:\n        udm_path = next(fpath.glob(\"*_udm2_clip.tif\"), None)\n    if not udm_path:\n        raise FileNotFoundError(f\"No matching UDM-2 TIFF files found in {fpath.resolve()} (.glob('*_udm2.tif'))\")\n\n    # See udm classes here: https://developers.planet.com/docs/data/udm-2/\n    da_udm = xr.open_dataarray(udm_path)\n\n    invalids = da_udm.sel(band=8).fillna(0) != 0\n    low_quality = da_udm.sel(band=[2, 3, 4, 5, 6]).max(axis=0) == 1\n    high_quality = ~low_quality &amp; ~invalids\n    qa_ds = xr.Dataset(coords={c: da_udm.coords[c] for c in da_udm.coords})\n    qa_ds[\"quality_data_mask\"] = (\n        xr.zeros_like(da_udm.sel(band=8)).where(invalids, 0).where(low_quality, 1).where(high_quality, 2)\n    )\n    qa_ds[\"quality_data_mask\"].attrs = {\n        \"data_source\": \"planet\",\n        \"long_name\": \"Quality data mask\",\n        \"description\": \"0 = Invalid, 1 = Low Quality, 2 = High Quality\",\n    }\n    return qa_ds\n</code></pre>"},{"location":"reference/darts_acquisition/load_planet_scene/","title":"darts_acquisition.load_planet_scene","text":"<p>Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files or a specific path to the TIFF file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Planet scene\", printer=logger.debug, print_kwargs=True)\ndef load_planet_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a PlanetScope satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files or a specific path to the TIFF file.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    # Check if the directory contains a PSOrthoTile or PSScene\n    planet_type = parse_planet_type(fpath)\n    logger.debug(f\"Loading Planet PS {planet_type.capitalize()} from {fpath.resolve()}\")\n\n    # Get imagepath\n    ps_image = next(fpath.glob(\"*_SR.tif\"), None)\n    if not ps_image:\n        ps_image = next(fpath.glob(\"*_SR_clip.tif\"), None)\n    if not ps_image:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR.tif'))\")\n\n    # Define band names and corresponding indices\n    planet_da = xr.open_dataarray(ps_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_planet = (\n        planet_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n    )\n    for var in ds_planet.variables:\n        ds_planet[var].assign_attrs(\n            {\n                \"long_name\": f\"PLANET {var.capitalize()}\",\n                \"data_source\": \"planet\",\n                \"planet_type\": planet_type,\n                \"units\": \"Reflectance\",\n            }\n        )\n    ds_planet.attrs = {\"tile_id\": fpath.parent.stem if planet_type == \"orthotile\" else fpath.stem}\n    return ds_planet\n</code></pre>"},{"location":"reference/darts_acquisition/load_s2_from_gee/","title":"darts_acquisition.load_s2_from_gee","text":"<p>Load a Sentinel 2 scene from Google Earth Engine and return it as an xarray dataset.</p> <p>Parameters:</p> <ul> <li> <code>img</code>               (<code>str | ee.Image</code>)           \u2013            <p>The Sentinel 2 image ID or the ee image object.</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>, default:                   <code>{'B2': 'blue', 'B3': 'green', 'B4': 'red', 'B8': 'nir'}</code> )           \u2013            <p>A mapping from bands to obtain. Will be renamed to the corresponding band names. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>scale_and_offset</code>               (<code>bool | tuple[float, float]</code>, default:                   <code>True</code> )           \u2013            <p>Whether to apply the scale and offset to the bands. If a tuple is provided, it will be used as the (<code>scale</code>, <code>offset</code>) values with <code>band * scale + offset</code>. If True, use the default values of <code>scale</code> = 0.0001 and <code>offset</code> = 0, taken from ee_extra. Defaults to True.</p> </li> <li> <code>cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cache directory. If None, no caching will be done. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Sentinel 2 scene from GEE\", printer=logger.debug, print_kwargs=[\"img\"])\ndef load_s2_from_gee(\n    img: str | ee.Image,\n    bands_mapping: dict = {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"},\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: Path | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 scene from Google Earth Engine and return it as an xarray dataset.\n\n    Args:\n        img (str | ee.Image): The Sentinel 2 image ID or the ee image object.\n        bands_mapping (dict[str, str], optional): A mapping from bands to obtain.\n            Will be renamed to the corresponding band names.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        scale_and_offset (bool | tuple[float, float], optional): Whether to apply the scale and offset to the bands.\n            If a tuple is provided, it will be used as the (`scale`, `offset`) values with `band * scale + offset`.\n            If True, use the default values of `scale` = 0.0001 and `offset` = 0, taken from ee_extra.\n            Defaults to True.\n        cache (Path | None, optional): The path to the cache directory. If None, no caching will be done.\n            Defaults to None.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    \"\"\"\n    if isinstance(img, str):\n        s2id = img\n        img = ee.Image(f\"COPERNICUS/S2_SR_HARMONIZED/{s2id}\")\n    else:\n        s2id = img.id().getInfo().split(\"/\")[-1]\n    logger.debug(f\"Loading Sentinel 2 tile {s2id=} from GEE\")\n\n    if \"SCL\" not in bands_mapping.keys():\n        bands_mapping[\"SCL\"] = \"scl\"\n\n    cache_file = None if cache is None else cache / f\"gee-s2srh-{s2id}-{''.join(bands_mapping.keys())}.nc\"\n    if cache_file is not None and cache_file.exists():\n        ds_s2 = xr.open_dataset(cache_file, engine=\"h5netcdf\").set_coords(\"spatial_ref\")\n        ds_s2.load()\n        logger.debug(f\"Loaded {s2id=} from cache.\")\n    else:\n        img = img.select(list(bands_mapping.keys()))\n        ds_s2 = xr.open_dataset(\n            img,\n            engine=\"ee\",\n            geometry=img.geometry(),\n            crs=img.select(0).projection().crs().getInfo(),\n            scale=10,\n        )\n        ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n        ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\").rename({\"X\": \"x\", \"Y\": \"y\"}).transpose(\"y\", \"x\")\n        ds_s2 = ds_s2.odc.assign_crs(ds_s2.attrs[\"crs\"])\n        logger.debug(\n            f\"Found dataset with shape {ds_s2.sizes} for tile {s2id=}.\"\n            \"Start downloading data from GEE. This may take a while.\"\n        )\n\n        with stopuhr.stopuhr(f\"Downloading data from GEE for {s2id=}\", printer=logger.debug):\n            ds_s2.load()\n            if cache_file is not None:\n                ds_s2.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2-gee\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    ds_s2 = convert_masks(ds_s2)\n\n    # For some reason, there are some spatially random nan values in the data, not only at the borders\n    # To workaround this, set all nan values to 0 and add this information to the quality_data_mask\n    # This workaround is quite computational expensive, but it works for now\n    # TODO: Find other solutions for this problem!\n    with stopuhr.stopuhr(f\"Fixing nan values in {s2id=}\", printer=logger.debug):\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[\"quality_data_mask\"] = xr.where(ds_s2[band].isnull(), 0, ds_s2[\"quality_data_mask\"])\n            ds_s2[band] = ds_s2[band].fillna(0)\n            # Turn real nan values (scl is nan) into invalid data\n            ds_s2[band] = ds_s2[band].where(~ds_s2[\"scl\"].isnull())\n\n    if scale_and_offset:\n        if isinstance(scale_and_offset, tuple):\n            scale, offset = scale_and_offset\n        else:\n            scale, offset = 0.0001, 0\n        logger.debug(f\"Applying {scale=} and {offset=} to {s2id=} optical data\")\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[band] = ds_s2[band] * scale + offset\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/load_s2_from_stac/","title":"darts_acquisition.load_s2_from_stac","text":"<p>Load a Sentinel 2 scene from the Copernicus STAC API and return it as an xarray dataset.</p> <p>Parameters:</p> <ul> <li> <code>s2id</code>               (<code>str</code>)           \u2013            <p>The Sentinel 2 image ID.</p> </li> <li> <code>bands_mapping</code>               (<code>dict[str, str]</code>, default:                   <code>{'B02_10m': 'blue', 'B03_10m': 'green', 'B04_10m': 'red', 'B08_10m': 'nir'}</code> )           \u2013            <p>A mapping from bands to obtain. Will be renamed to the corresponding band names. Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.</p> </li> <li> <code>scale_and_offset</code>               (<code>bool | tuple[float, float]</code>, default:                   <code>True</code> )           \u2013            <p>Whether to apply the scale and offset to the bands. If a tuple is provided, it will be used as the (<code>scale</code>, <code>offset</code>) values with <code>band * scale + offset</code>. If True, use the default values of <code>scale</code> = 0.0001 and <code>offset</code> = 0, taken from ee_extra. Defaults to True.</p> </li> <li> <code>cache</code>               (<code>pathlib.Path | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to the cache directory. If None, no caching will be done. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Sentinel 2 scene from STAC\", printer=logger.debug, print_kwargs=[\"s2id\"])\ndef load_s2_from_stac(\n    s2id: str,\n    bands_mapping: dict = {\"B02_10m\": \"blue\", \"B03_10m\": \"green\", \"B04_10m\": \"red\", \"B08_10m\": \"nir\"},\n    scale_and_offset: bool | tuple[float, float] = True,\n    cache: Path | None = None,\n) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 scene from the Copernicus STAC API and return it as an xarray dataset.\n\n    Args:\n        s2id (str): The Sentinel 2 image ID.\n        bands_mapping (dict[str, str], optional): A mapping from bands to obtain.\n            Will be renamed to the corresponding band names.\n            Defaults to {\"B2\": \"blue\", \"B3\": \"green\", \"B4\": \"red\", \"B8\": \"nir\"}.\n        scale_and_offset (bool | tuple[float, float], optional): Whether to apply the scale and offset to the bands.\n            If a tuple is provided, it will be used as the (`scale`, `offset`) values with `band * scale + offset`.\n            If True, use the default values of `scale` = 0.0001 and `offset` = 0, taken from ee_extra.\n            Defaults to True.\n        cache (Path | None, optional): The path to the cache directory. If None, no caching will be done.\n            Defaults to None.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    \"\"\"\n    if \"SCL_20m\" not in bands_mapping.keys():\n        bands_mapping[\"SCL_20m\"] = \"scl\"\n\n    catalog = Client.open(\"https://stac.dataspace.copernicus.eu/v1/\")\n    search = catalog.search(\n        collections=[\"sentinel-2-l2a\"],\n        ids=[s2id],\n    )\n\n    cache_file = None if cache is None else cache / f\"gee-s2srh-{s2id}-{''.join(bands_mapping.keys())}.nc\"\n    if cache_file is not None and cache_file.exists():\n        ds_s2 = xr.open_dataset(cache_file, engine=\"h5netcdf\").set_coords(\"spatial_ref\")\n        ds_s2.load()\n        logger.debug(f\"Loaded {s2id=} from cache.\")\n    else:\n        ds_s2 = xr.open_dataset(\n            search,\n            engine=\"stac\",\n            backend_kwargs={\"crs\": \"utm\", \"resolution\": 10, \"bands\": list(bands_mapping.keys())},\n        )\n        ds_s2.attrs[\"time\"] = str(ds_s2.time.values[0])\n        ds_s2 = ds_s2.isel(time=0).drop_vars(\"time\")\n        logger.debug(\n            f\"Found a dataset with shape {ds_s2.sizes} for tile {s2id=}.\"\n            \"Start downloading data from STAC. This may take a while.\"\n        )\n\n        with stopuhr.stopuhr(f\"Downloading data from STAC for {s2id=}\", printer=logger.debug):\n            # Need double loading since the first load transforms lazy-stac to dask and second actually downloads\n            ds_s2.load().load()\n            if cache_file is not None:\n                ds_s2.to_netcdf(cache_file, engine=\"h5netcdf\")\n\n    ds_s2 = ds_s2.rename_vars(bands_mapping)\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2-stac\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    ds_s2 = convert_masks(ds_s2)\n\n    if scale_and_offset:\n        if isinstance(scale_and_offset, tuple):\n            scale, offset = scale_and_offset\n        else:\n            scale, offset = 0.0001, 0\n        logger.debug(f\"Applying {scale=} and {offset=} to {s2id=} optical data\")\n        for band in set(bands_mapping.values()) - {\"scl\"}:\n            ds_s2[band] = ds_s2[band] * scale + offset\n\n    ds_s2.attrs[\"s2_tile_id\"] = s2id\n    ds_s2.attrs[\"tile_id\"] = s2id\n\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/load_s2_masks/","title":"darts_acquisition.load_s2_masks","text":"<p>Load the valid and quality data masks from a Sentinel 2 scene.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> <li> <code>reference_geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The reference geobox to reproject, resample and crop the masks data to.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A merged xarray Dataset containing two data masks: - 'valid_data_mask': A mask indicating valid (1) and no data (0). - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Sentinel 2 masks\", printer=logger.debug, print_kwargs=[\"fpath\"])\ndef load_s2_masks(fpath: str | Path, reference_geobox: GeoBox) -&gt; xr.Dataset:\n    \"\"\"Load the valid and quality data masks from a Sentinel 2 scene.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n        reference_geobox (GeoBox): The reference geobox to reproject, resample and crop the masks data to.\n\n\n    Returns:\n        xr.Dataset: A merged xarray Dataset containing two data masks:\n            - 'valid_data_mask': A mask indicating valid (1) and no data (0).\n            - 'quality_data_mask': A mask indicating high quality (1) and low quality (0).\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading data masks from {fpath.resolve()}\")\n\n    # TODO: SCL band in SR file\n    try:\n        scl_path = next(fpath.glob(\"*_SCL*.tif\"))\n    except StopIteration:\n        logger.warning(\"Found no data quality mask (SCL). No masking will occur.\")\n        valid_data_mask = (odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\") + 1).to_dataset(name=\"valid_data_mask\")\n        valid_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Valid Data Mask\"}\n        quality_data_mask = odc.geo.xr.xr_zeros(reference_geobox, dtype=\"uint8\").to_dataset(name=\"quality_data_mask\")\n        quality_data_mask.attrs = {\"data_source\": \"s2\", \"long_name\": \"Quality Data Mask\"}\n        qa_ds = xr.merge([valid_data_mask, quality_data_mask])\n        return qa_ds\n\n    # See scene classes here: https://custom-scripts.sentinel-hub.com/custom-scripts/sentinel-2/scene-classification/\n    da_scl = xr.open_dataarray(scl_path)\n\n    da_scl = da_scl.odc.reproject(reference_geobox, sampling=\"nearest\")\n\n    # Match crs\n    da_scl = da_scl.rio.write_crs(reference_geobox.crs)\n\n    da_scl = xr.Dataset({\"scl\": da_scl.sel(band=1).fillna(0).drop_vars(\"band\").astype(\"uint8\")})\n    da_scl = convert_masks(da_scl)\n\n    return da_scl\n</code></pre>"},{"location":"reference/darts_acquisition/load_s2_scene/","title":"darts_acquisition.load_s2_scene","text":"<p>Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The loaded dataset</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>@stopuhr.funkuhr(\"Loading Sentinel 2 scene from file\", printer=logger.debug, print_kwargs=True)\ndef load_s2_scene(fpath: str | Path) -&gt; xr.Dataset:\n    \"\"\"Load a Sentinel 2 satellite GeoTIFF file and return it as an xarray datset.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        xr.Dataset: The loaded dataset\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    # Convert to Path object if a string is provided\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n\n    logger.debug(f\"Loading Sentinel 2 scene from {fpath.resolve()}\")\n\n    # Get imagepath\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n\n    # Define band names and corresponding indices\n    s2_da = xr.open_dataarray(s2_image)\n\n    # Create a dataset with the bands\n    bands = [\"blue\", \"green\", \"red\", \"nir\"]\n    ds_s2 = s2_da.fillna(0).rio.write_nodata(0).astype(\"uint16\").assign_coords({\"band\": bands}).to_dataset(dim=\"band\")\n\n    for var in ds_s2.data_vars:\n        ds_s2[var].attrs[\"data_source\"] = \"s2\"\n        ds_s2[var].attrs[\"long_name\"] = f\"Sentinel 2 {var.capitalize()}\"\n        ds_s2[var].attrs[\"units\"] = \"Reflectance\"\n\n    planet_crop_id, s2_tile_id, tile_id = parse_s2_tile_id(fpath)\n    ds_s2.attrs[\"planet_crop_id\"] = planet_crop_id\n    ds_s2.attrs[\"s2_tile_id\"] = s2_tile_id\n    ds_s2.attrs[\"tile_id\"] = tile_id\n    return ds_s2\n</code></pre>"},{"location":"reference/darts_acquisition/load_tcvis/","title":"darts_acquisition.load_tcvis","text":"<p>Load the TCVIS for the given geobox, fetch new data from GEE if necessary.</p> <p>Parameters:</p> <ul> <li> <code>geobox</code>               (<code>odc.geo.geobox.GeoBox</code>)           \u2013            <p>The geobox to load the data for.</p> </li> <li> <code>data_dir</code>               (<code>pathlib.Path | str</code>)           \u2013            <p>The directory to store the downloaded data for faster access for consecutive calls.</p> </li> <li> <code>buffer</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The buffer around the geobox in pixels. Defaults to 0.</p> </li> <li> <code>persist</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If the data should be persisted in memory. If not, this will return a Dask backed Dataset. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The TCVIS dataset.</p> </li> </ul> Usage <p>Since the API of the <code>load_tcvis</code> is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:</p> <pre><code>import xarray as xr\nimport odc.geo.xr\n\nfrom darts_aquisition import load_tcvis\n\n# Assume \"optical\" is an already loaded s2 based dataarray\n\ntcvis = load_tcvis(\n    optical.odc.geobox,\n    \"/path/to/tcvis-parent-directory\",\n)\n\n# Now we can for example match the resolution and extent of the optical data:\ntcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n</code></pre> Source code in <code>darts-acquisition/src/darts_acquisition/tcvis.py</code> <pre><code>@stopuhr.funkuhr(\"Loading TCVIS\", printer=logger.debug, print_kwargs=True)\ndef load_tcvis(\n    geobox: GeoBox,\n    data_dir: Path | str,\n    buffer: int = 0,\n    persist: bool = True,\n) -&gt; xr.Dataset:\n    \"\"\"Load the TCVIS for the given geobox, fetch new data from GEE if necessary.\n\n    Args:\n        geobox (GeoBox): The geobox to load the data for.\n        data_dir (Path | str): The directory to store the downloaded data for faster access for consecutive calls.\n        buffer (int, optional): The buffer around the geobox in pixels. Defaults to 0.\n        persist (bool, optional): If the data should be persisted in memory.\n            If not, this will return a Dask backed Dataset. Defaults to True.\n\n    Returns:\n        xr.Dataset: The TCVIS dataset.\n\n    Usage:\n        Since the API of the `load_tcvis` is based on GeoBox, one can load a specific ROI based on an existing Xarray DataArray:\n\n        ```python\n        import xarray as xr\n        import odc.geo.xr\n\n        from darts_aquisition import load_tcvis\n\n        # Assume \"optical\" is an already loaded s2 based dataarray\n\n        tcvis = load_tcvis(\n            optical.odc.geobox,\n            \"/path/to/tcvis-parent-directory\",\n        )\n\n        # Now we can for example match the resolution and extent of the optical data:\n        tcvis = tcvis.odc.reproject(optical.odc.geobox, resampling=\"cubic\")\n        ```\n\n    \"\"\"  # noqa: E501\n    accessor = smart_geocubes.TCTrend(data_dir, create_icechunk_storage=False)\n\n    # We want to assume that the datacube is already created to be save in a multi-process environment\n    accessor.assert_created()\n\n    tcvis = accessor.load(geobox, buffer=buffer, persist=persist)\n\n    # Rename to follow our conventions\n    tcvis = tcvis.rename_vars(\n        {\n            \"TCB_slope\": \"tc_brightness\",\n            \"TCG_slope\": \"tc_greenness\",\n            \"TCW_slope\": \"tc_wetness\",\n        }\n    )\n\n    return tcvis\n</code></pre>"},{"location":"reference/darts_acquisition/parse_planet_type/","title":"darts_acquisition.parse_planet_type","text":"<p>Parse the type of Planet data from the directory path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory path to the Planet data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['orthotile', 'scene']</code>           \u2013            <p>Literal[\"orthotile\", \"scene\"]: The type of Planet data.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the Planet data type cannot be parsed from the file path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/planet.py</code> <pre><code>def parse_planet_type(fpath: Path) -&gt; Literal[\"orthotile\", \"scene\"]:\n    \"\"\"Parse the type of Planet data from the directory path.\n\n    Args:\n        fpath (Path): The directory path to the Planet data.\n\n    Returns:\n        Literal[\"orthotile\", \"scene\"]: The type of Planet data.\n\n    Raises:\n        ValueError: If the Planet data type cannot be parsed from the file path.\n\n    \"\"\"\n    # Cases for Scenes:\n    # - YYYYMMDD_HHMMSS_NN_XXXX\n    # - YYYYMMDD_HHMMSS_XXXX\n\n    # Cases for Orthotiles:\n    # NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n    # NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX\n\n    assert fpath.is_dir(), \"fpath must be the parent directory!\"\n\n    ps_name_parts = fpath.stem.split(\"_\")\n\n    if len(ps_name_parts) == 3:\n        # Must be scene or invalid\n        date, time, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and len(ident) == 4:\n            return \"scene\"\n\n    if len(ps_name_parts) == 4:\n        # Assume scene\n        date, time, n, ident = ps_name_parts\n        if _is_valid_date(date, \"%Y%m%d\") and _is_valid_date(time, \"%H%M%S\") and n.isdigit() and len(ident) == 4:\n            return \"scene\"\n        # Is not scene, assume orthotile\n        chunkid, tileid, date, ident = ps_name_parts\n        if chunkid.isdigit() and tileid.isdigit() and _is_valid_date(date, \"%Y-%m-%d\") and len(ident) == 4:\n            return \"orthotile\"\n\n    raise ValueError(\n        f\"Could not parse Planet data type from {fpath}.\"\n        f\"Expected a format of YYYYMMDD_HHMMSS_NN_XXXX or YYYYMMDD_HHMMSS_XXXX for scene, \"\n        \"or NNNNNNN/NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX or NNNNNNN_NNNNNNN_YYYY-MM-DD_XXXX for orthotile.\"\n        f\"Got {fpath.stem} instead.\"\n        \"Please ensure that the parent directory of the file is used, instead of the file itself.\"\n    )\n</code></pre>"},{"location":"reference/darts_acquisition/parse_s2_tile_id/","title":"darts_acquisition.parse_s2_tile_id","text":"<p>Parse the Sentinel 2 tile ID from a file path.</p> <p>Parameters:</p> <ul> <li> <code>fpath</code>               (<code>str | pathlib.Path</code>)           \u2013            <p>The path to the directory containing the TIFF files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, str, str]</code>           \u2013            <p>tuple[str, str, str]: A tuple containing the Planet crop ID, the Sentinel 2 tile ID and the combined tile ID.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>FileNotFoundError</code>             \u2013            <p>If no matching TIFF file is found in the specified path.</p> </li> </ul> Source code in <code>darts-acquisition/src/darts_acquisition/s2.py</code> <pre><code>def parse_s2_tile_id(fpath: str | Path) -&gt; tuple[str, str, str]:\n    \"\"\"Parse the Sentinel 2 tile ID from a file path.\n\n    Args:\n        fpath (str | Path): The path to the directory containing the TIFF files.\n\n    Returns:\n        tuple[str, str, str]: A tuple containing the Planet crop ID, the Sentinel 2 tile ID and the combined tile ID.\n\n    Raises:\n        FileNotFoundError: If no matching TIFF file is found in the specified path.\n\n    \"\"\"\n    fpath = fpath if isinstance(fpath, Path) else Path(fpath)\n    try:\n        s2_image = next(fpath.glob(\"*_SR*.tif\"))\n    except StopIteration:\n        raise FileNotFoundError(f\"No matching TIFF files found in {fpath.resolve()} (.glob('*_SR*.tif'))\")\n    planet_crop_id = fpath.stem\n    s2_tile_id = \"_\".join(s2_image.stem.split(\"_\")[:3])\n    tile_id = f\"{planet_crop_id}_{s2_tile_id}\"\n    return planet_crop_id, s2_tile_id, tile_id\n</code></pre>"},{"location":"reference/darts_ensemble/","title":"darts_ensemble","text":"<p>Inference and model ensembling for the DARTS dataset.</p> <p>Classes:</p> <ul> <li> <code>EnsembleV1</code>           \u2013            <p>DARTS v1 ensemble based on a list of models.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>__version__</code>           \u2013            </li> </ul> <ul> <li>EnsembleV1</li> </ul>"},{"location":"reference/darts_ensemble/#darts_ensemble.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1","title":"EnsembleV1","text":"<pre><code>EnsembleV1(\n    model_dict,\n    device: torch.device = darts_ensemble.ensemble_v1.DEFAULT_DEVICE,\n)\n</code></pre> <p>DARTS v1 ensemble based on a list of models.</p> <p>Initialize the ensemble.</p> <p>Parameters:</p> <ul> <li> <code>model_dict</code>               (<code>dict</code>)           \u2013            <p>The paths to model checkpoints to ensemble, the key is should be a model identifier to be written to outputs.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_ensemble.ensemble_v1.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Run the ensemble on the given tile.</p> </li> <li> <code>segment_tile</code>             \u2013              <p>Run inference on a tile.</p> </li> <li> <code>segment_tile_batched</code>             \u2013              <p>Run inference on a list of tiles.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>models</code>           \u2013            </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __init__(\n    self,\n    model_dict,\n    device: torch.device = DEFAULT_DEVICE,\n):\n    \"\"\"Initialize the ensemble.\n\n    Args:\n        model_dict (dict): The paths to model checkpoints to ensemble, the key is should be a model identifier\n            to be written to outputs.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_paths = {k: Path(v) for k, v in model_dict.items()}\n    logger.debug(\n        \"Loading models:\\n\" + \"\\n\".join([f\" - {k.upper()} model: {v.resolve()}\" for k, v in model_paths.items()])\n    )\n    self.models = {k: SMPSegmenter(v, device=device) for k, v in model_paths.items()}\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = {\n    k: darts_segmentation.segment.SMPSegmenter(\n        v,\n        device=darts_ensemble.ensemble_v1.EnsembleV1(\n            device\n        ),\n    )\n    for (k, v) in model_paths.items()\n}\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run the ensemble on the given tile.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile with the ensemble applied.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run the ensemble on the given tile.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        xr.Dataset: Output tile with the ensemble applied.\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    else:\n        raise ValueError(\"Input must be an xr.Dataset or a list of xr.Dataset.\")\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run inference on a tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>@stopuhr.funkuhr(\n    \"Ensemble inference\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\", \"keep_inputs\"],\n)\ndef segment_tile(\n    self,\n    tile: xr.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    probabilities = {}\n    for model_name, model in self.models.items():\n        probabilities[model_name] = model.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )[\"probabilities\"].copy()\n\n    # calculate the mean\n    tile[\"probabilities\"] = xr.concat(probabilities.values(), dim=\"model_probs\").mean(dim=\"model_probs\")\n\n    if keep_inputs:\n        for k, v in probabilities.items():\n            tile[f\"probabilities-{k}\"] = v\n\n    return tile\n</code></pre>"},{"location":"reference/darts_ensemble/#darts_ensemble.EnsembleV1.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    return [\n        self.segment_tile(\n            tile,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n        for tile in tiles\n    ]\n</code></pre>"},{"location":"reference/darts_ensemble/EnsembleV1/","title":"darts_ensemble.EnsembleV1","text":"<p>DARTS v1 ensemble based on a list of models.</p> <p>Initialize the ensemble.</p> <p>Parameters:</p> <ul> <li> <code>model_dict</code>               (<code>dict</code>)           \u2013            <p>The paths to model checkpoints to ensemble, the key is should be a model identifier to be written to outputs.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_ensemble.ensemble_v1.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __init__(\n    self,\n    model_dict,\n    device: torch.device = DEFAULT_DEVICE,\n):\n    \"\"\"Initialize the ensemble.\n\n    Args:\n        model_dict (dict): The paths to model checkpoints to ensemble, the key is should be a model identifier\n            to be written to outputs.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_paths = {k: Path(v) for k, v in model_dict.items()}\n    logger.debug(\n        \"Loading models:\\n\" + \"\\n\".join([f\" - {k.upper()} model: {v.resolve()}\" for k, v in model_paths.items()])\n    )\n    self.models = {k: SMPSegmenter(v, device=device) for k, v in model_paths.items()}\n</code></pre>"},{"location":"reference/darts_ensemble/EnsembleV1/#darts_ensemble.EnsembleV1.models","title":"models  <code>instance-attribute</code>","text":"<pre><code>models = {\n    k: darts_segmentation.segment.SMPSegmenter(\n        v,\n        device=darts_ensemble.ensemble_v1.EnsembleV1(\n            device\n        ),\n    )\n    for (k, v) in model_paths.items()\n}\n</code></pre>"},{"location":"reference/darts_ensemble/EnsembleV1/#darts_ensemble.EnsembleV1.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run the ensemble on the given tile.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile with the ensemble applied.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run the ensemble on the given tile.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        xr.Dataset: Output tile with the ensemble applied.\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n    else:\n        raise ValueError(\"Input must be an xr.Dataset or a list of xr.Dataset.\")\n</code></pre>"},{"location":"reference/darts_ensemble/EnsembleV1/#darts_ensemble.EnsembleV1.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run inference on a tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>@stopuhr.funkuhr(\n    \"Ensemble inference\",\n    printer=logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\", \"keep_inputs\"],\n)\ndef segment_tile(\n    self,\n    tile: xr.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    probabilities = {}\n    for model_name, model in self.models.items():\n        probabilities[model_name] = model.segment_tile(\n            tile, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )[\"probabilities\"].copy()\n\n    # calculate the mean\n    tile[\"probabilities\"] = xr.concat(probabilities.values(), dim=\"model_probs\").mean(dim=\"model_probs\")\n\n    if keep_inputs:\n        for k, v in probabilities.items():\n            tile[f\"probabilities-{k}\"] = v\n\n    return tile\n</code></pre>"},{"location":"reference/darts_ensemble/EnsembleV1/#darts_ensemble.EnsembleV1.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> <li> <code>keep_inputs</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to keep the input probabilities in the output. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-ensemble/src/darts_ensemble/ensemble_v1.py</code> <pre><code>def segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n    keep_inputs: bool = False,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n        keep_inputs (bool, optional): Whether to keep the input probabilities in the output. Defaults to False.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    return [\n        self.segment_tile(\n            tile,\n            patch_size=patch_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            reflection=reflection,\n            keep_inputs=keep_inputs,\n        )\n        for tile in tiles\n    ]\n</code></pre>"},{"location":"reference/darts_export/","title":"darts_export","text":"<p>Dataset export for the DARTS dataset.</p> <p>Functions:</p> <ul> <li> <code>export_tile</code>             \u2013              <p>Export a tile to a file.</p> </li> <li> <code>missing_outputs</code>             \u2013              <p>Check for missing output files in the given directory.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>__version__</code>           \u2013            </li> </ul> <ul> <li>export_tile</li> <li>missing_outputs</li> </ul>"},{"location":"reference/darts_export/#darts_export.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_export/#darts_export.export_tile","title":"export_tile","text":"<pre><code>export_tile(\n    tile: xarray.Dataset,\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n)\n</code></pre> <p>Export a tile to a file.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The tile to export.</p> </li> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path where to export to.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the band is not found in the tile.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>@stopuhr.funkuhr(\"Exporting tile\", logger.debug, print_kwargs=[\"bands\", \"ensemble_subsets\"])\ndef export_tile(  # noqa: C901\n    tile: xr.Dataset,\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n):\n    \"\"\"Export a tile to a file.\n\n    Args:\n        tile (xr.Dataset): The tile to export.\n        out_dir (Path): The path where to export to.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Raises:\n        ValueError: If the band is not found in the tile.\n\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                _export_polygonized(tile, out_dir, ensemble_subsets)\n            case \"binarized\":\n                _export_binarized(tile, out_dir, ensemble_subsets)\n            case \"probabilities\":\n                _export_probabilities(tile, out_dir, ensemble_subsets)\n            case \"extent\":\n                _export_vector(tile, \"extent\", out_dir, fname=\"prediction_extent\")\n            case \"thumbnail\":\n                _export_thumbnail(tile, out_dir)\n            case \"optical\":\n                _export_raster(tile, [\"red\", \"green\", \"blue\", \"nir\"], out_dir, fname=\"optical\")\n            case \"dem\":\n                _export_raster(tile, [\"slope\", \"relative_elevation\"], out_dir, fname=\"dem\")\n            case \"tcvis\":\n                _export_raster(tile, [\"tc_brightness\", \"tc_greenness\", \"tc_wetness\"], out_dir, fname=\"tcvis\")\n            case _:\n                if band not in tile.data_vars:\n                    raise ValueError(\n                        f\"Band {band} not found in tile for export. Available bands are: {list(tile.data_vars.keys())}\"\n                    )\n                # Export the band as a raster\n                _export_raster(tile, band, out_dir)\n</code></pre>"},{"location":"reference/darts_export/#darts_export.missing_outputs","title":"missing_outputs","text":"<pre><code>missing_outputs(\n    out_dir: pathlib.Path,\n    bands: list[str] = [\n        \"probabilities\",\n        \"binarized\",\n        \"polygonized\",\n        \"extent\",\n        \"thumbnail\",\n    ],\n    ensemble_subsets: list[str] = [],\n) -&gt; typing.Literal[\"all\", \"some\", \"none\"]\n</code></pre> <p>Check for missing output files in the given directory.</p> <p>Parameters:</p> <ul> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to check for missing files.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['all', 'some', 'none']</code>           \u2013            <p>Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files: - \"none\": No files are missing. - \"some\": Some files are missing, which one will be logged to debug. - \"all\": All files are missing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the output path is not a directory.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/check.py</code> <pre><code>def missing_outputs(  # noqa: C901\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n) -&gt; Literal[\"all\", \"some\", \"none\"]:\n    \"\"\"Check for missing output files in the given directory.\n\n    Args:\n        out_dir (Path): The directory to check for missing files.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Returns:\n        Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files:\n            - \"none\": No files are missing.\n            - \"some\": Some files are missing, which one will be logged to debug.\n            - \"all\": All files are missing.\n\n    Raises:\n        ValueError: If the output path is not a directory.\n\n    \"\"\"\n    if not out_dir.exists():\n        return []\n    if not out_dir.is_dir():\n        raise ValueError(f\"Output path {out_dir} is not a directory.\")\n    expected_files = []\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                expected_files += [\"prediction_segments.gpkg\"] + [\n                    f\"prediction_segments-{es}.gpkg\" for es in ensemble_subsets\n                ]\n                expected_files += [\"prediction_segments.parquet\"] + [\n                    f\"prediction_segments-{es}.parquet\" for es in ensemble_subsets\n                ]\n            case \"binarized\":\n                expected_files += [\"binarized.tif\"] + [f\"binarized-{es}.tif\" for es in ensemble_subsets]\n            case \"probabilities\":\n                expected_files += [\"probabilities.tif\"] + [f\"probabilities-{es}.tif\" for es in ensemble_subsets]\n            case \"extent\":\n                expected_files += [\"extent.gpkg\", \"extent.parquet\"]\n            case \"thumbnail\":\n                expected_files += [\"thumbnail.jpg\"]\n            case _:\n                expected_files += [f\"{band}.tif\"]\n\n    missing_files = _missing_files(out_dir, expected_files)\n    if len(missing_files) == 0:\n        return \"none\"\n    elif len(missing_files) == len(expected_files):\n        return \"all\"\n    else:\n        logger.debug(\n            f\"Missing files in {out_dir}: {', '.join(missing_files)}. Expected files: {', '.join(expected_files)}.\"\n        )\n        return \"some\"\n</code></pre>"},{"location":"reference/darts_export/export_tile/","title":"darts_export.export_tile","text":"<p>Export a tile to a file.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The tile to export.</p> </li> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path where to export to.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the band is not found in the tile.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/export.py</code> <pre><code>@stopuhr.funkuhr(\"Exporting tile\", logger.debug, print_kwargs=[\"bands\", \"ensemble_subsets\"])\ndef export_tile(  # noqa: C901\n    tile: xr.Dataset,\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n):\n    \"\"\"Export a tile to a file.\n\n    Args:\n        tile (xr.Dataset): The tile to export.\n        out_dir (Path): The path where to export to.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Raises:\n        ValueError: If the band is not found in the tile.\n\n    \"\"\"\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                _export_polygonized(tile, out_dir, ensemble_subsets)\n            case \"binarized\":\n                _export_binarized(tile, out_dir, ensemble_subsets)\n            case \"probabilities\":\n                _export_probabilities(tile, out_dir, ensemble_subsets)\n            case \"extent\":\n                _export_vector(tile, \"extent\", out_dir, fname=\"prediction_extent\")\n            case \"thumbnail\":\n                _export_thumbnail(tile, out_dir)\n            case \"optical\":\n                _export_raster(tile, [\"red\", \"green\", \"blue\", \"nir\"], out_dir, fname=\"optical\")\n            case \"dem\":\n                _export_raster(tile, [\"slope\", \"relative_elevation\"], out_dir, fname=\"dem\")\n            case \"tcvis\":\n                _export_raster(tile, [\"tc_brightness\", \"tc_greenness\", \"tc_wetness\"], out_dir, fname=\"tcvis\")\n            case _:\n                if band not in tile.data_vars:\n                    raise ValueError(\n                        f\"Band {band} not found in tile for export. Available bands are: {list(tile.data_vars.keys())}\"\n                    )\n                # Export the band as a raster\n                _export_raster(tile, band, out_dir)\n</code></pre>"},{"location":"reference/darts_export/missing_outputs/","title":"darts_export.missing_outputs","text":"<p>Check for missing output files in the given directory.</p> <p>Parameters:</p> <ul> <li> <code>out_dir</code>               (<code>pathlib.Path</code>)           \u2013            <p>The directory to check for missing files.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>, default:                   <code>['probabilities', 'binarized', 'polygonized', 'extent', 'thumbnail']</code> )           \u2013            <p>The bands to export. Defaults to [\"probabilities\"].</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to export. Defaults to [].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>typing.Literal['all', 'some', 'none']</code>           \u2013            <p>Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files: - \"none\": No files are missing. - \"some\": Some files are missing, which one will be logged to debug. - \"all\": All files are missing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the output path is not a directory.</p> </li> </ul> Source code in <code>darts-export/src/darts_export/check.py</code> <pre><code>def missing_outputs(  # noqa: C901\n    out_dir: Path,\n    bands: list[str] = [\"probabilities\", \"binarized\", \"polygonized\", \"extent\", \"thumbnail\"],\n    ensemble_subsets: list[str] = [],\n) -&gt; Literal[\"all\", \"some\", \"none\"]:\n    \"\"\"Check for missing output files in the given directory.\n\n    Args:\n        out_dir (Path): The directory to check for missing files.\n        bands (list[str], optional): The bands to export. Defaults to [\"probabilities\"].\n        ensemble_subsets (list[str], optional): The ensemble subsets to export. Defaults to [].\n\n    Returns:\n        Literal[\"all\", \"some\", \"none\"]: A string indicating the status of missing files:\n            - \"none\": No files are missing.\n            - \"some\": Some files are missing, which one will be logged to debug.\n            - \"all\": All files are missing.\n\n    Raises:\n        ValueError: If the output path is not a directory.\n\n    \"\"\"\n    if not out_dir.exists():\n        return []\n    if not out_dir.is_dir():\n        raise ValueError(f\"Output path {out_dir} is not a directory.\")\n    expected_files = []\n    for band in bands:\n        match band:\n            case \"polygonized\":\n                expected_files += [\"prediction_segments.gpkg\"] + [\n                    f\"prediction_segments-{es}.gpkg\" for es in ensemble_subsets\n                ]\n                expected_files += [\"prediction_segments.parquet\"] + [\n                    f\"prediction_segments-{es}.parquet\" for es in ensemble_subsets\n                ]\n            case \"binarized\":\n                expected_files += [\"binarized.tif\"] + [f\"binarized-{es}.tif\" for es in ensemble_subsets]\n            case \"probabilities\":\n                expected_files += [\"probabilities.tif\"] + [f\"probabilities-{es}.tif\" for es in ensemble_subsets]\n            case \"extent\":\n                expected_files += [\"extent.gpkg\", \"extent.parquet\"]\n            case \"thumbnail\":\n                expected_files += [\"thumbnail.jpg\"]\n            case _:\n                expected_files += [f\"{band}.tif\"]\n\n    missing_files = _missing_files(out_dir, expected_files)\n    if len(missing_files) == 0:\n        return \"none\"\n    elif len(missing_files) == len(expected_files):\n        return \"all\"\n    else:\n        logger.debug(\n            f\"Missing files in {out_dir}: {', '.join(missing_files)}. Expected files: {', '.join(expected_files)}.\"\n        )\n        return \"some\"\n</code></pre>"},{"location":"reference/darts_postprocessing/","title":"darts_postprocessing","text":"<p>Postprocessing steps for the DARTS dataset.</p> <p>Functions:</p> <ul> <li> <code>binarize</code>             \u2013              <p>Binarize the probabilities based on a threshold and a mask.</p> </li> <li> <code>erode_mask</code>             \u2013              <p>Erode the mask, also set the edges to invalid.</p> </li> <li> <code>prepare_export</code>             \u2013              <p>Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>__version__</code>           \u2013            </li> </ul> <ul> <li>binarize</li> <li>erode_mask</li> <li>prepare_export</li> </ul>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.binarize","title":"binarize","text":"<pre><code>binarize(\n    probs: xarray.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xarray.DataArray,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.DataArray\n</code></pre> <p>Binarize the probabilities based on a threshold and a mask.</p> Steps for binarization <ol> <li>Dilate the mask. This will dilate the edges of holes in the mask as well as the edges of the tile.</li> <li>Binarize the probabilities based on the threshold.</li> <li>Remove objects at which overlap with either the edge of the tile or the noData mask.</li> <li>Remove small objects.</li> </ol> <p>Parameters:</p> <ul> <li> <code>probs</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Probabilities to binarize.</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Threshold to binarize the probabilities.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>)           \u2013            <p>Minimum object size to keep.</p> </li> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Mask to apply to the binarized probabilities. Expects 0=negative, 1=postitive.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for removing small objects.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Binarized probabilities.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopuhr.funkuhr(\"Binarizing probabilities\", printer=logger.debug, print_kwargs=[\"threshold\", \"min_object_size\"])\ndef binarize(\n    probs: xr.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xr.DataArray,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xr.DataArray:\n    \"\"\"Binarize the probabilities based on a threshold and a mask.\n\n    Steps for binarization:\n        1. Dilate the mask. This will dilate the edges of holes in the mask as well as the edges of the tile.\n        2. Binarize the probabilities based on the threshold.\n        3. Remove objects at which overlap with either the edge of the tile or the noData mask.\n        4. Remove small objects.\n\n    Args:\n        probs (xr.DataArray): Probabilities to binarize.\n        threshold (float): Threshold to binarize the probabilities.\n        min_object_size (int): Minimum object size to keep.\n        mask (xr.DataArray): Mask to apply to the binarized probabilities. Expects 0=negative, 1=postitive.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for removing small objects.\n\n    Returns:\n        xr.DataArray: Binarized probabilities.\n\n    \"\"\"\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Where the output from the ensemble / segmentation is nan turn it into 0, else threshold it\n    # Also, where there was no valid input data, turn it into 0\n    binarized = (probs.fillna(0) &gt; threshold).astype(\"uint8\")\n\n    # Remove objects at which overlap with either the edge of the tile or the noData mask\n    labels = binarized.copy(data=label(binarized, connectivity=2))\n    edge_label_ids = np.unique(xr.where(~mask, labels, 0))\n    binarized = ~labels.isin(edge_label_ids) &amp; binarized\n\n    # Remove small objects with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving binarized to GPU:{device}.\")\n        # Check if binarized is dask, if not persist it, since remove_small_objects_gpu can't be calculated from\n        # cupy-dask arrays\n        if binarized.chunks is not None:\n            binarized = binarized.persist()\n        with cp.cuda.Device(device_nr):\n            binarized = binarized.cupy.as_cupy()\n            binarized.values = remove_small_objects_gpu(\n                binarized.astype(bool).expand_dims(\"batch\", 0).data, min_size=min_object_size\n            )[0]\n            binarized = binarized.cupy.as_numpy()\n            free_cupy()\n    else:\n        binarized.values = remove_small_objects(\n            binarized.astype(bool).expand_dims(\"batch\", 0).values, min_size=min_object_size\n        )[0]\n\n    # Convert back to int8\n    binarized = binarized.astype(\"uint8\")\n\n    return binarized\n</code></pre>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.erode_mask","title":"erode_mask","text":"<pre><code>erode_mask(\n    mask: xarray.DataArray,\n    size: int,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xarray.DataArray\n</code></pre> <p>Erode the mask, also set the edges to invalid.</p> <p>Parameters:</p> <ul> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>The mask to erode.</p> </li> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The size of the disk to use for erosion and the edge-cropping.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for erosion.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: The dilated and inverted mask.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopuhr.funkuhr(\"Eroding mask\", printer=logger.debug, print_kwargs=[\"size\"])\ndef erode_mask(mask: xr.DataArray, size: int, device: Literal[\"cuda\", \"cpu\"] | int) -&gt; xr.DataArray:\n    \"\"\"Erode the mask, also set the edges to invalid.\n\n    Args:\n        mask (xr.DataArray): The mask to erode.\n        size (int): The size of the disk to use for erosion and the edge-cropping.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for erosion.\n\n    Returns:\n        xr.DataArray: The dilated and inverted mask.\n\n    \"\"\"\n    # Clone mask to avoid in-place operations\n    mask = mask.copy()\n\n    # Change to dtype uint8 for faster skimage operations\n    mask = mask.astype(\"uint8\")\n\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Dilate the mask with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving mask to GPU:{device}.\")\n        # Check if mask is dask, if not persist it, since dilation can't be calculated from cupy-dask arrays\n        if mask.chunks is not None:\n            mask = mask.persist()\n        with cp.cuda.Device(device_nr):\n            mask = mask.cupy.as_cupy()\n            mask.values = binary_erosion_gpu(mask.data, disk_gpu(size))\n            mask = mask.cupy.as_numpy()\n            free_cupy()\n    else:\n        mask.values = binary_erosion(mask.values, disk(size))\n\n    # Mask edges\n    mask[:size, :] = 0\n    mask[-size:, :] = 0\n    mask[:, :size] = 0\n    mask[:, -size:] = 0\n\n    return mask\n</code></pre>"},{"location":"reference/darts_postprocessing/#darts_postprocessing.prepare_export","title":"prepare_export","text":"<pre><code>prepare_export(\n    tile: xarray.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int\n    | typing.Literal[\n        \"high_quality\", \"low_quality\", \"none\"\n    ] = 0,\n    ensemble_subsets: list[str] = [],\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_postprocessing.postprocess.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Input tile from inference and / or an ensemble.</p> </li> <li> <code>bin_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | str</code>, default:                   <code>0</code> )           \u2013            <p>The quality level to use for the mask. If a string maps to int. high_quality -&gt; 2, low_quality=1, none=0 (apply no masking). Defaults to 0.</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to use for the binarization. Defaults to [].</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_postprocessing.postprocess.DEFAULT_DEVICE</code> )           \u2013            <p>The device to use for dilation. Defaults to \"cuda\" if cuda for cucim is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopuhr.funkuhr(\n    \"Preparing export\",\n    printer=logger.debug,\n    print_kwargs=[\"bin_threshold\", \"mask_erosion_size\", \"min_object_size\", \"quality_level\", \"ensemble_subsets\"],\n)\ndef prepare_export(\n    tile: xr.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int | Literal[\"high_quality\", \"low_quality\", \"none\"] = 0,\n    ensemble_subsets: list[str] = [],\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.\n\n    Args:\n        tile (xr.Dataset): Input tile from inference and / or an ensemble.\n        bin_threshold (float, optional): The threshold to binarize the probabilities. Defaults to 0.5.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        min_object_size (int, optional): The minimum object size to keep in pixel. Defaults to 32.\n        quality_level (int | str, optional): The quality level to use for the mask. If a string maps to int.\n            high_quality -&gt; 2, low_quality=1, none=0 (apply no masking). Defaults to 0.\n        ensemble_subsets (list[str], optional): The ensemble subsets to use for the binarization.\n            Defaults to [].\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to use for dilation.\n            Defaults to \"cuda\" if cuda for cucim is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: Output tile.\n\n    \"\"\"\n    quality_level = (\n        quality_level\n        if isinstance(quality_level, int)\n        else {\"high_quality\": 2, \"low_quality\": 1, \"none\": 0}[quality_level]\n    )\n    mask = tile[\"quality_data_mask\"] &gt;= quality_level\n    if quality_level &gt; 0:\n        mask = erode_mask(mask, mask_erosion_size, device)  # 0=positive, 1=negative\n    tile[\"extent\"] = mask.copy()\n    tile[\"extent\"].attrs = {\n        \"long_name\": \"Extent of the segmentation\",\n    }\n\n    def _prep_layer(tile, layername, binarized_layer_name):\n        # Binarize the segmentation\n        tile[binarized_layer_name] = binarize(tile[layername], bin_threshold, min_object_size, mask, device)\n        tile[binarized_layer_name].attrs = {\n            \"long_name\": \"Binarized Segmentation\",\n        }\n\n        # Convert the probabilities to uint8\n        # Same but this time with 255 as no-data\n        # But first check if this step was already run\n        if tile[layername].max() &gt; 1:\n            return tile\n\n        intprobs = (tile[layername] * 100).fillna(255).astype(\"uint8\")\n        tile[layername] = xr.where(mask, intprobs, 255)\n        tile[layername].attrs = {\n            \"long_name\": \"Probabilities\",\n            \"units\": \"%\",\n        }\n        tile[layername] = tile[layername].rio.write_nodata(255)\n        return tile\n\n    tile = _prep_layer(tile, \"probabilities\", \"binarized_segmentation\")\n\n    # get the names of the model probabilities if available\n    # for example 'tcvis' from 'probabilities-tcvis'\n    for ensemble_subset in ensemble_subsets:\n        tile = _prep_layer(tile, f\"probabilities-{ensemble_subset}\", f\"binarized_segmentation-{ensemble_subset}\")\n\n    return tile\n</code></pre>"},{"location":"reference/darts_postprocessing/binarize/","title":"darts_postprocessing.binarize","text":"<p>Binarize the probabilities based on a threshold and a mask.</p> Steps for binarization <ol> <li>Dilate the mask. This will dilate the edges of holes in the mask as well as the edges of the tile.</li> <li>Binarize the probabilities based on the threshold.</li> <li>Remove objects at which overlap with either the edge of the tile or the noData mask.</li> <li>Remove small objects.</li> </ol> <p>Parameters:</p> <ul> <li> <code>probs</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Probabilities to binarize.</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>Threshold to binarize the probabilities.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>)           \u2013            <p>Minimum object size to keep.</p> </li> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>Mask to apply to the binarized probabilities. Expects 0=negative, 1=postitive.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for removing small objects.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: Binarized probabilities.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopuhr.funkuhr(\"Binarizing probabilities\", printer=logger.debug, print_kwargs=[\"threshold\", \"min_object_size\"])\ndef binarize(\n    probs: xr.DataArray,\n    threshold: float,\n    min_object_size: int,\n    mask: xr.DataArray,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n) -&gt; xr.DataArray:\n    \"\"\"Binarize the probabilities based on a threshold and a mask.\n\n    Steps for binarization:\n        1. Dilate the mask. This will dilate the edges of holes in the mask as well as the edges of the tile.\n        2. Binarize the probabilities based on the threshold.\n        3. Remove objects at which overlap with either the edge of the tile or the noData mask.\n        4. Remove small objects.\n\n    Args:\n        probs (xr.DataArray): Probabilities to binarize.\n        threshold (float): Threshold to binarize the probabilities.\n        min_object_size (int): Minimum object size to keep.\n        mask (xr.DataArray): Mask to apply to the binarized probabilities. Expects 0=negative, 1=postitive.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for removing small objects.\n\n    Returns:\n        xr.DataArray: Binarized probabilities.\n\n    \"\"\"\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Where the output from the ensemble / segmentation is nan turn it into 0, else threshold it\n    # Also, where there was no valid input data, turn it into 0\n    binarized = (probs.fillna(0) &gt; threshold).astype(\"uint8\")\n\n    # Remove objects at which overlap with either the edge of the tile or the noData mask\n    labels = binarized.copy(data=label(binarized, connectivity=2))\n    edge_label_ids = np.unique(xr.where(~mask, labels, 0))\n    binarized = ~labels.isin(edge_label_ids) &amp; binarized\n\n    # Remove small objects with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving binarized to GPU:{device}.\")\n        # Check if binarized is dask, if not persist it, since remove_small_objects_gpu can't be calculated from\n        # cupy-dask arrays\n        if binarized.chunks is not None:\n            binarized = binarized.persist()\n        with cp.cuda.Device(device_nr):\n            binarized = binarized.cupy.as_cupy()\n            binarized.values = remove_small_objects_gpu(\n                binarized.astype(bool).expand_dims(\"batch\", 0).data, min_size=min_object_size\n            )[0]\n            binarized = binarized.cupy.as_numpy()\n            free_cupy()\n    else:\n        binarized.values = remove_small_objects(\n            binarized.astype(bool).expand_dims(\"batch\", 0).values, min_size=min_object_size\n        )[0]\n\n    # Convert back to int8\n    binarized = binarized.astype(\"uint8\")\n\n    return binarized\n</code></pre>"},{"location":"reference/darts_postprocessing/erode_mask/","title":"darts_postprocessing.erode_mask","text":"<p>Erode the mask, also set the edges to invalid.</p> <p>Parameters:</p> <ul> <li> <code>mask</code>               (<code>xarray.DataArray</code>)           \u2013            <p>The mask to erode.</p> </li> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The size of the disk to use for erosion and the edge-cropping.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for erosion.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.DataArray</code>           \u2013            <p>xr.DataArray: The dilated and inverted mask.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopuhr.funkuhr(\"Eroding mask\", printer=logger.debug, print_kwargs=[\"size\"])\ndef erode_mask(mask: xr.DataArray, size: int, device: Literal[\"cuda\", \"cpu\"] | int) -&gt; xr.DataArray:\n    \"\"\"Erode the mask, also set the edges to invalid.\n\n    Args:\n        mask (xr.DataArray): The mask to erode.\n        size (int): The size of the disk to use for erosion and the edge-cropping.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for erosion.\n\n    Returns:\n        xr.DataArray: The dilated and inverted mask.\n\n    \"\"\"\n    # Clone mask to avoid in-place operations\n    mask = mask.copy()\n\n    # Change to dtype uint8 for faster skimage operations\n    mask = mask.astype(\"uint8\")\n\n    use_gpu = device == \"cuda\" or isinstance(device, int)\n\n    # Warn user if use_gpu is set but no GPU is available\n    if use_gpu and not CUCIM_AVAILABLE:\n        logger.warning(\n            f\"Device was set to {device}, but GPU acceleration is not available. Calculating TPI and slope on CPU.\"\n        )\n        use_gpu = False\n\n    # Dilate the mask with GPU\n    if use_gpu:\n        device_nr = device if isinstance(device, int) else 0\n        logger.debug(f\"Moving mask to GPU:{device}.\")\n        # Check if mask is dask, if not persist it, since dilation can't be calculated from cupy-dask arrays\n        if mask.chunks is not None:\n            mask = mask.persist()\n        with cp.cuda.Device(device_nr):\n            mask = mask.cupy.as_cupy()\n            mask.values = binary_erosion_gpu(mask.data, disk_gpu(size))\n            mask = mask.cupy.as_numpy()\n            free_cupy()\n    else:\n        mask.values = binary_erosion(mask.values, disk(size))\n\n    # Mask edges\n    mask[:size, :] = 0\n    mask[-size:, :] = 0\n    mask[:, :size] = 0\n    mask[:, -size:] = 0\n\n    return mask\n</code></pre>"},{"location":"reference/darts_postprocessing/prepare_export/","title":"darts_postprocessing.prepare_export","text":"<p>Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>Input tile from inference and / or an ensemble.</p> </li> <li> <code>bin_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold to binarize the probabilities. Defaults to 0.5.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>The size of the disk to use for mask erosion and the edge-cropping. Defaults to 10.</p> </li> <li> <code>min_object_size</code>               (<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The minimum object size to keep in pixel. Defaults to 32.</p> </li> <li> <code>quality_level</code>               (<code>int | str</code>, default:                   <code>0</code> )           \u2013            <p>The quality level to use for the mask. If a string maps to int. high_quality -&gt; 2, low_quality=1, none=0 (apply no masking). Defaults to 0.</p> </li> <li> <code>ensemble_subsets</code>               (<code>list[str]</code>, default:                   <code>[]</code> )           \u2013            <p>The ensemble subsets to use for the binarization. Defaults to [].</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_postprocessing.postprocess.DEFAULT_DEVICE</code> )           \u2013            <p>The device to use for dilation. Defaults to \"cuda\" if cuda for cucim is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: Output tile.</p> </li> </ul> Source code in <code>darts-postprocessing/src/darts_postprocessing/postprocess.py</code> <pre><code>@stopuhr.funkuhr(\n    \"Preparing export\",\n    printer=logger.debug,\n    print_kwargs=[\"bin_threshold\", \"mask_erosion_size\", \"min_object_size\", \"quality_level\", \"ensemble_subsets\"],\n)\ndef prepare_export(\n    tile: xr.Dataset,\n    bin_threshold: float = 0.5,\n    mask_erosion_size: int = 10,\n    min_object_size: int = 32,\n    quality_level: int | Literal[\"high_quality\", \"low_quality\", \"none\"] = 0,\n    ensemble_subsets: list[str] = [],\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Prepare the export, e.g. binarizes the data and convert the float probabilities to uint8.\n\n    Args:\n        tile (xr.Dataset): Input tile from inference and / or an ensemble.\n        bin_threshold (float, optional): The threshold to binarize the probabilities. Defaults to 0.5.\n        mask_erosion_size (int, optional): The size of the disk to use for mask erosion and the edge-cropping.\n            Defaults to 10.\n        min_object_size (int, optional): The minimum object size to keep in pixel. Defaults to 32.\n        quality_level (int | str, optional): The quality level to use for the mask. If a string maps to int.\n            high_quality -&gt; 2, low_quality=1, none=0 (apply no masking). Defaults to 0.\n        ensemble_subsets (list[str], optional): The ensemble subsets to use for the binarization.\n            Defaults to [].\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to use for dilation.\n            Defaults to \"cuda\" if cuda for cucim is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: Output tile.\n\n    \"\"\"\n    quality_level = (\n        quality_level\n        if isinstance(quality_level, int)\n        else {\"high_quality\": 2, \"low_quality\": 1, \"none\": 0}[quality_level]\n    )\n    mask = tile[\"quality_data_mask\"] &gt;= quality_level\n    if quality_level &gt; 0:\n        mask = erode_mask(mask, mask_erosion_size, device)  # 0=positive, 1=negative\n    tile[\"extent\"] = mask.copy()\n    tile[\"extent\"].attrs = {\n        \"long_name\": \"Extent of the segmentation\",\n    }\n\n    def _prep_layer(tile, layername, binarized_layer_name):\n        # Binarize the segmentation\n        tile[binarized_layer_name] = binarize(tile[layername], bin_threshold, min_object_size, mask, device)\n        tile[binarized_layer_name].attrs = {\n            \"long_name\": \"Binarized Segmentation\",\n        }\n\n        # Convert the probabilities to uint8\n        # Same but this time with 255 as no-data\n        # But first check if this step was already run\n        if tile[layername].max() &gt; 1:\n            return tile\n\n        intprobs = (tile[layername] * 100).fillna(255).astype(\"uint8\")\n        tile[layername] = xr.where(mask, intprobs, 255)\n        tile[layername].attrs = {\n            \"long_name\": \"Probabilities\",\n            \"units\": \"%\",\n        }\n        tile[layername] = tile[layername].rio.write_nodata(255)\n        return tile\n\n    tile = _prep_layer(tile, \"probabilities\", \"binarized_segmentation\")\n\n    # get the names of the model probabilities if available\n    # for example 'tcvis' from 'probabilities-tcvis'\n    for ensemble_subset in ensemble_subsets:\n        tile = _prep_layer(tile, f\"probabilities-{ensemble_subset}\", f\"binarized_segmentation-{ensemble_subset}\")\n\n    return tile\n</code></pre>"},{"location":"reference/darts_preprocessing/","title":"darts_preprocessing","text":"<p>Data preprocessing and feature engineering for the DARTS dataset.</p> <p>Functions:</p> <ul> <li> <code>calculate_ndvi</code>             \u2013              <p>Calculate NDVI from an xarray Dataset containing spectral bands.</p> </li> <li> <code>calculate_slope</code>             \u2013              <p>Calculate the slope of the terrain surface from an ArcticDEM Dataset.</p> </li> <li> <code>calculate_topographic_position_index</code>             \u2013              <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> </li> <li> <code>preprocess_legacy_fast</code>             \u2013              <p>Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>__version__</code>           \u2013            </li> </ul> <ul> <li>calculate_ndvi</li> <li>calculate_slope</li> <li>calculate_topographic_position_index</li> <li>preprocess_legacy_fast</li> </ul>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_ndvi","title":"calculate_ndvi","text":"<pre><code>calculate_ndvi(\n    planet_scene_dataset: xarray.Dataset,\n    nir_band: str = \"nir\",\n    red_band: str = \"red\",\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate NDVI from an xarray Dataset containing spectral bands.</p> Example <pre><code>ndvi_data = calculate_ndvi(planet_scene_dataset)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>planet_scene_dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The xarray Dataset containing the spectral bands, where the bands are indexed along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.</p> </li> <li> <code>nir_band</code>               (<code>str</code>, default:                   <code>'nir'</code> )           \u2013            <p>The name of the NIR band in the Dataset (default is \"nir\"). This name should correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".</p> </li> <li> <code>red_band</code>               (<code>str</code>, default:                   <code>'red'</code> )           \u2013            <p>The name of the Red band in the Dataset (default is \"red\"). This name should correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".</p> </li> </ul> Notes <p>NDVI (Normalized Difference Vegetation Index) is calculated using the formula:     NDVI = (NIR - Red) / (NIR + Red)</p> <p>This index is commonly used in remote sensing to assess vegetation health and density.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopuhr.funkuhr(\"Calculating NDVI\", printer=logger.debug, print_kwargs=[\"nir_band\", \"red_band\"])\ndef calculate_ndvi(planet_scene_dataset: xr.Dataset, nir_band: str = \"nir\", red_band: str = \"red\") -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI from an xarray Dataset containing spectral bands.\n\n    Example:\n        ```python\n        ndvi_data = calculate_ndvi(planet_scene_dataset)\n        ```\n\n    Args:\n        planet_scene_dataset (xr.Dataset): The xarray Dataset containing the spectral bands, where the bands are indexed\n            along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.\n        nir_band (str, optional): The name of the NIR band in the Dataset (default is \"nir\"). This name should\n            correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".\n        red_band (str, optional): The name of the Red band in the Dataset (default is \"red\"). This name should\n            correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".\n\n    Returns:\n        xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have\n            dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".\n\n\n    Notes:\n        NDVI (Normalized Difference Vegetation Index) is calculated using the formula:\n            NDVI = (NIR - Red) / (NIR + Red)\n\n        This index is commonly used in remote sensing to assess vegetation health and density.\n\n    \"\"\"\n    # Calculate NDVI using the formula\n    nir = planet_scene_dataset[nir_band].astype(\"float32\")\n    r = planet_scene_dataset[red_band].astype(\"float32\")\n    ndvi = (nir - r) / (nir + r)\n\n    # Scale to 0 - 20000 (for later conversion to uint16)\n    ndvi = (ndvi.clip(-1, 1) + 1) * 1e4\n    # Make nan to 0\n    ndvi = ndvi.fillna(0).rio.write_nodata(0)\n    # Convert to uint16\n    ndvi = ndvi.astype(\"uint16\")\n\n    ndvi = ndvi.assign_attrs({\"data_source\": \"planet\", \"long_name\": \"NDVI\"}).to_dataset(name=\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_slope","title":"calculate_slope","text":"<pre><code>calculate_slope(\n    arcticdem_ds: xarray.Dataset,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopuhr.funkuhr(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the slope of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.calculate_topographic_position_index","title":"calculate_topographic_position_index","text":"<pre><code>calculate_topographic_position_index(\n    arcticdem_ds: xarray.Dataset,\n    outer_radius: int,\n    inner_radius: int,\n) -&gt; xarray.Dataset\n</code></pre> <p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel in m.</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in m.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopuhr.funkuhr(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n        outer_radius (int, optional): The outer radius of the annulus kernel in m.\n        inner_radius (int, optional): The inner radius of the annulus kernel in m.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius_m = f\"{outer_radius}m\"\n    outer_radius_px = f\"{ceil(outer_radius / cellsize_x)}px\"\n    if inner_radius &gt; 0:\n        inner_radius_m = f\"{inner_radius}m\"\n        inner_radius_px = f\"{ceil(inner_radius / cellsize_x)}px\"\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius_m, inner_radius_m)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius_px}-{outer_radius_px} cells \"\n            f\"({inner_radius_m}-{outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of \"\n            f\"{inner_radius_px}-{outer_radius_px} ({inner_radius_m}-{outer_radius_m}) cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius_m)\n        attr_cell_description = (\n            f\"within a circle at a distance of {outer_radius_px} cells ({outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with circle kernel of {outer_radius_px} ({outer_radius_m}) cells.\"\n        )\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/#darts_preprocessing.preprocess_legacy_fast","title":"preprocess_legacy_fast","text":"<pre><code>preprocess_legacy_fast(\n    ds_merged: xarray.Dataset,\n    ds_arcticdem: xarray.Dataset,\n    ds_tcvis: xarray.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: typing.Literal[\"cuda\", \"cpu\"]\n    | int = darts_preprocessing.preprocess.DEFAULT_DEVICE,\n) -&gt; xarray.Dataset\n</code></pre> <p>Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>The main difference to preprocess_legacy is the new data concept of the arcticdem. Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw arcticdem data and calculates slope and relative elevation on the fly.</p> <p>Parameters:</p> <ul> <li> <code>ds_merged</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The TCVIS dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_preprocessing.preprocess.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>@stopuhr.funkuhr(\"Preprocessing\", printer=logger.debug)\ndef preprocess_legacy_fast(\n    ds_merged: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    The main difference to preprocess_legacy is the new data concept of the arcticdem.\n    Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw\n    arcticdem data and calculates slope and relative elevation on the fly.\n\n    Args:\n        ds_merged (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Calculate NDVI\n    ds_merged[\"ndvi\"] = calculate_ndvi(ds_merged).ndvi\n\n    # Reproject TCVIS to optical data\n    with stopuhr.stopuhr(\"Reprojecting TCVIS\", printer=logger.debug):\n        ds_tcvis = ds_tcvis.odc.reproject(ds_merged.odc.geobox, resampling=\"cubic\")\n\n    ds_merged[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_merged[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_merged[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    with stopuhr.stopuhr(\"Reprojecting ArcticDEM\", printer=logger.debug):\n        ds_arcticdem = ds_arcticdem.odc.reproject(ds_merged.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n\n    ds_arcticdem = preprocess_legacy_arcticdem_fast(ds_arcticdem, tpi_outer_radius, tpi_inner_radius, device)\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_merged.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_merged)\n\n    ds_merged[\"dem\"] = ds_arcticdem.dem\n    ds_merged[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_merged[\"slope\"] = ds_arcticdem.slope\n    ds_merged[\"arcticdem_data_mask\"] = ds_arcticdem.datamask\n\n    # Update datamask with arcticdem mask\n    # with xr.set_options(keep_attrs=True):\n    #     ds_merged[\"quality_data_mask\"] = ds_merged.quality_data_mask * ds_arcticdem.datamask\n    # ds_merged.quality_data_mask.attrs[\"data_source\"] += \" + ArcticDEM\"\n\n    return ds_merged\n</code></pre>"},{"location":"reference/darts_preprocessing/calculate_ndvi/","title":"darts_preprocessing.calculate_ndvi","text":"<p>Calculate NDVI from an xarray Dataset containing spectral bands.</p> Example <pre><code>ndvi_data = calculate_ndvi(planet_scene_dataset)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>planet_scene_dataset</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The xarray Dataset containing the spectral bands, where the bands are indexed along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.</p> </li> <li> <code>nir_band</code>               (<code>str</code>, default:                   <code>'nir'</code> )           \u2013            <p>The name of the NIR band in the Dataset (default is \"nir\"). This name should correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".</p> </li> <li> <code>red_band</code>               (<code>str</code>, default:                   <code>'red'</code> )           \u2013            <p>The name of the Red band in the Dataset (default is \"red\"). This name should correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".</p> </li> </ul> Notes <p>NDVI (Normalized Difference Vegetation Index) is calculated using the formula:     NDVI = (NIR - Red) / (NIR + Red)</p> <p>This index is commonly used in remote sensing to assess vegetation health and density.</p> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/indices.py</code> <pre><code>@stopuhr.funkuhr(\"Calculating NDVI\", printer=logger.debug, print_kwargs=[\"nir_band\", \"red_band\"])\ndef calculate_ndvi(planet_scene_dataset: xr.Dataset, nir_band: str = \"nir\", red_band: str = \"red\") -&gt; xr.Dataset:\n    \"\"\"Calculate NDVI from an xarray Dataset containing spectral bands.\n\n    Example:\n        ```python\n        ndvi_data = calculate_ndvi(planet_scene_dataset)\n        ```\n\n    Args:\n        planet_scene_dataset (xr.Dataset): The xarray Dataset containing the spectral bands, where the bands are indexed\n            along a dimension (e.g., 'band'). The Dataset should have dimensions including 'band', 'y', and 'x'.\n        nir_band (str, optional): The name of the NIR band in the Dataset (default is \"nir\"). This name should\n            correspond to the variable name for the NIR band in the 'band' dimension. Defaults to \"nir\".\n        red_band (str, optional): The name of the Red band in the Dataset (default is \"red\"). This name should\n            correspond to the variable name for the Red band in the 'band' dimension. Defaults to \"red\".\n\n    Returns:\n        xr.Dataset: A new Dataset containing the calculated NDVI values. The resulting Dataset will have\n            dimensions (band: 1, y: ..., x: ...) and will be named \"ndvi\".\n\n\n    Notes:\n        NDVI (Normalized Difference Vegetation Index) is calculated using the formula:\n            NDVI = (NIR - Red) / (NIR + Red)\n\n        This index is commonly used in remote sensing to assess vegetation health and density.\n\n    \"\"\"\n    # Calculate NDVI using the formula\n    nir = planet_scene_dataset[nir_band].astype(\"float32\")\n    r = planet_scene_dataset[red_band].astype(\"float32\")\n    ndvi = (nir - r) / (nir + r)\n\n    # Scale to 0 - 20000 (for later conversion to uint16)\n    ndvi = (ndvi.clip(-1, 1) + 1) * 1e4\n    # Make nan to 0\n    ndvi = ndvi.fillna(0).rio.write_nodata(0)\n    # Convert to uint16\n    ndvi = ndvi.astype(\"uint16\")\n\n    ndvi = ndvi.assign_attrs({\"data_source\": \"planet\", \"long_name\": \"NDVI\"}).to_dataset(name=\"ndvi\")\n    return ndvi\n</code></pre>"},{"location":"reference/darts_preprocessing/calculate_slope/","title":"darts_preprocessing.calculate_slope","text":"<p>Calculate the slope of the terrain surface from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopuhr.funkuhr(\"Calculating slope\", printer=logger.debug)\ndef calculate_slope(arcticdem_ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"Calculate the slope of the terrain surface from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated slope added as a new variable 'slope'.\n\n    \"\"\"\n    slope_deg = slope(arcticdem_ds.dem)\n    slope_deg.attrs = {\n        \"long_name\": \"Slope\",\n        \"units\": \"degrees\",\n        \"description\": \"The slope of the terrain surface in degrees.\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n    arcticdem_ds[\"slope\"] = slope_deg.compute()\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/calculate_topographic_position_index/","title":"darts_preprocessing.calculate_topographic_position_index","text":"<p>Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.</p> <p>Parameters:</p> <ul> <li> <code>arcticdem_ds</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM Dataset containing the 'dem' variable.</p> </li> <li> <code>outer_radius</code>               (<code>int</code>)           \u2013            <p>The outer radius of the annulus kernel in m.</p> </li> <li> <code>inner_radius</code>               (<code>int</code>)           \u2013            <p>The inner radius of the annulus kernel in m.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/engineering/arcticdem.py</code> <pre><code>@stopuhr.funkuhr(\"Calculating TPI\", printer=logger.debug, print_kwargs=[\"outer_radius\", \"inner_radius\"])\ndef calculate_topographic_position_index(arcticdem_ds: xr.Dataset, outer_radius: int, inner_radius: int) -&gt; xr.Dataset:\n    \"\"\"Calculate the Topographic Position Index (TPI) from an ArcticDEM Dataset.\n\n    Args:\n        arcticdem_ds (xr.Dataset): The ArcticDEM Dataset containing the 'dem' variable.\n        outer_radius (int, optional): The outer radius of the annulus kernel in m.\n        inner_radius (int, optional): The inner radius of the annulus kernel in m.\n\n    Returns:\n        xr.Dataset: The input Dataset with the calculated TPI added as a new variable 'tpi'.\n\n    \"\"\"\n    cellsize_x, cellsize_y = convolution.calc_cellsize(arcticdem_ds.dem)  # Should be equal to the resolution of the DEM\n    # Use an annulus kernel if inner_radius is greater than 0\n    outer_radius_m = f\"{outer_radius}m\"\n    outer_radius_px = f\"{ceil(outer_radius / cellsize_x)}px\"\n    if inner_radius &gt; 0:\n        inner_radius_m = f\"{inner_radius}m\"\n        inner_radius_px = f\"{ceil(inner_radius / cellsize_x)}px\"\n        kernel = convolution.annulus_kernel(cellsize_x, cellsize_y, outer_radius_m, inner_radius_m)\n        attr_cell_description = (\n            f\"within a ring at a distance of {inner_radius_px}-{outer_radius_px} cells \"\n            f\"({inner_radius_m}-{outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with annulus kernel of \"\n            f\"{inner_radius_px}-{outer_radius_px} ({inner_radius_m}-{outer_radius_m}) cells.\"\n        )\n    else:\n        kernel = convolution.circle_kernel(cellsize_x, cellsize_y, outer_radius_m)\n        attr_cell_description = (\n            f\"within a circle at a distance of {outer_radius_px} cells ({outer_radius_m}) away from the focal cell.\"\n        )\n        logger.debug(\n            f\"Calculating Topographic Position Index with circle kernel of {outer_radius_px} ({outer_radius_m}) cells.\"\n        )\n\n    if has_cuda_and_cupy() and arcticdem_ds.cupy.is_cupy:\n        kernel = cp.asarray(kernel)\n\n    tpi = arcticdem_ds.dem - convolution.convolution_2d(arcticdem_ds.dem, kernel) / kernel.sum()\n    tpi.attrs = {\n        \"long_name\": \"Topographic Position Index\",\n        \"units\": \"m\",\n        \"description\": \"The difference between the elevation of a cell and the mean elevation of the surrounding\"\n        f\"cells {attr_cell_description}\",\n        \"source\": \"ArcticDEM\",\n        \"_FillValue\": float(\"nan\"),\n    }\n\n    arcticdem_ds[\"tpi\"] = tpi.compute()\n\n    return arcticdem_ds\n</code></pre>"},{"location":"reference/darts_preprocessing/preprocess_legacy_fast/","title":"darts_preprocessing.preprocess_legacy_fast","text":"<p>Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.</p> <p>The processing steps are: - Calculate NDVI - Calculate slope and relative elevation from ArcticDEM - Merge everything into a single ds.</p> <p>The main difference to preprocess_legacy is the new data concept of the arcticdem. Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw arcticdem data and calculates slope and relative elevation on the fly.</p> <p>Parameters:</p> <ul> <li> <code>ds_merged</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.</p> </li> <li> <code>ds_arcticdem</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The ArcticDEM dataset.</p> </li> <li> <code>ds_tcvis</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The TCVIS dataset.</p> </li> <li> <code>tpi_outer_radius</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>The outer radius of the annulus kernel for the tpi calculation in m. Defaults to 100m.</p> </li> <li> <code>tpi_inner_radius</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The inner radius of the annulus kernel for the tpi calculation in m. Defaults to 0.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>, default:                   <code>darts_preprocessing.preprocess.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the tpi and slope calculations on. If \"cuda\" take the first device (0), if int take the specified device. Defaults to \"cuda\" if cuda is available, else \"cpu\".</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>xr.Dataset: The preprocessed dataset.</p> </li> </ul> Source code in <code>darts-preprocessing/src/darts_preprocessing/preprocess.py</code> <pre><code>@stopuhr.funkuhr(\"Preprocessing\", printer=logger.debug)\ndef preprocess_legacy_fast(\n    ds_merged: xr.Dataset,\n    ds_arcticdem: xr.Dataset,\n    ds_tcvis: xr.Dataset,\n    tpi_outer_radius: int = 100,\n    tpi_inner_radius: int = 0,\n    device: Literal[\"cuda\", \"cpu\"] | int = DEFAULT_DEVICE,\n) -&gt; xr.Dataset:\n    \"\"\"Preprocess optical data with legacy (DARTS v1) preprocessing steps, but with new data concepts.\n\n    The processing steps are:\n    - Calculate NDVI\n    - Calculate slope and relative elevation from ArcticDEM\n    - Merge everything into a single ds.\n\n    The main difference to preprocess_legacy is the new data concept of the arcticdem.\n    Instead of using already preprocessed arcticdem data which are loaded from a VRT, this step expects the raw\n    arcticdem data and calculates slope and relative elevation on the fly.\n\n    Args:\n        ds_merged (xr.Dataset): The Planet scene optical data or Sentinel 2 scene optical dataset including data_masks.\n        ds_arcticdem (xr.Dataset): The ArcticDEM dataset.\n        ds_tcvis (xr.Dataset): The TCVIS dataset.\n        tpi_outer_radius (int, optional): The outer radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 100m.\n        tpi_inner_radius (int, optional): The inner radius of the annulus kernel for the tpi calculation\n            in m. Defaults to 0.\n        device (Literal[\"cuda\", \"cpu\"] | int, optional): The device to run the tpi and slope calculations on.\n            If \"cuda\" take the first device (0), if int take the specified device.\n            Defaults to \"cuda\" if cuda is available, else \"cpu\".\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n\n    \"\"\"\n    # Calculate NDVI\n    ds_merged[\"ndvi\"] = calculate_ndvi(ds_merged).ndvi\n\n    # Reproject TCVIS to optical data\n    with stopuhr.stopuhr(\"Reprojecting TCVIS\", printer=logger.debug):\n        ds_tcvis = ds_tcvis.odc.reproject(ds_merged.odc.geobox, resampling=\"cubic\")\n\n    ds_merged[\"tc_brightness\"] = ds_tcvis.tc_brightness\n    ds_merged[\"tc_greenness\"] = ds_tcvis.tc_greenness\n    ds_merged[\"tc_wetness\"] = ds_tcvis.tc_wetness\n\n    # Calculate TPI and slope from ArcticDEM\n    with stopuhr.stopuhr(\"Reprojecting ArcticDEM\", printer=logger.debug):\n        ds_arcticdem = ds_arcticdem.odc.reproject(ds_merged.odc.geobox.buffered(tpi_outer_radius), resampling=\"cubic\")\n\n    ds_arcticdem = preprocess_legacy_arcticdem_fast(ds_arcticdem, tpi_outer_radius, tpi_inner_radius, device)\n    ds_arcticdem = ds_arcticdem.odc.crop(ds_merged.odc.geobox.extent)\n    # For some reason, we need to reindex, because the reproject + crop of the arcticdem sometimes results\n    # in floating point errors. These error are at the order of 1e-10, hence, way below millimeter precision.\n    ds_arcticdem = ds_arcticdem.reindex_like(ds_merged)\n\n    ds_merged[\"dem\"] = ds_arcticdem.dem\n    ds_merged[\"relative_elevation\"] = ds_arcticdem.tpi\n    ds_merged[\"slope\"] = ds_arcticdem.slope\n    ds_merged[\"arcticdem_data_mask\"] = ds_arcticdem.datamask\n\n    # Update datamask with arcticdem mask\n    # with xr.set_options(keep_attrs=True):\n    #     ds_merged[\"quality_data_mask\"] = ds_merged.quality_data_mask * ds_arcticdem.datamask\n    # ds_merged.quality_data_mask.attrs[\"data_source\"] += \" + ArcticDEM\"\n\n    return ds_merged\n</code></pre>"},{"location":"reference/darts_segmentation/","title":"darts_segmentation","text":"<p>Image segmentation of thaw-slumps for the DARTS dataset.</p> <p>Classes:</p> <ul> <li> <code>SMPSegmenter</code>           \u2013            <p>An actor that keeps a model as its state and segments tiles.</p> </li> <li> <code>SMPSegmenterConfig</code>           \u2013            <p>Configuration for the segmentor.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>create_patches</code>             \u2013              <p>Create patches from a tensor.</p> </li> <li> <code>patch_coords</code>             \u2013              <p>Yield patch coordinates based on height, width, patch size and margin size.</p> </li> <li> <code>predict_in_patches</code>             \u2013              <p>Predict on a tensor.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>__version__</code>           \u2013            </li> </ul> <ul> <li>create_patches</li> <li>patch_coords</li> <li>predict_in_patches</li> <li>SMPSegmenter</li> <li>SMPSegmenterConfig</li> </ul>"},{"location":"reference/darts_segmentation/#darts_segmentation.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenter","title":"SMPSegmenter","text":"<pre><code>SMPSegmenter(\n    model_checkpoint: pathlib.Path | str,\n    device: torch.device = darts_segmentation.segment.DEFAULT_DEVICE,\n)\n</code></pre> <p>An actor that keeps a model as its state and segments tiles.</p> <p>Initialize the segmenter.</p> <p>Parameters:</p> <ul> <li> <code>model_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the model checkpoint.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_segmentation.segment.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__call__</code>             \u2013              <p>Run inference on a single tile or a list of tiles.</p> </li> <li> <code>segment_tile</code>             \u2013              <p>Run inference on a tile.</p> </li> <li> <code>segment_tile_batched</code>             \u2013              <p>Run inference on a list of tiles.</p> </li> <li> <code>tile2tensor</code>             \u2013              <p>Take a tile and convert it to a pytorch tensor.</p> </li> <li> <code>tile2tensor_batched</code>             \u2013              <p>Take a list of tiles and convert them to a pytorch tensor.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>config</code>               (<code>darts_segmentation.segment.SMPSegmenterConfig</code>)           \u2013            </li> <li> <code>device</code>               (<code>torch.device</code>)           \u2013            </li> <li> <code>model</code>               (<code>torch.nn.Module</code>)           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __init__(self, model_checkpoint: Path | str, device: torch.device = DEFAULT_DEVICE):\n    \"\"\"Initialize the segmenter.\n\n    Args:\n        model_checkpoint (Path): The path to the model checkpoint.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_checkpoint = model_checkpoint if isinstance(model_checkpoint, Path) else Path(model_checkpoint)\n    self.device = device\n    ckpt = torch.load(model_checkpoint, map_location=self.device)\n    self.config = validate_config(ckpt[\"config\"])\n    # Overwrite the encoder weights with None, because we load our own\n    self.config[\"model\"] |= {\"encoder_weights\": None}\n    self.model = smp.create_model(**self.config[\"model\"])\n    self.model.to(self.device)\n    self.model.load_state_dict(ckpt[\"statedict\"])\n    self.model.eval()\n\n    logger.debug(\n        f\"Successfully loaded model from {model_checkpoint.resolve()} with inputs: \"\n        f\"{self.config['input_combination']}\"\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenter.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: darts_segmentation.segment.SMPSegmenterConfig = (\n    darts_segmentation.segment.validate_config(\n        ckpt[\"config\"]\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenter.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: torch.device = (\n    darts_segmentation.segment.SMPSegmenter(device)\n)\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenter.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: torch.nn.Module = (\n    segmentation_models_pytorch.create_model(\n        **darts_segmentation.segment.SMPSegmenter(\n            self\n        ).config[\"model\"]\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenter.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset | list[xarray.Dataset]\n</code></pre> <p>Run inference on a single tile or a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>A single tile or a list of tiles augmented by a predicted <code>probabilities</code> layer, depending on the input.</p> </li> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>Each <code>probability</code> has type float32 and range [0, 1].</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xr.Dataset | list[xr.Dataset]:\n    \"\"\"Run inference on a single tile or a list of tiles.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n        Each `probability` has type float32 and range [0, 1].\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    else:\n        raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenter.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run inference on a tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@stopuhr.funkuhr(\n    \"Segmenting tile\",\n    logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\"],\n)\ndef segment_tile(\n    self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tile to a tensor\n    tensor_tile = self.tile2tensor(tile)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tile = tensor_tile.unsqueeze(0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n    ).squeeze(0)\n\n    # Highly sophisticated DL-based predictor\n    # TODO: is there a better way to pass metadata?\n    tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n    tile[\"probabilities\"].attrs = {\"long_name\": \"Probabilities\"}\n    tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tile, probabilities\n    free_torch()\n\n    return tile\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenter.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@stopuhr.funkuhr(\n    \"Segmenting tiles\",\n    logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\"],\n)\ndef segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tiles to tensors\n    # TODO: maybe create a batched tile2tensor function?\n    # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n    tensor_tiles = self.tile2tensor_batched(tiles)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n    )\n\n    # Highly sophisticated DL-based predictor\n    for tile, probs in zip(tiles, probabilities):\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\"long_name\": \"Probabilities\"}\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tiles, probabilities\n    free_torch()\n\n    return tiles\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenter.tile2tensor","title":"tile2tensor","text":"<pre><code>tile2tensor(tile: xarray.Dataset) -&gt; torch.Tensor\n</code></pre> <p>Take a tile and convert it to a pytorch tensor.</p> <p>Respects the input combination from the config.</p> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n    \"\"\"Take a tile and convert it to a pytorch tensor.\n\n    Respects the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n    # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        band_data = tile[feature_name]\n        # Normalize the band data\n        band_data = band_data * norm\n        bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n    return torch.stack(bands, dim=0)\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenter.tile2tensor_batched","title":"tile2tensor_batched","text":"<pre><code>tile2tensor_batched(\n    tiles: list[xarray.Dataset],\n) -&gt; torch.Tensor\n</code></pre> <p>Take a list of tiles and convert them to a pytorch tensor.</p> <p>Respects the the input combination from the config.</p> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n    \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n    Respects the the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        for tile in tiles:\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n    # TODO: Test this\n    return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenterConfig","title":"SMPSegmenterConfig","text":"<p>               Bases: <code>typing.TypedDict</code></p> <p>Configuration for the segmentor.</p> <p>Attributes:</p> <ul> <li> <code>input_combination</code>               (<code>list[str]</code>)           \u2013            </li> <li> <code>model</code>               (<code>dict[str, typing.Any]</code>)           \u2013            </li> <li> <code>norm_factors</code>               (<code>dict[str, float]</code>)           \u2013            </li> </ul>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenterConfig.input_combination","title":"input_combination  <code>instance-attribute</code>","text":"<pre><code>input_combination: list[str]\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenterConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: dict[str, typing.Any]\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.SMPSegmenterConfig.norm_factors","title":"norm_factors  <code>instance-attribute</code>","text":"<pre><code>norm_factors: dict[str, float]\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.create_patches","title":"create_patches","text":"<pre><code>create_patches(\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    return_coords: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Create patches from a tensor.</p> <p>Parameters:</p> <ul> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>return_coords</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.patch_coords","title":"patch_coords","text":"<pre><code>patch_coords(\n    h: int, w: int, patch_size: int, overlap: int\n) -&gt; collections.abc.Generator[\n    tuple[int, int, int, int], None, None\n]\n</code></pre> <p>Yield patch coordinates based on height, width, patch size and margin size.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>int</code>)           \u2013            <p>Height of the image.</p> </li> <li> <code>w</code>               (<code>int</code>)           \u2013            <p>Width of the image.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Patch size.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>Margin size.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[int, int, int, int]</code>           \u2013            <p>tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def patch_coords(h: int, w: int, patch_size: int, overlap: int) -&gt; Generator[tuple[int, int, int, int], None, None]:\n    \"\"\"Yield patch coordinates based on height, width, patch size and margin size.\n\n    Args:\n        h (int): Height of the image.\n        w (int): Width of the image.\n        patch_size (int): Patch size.\n        overlap (int): Margin size.\n\n    Yields:\n        tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.\n\n    \"\"\"\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            yield y, x, patch_idx_y, patch_idx_x\n</code></pre>"},{"location":"reference/darts_segmentation/#darts_segmentation.predict_in_patches","title":"predict_in_patches","text":"<pre><code>predict_in_patches(\n    model: torch.nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor\n</code></pre> <p>Predict on a tensor.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>torch.nn.Module</code>)           \u2013            <p>The model to use for prediction.</p> </li> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> </li> <li> <code>reflection</code>               (<code>int</code>)           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>torch.device</code> )           \u2013            <p>The device to use for the prediction.</p> </li> <li> <code>return_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>The predicted tensor.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    n_skipped = 0\n    for i, batch in enumerate(patches):\n        # If batch contains only nans, skip it\n        # TODO: This doesn't work as expected -&gt; check if torch.isnan(batch).all() is correct\n        if torch.isnan(batch).all(axis=0).any():\n            patched_probabilities[i * batch_size : (i + 1) * batch_size] = 0\n            n_skipped += 1\n            continue\n        # If batch contains some nans, replace them with zeros\n        batch[torch.isnan(batch)] = 0\n\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    if n_skipped &gt; 0:\n        logger.debug(f\"Skipped {n_skipped} batches because they only contained NaNs\")\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenter/","title":"darts_segmentation.SMPSegmenter","text":"<p>An actor that keeps a model as its state and segments tiles.</p> <p>Initialize the segmenter.</p> <p>Parameters:</p> <ul> <li> <code>model_checkpoint</code>               (<code>pathlib.Path</code>)           \u2013            <p>The path to the model checkpoint.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>darts_segmentation.segment.DEFAULT_DEVICE</code> )           \u2013            <p>The device to run the model on. Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __init__(self, model_checkpoint: Path | str, device: torch.device = DEFAULT_DEVICE):\n    \"\"\"Initialize the segmenter.\n\n    Args:\n        model_checkpoint (Path): The path to the model checkpoint.\n        device (torch.device): The device to run the model on.\n            Defaults to torch.device(\"cuda\") if cuda is available, else torch.device(\"cpu\").\n\n    \"\"\"\n    model_checkpoint = model_checkpoint if isinstance(model_checkpoint, Path) else Path(model_checkpoint)\n    self.device = device\n    ckpt = torch.load(model_checkpoint, map_location=self.device)\n    self.config = validate_config(ckpt[\"config\"])\n    # Overwrite the encoder weights with None, because we load our own\n    self.config[\"model\"] |= {\"encoder_weights\": None}\n    self.model = smp.create_model(**self.config[\"model\"])\n    self.model.to(self.device)\n    self.model.load_state_dict(ckpt[\"statedict\"])\n    self.model.eval()\n\n    logger.debug(\n        f\"Successfully loaded model from {model_checkpoint.resolve()} with inputs: \"\n        f\"{self.config['input_combination']}\"\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenter/#darts_segmentation.SMPSegmenter.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: darts_segmentation.segment.SMPSegmenterConfig = (\n    darts_segmentation.segment.validate_config(\n        ckpt[\"config\"]\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenter/#darts_segmentation.SMPSegmenter.device","title":"device  <code>instance-attribute</code>","text":"<pre><code>device: torch.device = (\n    darts_segmentation.segment.SMPSegmenter(device)\n)\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenter/#darts_segmentation.SMPSegmenter.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: torch.nn.Module = (\n    segmentation_models_pytorch.create_model(\n        **darts_segmentation.segment.SMPSegmenter(\n            self\n        ).config[\"model\"]\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenter/#darts_segmentation.SMPSegmenter.__call__","title":"__call__","text":"<pre><code>__call__(\n    input: xarray.Dataset | list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset | list[xarray.Dataset]\n</code></pre> <p>Run inference on a single tile or a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>input</code>               (<code>xarray.Dataset | list[xarray.Dataset]</code>)           \u2013            <p>A single tile or a list of tiles.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>A single tile or a list of tiles augmented by a predicted <code>probabilities</code> layer, depending on the input.</p> </li> <li> <code>xarray.Dataset | list[xarray.Dataset]</code>           \u2013            <p>Each <code>probability</code> has type float32 and range [0, 1].</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>in case the input is not an xr.Dataset or a list of xr.Dataset</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def __call__(\n    self,\n    input: xr.Dataset | list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xr.Dataset | list[xr.Dataset]:\n    \"\"\"Run inference on a single tile or a list of tiles.\n\n    Args:\n        input (xr.Dataset | list[xr.Dataset]): A single tile or a list of tiles.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A single tile or a list of tiles augmented by a predicted `probabilities` layer, depending on the input.\n        Each `probability` has type float32 and range [0, 1].\n\n    Raises:\n        ValueError: in case the input is not an xr.Dataset or a list of xr.Dataset\n\n    \"\"\"\n    if isinstance(input, xr.Dataset):\n        return self.segment_tile(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    elif isinstance(input, list):\n        return self.segment_tile_batched(\n            input, patch_size=patch_size, overlap=overlap, batch_size=batch_size, reflection=reflection\n        )\n    else:\n        raise ValueError(f\"Expected xr.Dataset or list of xr.Dataset, got {type(input)}\")\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenter/#darts_segmentation.SMPSegmenter.segment_tile","title":"segment_tile","text":"<pre><code>segment_tile(\n    tile: xarray.Dataset,\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; xarray.Dataset\n</code></pre> <p>Run inference on a tile.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>xarray.Dataset</code>           \u2013            <p>Input tile augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@stopuhr.funkuhr(\n    \"Segmenting tile\",\n    logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\"],\n)\ndef segment_tile(\n    self, tile: xr.Dataset, patch_size: int = 1024, overlap: int = 16, batch_size: int = 8, reflection: int = 0\n) -&gt; xr.Dataset:\n    \"\"\"Run inference on a tile.\n\n    Args:\n        tile: The input tile, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        Input tile augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tile to a tensor\n    tensor_tile = self.tile2tensor(tile)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tile = tensor_tile.unsqueeze(0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tile, patch_size, overlap, batch_size, reflection, self.device\n    ).squeeze(0)\n\n    # Highly sophisticated DL-based predictor\n    # TODO: is there a better way to pass metadata?\n    tile[\"probabilities\"] = tile[\"red\"].copy(data=probabilities.cpu().numpy())\n    tile[\"probabilities\"].attrs = {\"long_name\": \"Probabilities\"}\n    tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tile, probabilities\n    free_torch()\n\n    return tile\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenter/#darts_segmentation.SMPSegmenter.segment_tile_batched","title":"segment_tile_batched","text":"<pre><code>segment_tile_batched(\n    tiles: list[xarray.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xarray.Dataset]\n</code></pre> <p>Run inference on a list of tiles.</p> <p>Parameters:</p> <ul> <li> <code>tiles</code>               (<code>list[xarray.Dataset]</code>)           \u2013            <p>The input tiles, containing preprocessed, harmonized data.</p> </li> <li> <code>patch_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>The size of the patches. Defaults to 1024.</p> </li> <li> <code>overlap</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The size of the overlap. Defaults to 16.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.</p> </li> <li> <code>reflection</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[xarray.Dataset]</code>           \u2013            <p>A list of input tiles augmented by a predicted <code>probabilities</code> layer with type float32 and range [0, 1].</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>@stopuhr.funkuhr(\n    \"Segmenting tiles\",\n    logger.debug,\n    print_kwargs=[\"patch_size\", \"overlap\", \"batch_size\", \"reflection\"],\n)\ndef segment_tile_batched(\n    self,\n    tiles: list[xr.Dataset],\n    patch_size: int = 1024,\n    overlap: int = 16,\n    batch_size: int = 8,\n    reflection: int = 0,\n) -&gt; list[xr.Dataset]:\n    \"\"\"Run inference on a list of tiles.\n\n    Args:\n        tiles: The input tiles, containing preprocessed, harmonized data.\n        patch_size (int): The size of the patches. Defaults to 1024.\n        overlap (int): The size of the overlap. Defaults to 16.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches. Defaults to 8.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor. Defaults to 0.\n\n    Returns:\n        A list of input tiles augmented by a predicted `probabilities` layer with type float32 and range [0, 1].\n\n    \"\"\"\n    # Convert the tiles to tensors\n    # TODO: maybe create a batched tile2tensor function?\n    # tensor_tiles = [self.tile2tensor(tile).to(self.dev) for tile in tiles]\n    tensor_tiles = self.tile2tensor_batched(tiles)\n\n    # Create a batch dimension, because predict expects it\n    tensor_tiles = torch.stack(tensor_tiles, dim=0)\n\n    probabilities = predict_in_patches(\n        self.model, tensor_tiles, patch_size, overlap, batch_size, reflection, self.device\n    )\n\n    # Highly sophisticated DL-based predictor\n    for tile, probs in zip(tiles, probabilities):\n        # TODO: is there a better way to pass metadata?\n        tile[\"probabilities\"] = tile[\"red\"].copy(data=probs.cpu().numpy())\n        tile[\"probabilities\"].attrs = {\"long_name\": \"Probabilities\"}\n        tile[\"probabilities\"] = tile[\"probabilities\"].fillna(float(\"nan\")).rio.write_nodata(float(\"nan\"))\n\n    # Cleanup cuda memory\n    del tensor_tiles, probabilities\n    free_torch()\n\n    return tiles\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenter/#darts_segmentation.SMPSegmenter.tile2tensor","title":"tile2tensor","text":"<pre><code>tile2tensor(tile: xarray.Dataset) -&gt; torch.Tensor\n</code></pre> <p>Take a tile and convert it to a pytorch tensor.</p> <p>Respects the input combination from the config.</p> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor(self, tile: xr.Dataset) -&gt; torch.Tensor:\n    \"\"\"Take a tile and convert it to a pytorch tensor.\n\n    Respects the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    # e.g. input_combination: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n    # tile.data_vars: [\"red\", \"green\", \"blue\", \"relative_elevation\", ...]\n\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        band_data = tile[feature_name]\n        # Normalize the band data\n        band_data = band_data * norm\n        bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n\n    return torch.stack(bands, dim=0)\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenter/#darts_segmentation.SMPSegmenter.tile2tensor_batched","title":"tile2tensor_batched","text":"<pre><code>tile2tensor_batched(\n    tiles: list[xarray.Dataset],\n) -&gt; torch.Tensor\n</code></pre> <p>Take a list of tiles and convert them to a pytorch tensor.</p> <p>Respects the the input combination from the config.</p> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>A torch tensor for the full tile consisting of the bands specified in <code>self.band_combination</code>.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/segment.py</code> <pre><code>def tile2tensor_batched(self, tiles: list[xr.Dataset]) -&gt; torch.Tensor:\n    \"\"\"Take a list of tiles and convert them to a pytorch tensor.\n\n    Respects the the input combination from the config.\n\n    Returns:\n        A torch tensor for the full tile consisting of the bands specified in `self.band_combination`.\n\n    \"\"\"\n    bands = []\n    for feature_name in self.config[\"input_combination\"]:\n        norm = self.config[\"norm_factors\"][feature_name]\n        for tile in tiles:\n            band_data = tile[feature_name]\n            # Normalize the band data\n            band_data = band_data * norm\n            bands.append(torch.from_numpy(band_data.to_numpy().astype(\"float32\")))\n    # TODO: Test this\n    return torch.stack(bands, dim=0).reshape(len(tiles), len(self.config[\"input_combination\"]), *bands[0].shape)\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenterConfig/","title":"darts_segmentation.SMPSegmenterConfig","text":"<p>               Bases: <code>typing.TypedDict</code></p> <p>Configuration for the segmentor.</p>"},{"location":"reference/darts_segmentation/SMPSegmenterConfig/#darts_segmentation.SMPSegmenterConfig.input_combination","title":"input_combination  <code>instance-attribute</code>","text":"<pre><code>input_combination: list[str]\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenterConfig/#darts_segmentation.SMPSegmenterConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: dict[str, typing.Any]\n</code></pre>"},{"location":"reference/darts_segmentation/SMPSegmenterConfig/#darts_segmentation.SMPSegmenterConfig.norm_factors","title":"norm_factors  <code>instance-attribute</code>","text":"<pre><code>norm_factors: dict[str, float]\n</code></pre>"},{"location":"reference/darts_segmentation/create_patches/","title":"darts_segmentation.create_patches","text":"<p>Create patches from a tensor.</p> <p>Parameters:</p> <ul> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>return_coords</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the coordinates of the patches. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef create_patches(\n    tensor_tiles: torch.Tensor, patch_size: int, overlap: int, return_coords: bool = False\n) -&gt; torch.Tensor:\n    \"\"\"Create patches from a tensor.\n\n    Args:\n        tensor_tiles (torch.Tensor): The input tensor. Shape: (BS, C, H, W).\n        patch_size (int, optional): The size of the patches.\n        overlap (int, optional): The size of the overlap.\n        return_coords (bool, optional): Whether to return the coordinates of the patches.\n            Can be used for debugging. Defaults to False.\n\n    Returns:\n        torch.Tensor: The patches. Shape: (BS, N_h, N_w, C, patch_size, patch_size).\n\n    \"\"\"\n    logger.debug(\n        f\"Creating patches from a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size} and overlap {overlap}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    bs, c, h, w = tensor_tiles.shape\n    assert h &gt; patch_size &gt; overlap\n    assert w &gt; patch_size &gt; overlap\n\n    step_size = patch_size - overlap\n\n    # The problem with unfold is that is cuts off the last patch if it doesn't fit exactly\n    # Padding could help, but then the next problem is that the view needs to get reshaped (copied in memory)\n    # to fit the model input shape. Such a complex view can't be inserted into the model.\n    # Since we need, doing it manually is currently our best choice, since be can avoid the padding.\n    # patches = (\n    #     tensor_tiles.unfold(2, patch_size, step_size).unfold(3, patch_size, step_size).transpose(1, 2).transpose(2, 3)\n    # )\n    # return patches\n\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = torch.zeros((bs, nh, nw, c, patch_size, patch_size), device=tensor_tiles.device)\n    coords = torch.zeros((nh, nw, 5))\n    for i, (y, x, patch_idx_h, patch_idx_w) in enumerate(patch_coords(h, w, patch_size, overlap)):\n        patches[:, patch_idx_h, patch_idx_w, :] = tensor_tiles[:, :, y : y + patch_size, x : x + patch_size]\n        coords[patch_idx_h, patch_idx_w, :] = torch.tensor([i, y, x, patch_idx_h, patch_idx_w])\n\n    if return_coords:\n        return patches, coords\n    else:\n        return patches\n</code></pre>"},{"location":"reference/darts_segmentation/patch_coords/","title":"darts_segmentation.patch_coords","text":"<p>Yield patch coordinates based on height, width, patch size and margin size.</p> <p>Parameters:</p> <ul> <li> <code>h</code>               (<code>int</code>)           \u2013            <p>Height of the image.</p> </li> <li> <code>w</code>               (<code>int</code>)           \u2013            <p>Width of the image.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>Patch size.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>Margin size.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[int, int, int, int]</code>           \u2013            <p>tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>def patch_coords(h: int, w: int, patch_size: int, overlap: int) -&gt; Generator[tuple[int, int, int, int], None, None]:\n    \"\"\"Yield patch coordinates based on height, width, patch size and margin size.\n\n    Args:\n        h (int): Height of the image.\n        w (int): Width of the image.\n        patch_size (int): Patch size.\n        overlap (int): Margin size.\n\n    Yields:\n        tuple[int, int, int, int]: The patch coordinates y, x, patch_idx_y and patch_idx_x.\n\n    \"\"\"\n    step_size = patch_size - overlap\n    # Substract the overlap from h and w so that an exact match of the last patch won't create a duplicate\n    for patch_idx_y, y in enumerate(range(0, h - overlap, step_size)):\n        for patch_idx_x, x in enumerate(range(0, w - overlap, step_size)):\n            if y + patch_size &gt; h:\n                y = h - patch_size\n            if x + patch_size &gt; w:\n                x = w - patch_size\n            yield y, x, patch_idx_y, patch_idx_x\n</code></pre>"},{"location":"reference/darts_segmentation/predict_in_patches/","title":"darts_segmentation.predict_in_patches","text":"<p>Predict on a tensor.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>torch.nn.Module</code>)           \u2013            <p>The model to use for prediction.</p> </li> <li> <code>tensor_tiles</code>               (<code>torch.Tensor</code>)           \u2013            <p>The input tensor. Shape: (BS, C, H, W).</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>The batch size for the prediction, NOT the batch_size of input tiles. Tensor will be sliced into patches and these again will be infered in batches.</p> </li> <li> <code>reflection</code>               (<code>int</code>)           \u2013            <p>Reflection-Padding which will be applied to the edges of the tensor.</p> </li> <li> <code>device</code>               (<code>torch.device</code>, default:                   <code>torch.device</code> )           \u2013            <p>The device to use for the prediction.</p> </li> <li> <code>return_weights</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return the weights. Can be used for debugging. Defaults to False.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>torch.Tensor</code>           \u2013            <p>The predicted tensor.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/utils.py</code> <pre><code>@torch.no_grad()\ndef predict_in_patches(\n    model: nn.Module,\n    tensor_tiles: torch.Tensor,\n    patch_size: int,\n    overlap: int,\n    batch_size: int,\n    reflection: int,\n    device=torch.device,\n    return_weights: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Predict on a tensor.\n\n    Args:\n        model: The model to use for prediction.\n        tensor_tiles: The input tensor. Shape: (BS, C, H, W).\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        batch_size (int): The batch size for the prediction, NOT the batch_size of input tiles.\n            Tensor will be sliced into patches and these again will be infered in batches.\n        reflection (int): Reflection-Padding which will be applied to the edges of the tensor.\n        device (torch.device): The device to use for the prediction.\n        return_weights (bool, optional): Whether to return the weights. Can be used for debugging. Defaults to False.\n\n    Returns:\n        The predicted tensor.\n\n    \"\"\"\n    logger.debug(\n        f\"Predicting on a tensor with shape {tensor_tiles.shape} \"\n        f\"with patch_size {patch_size}, overlap {overlap} and batch_size {batch_size} on device {device}\"\n    )\n    assert tensor_tiles.dim() == 4, f\"Expects tensor_tiles to has shape (BS, C, H, W), got {tensor_tiles.shape}\"\n    # Add a 1px + reflection border to avoid pixel loss when applying the soft margin and to reduce edge-artefacts\n    p = 1 + reflection\n    tensor_tiles = torch.nn.functional.pad(tensor_tiles, (p, p, p, p), mode=\"reflect\")\n    bs, c, h, w = tensor_tiles.shape\n    step_size = patch_size - overlap\n    nh, nw = math.ceil((h - overlap) / step_size), math.ceil((w - overlap) / step_size)\n\n    # Create Patches of size (BS, N_h, N_w, C, patch_size, patch_size)\n    patches = create_patches(tensor_tiles, patch_size=patch_size, overlap=overlap)\n\n    # Flatten the patches so they fit to the model\n    # (BS, N_h, N_w, C, patch_size, patch_size) -&gt; (BS * N_h * N_w, C, patch_size, patch_size)\n    patches = patches.view(bs * nh * nw, c, patch_size, patch_size)\n\n    # Create a soft margin for the patches\n    margin_ramp = torch.cat(\n        [\n            torch.linspace(0, 1, overlap),\n            torch.ones(patch_size - 2 * overlap),\n            torch.linspace(1, 0, overlap),\n        ]\n    )\n    soft_margin = margin_ramp.reshape(1, 1, patch_size) * margin_ramp.reshape(1, patch_size, 1)\n    soft_margin = soft_margin.to(patches.device)\n\n    # Infer logits with model and turn into probabilities with sigmoid in a batched manner\n    # TODO: check with ingmar and jonas if moving all patches to the device at the same time is a good idea\n    patched_probabilities = torch.zeros_like(patches[:, 0, :, :])\n    patches = patches.split(batch_size)\n    n_skipped = 0\n    for i, batch in enumerate(patches):\n        # If batch contains only nans, skip it\n        # TODO: This doesn't work as expected -&gt; check if torch.isnan(batch).all() is correct\n        if torch.isnan(batch).all(axis=0).any():\n            patched_probabilities[i * batch_size : (i + 1) * batch_size] = 0\n            n_skipped += 1\n            continue\n        # If batch contains some nans, replace them with zeros\n        batch[torch.isnan(batch)] = 0\n\n        batch = batch.to(device)\n        # logger.debug(f\"Predicting on batch {i + 1}/{len(patches)}\")\n        patched_probabilities[i * batch_size : (i + 1) * batch_size] = (\n            torch.sigmoid(model(batch)).squeeze(1).to(patched_probabilities.device)\n        )\n        batch = batch.to(patched_probabilities.device)  # Transfer back to the original device to avoid memory leaks\n\n    if n_skipped &gt; 0:\n        logger.debug(f\"Skipped {n_skipped} batches because they only contained NaNs\")\n\n    patched_probabilities = patched_probabilities.view(bs, nh, nw, patch_size, patch_size)\n\n    # Reconstruct the image from the patches\n    prediction = torch.zeros(bs, h, w, device=tensor_tiles.device)\n    weights = torch.zeros(bs, h, w, device=tensor_tiles.device)\n\n    for y, x, patch_idx_h, patch_idx_w in patch_coords(h, w, patch_size, overlap):\n        patch = patched_probabilities[:, patch_idx_h, patch_idx_w]\n        prediction[:, y : y + patch_size, x : x + patch_size] += patch * soft_margin\n        weights[:, y : y + patch_size, x : x + patch_size] += soft_margin\n\n    # Avoid division by zero\n    weights = torch.where(weights == 0, torch.ones_like(weights), weights)\n    prediction = prediction / weights\n\n    # Remove the 1px border and the padding\n    prediction = prediction[:, p:-p, p:-p]\n\n    if return_weights:\n        return prediction, weights\n    else:\n        return prediction\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/","title":"darts_segmentation.metrics","text":"<p>Own metrics for segmentation tasks.</p> <p>Classes:</p> <ul> <li> <code>BinaryBoundaryIoU</code>           \u2013            <p>Binary Boundary IoU metric for binary segmentation tasks.</p> </li> <li> <code>BinaryInstanceAccuracy</code>           \u2013            <p>Binary instance accuracy metric.</p> </li> <li> <code>BinaryInstanceAveragePrecision</code>           \u2013            <p>Compute the average precision for binary instance segmentation.</p> </li> <li> <code>BinaryInstanceConfusionMatrix</code>           \u2013            <p>Binary instance confusion matrix metric.</p> </li> <li> <code>BinaryInstanceF1Score</code>           \u2013            <p>Binary instance F1 score metric.</p> </li> <li> <code>BinaryInstanceFBetaScore</code>           \u2013            <p>Binary instance F-beta score metric.</p> </li> <li> <code>BinaryInstancePrecision</code>           \u2013            <p>Binary instance precision metric.</p> </li> <li> <code>BinaryInstancePrecisionRecallCurve</code>           \u2013            <p>Compute the precision-recall curve for binary instance segmentation.</p> </li> <li> <code>BinaryInstanceRecall</code>           \u2013            <p>Binary instance recall metric.</p> </li> <li> <code>BinaryInstanceStatScores</code>           \u2013            <p>Base class for binary instance segmentation metrics.</p> </li> </ul> <ul> <li>BinaryBoundaryIoU</li> <li>BinaryInstanceAccuracy</li> <li>BinaryInstanceAveragePrecision</li> <li>BinaryInstanceConfusionMatrix</li> <li>BinaryInstanceF1Score</li> <li>BinaryInstanceFBetaScore</li> <li>BinaryInstancePrecision</li> <li>BinaryInstancePrecisionRecallCurve</li> <li>BinaryInstanceRecall</li> <li>BinaryInstanceStatScores</li> </ul>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU","title":"BinaryBoundaryIoU","text":"<pre><code>BinaryBoundaryIoU(\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Unpack[\n        darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs\n    ],\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Binary Boundary IoU metric for binary segmentation tasks.</p> <p>This metric is similar to the Binary Intersection over Union (IoU or Jaccard Index) metric, but instead of comparing all pixels it only compares the boundaries of each foreground object.</p> <p>Create a new instance of the BinaryBoundaryIoU metric.</p> <p>Please see the torchmetrics docs for more info about the **kwargs.</p> <p>Parameters:</p> <ul> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class.  Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>**kwargs</code>               (<code>typing.Unpack[darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the metric.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>zero_division</code>               (<code>int</code>)           \u2013            <p>Value to return when there is a zero division. Default is 0.</p> </li> <li> <code>compute_on_cpu</code>               (<code>bool</code>)           \u2013            <p>If metric state should be stored on CPU during computations. Only works for list states.</p> </li> <li> <code>dist_sync_on_step</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize on <code>forward()</code>. Default is <code>False</code>.</p> </li> <li> <code>process_group</code>               (<code>str</code>)           \u2013            <p>The process group on which the synchronization is called. Default is the world.</p> </li> <li> <code>dist_sync_fn</code>               (<code>callable</code>)           \u2013            <p>Function that performs the allgather option on the metric state. Default is a custom implementation that calls <code>torch.distributed.all_gather</code> internally.</p> </li> <li> <code>distributed_available_fn</code>               (<code>callable</code>)           \u2013            <p>Function that checks if the distributed backend is available. Defaults to a check of <code>torch.distributed.is_available()</code> and <code>torch.distributed.is_initialized()</code>.</p> </li> <li> <code>sync_on_compute</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize when <code>compute</code> is called. Default is <code>True</code>.</p> </li> <li> <code>compute_with_cache</code>               (<code>bool</code>)           \u2013            <p>If results from <code>compute</code> should be cached. Default is <code>True</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If dilation is not a float or int.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              <p>Compute the metric.</p> </li> <li> <code>update</code>             \u2013              <p>Update the metric state.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>dilation</code>           \u2013            </li> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool | None</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>intersection</code>               (<code>torch.Tensor | list[torch.Tensor]</code>)           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>multidim_average</code>           \u2013            </li> <li> <code>plot_lower_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>plot_upper_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>threshold</code>           \u2013            </li> <li> <code>union</code>               (<code>torch.Tensor | list[torch.Tensor]</code>)           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> <li> <code>zero_division</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def __init__(\n    self,\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Unpack[BinaryBoundaryIoUKwargs],\n):\n    \"\"\"Create a new instance of the BinaryBoundaryIoU metric.\n\n    Please see the\n    [torchmetrics docs](https://lightning.ai/docs/torchmetrics/stable/pages/overview.html#metric-kwargs)\n    for more info about the **kwargs.\n\n    Args:\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class.  Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        **kwargs: Additional keyword arguments for the metric.\n\n    Keyword Args:\n        zero_division (int):\n            Value to return when there is a zero division. Default is 0.\n        compute_on_cpu (bool):\n            If metric state should be stored on CPU during computations. Only works for list states.\n        dist_sync_on_step (bool):\n            If metric state should synchronize on ``forward()``. Default is ``False``.\n        process_group (str):\n            The process group on which the synchronization is called. Default is the world.\n        dist_sync_fn (callable):\n            Function that performs the allgather option on the metric state. Default is a custom\n            implementation that calls ``torch.distributed.all_gather`` internally.\n        distributed_available_fn (callable):\n            Function that checks if the distributed backend is available. Defaults to a\n            check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n        sync_on_compute (bool):\n            If metric state should synchronize when ``compute`` is called. Default is ``True``.\n        compute_with_cache (bool):\n            If results from ``compute`` should be cached. Default is ``True``.\n\n    Raises:\n        ValueError: If dilation is not a float or int.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super().__init__(**kwargs)\n\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not isinstance(dilation, float | int):\n            raise ValueError(f\"Expected argument `dilation` to be a float or int, but got {dilation}.\")\n\n    self.dilation = dilation\n    self.threshold = threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if multidim_average == \"samplewise\":\n        self.add_state(\"intersection\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"union\", default=[], dist_reduce_fx=\"cat\")\n    else:\n        self.add_state(\"intersection\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.dilation","title":"dilation  <code>instance-attribute</code>","text":"<pre><code>dilation = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    dilation\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.intersection","title":"intersection  <code>instance-attribute</code>","text":"<pre><code>intersection: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.union","title":"union  <code>instance-attribute</code>","text":"<pre><code>union: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>torch.Tensor</code> )          \u2013            <p>The computed metric.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\n\n    Returns:\n        Tensor: The computed metric.\n\n    \"\"\"\n    if self.multidim_average == \"global\":\n        return self.intersection / self.union\n    else:\n        self.intersection = torch.tensor(self.intersection)\n        self.union = torch.tensor(self.union)\n        return self.intersection / self.union\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryBoundaryIoU.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input arguments are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input shapes are invalid.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If the input arguments are invalid.\n        ValueError: If the input shapes are invalid.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.shape == target.shape:\n            raise ValueError(\n                f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\"\n            )\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.\")\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    target = target.to(torch.uint8)\n    preds = preds.to(torch.uint8)\n\n    target_boundary = get_boundary((target == 1).to(torch.uint8), self.dilation, self.validate_args)\n    preds_boundary = get_boundary(preds, self.dilation, self.validate_args)\n\n    intersection = target_boundary &amp; preds_boundary\n    union = target_boundary | preds_boundary\n\n    if self.ignore_index is not None:\n        # Important that this is NOT the boundary, but the original mask\n        valid_idx = target != self.ignore_index\n        intersection &amp;= valid_idx\n        union &amp;= valid_idx\n\n    intersection = intersection.sum().item()\n    union = union.sum().item()\n\n    if self.multidim_average == \"global\":\n        self.intersection += intersection\n        self.union += union\n    else:\n        self.intersection.append(intersection)\n        self.union.append(union)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy","title":"BinaryInstanceAccuracy","text":"<pre><code>BinaryInstanceAccuracy(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance accuracy metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              </li> <li> <code>plot</code>             \u2013              </li> <li> <code>update</code>             \u2013              <p>Update the metric state.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool | None</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>matching_threshold</code>           \u2013            </li> <li> <code>multidim_average</code>           \u2013            </li> <li> <code>plot_lower_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>plot_upper_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>threshold</code>           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> <li> <code>zero_division</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _accuracy_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAccuracy.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision","title":"BinaryInstanceAveragePrecision","text":"<pre><code>BinaryInstanceAveragePrecision(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve</code></p> <p>Compute the average precision for binary instance segmentation.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              </li> <li> <code>plot</code>             \u2013              </li> <li> <code>update</code>             \u2013              <p>Update metric states.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>confmat</code>               (<code>torch.Tensor</code>)           \u2013            </li> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>matching_threshold</code>           \u2013            </li> <li> <code>plot_lower_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>plot_upper_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>preds</code>               (<code>list[torch.Tensor]</code>)           \u2013            </li> <li> <code>target</code>               (<code>list[torch.Tensor]</code>)           \u2013            </li> <li> <code>thesholds</code>               (<code>torch.Tensor</code>)           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tensor:  # type: ignore[override]  # noqa: D102\n    return _binary_average_precision_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # type: ignore[override]  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix","title":"BinaryInstanceConfusionMatrix","text":"<pre><code>BinaryInstanceConfusionMatrix(\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance confusion matrix metric.</p> <p>Create a new instance of the BinaryInstanceConfusionMatrix metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, return the confusion matrix normalized by the number of instances. If False, return the confusion matrix without normalization. Defaults to None.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalize</code> is not a bool.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              </li> <li> <code>plot</code>             \u2013              </li> <li> <code>update</code>             \u2013              <p>Update the metric state.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool | None</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>matching_threshold</code>           \u2013            </li> <li> <code>multidim_average</code>           \u2013            </li> <li> <code>normalize</code>           \u2013            </li> <li> <code>threshold</code>           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> <li> <code>zero_division</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceConfusionMatrix metric.\n\n    Args:\n        normalize (bool, optional): If True, return the confusion matrix normalized by the number of instances.\n            If False, return the confusion matrix without normalization. Defaults to None.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `normalize` is not a bool.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        matching_threshold=matching_threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if normalize is not None and not isinstance(normalize, bool):\n        raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.normalize","title":"normalize  <code>instance-attribute</code>","text":"<pre><code>normalize = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix(\n    normalize\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    # tn is always 0\n    if self.normalize:\n        all = tp + fp + fn\n        return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n    else:\n        return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n    add_text: bool = True,\n    labels: list[str] | None = None,\n    cmap: torchmetrics.utilities.plot._CMAP_TYPE\n    | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n    add_text: bool = True,\n    labels: list[str] | None = None,  # type: ignore\n    cmap: _CMAP_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    val = val or self.compute()\n    if not isinstance(val, Tensor):\n        raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n    fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n    return fig, ax\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score","title":"BinaryInstanceF1Score","text":"<pre><code>BinaryInstanceF1Score(\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore</code></p> <p>Binary instance F1 score metric.</p> <p>Create a new instance of the BinaryInstanceF1Score metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              </li> <li> <code>plot</code>             \u2013              </li> <li> <code>update</code>             \u2013              <p>Update the metric state.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>beta</code>           \u2013            </li> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool | None</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>matching_threshold</code>           \u2013            </li> <li> <code>multidim_average</code>           \u2013            </li> <li> <code>plot_lower_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>plot_upper_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>threshold</code>           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> <li> <code>zero_division</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceF1Score metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        beta=1.0,\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=validate_args,\n        zero_division=zero_division,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceF1Score.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore","title":"BinaryInstanceFBetaScore","text":"<pre><code>BinaryInstanceFBetaScore(\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance F-beta score metric.</p> <p>Create a new instance of the BinaryInstanceFBetaScore metric.</p> <p>Parameters:</p> <ul> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The beta parameter for the F-beta score.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              </li> <li> <code>plot</code>             \u2013              </li> <li> <code>update</code>             \u2013              <p>Update the metric state.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>beta</code>           \u2013            </li> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool | None</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>matching_threshold</code>           \u2013            </li> <li> <code>multidim_average</code>           \u2013            </li> <li> <code>plot_lower_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>plot_upper_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>threshold</code>           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> <li> <code>zero_division</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceFBetaScore metric.\n\n    Args:\n        beta (float): The beta parameter for the F-beta score.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if validate_args:\n        _binary_fbeta_score_arg_validation(beta, threshold, multidim_average, ignore_index, zero_division)\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n    self.beta = beta\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceFBetaScore.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision","title":"BinaryInstancePrecision","text":"<pre><code>BinaryInstancePrecision(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance precision metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              </li> <li> <code>plot</code>             \u2013              </li> <li> <code>update</code>             \u2013              <p>Update the metric state.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool | None</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>matching_threshold</code>           \u2013            </li> <li> <code>multidim_average</code>           \u2013            </li> <li> <code>plot_lower_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>plot_upper_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>threshold</code>           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> <li> <code>zero_division</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"precision\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve","title":"BinaryInstancePrecisionRecallCurve","text":"<pre><code>BinaryInstancePrecisionRecallCurve(\n    thresholds: int | list[float] | torch.Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.Metric</code></p> <p>Compute the precision-recall curve for binary instance segmentation.</p> <p>This metric works similar to <code>torchmetrics.classification.PrecisionRecallCurve</code>, with two key differences: 1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.     Instead of calculating the values for each pixel. 2. The \"thresholds\" argument is required.     Calculating the thresholds at the compute stage would cost to much memory for this usecase.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              </li> <li> <code>plot</code>             \u2013              </li> <li> <code>update</code>             \u2013              <p>Update metric states.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>confmat</code>               (<code>torch.Tensor</code>)           \u2013            </li> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool | None</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>matching_threshold</code>           \u2013            </li> <li> <code>preds</code>               (<code>list[torch.Tensor]</code>)           \u2013            </li> <li> <code>target</code>               (<code>list[torch.Tensor]</code>)           \u2013            </li> <li> <code>thesholds</code>               (<code>torch.Tensor</code>)           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.compute","title":"compute","text":"<pre><code>compute() -&gt; tuple[\n    torch.Tensor, torch.Tensor, torch.Tensor\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; tuple[Tensor, Tensor, Tensor]:  # noqa: D102\n    return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.plot","title":"plot","text":"<pre><code>plot(\n    curve: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n    | None = None,\n    score: torch.Tensor | bool | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    curve: tuple[Tensor, Tensor, Tensor] | None = None,\n    score: Tensor | bool | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    curve_computed = curve or self.compute()\n    # switch order as the standard way is recall along x-axis and precision along y-axis\n    curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n    score = (\n        _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n        if not curve and score is True\n        else None\n    )\n    return plot_curve(\n        curve_computed, score=score, ax=ax, label_names=(\"Recall\", \"Precision\"), name=self.__class__.__name__\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall","title":"BinaryInstanceRecall","text":"<pre><code>BinaryInstanceRecall(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance recall metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              </li> <li> <code>plot</code>             \u2013              </li> <li> <code>update</code>             \u2013              <p>Update the metric state.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool | None</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>matching_threshold</code>           \u2013            </li> <li> <code>multidim_average</code>           \u2013            </li> <li> <code>plot_lower_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>plot_upper_bound</code>               (<code>float</code>)           \u2013            </li> <li> <code>threshold</code>           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> <li> <code>zero_division</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"recall\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceRecall.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores","title":"BinaryInstanceStatScores","text":"<pre><code>BinaryInstanceStatScores(\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: typing.Literal[\n        \"global\", \"samplewise\"\n    ] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: typing.Any,\n)\n</code></pre> <p>               Bases: <code>torchmetrics.classification.stat_scores._AbstractStatScores</code></p> <p>Base class for binary instance segmentation metrics.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>compute</code>             \u2013              </li> <li> <code>update</code>             \u2013              <p>Update the metric state.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>full_state_update</code>               (<code>bool</code>)           \u2013            </li> <li> <code>higher_is_better</code>               (<code>bool | None</code>)           \u2013            </li> <li> <code>ignore_index</code>           \u2013            </li> <li> <code>is_differentiable</code>               (<code>bool</code>)           \u2013            </li> <li> <code>matching_threshold</code>           \u2013            </li> <li> <code>multidim_average</code>           \u2013            </li> <li> <code>threshold</code>           \u2013            </li> <li> <code>validate_args</code>           \u2013            </li> <li> <code>zero_division</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _binary_stat_scores_compute(tp, fp, tn, fn, self.multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/#darts_segmentation.metrics.BinaryInstanceStatScores.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/","title":"darts_segmentation.metrics.BinaryBoundaryIoU","text":"<p>               Bases: <code>torchmetrics.Metric</code></p> <p>Binary Boundary IoU metric for binary segmentation tasks.</p> <p>This metric is similar to the Binary Intersection over Union (IoU or Jaccard Index) metric, but instead of comparing all pixels it only compares the boundaries of each foreground object.</p> <p>Create a new instance of the BinaryBoundaryIoU metric.</p> <p>Please see the torchmetrics docs for more info about the **kwargs.</p> <p>Parameters:</p> <ul> <li> <code>dilation</code>               (<code>float | int</code>, default:                   <code>0.02</code> )           \u2013            <p>The dilation (factor) / width of the boundary. Dilation in pixels if int, else ratio to calculate <code>dilation = dilation_ratio * image_diagonal</code>. Default: 0.02</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class.  Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>**kwargs</code>               (<code>typing.Unpack[darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoUKwargs]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments for the metric.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>zero_division</code>               (<code>int</code>)           \u2013            <p>Value to return when there is a zero division. Default is 0.</p> </li> <li> <code>compute_on_cpu</code>               (<code>bool</code>)           \u2013            <p>If metric state should be stored on CPU during computations. Only works for list states.</p> </li> <li> <code>dist_sync_on_step</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize on <code>forward()</code>. Default is <code>False</code>.</p> </li> <li> <code>process_group</code>               (<code>str</code>)           \u2013            <p>The process group on which the synchronization is called. Default is the world.</p> </li> <li> <code>dist_sync_fn</code>               (<code>callable</code>)           \u2013            <p>Function that performs the allgather option on the metric state. Default is a custom implementation that calls <code>torch.distributed.all_gather</code> internally.</p> </li> <li> <code>distributed_available_fn</code>               (<code>callable</code>)           \u2013            <p>Function that checks if the distributed backend is available. Defaults to a check of <code>torch.distributed.is_available()</code> and <code>torch.distributed.is_initialized()</code>.</p> </li> <li> <code>sync_on_compute</code>               (<code>bool</code>)           \u2013            <p>If metric state should synchronize when <code>compute</code> is called. Default is <code>True</code>.</p> </li> <li> <code>compute_with_cache</code>               (<code>bool</code>)           \u2013            <p>If results from <code>compute</code> should be cached. Default is <code>True</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If dilation is not a float or int.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def __init__(\n    self,\n    dilation: float | int = 0.02,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Unpack[BinaryBoundaryIoUKwargs],\n):\n    \"\"\"Create a new instance of the BinaryBoundaryIoU metric.\n\n    Please see the\n    [torchmetrics docs](https://lightning.ai/docs/torchmetrics/stable/pages/overview.html#metric-kwargs)\n    for more info about the **kwargs.\n\n    Args:\n        dilation (float | int, optional): The dilation (factor) / width of the boundary.\n            Dilation in pixels if int, else ratio to calculate `dilation = dilation_ratio * image_diagonal`.\n            Default: 0.02\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class.  Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        **kwargs: Additional keyword arguments for the metric.\n\n    Keyword Args:\n        zero_division (int):\n            Value to return when there is a zero division. Default is 0.\n        compute_on_cpu (bool):\n            If metric state should be stored on CPU during computations. Only works for list states.\n        dist_sync_on_step (bool):\n            If metric state should synchronize on ``forward()``. Default is ``False``.\n        process_group (str):\n            The process group on which the synchronization is called. Default is the world.\n        dist_sync_fn (callable):\n            Function that performs the allgather option on the metric state. Default is a custom\n            implementation that calls ``torch.distributed.all_gather`` internally.\n        distributed_available_fn (callable):\n            Function that checks if the distributed backend is available. Defaults to a\n            check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.\n        sync_on_compute (bool):\n            If metric state should synchronize when ``compute`` is called. Default is ``True``.\n        compute_with_cache (bool):\n            If results from ``compute`` should be cached. Default is ``True``.\n\n    Raises:\n        ValueError: If dilation is not a float or int.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super().__init__(**kwargs)\n\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not isinstance(dilation, float | int):\n            raise ValueError(f\"Expected argument `dilation` to be a float or int, but got {dilation}.\")\n\n    self.dilation = dilation\n    self.threshold = threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    if multidim_average == \"samplewise\":\n        self.add_state(\"intersection\", default=[], dist_reduce_fx=\"cat\")\n        self.add_state(\"union\", default=[], dist_reduce_fx=\"cat\")\n    else:\n        self.add_state(\"intersection\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"union\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.dilation","title":"dilation  <code>instance-attribute</code>","text":"<pre><code>dilation = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    dilation\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.intersection","title":"intersection  <code>instance-attribute</code>","text":"<pre><code>intersection: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.union","title":"union  <code>instance-attribute</code>","text":"<pre><code>union: torch.Tensor | list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.boundary_iou.BinaryBoundaryIoU(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> <p>Compute the metric.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code> (              <code>torch.Tensor</code> )          \u2013            <p>The computed metric.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def compute(self) -&gt; Tensor:\n    \"\"\"Compute the metric.\n\n    Returns:\n        Tensor: The computed metric.\n\n    \"\"\"\n    if self.multidim_average == \"global\":\n        return self.intersection / self.union\n    else:\n        self.intersection = torch.tensor(self.intersection)\n        self.union = torch.tensor(self.union)\n        return self.intersection / self.union\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryBoundaryIoU/#darts_segmentation.metrics.BinaryBoundaryIoU.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input arguments are invalid.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input shapes are invalid.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/boundary_iou.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If the input arguments are invalid.\n        ValueError: If the input shapes are invalid.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.shape == target.shape:\n            raise ValueError(\n                f\"Expected `preds` and `target` to have the same shape, but got {preds.shape} and {target.shape}.\"\n            )\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions, but got {preds.dim()}.\")\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    target = target.to(torch.uint8)\n    preds = preds.to(torch.uint8)\n\n    target_boundary = get_boundary((target == 1).to(torch.uint8), self.dilation, self.validate_args)\n    preds_boundary = get_boundary(preds, self.dilation, self.validate_args)\n\n    intersection = target_boundary &amp; preds_boundary\n    union = target_boundary | preds_boundary\n\n    if self.ignore_index is not None:\n        # Important that this is NOT the boundary, but the original mask\n        valid_idx = target != self.ignore_index\n        intersection &amp;= valid_idx\n        union &amp;= valid_idx\n\n    intersection = intersection.sum().item()\n    union = union.sum().item()\n\n    if self.multidim_average == \"global\":\n        self.intersection += intersection\n        self.union += union\n    else:\n        self.intersection.append(intersection)\n        self.union.append(union)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/","title":"darts_segmentation.metrics.BinaryInstanceAccuracy","text":"<p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance accuracy metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _accuracy_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAccuracy/#darts_segmentation.metrics.BinaryInstanceAccuracy.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/","title":"darts_segmentation.metrics.BinaryInstanceAveragePrecision","text":"<p>               Bases: <code>darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve</code></p> <p>Compute the average precision for binary instance segmentation.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; Tensor:  # type: ignore[override]  # noqa: D102\n    return _binary_average_precision_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # type: ignore[override]  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceAveragePrecision/#darts_segmentation.metrics.BinaryInstanceAveragePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/","title":"darts_segmentation.metrics.BinaryInstanceConfusionMatrix","text":"<p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance confusion matrix metric.</p> <p>Create a new instance of the BinaryInstanceConfusionMatrix metric.</p> <p>Parameters:</p> <ul> <li> <code>normalize</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>If True, return the confusion matrix normalized by the number of instances. If False, return the confusion matrix without normalization. Defaults to None.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>normalize</code> is not a bool.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    normalize: bool | None = None,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceConfusionMatrix metric.\n\n    Args:\n        normalize (bool, optional): If True, return the confusion matrix normalized by the number of instances.\n            If False, return the confusion matrix without normalization. Defaults to None.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `normalize` is not a bool.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        matching_threshold=matching_threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if normalize is not None and not isinstance(normalize, bool):\n        raise ValueError(f\"Argument `normalize` needs to be of bool type but got {type(normalize)}\")\n    self.normalize = normalize\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.normalize","title":"normalize  <code>instance-attribute</code>","text":"<pre><code>normalize = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceConfusionMatrix(\n    normalize\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    # tn is always 0\n    if self.normalize:\n        all = tp + fp + fn\n        return torch.tensor([[0, fp / all], [fn / all, tp / all]], device=tp.device)\n    else:\n        return torch.tensor([[tn, fp], [fn, tp]], device=tp.device)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n    add_text: bool = True,\n    labels: list[str] | None = None,\n    cmap: torchmetrics.utilities.plot._CMAP_TYPE\n    | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n    add_text: bool = True,\n    labels: list[str] | None = None,  # type: ignore\n    cmap: _CMAP_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    val = val or self.compute()\n    if not isinstance(val, Tensor):\n        raise TypeError(f\"Expected val to be a single tensor but got {val}\")\n    fig, ax = plot_confusion_matrix(val, ax=ax, add_text=add_text, labels=labels, cmap=cmap)\n    return fig, ax\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceConfusionMatrix/#darts_segmentation.metrics.BinaryInstanceConfusionMatrix.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/","title":"darts_segmentation.metrics.BinaryInstanceF1Score","text":"<p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore</code></p> <p>Binary instance F1 score metric.</p> <p>Create a new instance of the BinaryInstanceF1Score metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceF1Score metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        beta=1.0,\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=validate_args,\n        zero_division=zero_division,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceF1Score/#darts_segmentation.metrics.BinaryInstanceF1Score.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/","title":"darts_segmentation.metrics.BinaryInstanceFBetaScore","text":"<p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance F-beta score metric.</p> <p>Create a new instance of the BinaryInstanceFBetaScore metric.</p> <p>Parameters:</p> <ul> <li> <code>beta</code>               (<code>float</code>)           \u2013            <p>The beta parameter for the F-beta score.</p> </li> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>zero_division</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Value to return when there is a zero division. Defaults to 0.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    beta: float,\n    threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    zero_division: float = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceFBetaScore metric.\n\n    Args:\n        beta (float): The beta parameter for the F-beta score.\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        zero_division (float, optional): Value to return when there is a zero division. Defaults to 0.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    \"\"\"\n    super().__init__(\n        threshold=threshold,\n        multidim_average=multidim_average,\n        ignore_index=ignore_index,\n        validate_args=False,\n        **kwargs,\n    )\n    if validate_args:\n        _binary_fbeta_score_arg_validation(beta, threshold, multidim_average, ignore_index, zero_division)\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n    self.beta = beta\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.beta","title":"beta  <code>instance-attribute</code>","text":"<pre><code>beta = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    beta\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceFBetaScore(\n    zero_division\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _fbeta_reduce(\n        tp,\n        fp,\n        tn,\n        fn,\n        self.beta,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceFBetaScore/#darts_segmentation.metrics.BinaryInstanceFBetaScore.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/","title":"darts_segmentation.metrics.BinaryInstancePrecision","text":"<p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance precision metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"precision\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecision/#darts_segmentation.metrics.BinaryInstancePrecision.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/","title":"darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve","text":"<p>               Bases: <code>torchmetrics.Metric</code></p> <p>Compute the precision-recall curve for binary instance segmentation.</p> <p>This metric works similar to <code>torchmetrics.classification.PrecisionRecallCurve</code>, with two key differences: 1. It calculates the tp, fp, fn values for each instance (blob) in the batch, and then aggregates them.     Instead of calculating the values for each pixel. 2. The \"thresholds\" argument is required.     Calculating the thresholds at the compute stage would cost to much memory for this usecase.</p> <p>Create a new instance of the BinaryInstancePrecisionRecallCurve metric.</p> <p>Parameters:</p> <ul> <li> <code>thresholds</code>               (<code>int | list[float] | torch.Tensor</code>, default:                   <code>None</code> )           \u2013            <p>The thresholds to use for the curve. Defaults to None.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If thresholds is None.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def __init__(\n    self,\n    thresholds: int | list[float] | Tensor = None,\n    matching_threshold: float = 0.5,\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstancePrecisionRecallCurve metric.\n\n    Args:\n        thresholds (int | list[float] | Tensor, optional): The thresholds to use for the curve. Defaults to None.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If thresholds is None.\n\n    \"\"\"\n    super().__init__(**kwargs)\n    if validate_args:\n        _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n        if thresholds is None:\n            raise ValueError(\"Argument `thresholds` must be provided for this metric.\")\n\n    self.matching_threshold = matching_threshold\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n\n    thresholds = _adjust_threshold_arg(thresholds)\n    self.register_buffer(\"thresholds\", thresholds, persistent=False)\n    self.add_state(\"confmat\", default=torch.zeros(len(thresholds), 2, 2, dtype=torch.long), dist_reduce_fx=\"sum\")\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.confmat","title":"confmat  <code>instance-attribute</code>","text":"<pre><code>confmat: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.preds","title":"preds  <code>instance-attribute</code>","text":"<pre><code>preds: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.target","title":"target  <code>instance-attribute</code>","text":"<pre><code>target: list[torch.Tensor]\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.thesholds","title":"thesholds  <code>instance-attribute</code>","text":"<pre><code>thesholds: torch.Tensor\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_prc.BinaryInstancePrecisionRecallCurve(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.compute","title":"compute","text":"<pre><code>compute() -&gt; tuple[\n    torch.Tensor, torch.Tensor, torch.Tensor\n]\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def compute(self) -&gt; tuple[Tensor, Tensor, Tensor]:  # noqa: D102\n    return _binary_precision_recall_curve_compute(self.confmat, self.thresholds)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.plot","title":"plot","text":"<pre><code>plot(\n    curve: tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n    | None = None,\n    score: torch.Tensor | bool | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    curve: tuple[Tensor, Tensor, Tensor] | None = None,\n    score: Tensor | bool | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    curve_computed = curve or self.compute()\n    # switch order as the standard way is recall along x-axis and precision along y-axis\n    curve_computed = (curve_computed[1], curve_computed[0], curve_computed[2])\n\n    score = (\n        _auc_compute_without_check(curve_computed[0], curve_computed[1], direction=-1.0)\n        if not curve and score is True\n        else None\n    )\n    return plot_curve(\n        curve_computed, score=score, ax=ax, label_names=(\"Recall\", \"Precision\"), name=self.__class__.__name__\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstancePrecisionRecallCurve/#darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update metric states.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>The predicted mask. Shape: (batch_size, height, width)</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>The target mask. Shape: (batch_size, height, width)</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If preds and target have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_prc.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update metric states.\n\n    Args:\n        preds (Tensor): The predicted mask. Shape: (batch_size, height, width)\n        target (Tensor): The target mask. Shape: (batch_size, height, width)\n\n    Raises:\n        ValueError: If preds and target have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_precision_recall_curve_tensor_validation(preds, target, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n        preds = preds.sigmoid()\n\n    if self.ignore_index is not None:\n        target = (target == 1).to(torch.uint8)\n\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n\n    len_t = len(self.thresholds)\n    confmat = self.thresholds.new_zeros((len_t, 2, 2), dtype=torch.int64)\n    for i in range(len_t):\n        preds_i = preds &gt;= self.thresholds[i]\n\n        if self.ignore_index is not None:\n            invalid_idx = target == self.ignore_index\n            preds_i = preds_i.clone()\n            preds_i[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n\n        instance_list_preds_i = mask_to_instances(preds_i.to(torch.uint8), self.validate_args)\n        for target_i, preds_i in zip(instance_list_target, instance_list_preds_i):\n            tp, fp, fn = match_instances(\n                target_i,\n                preds_i,\n                match_threshold=self.matching_threshold,\n                validate_args=self.validate_args,\n            )\n            confmat[i, 1, 1] += tp\n            confmat[i, 0, 1] += fp\n            confmat[i, 1, 0] += fn\n    self.confmat += confmat\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/","title":"darts_segmentation.metrics.BinaryInstanceRecall","text":"<p>               Bases: <code>darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores</code></p> <p>Binary instance recall metric.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = True\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.plot_lower_bound","title":"plot_lower_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_lower_bound: float = 0.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.plot_upper_bound","title":"plot_upper_bound  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>plot_upper_bound: float = 1.0\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _precision_recall_reduce(\n        \"recall\",\n        tp,\n        fp,\n        tn,\n        fn,\n        average=\"binary\",\n        multidim_average=self.multidim_average,\n        zero_division=self.zero_division,\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.plot","title":"plot","text":"<pre><code>plot(\n    val: torch.Tensor\n    | collections.abc.Sequence[torch.Tensor]\n    | None = None,\n    ax: torchmetrics.utilities.plot._AX_TYPE | None = None,\n) -&gt; torchmetrics.utilities.plot._PLOT_OUT_TYPE\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def plot(  # noqa: D102\n    self,\n    val: Tensor | Sequence[Tensor] | None = None,\n    ax: _AX_TYPE | None = None,  # type: ignore\n) -&gt; _PLOT_OUT_TYPE:  # type: ignore\n    return self._plot(val, ax)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceRecall/#darts_segmentation.metrics.BinaryInstanceRecall.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/","title":"darts_segmentation.metrics.BinaryInstanceStatScores","text":"<p>               Bases: <code>torchmetrics.classification.stat_scores._AbstractStatScores</code></p> <p>Base class for binary instance segmentation metrics.</p> <p>Create a new instance of the BinaryInstanceStatScores metric.</p> <p>Parameters:</p> <ul> <li> <code>threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>Threshold for binarizing the prediction. Has no effect if the prediction is already binarized. Defaults to 0.5.</p> </li> <li> <code>matching_threshold</code>               (<code>float</code>, default:                   <code>0.5</code> )           \u2013            <p>The threshold for matching instances. Defaults to 0.5.</p> </li> <li> <code>multidim_average</code>               (<code>typing.Literal['global', 'samplewise']</code>, default:                   <code>'global'</code> )           \u2013            <p>How the average over multiple batches is calculated. Defaults to \"global\".</p> </li> <li> <code>ignore_index</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Ignores an invalid class. Defaults to None.</p> </li> <li> <code>validate_args</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Weather to validate inputs. Defaults to True.</p> </li> <li> <code>kwargs</code>               (<code>typing.Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments for the Metric class, regarding compute-methods. Please refer to torchmetrics for more examples.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>matching_threshold</code> is not a float in the [0,1] range.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def __init__(\n    self,\n    threshold: float = 0.5,\n    matching_threshold: float = 0.5,\n    multidim_average: Literal[\"global\", \"samplewise\"] = \"global\",\n    ignore_index: int | None = None,\n    validate_args: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Create a new instance of the BinaryInstanceStatScores metric.\n\n    Args:\n        threshold (float, optional): Threshold for binarizing the prediction.\n            Has no effect if the prediction is already binarized. Defaults to 0.5.\n        matching_threshold (float, optional): The threshold for matching instances. Defaults to 0.5.\n        multidim_average (Literal[\"global\", \"samplewise\"], optional): How the average over multiple batches is\n            calculated. Defaults to \"global\".\n        ignore_index (int | None, optional): Ignores an invalid class. Defaults to None.\n        validate_args (bool, optional): Weather to validate inputs. Defaults to True.\n        kwargs: Additional arguments for the Metric class, regarding compute-methods.\n            Please refer to torchmetrics for more examples.\n\n    Raises:\n        ValueError: If `matching_threshold` is not a float in the [0,1] range.\n\n    \"\"\"\n    zero_division = kwargs.pop(\"zero_division\", 0)\n    super(_AbstractStatScores, self).__init__(**kwargs)\n    if validate_args:\n        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index, zero_division)\n        if not (isinstance(matching_threshold, float) and (0 &lt;= matching_threshold &lt;= 1)):\n            raise ValueError(\n                f\"Expected arg `matching_threshold` to be a float in the [0,1] range, but got {matching_threshold}.\"\n            )\n\n    self.threshold = threshold\n    self.matching_threshold = matching_threshold\n    self.multidim_average = multidim_average\n    self.ignore_index = ignore_index\n    self.validate_args = validate_args\n    self.zero_division = zero_division\n\n    self._create_state(size=1, multidim_average=multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.full_state_update","title":"full_state_update  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>full_state_update: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.higher_is_better","title":"higher_is_better  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>higher_is_better: bool | None = None\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.ignore_index","title":"ignore_index  <code>instance-attribute</code>","text":"<pre><code>ignore_index = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    ignore_index\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.is_differentiable","title":"is_differentiable  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>is_differentiable: bool = False\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.matching_threshold","title":"matching_threshold  <code>instance-attribute</code>","text":"<pre><code>matching_threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    matching_threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.multidim_average","title":"multidim_average  <code>instance-attribute</code>","text":"<pre><code>multidim_average = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    multidim_average\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.threshold","title":"threshold  <code>instance-attribute</code>","text":"<pre><code>threshold = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    threshold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.validate_args","title":"validate_args  <code>instance-attribute</code>","text":"<pre><code>validate_args = darts_segmentation.metrics.binary_instance_stat_scores.BinaryInstanceStatScores(\n    validate_args\n)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.zero_division","title":"zero_division  <code>instance-attribute</code>","text":"<pre><code>zero_division = zero_division\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.compute","title":"compute","text":"<pre><code>compute() -&gt; torch.Tensor\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def compute(self) -&gt; Tensor:  # noqa: D102\n    tp, fp, tn, fn = self._final_state()\n    return _binary_stat_scores_compute(tp, fp, tn, fn, self.multidim_average)\n</code></pre>"},{"location":"reference/darts_segmentation/metrics/BinaryInstanceStatScores/#darts_segmentation.metrics.BinaryInstanceStatScores.update","title":"update","text":"<pre><code>update(preds: torch.Tensor, target: torch.Tensor) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and then binarized using the threshold. If the predictions are probabilities, they are binarized using the threshold.</p> <p>Parameters:</p> <ul> <li> <code>preds</code>               (<code>torch.Tensor</code>)           \u2013            <p>Predictions from model (logits or probabilities).</p> </li> <li> <code>target</code>               (<code>torch.Tensor</code>)           \u2013            <p>Ground truth labels.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If <code>preds</code> and <code>target</code> have different shapes.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the input targets are not binary masks.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/metrics/binary_instance_stat_scores.py</code> <pre><code>def update(self, preds: Tensor, target: Tensor) -&gt; None:\n    \"\"\"Update the metric state.\n\n    If the predictions are logits (not between 0 and 1), they are converted to probabilities using a sigmoid and\n    then binarized using the threshold.\n    If the predictions are probabilities, they are binarized using the threshold.\n\n    Args:\n        preds (Tensor): Predictions from model (logits or probabilities).\n        target (Tensor): Ground truth labels.\n\n    Raises:\n        ValueError: If `preds` and `target` have different shapes.\n        ValueError: If the input targets are not binary masks.\n\n    \"\"\"\n    if self.validate_args:\n        _binary_stat_scores_tensor_validation(preds, target, self.multidim_average, self.ignore_index)\n        if not preds.dim() == 3:\n            raise ValueError(f\"Expected `preds` and `target` to have 3 dimensions (BHW), but got {preds.dim()}.\")\n        if self.ignore_index is None and target.max() &gt; 1:\n            raise ValueError(\n                \"Expected binary mask, got more than 1 unique value in target.\"\n                \" You can set 'ignore_index' to ignore a class.\"\n            )\n\n    # Format\n    if preds.is_floating_point():\n        if not torch.all((preds &gt;= 0) * (preds &lt;= 1)):\n            # preds is logits, convert with sigmoid\n            preds = preds.sigmoid()\n        preds = preds &gt; self.threshold\n\n    if self.ignore_index is not None:\n        invalid_idx = target == self.ignore_index\n        preds = preds.clone()\n        preds[invalid_idx] = 0  # This will prevent from counting instances in the ignored area\n        target = (target == 1).to(torch.uint8)\n\n    # Update state\n    instance_list_target = mask_to_instances(target.to(torch.uint8), self.validate_args)\n    instance_list_preds = mask_to_instances(preds.to(torch.uint8), self.validate_args)\n\n    for target_i, preds_i in zip(instance_list_target, instance_list_preds):\n        tp, fp, fn = match_instances(\n            target_i,\n            preds_i,\n            match_threshold=self.matching_threshold,\n            validate_args=self.validate_args,\n        )\n        self._update_state(tp, fp, 0, fn)\n</code></pre>"},{"location":"reference/darts_segmentation/training/","title":"darts_segmentation.training","text":"<p>Training related functions and classes for Image Segmentation.</p> <p>Classes:</p> <ul> <li> <code>BinarySegmentationMetrics</code>           \u2013            <p>Callback for validation metrics and visualizations.</p> </li> <li> <code>DartsDataModule</code>           \u2013            </li> <li> <code>DartsDataset</code>           \u2013            </li> <li> <code>DartsDatasetInMemory</code>           \u2013            </li> <li> <code>DartsDatasetZarr</code>           \u2013            </li> <li> <code>SMPSegmenter</code>           \u2013            <p>Lightning module for training a segmentation model using the segmentation_models_pytorch library.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>create_training_patches</code>             \u2013              <p>Create training patches from a tile and labels.</p> </li> </ul> <ul> <li>create_training_patches</li> <li>BinarySegmentationMetrics</li> <li>DartsDataModule</li> <li>DartsDataset</li> <li>DartsDatasetInMemory</li> <li>DartsDatasetZarr</li> <li>SMPSegmenter</li> </ul>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics","title":"BinarySegmentationMetrics","text":"<pre><code>BinarySegmentationMetrics(\n    *,\n    input_combination: list[str],\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n    is_crossval: bool = False,\n)\n</code></pre> <p>               Bases: <code>lightning.pytorch.callbacks.Callback</code></p> <p>Callback for validation metrics and visualizations.</p> <p>Initialize the ValidationCallback.</p> <p>Parameters:</p> <ul> <li> <code>input_combination</code>               (<code>list[str]</code>)           \u2013            <p>List of input names to combine for the visualization.</p> </li> <li> <code>val_set</code>               (<code>str</code>, default:                   <code>'val'</code> )           \u2013            <p>Name of the validation set. Only used for naming the validation metrics. Defaults to \"val\".</p> </li> <li> <code>test_set</code>               (<code>str</code>, default:                   <code>'test'</code> )           \u2013            <p>Name of the test set. Only used for naming the test metrics. Defaults to \"test\".</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>is_crossval</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether the training is done with cross-validation. This will change the logging behavior of scalar metrics from logging to {val_set} to just \"val\". The logging behaviour of the samples is not affected. Defaults to False.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>is_val_plot_epoch</code>             \u2013              <p>Check if the current epoch is an epoch where validation samples should be plotted.</p> </li> <li> <code>on_test_batch_end</code>             \u2013              </li> <li> <code>on_test_epoch_end</code>             \u2013              </li> <li> <code>on_train_batch_end</code>             \u2013              </li> <li> <code>on_train_epoch_end</code>             \u2013              </li> <li> <code>on_validation_batch_end</code>             \u2013              </li> <li> <code>on_validation_epoch_end</code>             \u2013              </li> <li> <code>setup</code>             \u2013              <p>Setups the callback.</p> </li> <li> <code>teardown</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>input_combination</code>           \u2013            </li> <li> <code>is_crossval</code>           \u2013            </li> <li> <code>pl_module</code>               (<code>lightning.LightningModule</code>)           \u2013            </li> <li> <code>plot_every_n_val_epochs</code>           \u2013            </li> <li> <code>stage</code>               (<code>darts_segmentation.training.callbacks.Stage</code>)           \u2013            </li> <li> <code>test_cmx</code>               (<code>torchmetrics.ConfusionMatrix</code>)           \u2013            </li> <li> <code>test_instance_cmx</code>               (<code>darts_segmentation.metrics.BinaryInstanceConfusionMatrix</code>)           \u2013            </li> <li> <code>test_instance_prc</code>               (<code>darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve</code>)           \u2013            </li> <li> <code>test_metrics</code>               (<code>torchmetrics.MetricCollection</code>)           \u2013            </li> <li> <code>test_prc</code>               (<code>torchmetrics.PrecisionRecallCurve</code>)           \u2013            </li> <li> <code>test_roc</code>               (<code>torchmetrics.ROC</code>)           \u2013            </li> <li> <code>test_set</code>           \u2013            </li> <li> <code>train_metrics</code>               (<code>torchmetrics.MetricCollection</code>)           \u2013            </li> <li> <code>trainer</code>               (<code>lightning.Trainer</code>)           \u2013            </li> <li> <code>val_cmx</code>               (<code>torchmetrics.ConfusionMatrix</code>)           \u2013            </li> <li> <code>val_metrics</code>               (<code>torchmetrics.MetricCollection</code>)           \u2013            </li> <li> <code>val_prc</code>               (<code>torchmetrics.PrecisionRecallCurve</code>)           \u2013            </li> <li> <code>val_roc</code>               (<code>torchmetrics.ROC</code>)           \u2013            </li> <li> <code>val_set</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    *,\n    input_combination: list[str],\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n    is_crossval: bool = False,\n):\n    \"\"\"Initialize the ValidationCallback.\n\n    Args:\n        input_combination (list[str]): List of input names to combine for the visualization.\n        val_set (str, optional): Name of the validation set. Only used for naming the validation metrics.\n            Defaults to \"val\".\n        test_set (str, optional): Name of the test set. Only used for naming the test metrics. Defaults to \"test\".\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        is_crossval (bool, optional): Whether the training is done with cross-validation.\n            This will change the logging behavior of scalar metrics from logging to {val_set} to just \"val\".\n            The logging behaviour of the samples is not affected.\n            Defaults to False.\n\n    \"\"\"\n    assert \"/\" not in val_set, \"val_set must not contain '/'\"\n    assert \"/\" not in test_set, \"test_set must not contain '/'\"\n    self.val_set = val_set\n    self.test_set = test_set\n    self.plot_every_n_val_epochs = plot_every_n_val_epochs\n    self.input_combination = input_combination\n    self.is_crossval = is_crossval\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.input_combination","title":"input_combination  <code>instance-attribute</code>","text":"<pre><code>input_combination = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    input_combination\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.is_crossval","title":"is_crossval  <code>instance-attribute</code>","text":"<pre><code>is_crossval = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    is_crossval\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.pl_module","title":"pl_module  <code>instance-attribute</code>","text":"<pre><code>pl_module: lightning.LightningModule\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    plot_every_n_val_epochs\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.stage","title":"stage  <code>instance-attribute</code>","text":"<pre><code>stage: darts_segmentation.training.callbacks.Stage\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.test_cmx","title":"test_cmx  <code>instance-attribute</code>","text":"<pre><code>test_cmx: torchmetrics.ConfusionMatrix\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.test_instance_cmx","title":"test_instance_cmx  <code>instance-attribute</code>","text":"<pre><code>test_instance_cmx: (\n    darts_segmentation.metrics.BinaryInstanceConfusionMatrix\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.test_instance_prc","title":"test_instance_prc  <code>instance-attribute</code>","text":"<pre><code>test_instance_prc: darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.test_metrics","title":"test_metrics  <code>instance-attribute</code>","text":"<pre><code>test_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.test_prc","title":"test_prc  <code>instance-attribute</code>","text":"<pre><code>test_prc: torchmetrics.PrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.test_roc","title":"test_roc  <code>instance-attribute</code>","text":"<pre><code>test_roc: torchmetrics.ROC\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.test_set","title":"test_set  <code>instance-attribute</code>","text":"<pre><code>test_set = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    test_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.train_metrics","title":"train_metrics  <code>instance-attribute</code>","text":"<pre><code>train_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: lightning.Trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.val_cmx","title":"val_cmx  <code>instance-attribute</code>","text":"<pre><code>val_cmx: torchmetrics.ConfusionMatrix\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.val_metrics","title":"val_metrics  <code>instance-attribute</code>","text":"<pre><code>val_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.val_prc","title":"val_prc  <code>instance-attribute</code>","text":"<pre><code>val_prc: torchmetrics.PrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.val_roc","title":"val_roc  <code>instance-attribute</code>","text":"<pre><code>val_roc: torchmetrics.ROC\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.val_set","title":"val_set  <code>instance-attribute</code>","text":"<pre><code>val_set = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    val_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.is_val_plot_epoch","title":"is_val_plot_epoch","text":"<pre><code>is_val_plot_epoch(\n    current_epoch: int, check_val_every_n_epoch: int | None\n) -&gt; bool\n</code></pre> <p>Check if the current epoch is an epoch where validation samples should be plotted.</p> <p>Parameters:</p> <ul> <li> <code>current_epoch</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int | None</code>)           \u2013            <p>The number of epochs to check for plotting. If None, no plotting is done.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the current epoch is a plot epoch, False otherwise.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def is_val_plot_epoch(self, current_epoch: int, check_val_every_n_epoch: int | None) -&gt; bool:\n    \"\"\"Check if the current epoch is an epoch where validation samples should be plotted.\n\n    Args:\n        current_epoch (int): The current epoch.\n        check_val_every_n_epoch (int | None): The number of epochs to check for plotting.\n            If None, no plotting is done.\n\n    Returns:\n        bool: True if the current epoch is a plot epoch, False otherwise.\n\n    \"\"\"\n    if check_val_every_n_epoch is None:\n        return False\n    n = self.plot_every_n_val_epochs * check_val_every_n_epoch\n    return ((current_epoch + 1) % n) == 0 or current_epoch == 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.on_test_batch_end","title":"on_test_batch_end","text":"<pre><code>on_test_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    pl_module.log(f\"{self.test_set}/loss\", outputs[\"loss\"])\n    x, y = batch\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'test_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    pl_module.test_metrics.update(y_hat, y)\n    pl_module.test_roc.update(y_hat, y)\n    pl_module.test_prc.update(y_hat, y)\n    pl_module.test_cmx.update(y_hat, y)\n    pl_module.test_instance_prc.update(y_hat, y)\n    pl_module.test_instance_cmx.update(y_hat, y)\n\n    # Create figures for the samples (plot at maximum 24)\n    is_last_batch = trainer.num_val_batches == (batch_idx + 1)\n    max_batch_idx = (24 // x.shape[0]) - 1  # Does only work if NOT last batch, since last batch may be smaller\n    # If num_val_batches is 1 then this batch is the last one, but we still want to log it. despite its size\n    # Will plot the first 24 samples of the first batch if batch-size is larger than 24\n    should_log_batch = (\n        (max_batch_idx &gt;= batch_idx and not is_last_batch)\n        or trainer.num_val_batches == 1\n        or (max_batch_idx == -1 and batch_idx == 0)\n    )\n    if should_log_batch:\n        for i in range(min(x.shape[0], 24)):\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.input_combination)\n            for pllogger in pl_module.loggers:\n                if isinstance(pllogger, CSVLogger):\n                    fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.test_set}-samples\"\n                    fig_dir.mkdir(exist_ok=True, parents=True)\n                    fig.savefig(fig_dir / f\"sample_{pl_module.global_step}_{batch_idx}_{i}.png\")\n                if isinstance(pllogger, WandbLogger):\n                    wandb_run: Run = pllogger.experiment\n                    # We don't commit the log yet, so that the step is increased with the next lightning log\n                    # Which happens at the end of the validation epoch\n                    img_name = f\"{self.test_set}-samples/sample_{batch_idx}_{i}\"\n                    wandb_run.log({img_name: wandb.Image(fig)}, commit=False)\n            fig.clear()\n            plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    pl_module.test_cmx.compute()\n    pl_module.test_roc.compute()\n    pl_module.test_prc.compute()\n    pl_module.test_instance_prc.compute()\n    pl_module.test_instance_cmx.compute()\n\n    # Plot roc, prc and confusion matrix to disk and wandb\n    fig_cmx, _ = pl_module.test_cmx.plot(cmap=\"Blues\")\n    fig_roc, _ = pl_module.test_roc.plot(score=True)\n    fig_prc, _ = pl_module.test_prc.plot(score=True)\n    fig_instance_cmx, _ = pl_module.test_instance_cmx.plot(cmap=\"Blues\")\n    fig_instance_prc, _ = pl_module.test_instance_prc.plot(score=True)\n\n    # Check for a wandb or csv logger to log the images\n    for pllogger in pl_module.loggers:\n        if isinstance(pllogger, CSVLogger):\n            fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.test_set}-samples\"\n            fig_dir.mkdir(exist_ok=True, parents=True)\n            fig_cmx.savefig(fig_dir / f\"cmx_{pl_module.global_step}.png\")\n            fig_roc.savefig(fig_dir / f\"roc_{pl_module.global_step}.png\")\n            fig_prc.savefig(fig_dir / f\"prc_{pl_module.global_step}.png\")\n            fig_instance_cmx.savefig(fig_dir / f\"instance_cmx_{pl_module.global_step}.png\")\n            fig_instance_prc.savefig(fig_dir / f\"instance_prc_{pl_module.global_step}.png\")\n        if isinstance(pllogger, WandbLogger):\n            wandb_run: Run = pllogger.experiment\n            wandb_run.log({f\"{self.test_set}/cmx\": wandb.Image(fig_cmx)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/roc\": wandb.Image(fig_roc)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/prc\": wandb.Image(fig_prc)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/instance_cmx\": wandb.Image(fig_instance_cmx)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/instance_prc\": wandb.Image(fig_instance_prc)}, commit=False)\n\n    fig_cmx.clear()\n    fig_roc.clear()\n    fig_prc.clear()\n    fig_instance_cmx.clear()\n    fig_instance_prc.clear()\n    plt.close(\"all\")\n\n    # This will also commit the accumulated plots\n    pl_module.log_dict(pl_module.test_metrics.compute())\n\n    pl_module.test_metrics.reset()\n    pl_module.test_roc.reset()\n    pl_module.test_prc.reset()\n    pl_module.test_cmx.reset()\n    pl_module.test_instance_prc.reset()\n    pl_module.test_instance_cmx.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.on_train_batch_end","title":"on_train_batch_end","text":"<pre><code>on_train_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_train_batch_end(self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx):  # noqa: D102\n    pl_module.log(\"train/loss\", outputs[\"loss\"])\n    _, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'training_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n    pl_module.train_metrics(y_hat, y)\n    pl_module.log_dict(pl_module.train_metrics, on_step=True, on_epoch=False)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    pl_module.train_metrics.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.on_validation_batch_end","title":"on_validation_batch_end","text":"<pre><code>on_validation_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    pl_module.log(f\"{self._val_prefix}/loss\", outputs[\"loss\"])\n    x, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'validation_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    pl_module.val_metrics.update(y_hat, y)\n    pl_module.val_roc.update(y_hat, y)\n    pl_module.val_prc.update(y_hat, y)\n    pl_module.val_cmx.update(y_hat, y)\n\n    # Create figures for the samples (plot at maximum 24)\n    is_last_batch = trainer.num_val_batches == (batch_idx + 1)\n    max_batch_idx = (24 // x.shape[0]) - 1  # Does only work if NOT last batch, since last batch may be smaller\n    # If num_val_batches is 1 then this batch is the last one, but we still want to log it. despite its size\n    # Will plot the first 24 samples of the first batch if batch-size is larger than 24\n    should_log_batch = (\n        (max_batch_idx &gt;= batch_idx and not is_last_batch)\n        or trainer.num_val_batches == 1\n        or (max_batch_idx == -1 and batch_idx == 0)\n    )\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if is_val_plot_epoch and should_log_batch:\n        for i in range(min(x.shape[0], 24)):\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.input_combination)\n            for pllogger in pl_module.loggers:\n                if isinstance(pllogger, CSVLogger):\n                    fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.val_set}-samples\"\n                    fig_dir.mkdir(exist_ok=True, parents=True)\n                    fig.savefig(fig_dir / f\"sample_{pl_module.global_step}_{batch_idx}_{i}.png\")\n                if isinstance(pllogger, WandbLogger):\n                    wandb_run: Run = pllogger.experiment\n                    # We don't commit the log yet, so that the step is increased with the next lightning log\n                    # Which happens at the end of the validation epoch\n                    img_name = f\"{self.val_set}-samples/sample_{batch_idx}_{i}\"\n                    wandb_run.log({img_name: wandb.Image(fig)}, commit=False)\n            fig.clear()\n            plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    # Only do this every self.plot_every_n_val_epochs epochs\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if is_val_plot_epoch:\n        pl_module.val_cmx.compute()\n        pl_module.val_roc.compute()\n        pl_module.val_prc.compute()\n\n        # Plot roc, prc and confusion matrix to disk and wandb\n        fig_cmx, _ = pl_module.val_cmx.plot(cmap=\"Blues\")\n        fig_roc, _ = pl_module.val_roc.plot(score=True)\n        fig_prc, _ = pl_module.val_prc.plot(score=True)\n\n        # Check for a wandb or csv logger to log the images\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self._val_prefix}-samples\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig_cmx.savefig(fig_dir / f\"cmx_{pl_module.global_step}png\")\n                fig_roc.savefig(fig_dir / f\"roc_{pl_module.global_step}png\")\n                fig_prc.savefig(fig_dir / f\"prc_{pl_module.global_step}.png\")\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                wandb_run.log({f\"{self._val_prefix}/cmx\": wandb.Image(fig_cmx)}, commit=False)\n                wandb_run.log({f\"{self._val_prefix}/roc\": wandb.Image(fig_roc)}, commit=False)\n                wandb_run.log({f\"{self._val_prefix}/prc\": wandb.Image(fig_prc)}, commit=False)\n\n        fig_cmx.clear()\n        fig_roc.clear()\n        fig_prc.clear()\n        plt.close(\"all\")\n\n    # This will also commit the accumulated plots\n    pl_module.log_dict(pl_module.val_metrics.compute())\n\n    pl_module.val_metrics.reset()\n    pl_module.val_roc.reset()\n    pl_module.val_prc.reset()\n    pl_module.val_cmx.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.setup","title":"setup","text":"<pre><code>setup(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> <p>Setups the callback.</p> <p>Creates metrics required for the specific stage:</p> <ul> <li>For the \"fit\" stage, creates training and validation metrics and visualizations.</li> <li>For the \"validate\" stage, only creates validation metrics and visualizations.</li> <li>For the \"test\" stage, only creates test metrics and visualizations.</li> <li>For the \"predict\" stage, no metrics or visualizations are created.</li> </ul> <p>Always maps the trainer and pl_module to the callback.</p> <p>Training and validation metrics are \"simple\" metrics from torchmetrics. The validation visualizations are more complex metrics from torchmetrics. The test metrics and vsiualizations are the same as the validation ones, and also include custom \"Instance\" metrics.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>lightning.Trainer</code>)           \u2013            <p>The lightning trainer.</p> </li> <li> <code>pl_module</code>               (<code>lightning.LightningModule</code>)           \u2013            <p>The lightning module.</p> </li> <li> <code>stage</code>               (<code>typing.Literal['fit', 'validate', 'test', 'predict']</code>)           \u2013            <p>The current stage. One of: \"fit\", \"validate\", \"test\", \"predict\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):\n    \"\"\"Setups the callback.\n\n    Creates metrics required for the specific stage:\n\n    - For the \"fit\" stage, creates training and validation metrics and visualizations.\n    - For the \"validate\" stage, only creates validation metrics and visualizations.\n    - For the \"test\" stage, only creates test metrics and visualizations.\n    - For the \"predict\" stage, no metrics or visualizations are created.\n\n    Always maps the trainer and pl_module to the callback.\n\n    Training and validation metrics are \"simple\" metrics from torchmetrics.\n    The validation visualizations are more complex metrics from torchmetrics.\n    The test metrics and vsiualizations are the same as the validation ones,\n    and also include custom \"Instance\" metrics.\n\n    Args:\n        trainer (Trainer): The lightning trainer.\n        pl_module (LightningModule): The lightning module.\n        stage (Literal[\"fit\", \"validate\", \"test\", \"predict\"]): The current stage.\n            One of: \"fit\", \"validate\", \"test\", \"predict\".\n\n    \"\"\"\n    # Save references to the trainer and pl_module\n    self.trainer = trainer\n    self.pl_module = pl_module\n    self.stage = stage\n\n    # We don't want to use memory in the predict stage\n    if stage == \"predict\":\n        return\n\n    metric_kwargs = {\"task\": \"binary\", \"validate_args\": False, \"ignore_index\": 2}\n    metrics = MetricCollection(\n        {\n            \"Accuracy\": Accuracy(**metric_kwargs),\n            \"Precision\": Precision(**metric_kwargs),\n            \"Specificity\": Specificity(**metric_kwargs),\n            \"Recall\": Recall(**metric_kwargs),\n            \"F1Score\": F1Score(**metric_kwargs),\n            \"JaccardIndex\": JaccardIndex(**metric_kwargs),\n            \"CohenKappa\": CohenKappa(**metric_kwargs),\n            \"HammingDistance\": HammingDistance(**metric_kwargs),\n        }\n    )\n\n    added_metrics: list[str] = []\n\n    # Train metrics only for the fit stage\n    if stage == \"fit\":\n        pl_module.train_metrics = metrics.clone(prefix=\"train/\")\n        added_metrics += list(pl_module.train_metrics.keys())\n    # Validation metrics and visualizations for the fit and validate stages\n    if stage == \"fit\" or stage == \"validate\":\n        pl_module.val_metrics = metrics.clone(prefix=f\"{self._val_prefix}/\")\n        pl_module.val_metrics.add_metrics(\n            {\n                \"AUROC\": AUROC(thresholds=20, **metric_kwargs),\n                \"AveragePrecision\": AveragePrecision(thresholds=20, **metric_kwargs),\n            }\n        )\n        pl_module.val_roc = ROC(thresholds=20, **metric_kwargs)\n        pl_module.val_prc = PrecisionRecallCurve(thresholds=20, **metric_kwargs)\n        pl_module.val_cmx = ConfusionMatrix(normalize=\"true\", **metric_kwargs)\n        added_metrics += list(pl_module.val_metrics.keys())\n        added_metrics += [f\"{self._val_prefix}/{m}\" for m in [\"roc\", \"prc\", \"cmx\"]]\n\n    # Test metrics and visualizations for the test stage\n    if stage == \"test\":\n        pl_module.test_metrics = metrics.clone(prefix=f\"{pl_module.test_set}/\")\n        pl_module.test_metrics.add_metrics(\n            {\n                \"AUROC\": AUROC(thresholds=20, **metric_kwargs),\n                \"AveragePrecision\": AveragePrecision(thresholds=20, **metric_kwargs),\n            }\n        )\n        pl_module.test_roc = ROC(thresholds=20, **metric_kwargs)\n        pl_module.test_prc = PrecisionRecallCurve(thresholds=20, **metric_kwargs)\n        pl_module.test_cmx = ConfusionMatrix(normalize=\"true\", **metric_kwargs)\n\n        # Instance Metrics\n        instance_metric_kwargs = {\"validate_args\": False, \"ignore_index\": 2, \"matching_threshold\": 0.3}\n        pl_module.test_metrics.add_metrics(\n            {\n                \"InstanceAccuracy\": BinaryInstanceAccuracy(**instance_metric_kwargs),\n                \"InstancePrecision\": BinaryInstancePrecision(**instance_metric_kwargs),\n                \"InstanceRecall\": BinaryInstanceRecall(**instance_metric_kwargs),\n                \"InstanceF1Score\": BinaryInstanceF1Score(**instance_metric_kwargs),\n                \"InstanceAveragePrecision\": BinaryInstanceAveragePrecision(thresholds=20, **instance_metric_kwargs),\n            }\n        )\n        boundary_metric_kwargs = {\"validate_args\": False, \"ignore_index\": 2}\n        pl_module.test_metrics.add_metrics(\n            {\n                \"InstanceBoundaryIoU\": BinaryBoundaryIoU(**boundary_metric_kwargs),\n            }\n        )\n        pl_module.test_instance_prc = BinaryInstancePrecisionRecallCurve(thresholds=20, **instance_metric_kwargs)\n        pl_module.test_instance_cmx = BinaryInstanceConfusionMatrix(normalize=True, **instance_metric_kwargs)\n\n        added_metrics += list(pl_module.test_metrics.keys())\n        added_metrics += [f\"{self.test_set}/{m}\" for m in [\"roc\", \"prc\", \"cmx\", \"instance_prc\", \"instance_cmx\"]]\n\n    # Log the added metrics\n    sep = \"\\n\\t- \"\n    logger.debug(f\"Added metrics:{sep + sep.join(added_metrics)}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.BinarySegmentationMetrics.teardown","title":"teardown","text":"<pre><code>teardown(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def teardown(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):  # noqa: D102\n    # Delete the references to the trainer and pl_module\n    del self.trainer\n    del self.pl_module\n    del self.stage\n\n    # No need to delete anything if we are in the predict stage\n    if stage == \"predict\":\n        return\n\n    if stage == \"fit\":\n        del pl_module.train_metrics\n\n    if stage == \"fit\" or stage == \"validate\":\n        del pl_module.val_metrics\n        del pl_module.val_roc\n        del pl_module.val_prc\n        del pl_module.val_cmx\n\n    if stage == \"test\":\n        del pl_module.test_metrics\n        del pl_module.test_roc\n        del pl_module.test_prc\n        del pl_module.test_cmx\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule","title":"DartsDataModule","text":"<pre><code>DartsDataModule(\n    data_dir: pathlib.Path,\n    batch_size: int,\n    fold: int = 0,\n    augment: bool = True,\n    num_workers: int = 0,\n    in_memory: bool = False,\n)\n</code></pre> <p>               Bases: <code>lightning.LightningDataModule</code></p> <p>Methods:</p> <ul> <li> <code>setup</code>             \u2013              </li> <li> <code>test_dataloader</code>             \u2013              </li> <li> <code>train_dataloader</code>             \u2013              </li> <li> <code>val_dataloader</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>augment</code>           \u2013            </li> <li> <code>batch_size</code>           \u2013            </li> <li> <code>data_dir</code>           \u2013            </li> <li> <code>fold</code>           \u2013            </li> <li> <code>in_memory</code>           \u2013            </li> <li> <code>nsamples</code>           \u2013            </li> <li> <code>num_workers</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path,\n    batch_size: int,\n    fold: int = 0,  # Not used for test\n    augment: bool = True,  # Not used for test\n    num_workers: int = 0,\n    in_memory: bool = False,\n):\n    super().__init__()\n    self.save_hyperparameters()\n    self.data_dir = data_dir\n    self.batch_size = batch_size\n    self.fold = fold\n    self.augment = augment\n    self.num_workers = num_workers\n    self.in_memory = in_memory\n\n    data_dir = Path(data_dir)\n\n    store = zarr.storage.DirectoryStore(data_dir)\n    zroot = zarr.group(store=store)\n    self.nsamples = len(zroot[\"x\"])\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.augment","title":"augment  <code>instance-attribute</code>","text":"<pre><code>augment = darts_segmentation.training.data.DartsDataModule(\n    augment\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = (\n    darts_segmentation.training.data.DartsDataModule(\n        batch_size\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.data_dir","title":"data_dir  <code>instance-attribute</code>","text":"<pre><code>data_dir = darts_segmentation.training.data.DartsDataModule(\n    data_dir\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.fold","title":"fold  <code>instance-attribute</code>","text":"<pre><code>fold = darts_segmentation.training.data.DartsDataModule(\n    fold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.in_memory","title":"in_memory  <code>instance-attribute</code>","text":"<pre><code>in_memory = (\n    darts_segmentation.training.data.DartsDataModule(\n        in_memory\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.nsamples","title":"nsamples  <code>instance-attribute</code>","text":"<pre><code>nsamples = len(zroot['x'])\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.num_workers","title":"num_workers  <code>instance-attribute</code>","text":"<pre><code>num_workers = (\n    darts_segmentation.training.data.DartsDataModule(\n        num_workers\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.setup","title":"setup","text":"<pre><code>setup(\n    stage: typing.Literal[\n        \"fit\", \"validate\", \"test\", \"predict\"\n    ]\n    | None = None,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def setup(self, stage: Literal[\"fit\", \"validate\", \"test\", \"predict\"] | None = None):\n    if stage in [\"fit\", \"validate\"]:\n        kf = KFold(n_splits=5)\n        train_idx, val_idx = list(kf.split(range(self.nsamples)))[self.fold]\n\n        dsclass = DartsDatasetInMemory if self.in_memory else DartsDatasetZarr\n        self.train = dsclass(self.data_dir, self.augment, train_idx)\n        self.val = dsclass(self.data_dir, False, val_idx)\n    if stage == \"test\":\n        dsclass = DartsDatasetInMemory if self.in_memory else DartsDatasetZarr\n        self.test = dsclass(self.data_dir, False)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.test_dataloader","title":"test_dataloader","text":"<pre><code>test_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def test_dataloader(self):\n    return DataLoader(self.test, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def train_dataloader(self):\n    return DataLoader(self.train, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataModule.val_dataloader","title":"val_dataloader","text":"<pre><code>val_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def val_dataloader(self):\n    return DataLoader(self.val, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataset","title":"DartsDataset","text":"<pre><code>DartsDataset(\n    data_dir: pathlib.Path | str,\n    augment: bool,\n    indices: list[int] | None = None,\n)\n</code></pre> <p>               Bases: <code>torch.utils.data.Dataset</code></p> <p>Methods:</p> <ul> <li> <code>__getitem__</code>             \u2013              </li> <li> <code>__len__</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>transform</code>           \u2013            </li> <li> <code>x_files</code>           \u2013            </li> <li> <code>y_files</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(self, data_dir: Path | str, augment: bool, indices: list[int] | None = None):\n    if isinstance(data_dir, str):\n        data_dir = Path(data_dir)\n\n    self.x_files = sorted((data_dir / \"x\").glob(\"*.pt\"))\n    self.y_files = sorted((data_dir / \"y\").glob(\"*.pt\"))\n    assert len(self.x_files) == len(self.y_files), (\n        f\"Dataset corrupted! Got {len(self.x_files)=} and {len(self.y_files)=}!\"\n    )\n    if indices is not None:\n        self.x_files = [self.x_files[i] for i in indices]\n        self.y_files = [self.y_files[i] for i in indices]\n\n    self.transform = (\n        A.Compose(\n            [\n                A.HorizontalFlip(),\n                A.VerticalFlip(),\n                A.RandomRotate90(),\n                # A.Blur(),\n                A.RandomBrightnessContrast(),\n                A.MultiplicativeNoise(per_channel=True, elementwise=True),\n                # ToTensorV2(),\n            ]\n        )\n        if augment\n        else None\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataset.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = (\n    albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(),\n            albumentations.VerticalFlip(),\n            albumentations.RandomRotate90(),\n            albumentations.RandomBrightnessContrast(),\n            albumentations.MultiplicativeNoise(\n                per_channel=True, elementwise=True\n            ),\n        ]\n    )\n    if darts_segmentation.training.data.DartsDataset(\n        augment\n    )\n    else None\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataset.x_files","title":"x_files  <code>instance-attribute</code>","text":"<pre><code>x_files = sorted(\n    darts_segmentation.training.data.DartsDataset(data_dir)\n    / \"x\".glob(\"*.pt\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataset.y_files","title":"y_files  <code>instance-attribute</code>","text":"<pre><code>y_files = sorted(\n    darts_segmentation.training.data.DartsDataset(data_dir)\n    / \"y\".glob(\"*.pt\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    xfile = self.x_files[idx]\n    yfile = self.y_files[idx]\n    assert xfile.stem == yfile.stem, f\"Dataset corrupted! Files must have the same name, but got {xfile=} {yfile=}!\"\n\n    x = torch.load(xfile).numpy()\n    y = torch.load(yfile).int().numpy()\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.x_files)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetInMemory","title":"DartsDatasetInMemory","text":"<pre><code>DartsDatasetInMemory(\n    data_dir: pathlib.Path | str,\n    augment: bool,\n    indices: list[int] | None = None,\n)\n</code></pre> <p>               Bases: <code>torch.utils.data.Dataset</code></p> <p>Methods:</p> <ul> <li> <code>__getitem__</code>             \u2013              </li> <li> <code>__len__</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>transform</code>           \u2013            </li> <li> <code>x</code>           \u2013            </li> <li> <code>y</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(self, data_dir: Path | str, augment: bool, indices: list[int] | None = None):\n    if isinstance(data_dir, str):\n        data_dir = Path(data_dir)\n\n    x_files = sorted((data_dir / \"x\").glob(\"*.pt\"))\n    y_files = sorted((data_dir / \"y\").glob(\"*.pt\"))\n    assert len(x_files) == len(y_files), f\"Dataset corrupted! Got {len(x_files)=} and {len(y_files)=}!\"\n    if indices is not None:\n        x_files = [x_files[i] for i in indices]\n        y_files = [y_files[i] for i in indices]\n\n    self.x = []\n    self.y = []\n    for xfile, yfile in zip(x_files, y_files):\n        assert xfile.stem == yfile.stem, (\n            f\"Dataset corrupted! Files must have the same name, but got {xfile=} {yfile=}!\"\n        )\n        x = torch.load(xfile).numpy()\n        y = torch.load(yfile).int().numpy()\n        self.x.append(x)\n        self.y.append(y)\n\n    self.transform = (\n        A.Compose(\n            [\n                A.HorizontalFlip(),\n                A.VerticalFlip(),\n                A.RandomRotate90(),\n                # A.Blur(),\n                A.RandomBrightnessContrast(),\n                A.MultiplicativeNoise(per_channel=True, elementwise=True),\n                # ToTensorV2(),\n            ]\n        )\n        if augment\n        else None\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetInMemory.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = (\n    albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(),\n            albumentations.VerticalFlip(),\n            albumentations.RandomRotate90(),\n            albumentations.RandomBrightnessContrast(),\n            albumentations.MultiplicativeNoise(\n                per_channel=True, elementwise=True\n            ),\n        ]\n    )\n    if darts_segmentation.training.data.DartsDatasetInMemory(\n        augment\n    )\n    else None\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetInMemory.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x = []\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetInMemory.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y = []\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetInMemory.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    x = self.x[idx]\n    y = self.y[idx]\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetInMemory.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.x)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetZarr","title":"DartsDatasetZarr","text":"<pre><code>DartsDatasetZarr(\n    data_dir: pathlib.Path | str,\n    augment: bool,\n    indices: list[int] | None = None,\n)\n</code></pre> <p>               Bases: <code>torch.utils.data.Dataset</code></p> <p>Methods:</p> <ul> <li> <code>__getitem__</code>             \u2013              </li> <li> <code>__len__</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>indices</code>           \u2013            </li> <li> <code>transform</code>           \u2013            </li> <li> <code>zroot</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(self, data_dir: Path | str, augment: bool, indices: list[int] | None = None):\n    if isinstance(data_dir, str):\n        data_dir = Path(data_dir)\n\n    store = zarr.storage.LocalStore(data_dir)\n    self.zroot = zarr.group(store=store)\n\n    assert \"x\" in self.zroot and \"y\" in self.zroot, (\n        f\"Dataset corrupted! {self.zroot.info=} must contain 'x' or 'y' arrays!\"\n    )\n\n    self.indices = indices if indices is not None else list(range(self.zroot[\"x\"].shape[0]))\n\n    self.transform = (\n        A.Compose(\n            [\n                A.HorizontalFlip(),\n                A.VerticalFlip(),\n                A.RandomRotate90(),\n                # A.Blur(),\n                A.RandomBrightnessContrast(),\n                A.MultiplicativeNoise(per_channel=True, elementwise=True),\n                # ToTensorV2(),\n            ]\n        )\n        if augment\n        else None\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetZarr.indices","title":"indices  <code>instance-attribute</code>","text":"<pre><code>indices = (\n    darts_segmentation.training.data.DartsDatasetZarr(\n        indices\n    )\n    if darts_segmentation.training.data.DartsDatasetZarr(\n        indices\n    )\n    is not None\n    else list(\n        range(\n            darts_segmentation.training.data.DartsDatasetZarr(\n                self\n            )\n            .zroot[\"x\"]\n            .shape[0]\n        )\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetZarr.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = (\n    albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(),\n            albumentations.VerticalFlip(),\n            albumentations.RandomRotate90(),\n            albumentations.RandomBrightnessContrast(),\n            albumentations.MultiplicativeNoise(\n                per_channel=True, elementwise=True\n            ),\n        ]\n    )\n    if darts_segmentation.training.data.DartsDatasetZarr(\n        augment\n    )\n    else None\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetZarr.zroot","title":"zroot  <code>instance-attribute</code>","text":"<pre><code>zroot = zarr.group(store=store)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetZarr.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    i = self.indices[idx]\n\n    x = self.zroot[\"x\"][i]\n    y = self.zroot[\"y\"][i]\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.DartsDatasetZarr.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.indices)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.SMPSegmenter","title":"SMPSegmenter","text":"<pre><code>SMPSegmenter(\n    config: darts_segmentation.segment.SMPSegmenterConfig,\n    learning_rate: float = 1e-05,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    **kwargs: dict[str, typing.Any],\n)\n</code></pre> <p>               Bases: <code>lightning.LightningModule</code></p> <p>Lightning module for training a segmentation model using the segmentation_models_pytorch library.</p> <p>Initialize the SMPSegmenter.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>darts_segmentation.segment.SMPSegmenterConfig</code>)           \u2013            <p>Configuration for the segmentation model.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>1e-05</code> )           \u2013            <p>Initial learning rate. Defaults to 1e-5.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>kwargs</code>               (<code>dict[str, typing.Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments which should be saved to the hyperparameter file.</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>__repr__</code>             \u2013              </li> <li> <code>configure_optimizers</code>             \u2013              </li> <li> <code>on_train_epoch_end</code>             \u2013              </li> <li> <code>test_step</code>             \u2013              </li> <li> <code>training_step</code>             \u2013              </li> <li> <code>validation_step</code>             \u2013              </li> </ul> <p>Attributes:</p> <ul> <li> <code>loss_fn</code>           \u2013            </li> <li> <code>model</code>           \u2013            </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def __init__(\n    self,\n    config: SMPSegmenterConfig,\n    learning_rate: float = 1e-5,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Initialize the SMPSegmenter.\n\n    Args:\n        config (SMPSegmenterConfig): Configuration for the segmentation model.\n        learning_rate (float, optional): Initial learning rate. Defaults to 1e-5.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        kwargs (dict[str, Any]): Additional keyword arguments which should be saved to the hyperparameter file.\n\n    \"\"\"\n    super().__init__()\n\n    # This saves config, learning_rate and gamma under self.hparams\n    self.save_hyperparameters(ignore=[\"test_set\", \"val_set\"])\n    self.model = smp.create_model(**config[\"model\"], activation=\"sigmoid\")\n\n    # Assumes that the training preparation was done with setting invalid pixels in the mask to 2\n    self.loss_fn = smp.losses.FocalLoss(\n        mode=\"binary\", alpha=focal_loss_alpha, gamma=focal_loss_gamma, ignore_index=2\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.SMPSegmenter.loss_fn","title":"loss_fn  <code>instance-attribute</code>","text":"<pre><code>loss_fn = segmentation_models_pytorch.losses.FocalLoss(\n    mode=\"binary\",\n    alpha=darts_segmentation.training.module.SMPSegmenter(\n        focal_loss_alpha\n    ),\n    gamma=darts_segmentation.training.module.SMPSegmenter(\n        focal_loss_gamma\n    ),\n    ignore_index=2,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.SMPSegmenter.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = segmentation_models_pytorch.create_model(\n    **darts_segmentation.training.module.SMPSegmenter(\n        config\n    )[\"model\"],\n    activation=\"sigmoid\",\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.SMPSegmenter.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def __repr__(self):  # noqa: D105\n    return f\"SMPSegmenter({self.hparams['config']['model']})\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.SMPSegmenter.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def configure_optimizers(self):  # noqa: D102\n    optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.hparams.gamma)\n    return [optimizer], [scheduler]\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.SMPSegmenter.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def on_train_epoch_end(self):  # noqa: D102\n    self.log(\"learning_rate\", self.lr_schedulers().get_last_lr()[0])\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.SMPSegmenter.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def test_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.SMPSegmenter.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def training_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.SMPSegmenter.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def validation_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/#darts_segmentation.training.create_training_patches","title":"create_training_patches","text":"<pre><code>create_training_patches(\n    tile: xarray.Dataset,\n    labels: geopandas.GeoDataFrame,\n    bands: list[str],\n    norm_factors: dict[str, float],\n    patch_size: int,\n    overlap: int,\n    exclude_nopositive: bool,\n    exclude_nan: bool,\n    device: typing.Literal[\"cuda\", \"cpu\"] | int,\n    mask_erosion_size: int,\n) -&gt; collections.abc.Generator[\n    tuple[torch.tensor, torch.tensor]\n]\n</code></pre> <p>Create training patches from a tile and labels.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>labels</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The labels to be used for training.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the tile.</p> </li> <li> <code>norm_factors</code>               (<code>dict[str, float]</code>)           \u2013            <p>The normalization factors for the bands.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>)           \u2013            <p>Whether to exclude patches where the labels do not contain positives.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>)           \u2013            <p>Whether to exclude patches where the input data has nan values.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for the erosion.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>)           \u2013            <p>The size of the disk to use for erosion.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>collections.abc.Generator[tuple[torch.tensor, torch.tensor]]</code>           \u2013            <p>Generator[tuple[torch.tensor, torch.tensor]]: A tuple containing the input and the labels as pytorch tensors. The input has the format (C, H, W), the labels (H, W).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a band is not found in the preprocessed data.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def create_training_patches(\n    tile: xr.Dataset,\n    labels: gpd.GeoDataFrame,\n    bands: list[str],\n    norm_factors: dict[str, float],\n    patch_size: int,\n    overlap: int,\n    exclude_nopositive: bool,\n    exclude_nan: bool,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n    mask_erosion_size: int,\n) -&gt; Generator[tuple[torch.tensor, torch.tensor]]:\n    \"\"\"Create training patches from a tile and labels.\n\n    Args:\n        tile (xr.Dataset): The input tile, containing preprocessed, harmonized data.\n        labels (gpd.GeoDataFrame): The labels to be used for training.\n        bands (list[str]): The bands to be used for training. Must be present in the tile.\n        norm_factors (dict[str, float]): The normalization factors for the bands.\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        exclude_nopositive (bool): Whether to exclude patches where the labels do not contain positives.\n        exclude_nan (bool): Whether to exclude patches where the input data has nan values.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for the erosion.\n        mask_erosion_size (int): The size of the disk to use for erosion.\n\n    Yields:\n        Generator[tuple[torch.tensor, torch.tensor]]: A tuple containing the input and the labels as pytorch tensors.\n            The input has the format (C, H, W), the labels (H, W).\n\n    Raises:\n        ValueError: If a band is not found in the preprocessed data.\n\n    \"\"\"\n    if len(labels) == 0 and exclude_nopositive:\n        logger.warning(\"No labels found in the labels GeoDataFrame. Skipping.\")\n        return\n\n    # Rasterize the labels\n    if len(labels) &gt; 0:\n        labels_rasterized = 1 - make_geocube(labels, measurements=[\"id\"], like=tile).id.isnull()\n    else:\n        labels_rasterized = xr.zeros_like(tile[\"valid_data_mask\"])\n\n    # Filter out the nodata values (class 2 -&gt; invalid data)\n    mask = erode_mask(tile[\"valid_data_mask\"], mask_erosion_size, device)\n    mask = tile[\"valid_data_mask\"]\n    labels_rasterized = xr.where(mask, labels_rasterized, 2)\n\n    # Normalize the bands and clip the values\n    for band in bands:\n        if band not in tile:\n            raise ValueError(f\"Band '{band}' not found in the preprocessed data.\")\n        with xr.set_options(keep_attrs=True):\n            tile[band] = tile[band] * norm_factors[band]\n            tile[band] = tile[band].clip(0, 1)\n\n    # Replace invalid values with nan (used for nan check later on)\n    tile = xr.where(tile[\"valid_data_mask\"], tile, float(\"nan\"))\n\n    # Convert to dataaray and select the bands (bands are now in specified order)\n    tile = tile.to_dataarray(dim=\"band\").sel(band=bands)\n\n    # Transpose to (C, H, W)\n    tile = tile.transpose(\"band\", \"y\", \"x\")\n    labels_rasterized = labels_rasterized.transpose(\"y\", \"x\")\n\n    # Convert to tensor\n    tensor_tile = torch.tensor(tile.values).float()\n    tensor_labels = torch.tensor(labels_rasterized.values).float()\n\n    assert tensor_tile.dim() == 3, f\"Expects tensor_tile to has shape (C, H, W), got {tensor_tile.shape}\"\n    assert tensor_labels.dim() == 2, f\"Expects tensor_labels to has shape (H, W), got {tensor_labels.shape}\"\n\n    # Create patches\n    tensor_patches = create_patches(tensor_tile.unsqueeze(0), patch_size, overlap)\n    tensor_patches = tensor_patches.reshape(-1, len(bands), patch_size, patch_size)\n    tensor_labels = create_patches(tensor_labels.unsqueeze(0).unsqueeze(0), patch_size, overlap)\n    tensor_labels = tensor_labels.reshape(-1, patch_size, patch_size)\n\n    # Turn the patches into a list of tuples\n    n_patches = tensor_patches.shape[0]\n    for i in range(n_patches):\n        x = tensor_patches[i]\n        y = tensor_labels[i]\n\n        if exclude_nopositive and not (y == 1).any():\n            continue\n\n        if exclude_nan and torch.isnan(x).any():\n            continue\n\n        # Skip where there are less than 10% visible pixel\n        if ((y != 2).sum() / y.numel()) &lt; 0.1:\n            continue\n\n        # Skip patches where everything is nan\n        if torch.isnan(x).all():\n            continue\n\n        # Convert all nan values to 0\n        x[torch.isnan(x)] = 0\n\n        logger.debug(f\"Yielding patch {i} with\\n\\t{x=}\\n\\t{y=}\")\n        yield x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/","title":"darts_segmentation.training.BinarySegmentationMetrics","text":"<p>               Bases: <code>lightning.pytorch.callbacks.Callback</code></p> <p>Callback for validation metrics and visualizations.</p> <p>Initialize the ValidationCallback.</p> <p>Parameters:</p> <ul> <li> <code>input_combination</code>               (<code>list[str]</code>)           \u2013            <p>List of input names to combine for the visualization.</p> </li> <li> <code>val_set</code>               (<code>str</code>, default:                   <code>'val'</code> )           \u2013            <p>Name of the validation set. Only used for naming the validation metrics. Defaults to \"val\".</p> </li> <li> <code>test_set</code>               (<code>str</code>, default:                   <code>'test'</code> )           \u2013            <p>Name of the test set. Only used for naming the test metrics. Defaults to \"test\".</p> </li> <li> <code>plot_every_n_val_epochs</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Plot validation samples every n epochs. Defaults to 5.</p> </li> <li> <code>is_crossval</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether the training is done with cross-validation. This will change the logging behavior of scalar metrics from logging to {val_set} to just \"val\". The logging behaviour of the samples is not affected. Defaults to False.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def __init__(\n    self,\n    *,\n    input_combination: list[str],\n    val_set: str = \"val\",\n    test_set: str = \"test\",\n    plot_every_n_val_epochs: int = 5,\n    is_crossval: bool = False,\n):\n    \"\"\"Initialize the ValidationCallback.\n\n    Args:\n        input_combination (list[str]): List of input names to combine for the visualization.\n        val_set (str, optional): Name of the validation set. Only used for naming the validation metrics.\n            Defaults to \"val\".\n        test_set (str, optional): Name of the test set. Only used for naming the test metrics. Defaults to \"test\".\n        plot_every_n_val_epochs (int, optional): Plot validation samples every n epochs. Defaults to 5.\n        is_crossval (bool, optional): Whether the training is done with cross-validation.\n            This will change the logging behavior of scalar metrics from logging to {val_set} to just \"val\".\n            The logging behaviour of the samples is not affected.\n            Defaults to False.\n\n    \"\"\"\n    assert \"/\" not in val_set, \"val_set must not contain '/'\"\n    assert \"/\" not in test_set, \"test_set must not contain '/'\"\n    self.val_set = val_set\n    self.test_set = test_set\n    self.plot_every_n_val_epochs = plot_every_n_val_epochs\n    self.input_combination = input_combination\n    self.is_crossval = is_crossval\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics._val_prefix","title":"_val_prefix  <code>property</code>","text":"<pre><code>_val_prefix\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.input_combination","title":"input_combination  <code>instance-attribute</code>","text":"<pre><code>input_combination = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    input_combination\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.is_crossval","title":"is_crossval  <code>instance-attribute</code>","text":"<pre><code>is_crossval = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    is_crossval\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.pl_module","title":"pl_module  <code>instance-attribute</code>","text":"<pre><code>pl_module: lightning.LightningModule\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.plot_every_n_val_epochs","title":"plot_every_n_val_epochs  <code>instance-attribute</code>","text":"<pre><code>plot_every_n_val_epochs = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    plot_every_n_val_epochs\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.stage","title":"stage  <code>instance-attribute</code>","text":"<pre><code>stage: darts_segmentation.training.callbacks.Stage\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.test_cmx","title":"test_cmx  <code>instance-attribute</code>","text":"<pre><code>test_cmx: torchmetrics.ConfusionMatrix\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.test_instance_cmx","title":"test_instance_cmx  <code>instance-attribute</code>","text":"<pre><code>test_instance_cmx: (\n    darts_segmentation.metrics.BinaryInstanceConfusionMatrix\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.test_instance_prc","title":"test_instance_prc  <code>instance-attribute</code>","text":"<pre><code>test_instance_prc: darts_segmentation.metrics.BinaryInstancePrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.test_metrics","title":"test_metrics  <code>instance-attribute</code>","text":"<pre><code>test_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.test_prc","title":"test_prc  <code>instance-attribute</code>","text":"<pre><code>test_prc: torchmetrics.PrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.test_roc","title":"test_roc  <code>instance-attribute</code>","text":"<pre><code>test_roc: torchmetrics.ROC\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.test_set","title":"test_set  <code>instance-attribute</code>","text":"<pre><code>test_set = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    test_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.train_metrics","title":"train_metrics  <code>instance-attribute</code>","text":"<pre><code>train_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.trainer","title":"trainer  <code>instance-attribute</code>","text":"<pre><code>trainer: lightning.Trainer\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.val_cmx","title":"val_cmx  <code>instance-attribute</code>","text":"<pre><code>val_cmx: torchmetrics.ConfusionMatrix\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.val_metrics","title":"val_metrics  <code>instance-attribute</code>","text":"<pre><code>val_metrics: torchmetrics.MetricCollection\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.val_prc","title":"val_prc  <code>instance-attribute</code>","text":"<pre><code>val_prc: torchmetrics.PrecisionRecallCurve\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.val_roc","title":"val_roc  <code>instance-attribute</code>","text":"<pre><code>val_roc: torchmetrics.ROC\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.val_set","title":"val_set  <code>instance-attribute</code>","text":"<pre><code>val_set = darts_segmentation.training.callbacks.BinarySegmentationMetrics(\n    val_set\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.is_val_plot_epoch","title":"is_val_plot_epoch","text":"<pre><code>is_val_plot_epoch(\n    current_epoch: int, check_val_every_n_epoch: int | None\n) -&gt; bool\n</code></pre> <p>Check if the current epoch is an epoch where validation samples should be plotted.</p> <p>Parameters:</p> <ul> <li> <code>current_epoch</code>               (<code>int</code>)           \u2013            <p>The current epoch.</p> </li> <li> <code>check_val_every_n_epoch</code>               (<code>int | None</code>)           \u2013            <p>The number of epochs to check for plotting. If None, no plotting is done.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the current epoch is a plot epoch, False otherwise.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def is_val_plot_epoch(self, current_epoch: int, check_val_every_n_epoch: int | None) -&gt; bool:\n    \"\"\"Check if the current epoch is an epoch where validation samples should be plotted.\n\n    Args:\n        current_epoch (int): The current epoch.\n        check_val_every_n_epoch (int | None): The number of epochs to check for plotting.\n            If None, no plotting is done.\n\n    Returns:\n        bool: True if the current epoch is a plot epoch, False otherwise.\n\n    \"\"\"\n    if check_val_every_n_epoch is None:\n        return False\n    n = self.plot_every_n_val_epochs * check_val_every_n_epoch\n    return ((current_epoch + 1) % n) == 0 or current_epoch == 0\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.on_test_batch_end","title":"on_test_batch_end","text":"<pre><code>on_test_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    pl_module.log(f\"{self.test_set}/loss\", outputs[\"loss\"])\n    x, y = batch\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'test_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    pl_module.test_metrics.update(y_hat, y)\n    pl_module.test_roc.update(y_hat, y)\n    pl_module.test_prc.update(y_hat, y)\n    pl_module.test_cmx.update(y_hat, y)\n    pl_module.test_instance_prc.update(y_hat, y)\n    pl_module.test_instance_cmx.update(y_hat, y)\n\n    # Create figures for the samples (plot at maximum 24)\n    is_last_batch = trainer.num_val_batches == (batch_idx + 1)\n    max_batch_idx = (24 // x.shape[0]) - 1  # Does only work if NOT last batch, since last batch may be smaller\n    # If num_val_batches is 1 then this batch is the last one, but we still want to log it. despite its size\n    # Will plot the first 24 samples of the first batch if batch-size is larger than 24\n    should_log_batch = (\n        (max_batch_idx &gt;= batch_idx and not is_last_batch)\n        or trainer.num_val_batches == 1\n        or (max_batch_idx == -1 and batch_idx == 0)\n    )\n    if should_log_batch:\n        for i in range(min(x.shape[0], 24)):\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.input_combination)\n            for pllogger in pl_module.loggers:\n                if isinstance(pllogger, CSVLogger):\n                    fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.test_set}-samples\"\n                    fig_dir.mkdir(exist_ok=True, parents=True)\n                    fig.savefig(fig_dir / f\"sample_{pl_module.global_step}_{batch_idx}_{i}.png\")\n                if isinstance(pllogger, WandbLogger):\n                    wandb_run: Run = pllogger.experiment\n                    # We don't commit the log yet, so that the step is increased with the next lightning log\n                    # Which happens at the end of the validation epoch\n                    img_name = f\"{self.test_set}-samples/sample_{batch_idx}_{i}\"\n                    wandb_run.log({img_name: wandb.Image(fig)}, commit=False)\n            fig.clear()\n            plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.on_test_epoch_end","title":"on_test_epoch_end","text":"<pre><code>on_test_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_test_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    pl_module.test_cmx.compute()\n    pl_module.test_roc.compute()\n    pl_module.test_prc.compute()\n    pl_module.test_instance_prc.compute()\n    pl_module.test_instance_cmx.compute()\n\n    # Plot roc, prc and confusion matrix to disk and wandb\n    fig_cmx, _ = pl_module.test_cmx.plot(cmap=\"Blues\")\n    fig_roc, _ = pl_module.test_roc.plot(score=True)\n    fig_prc, _ = pl_module.test_prc.plot(score=True)\n    fig_instance_cmx, _ = pl_module.test_instance_cmx.plot(cmap=\"Blues\")\n    fig_instance_prc, _ = pl_module.test_instance_prc.plot(score=True)\n\n    # Check for a wandb or csv logger to log the images\n    for pllogger in pl_module.loggers:\n        if isinstance(pllogger, CSVLogger):\n            fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.test_set}-samples\"\n            fig_dir.mkdir(exist_ok=True, parents=True)\n            fig_cmx.savefig(fig_dir / f\"cmx_{pl_module.global_step}.png\")\n            fig_roc.savefig(fig_dir / f\"roc_{pl_module.global_step}.png\")\n            fig_prc.savefig(fig_dir / f\"prc_{pl_module.global_step}.png\")\n            fig_instance_cmx.savefig(fig_dir / f\"instance_cmx_{pl_module.global_step}.png\")\n            fig_instance_prc.savefig(fig_dir / f\"instance_prc_{pl_module.global_step}.png\")\n        if isinstance(pllogger, WandbLogger):\n            wandb_run: Run = pllogger.experiment\n            wandb_run.log({f\"{self.test_set}/cmx\": wandb.Image(fig_cmx)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/roc\": wandb.Image(fig_roc)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/prc\": wandb.Image(fig_prc)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/instance_cmx\": wandb.Image(fig_instance_cmx)}, commit=False)\n            wandb_run.log({f\"{self.test_set}/instance_prc\": wandb.Image(fig_instance_prc)}, commit=False)\n\n    fig_cmx.clear()\n    fig_roc.clear()\n    fig_prc.clear()\n    fig_instance_cmx.clear()\n    fig_instance_prc.clear()\n    plt.close(\"all\")\n\n    # This will also commit the accumulated plots\n    pl_module.log_dict(pl_module.test_metrics.compute())\n\n    pl_module.test_metrics.reset()\n    pl_module.test_roc.reset()\n    pl_module.test_prc.reset()\n    pl_module.test_cmx.reset()\n    pl_module.test_instance_prc.reset()\n    pl_module.test_instance_cmx.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.on_train_batch_end","title":"on_train_batch_end","text":"<pre><code>on_train_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_train_batch_end(self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx):  # noqa: D102\n    pl_module.log(\"train/loss\", outputs[\"loss\"])\n    _, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'training_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n    pl_module.train_metrics(y_hat, y)\n    pl_module.log_dict(pl_module.train_metrics, on_step=True, on_epoch=False)\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    pl_module.train_metrics.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.on_validation_batch_end","title":"on_validation_batch_end","text":"<pre><code>on_validation_batch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    outputs,\n    batch,\n    batch_idx,\n    dataloader_idx=0,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_batch_end(  # noqa: D102\n    self, trainer: Trainer, pl_module: LightningModule, outputs, batch, batch_idx, dataloader_idx=0\n):\n    pl_module.log(f\"{self._val_prefix}/loss\", outputs[\"loss\"])\n    x, y = batch\n    # Expect the output to has a tensor called \"y_hat\"\n    assert \"y_hat\" in outputs, (\n        \"Output does not contain 'y_hat' tensor.\"\n        \" Please make sure the 'validation_step' method returns a dict with 'y_hat' and 'loss' keys.\"\n        \" The 'y_hat' should be the model's prediction (a pytorch tensor of shape [B, C, H, W]).\"\n        \" The 'loss' should be the loss value (a scalar tensor).\",\n    )\n    y_hat = outputs[\"y_hat\"]\n\n    pl_module.val_metrics.update(y_hat, y)\n    pl_module.val_roc.update(y_hat, y)\n    pl_module.val_prc.update(y_hat, y)\n    pl_module.val_cmx.update(y_hat, y)\n\n    # Create figures for the samples (plot at maximum 24)\n    is_last_batch = trainer.num_val_batches == (batch_idx + 1)\n    max_batch_idx = (24 // x.shape[0]) - 1  # Does only work if NOT last batch, since last batch may be smaller\n    # If num_val_batches is 1 then this batch is the last one, but we still want to log it. despite its size\n    # Will plot the first 24 samples of the first batch if batch-size is larger than 24\n    should_log_batch = (\n        (max_batch_idx &gt;= batch_idx and not is_last_batch)\n        or trainer.num_val_batches == 1\n        or (max_batch_idx == -1 and batch_idx == 0)\n    )\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if is_val_plot_epoch and should_log_batch:\n        for i in range(min(x.shape[0], 24)):\n            fig, _ = plot_sample(x[i], y[i], y_hat[i], self.input_combination)\n            for pllogger in pl_module.loggers:\n                if isinstance(pllogger, CSVLogger):\n                    fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self.val_set}-samples\"\n                    fig_dir.mkdir(exist_ok=True, parents=True)\n                    fig.savefig(fig_dir / f\"sample_{pl_module.global_step}_{batch_idx}_{i}.png\")\n                if isinstance(pllogger, WandbLogger):\n                    wandb_run: Run = pllogger.experiment\n                    # We don't commit the log yet, so that the step is increased with the next lightning log\n                    # Which happens at the end of the validation epoch\n                    img_name = f\"{self.val_set}-samples/sample_{batch_idx}_{i}\"\n                    wandb_run.log({img_name: wandb.Image(fig)}, commit=False)\n            fig.clear()\n            plt.close(fig)\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.on_validation_epoch_end","title":"on_validation_epoch_end","text":"<pre><code>on_validation_epoch_end(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):  # noqa: D102\n    # Only do this every self.plot_every_n_val_epochs epochs\n    is_val_plot_epoch = self.is_val_plot_epoch(pl_module.current_epoch, trainer.check_val_every_n_epoch)\n    if is_val_plot_epoch:\n        pl_module.val_cmx.compute()\n        pl_module.val_roc.compute()\n        pl_module.val_prc.compute()\n\n        # Plot roc, prc and confusion matrix to disk and wandb\n        fig_cmx, _ = pl_module.val_cmx.plot(cmap=\"Blues\")\n        fig_roc, _ = pl_module.val_roc.plot(score=True)\n        fig_prc, _ = pl_module.val_prc.plot(score=True)\n\n        # Check for a wandb or csv logger to log the images\n        for pllogger in pl_module.loggers:\n            if isinstance(pllogger, CSVLogger):\n                fig_dir = Path(pllogger.log_dir) / \"figures\" / f\"{self._val_prefix}-samples\"\n                fig_dir.mkdir(exist_ok=True, parents=True)\n                fig_cmx.savefig(fig_dir / f\"cmx_{pl_module.global_step}png\")\n                fig_roc.savefig(fig_dir / f\"roc_{pl_module.global_step}png\")\n                fig_prc.savefig(fig_dir / f\"prc_{pl_module.global_step}.png\")\n            if isinstance(pllogger, WandbLogger):\n                wandb_run: Run = pllogger.experiment\n                wandb_run.log({f\"{self._val_prefix}/cmx\": wandb.Image(fig_cmx)}, commit=False)\n                wandb_run.log({f\"{self._val_prefix}/roc\": wandb.Image(fig_roc)}, commit=False)\n                wandb_run.log({f\"{self._val_prefix}/prc\": wandb.Image(fig_prc)}, commit=False)\n\n        fig_cmx.clear()\n        fig_roc.clear()\n        fig_prc.clear()\n        plt.close(\"all\")\n\n    # This will also commit the accumulated plots\n    pl_module.log_dict(pl_module.val_metrics.compute())\n\n    pl_module.val_metrics.reset()\n    pl_module.val_roc.reset()\n    pl_module.val_prc.reset()\n    pl_module.val_cmx.reset()\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.setup","title":"setup","text":"<pre><code>setup(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> <p>Setups the callback.</p> <p>Creates metrics required for the specific stage:</p> <ul> <li>For the \"fit\" stage, creates training and validation metrics and visualizations.</li> <li>For the \"validate\" stage, only creates validation metrics and visualizations.</li> <li>For the \"test\" stage, only creates test metrics and visualizations.</li> <li>For the \"predict\" stage, no metrics or visualizations are created.</li> </ul> <p>Always maps the trainer and pl_module to the callback.</p> <p>Training and validation metrics are \"simple\" metrics from torchmetrics. The validation visualizations are more complex metrics from torchmetrics. The test metrics and vsiualizations are the same as the validation ones, and also include custom \"Instance\" metrics.</p> <p>Parameters:</p> <ul> <li> <code>trainer</code>               (<code>lightning.Trainer</code>)           \u2013            <p>The lightning trainer.</p> </li> <li> <code>pl_module</code>               (<code>lightning.LightningModule</code>)           \u2013            <p>The lightning module.</p> </li> <li> <code>stage</code>               (<code>typing.Literal['fit', 'validate', 'test', 'predict']</code>)           \u2013            <p>The current stage. One of: \"fit\", \"validate\", \"test\", \"predict\".</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def setup(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):\n    \"\"\"Setups the callback.\n\n    Creates metrics required for the specific stage:\n\n    - For the \"fit\" stage, creates training and validation metrics and visualizations.\n    - For the \"validate\" stage, only creates validation metrics and visualizations.\n    - For the \"test\" stage, only creates test metrics and visualizations.\n    - For the \"predict\" stage, no metrics or visualizations are created.\n\n    Always maps the trainer and pl_module to the callback.\n\n    Training and validation metrics are \"simple\" metrics from torchmetrics.\n    The validation visualizations are more complex metrics from torchmetrics.\n    The test metrics and vsiualizations are the same as the validation ones,\n    and also include custom \"Instance\" metrics.\n\n    Args:\n        trainer (Trainer): The lightning trainer.\n        pl_module (LightningModule): The lightning module.\n        stage (Literal[\"fit\", \"validate\", \"test\", \"predict\"]): The current stage.\n            One of: \"fit\", \"validate\", \"test\", \"predict\".\n\n    \"\"\"\n    # Save references to the trainer and pl_module\n    self.trainer = trainer\n    self.pl_module = pl_module\n    self.stage = stage\n\n    # We don't want to use memory in the predict stage\n    if stage == \"predict\":\n        return\n\n    metric_kwargs = {\"task\": \"binary\", \"validate_args\": False, \"ignore_index\": 2}\n    metrics = MetricCollection(\n        {\n            \"Accuracy\": Accuracy(**metric_kwargs),\n            \"Precision\": Precision(**metric_kwargs),\n            \"Specificity\": Specificity(**metric_kwargs),\n            \"Recall\": Recall(**metric_kwargs),\n            \"F1Score\": F1Score(**metric_kwargs),\n            \"JaccardIndex\": JaccardIndex(**metric_kwargs),\n            \"CohenKappa\": CohenKappa(**metric_kwargs),\n            \"HammingDistance\": HammingDistance(**metric_kwargs),\n        }\n    )\n\n    added_metrics: list[str] = []\n\n    # Train metrics only for the fit stage\n    if stage == \"fit\":\n        pl_module.train_metrics = metrics.clone(prefix=\"train/\")\n        added_metrics += list(pl_module.train_metrics.keys())\n    # Validation metrics and visualizations for the fit and validate stages\n    if stage == \"fit\" or stage == \"validate\":\n        pl_module.val_metrics = metrics.clone(prefix=f\"{self._val_prefix}/\")\n        pl_module.val_metrics.add_metrics(\n            {\n                \"AUROC\": AUROC(thresholds=20, **metric_kwargs),\n                \"AveragePrecision\": AveragePrecision(thresholds=20, **metric_kwargs),\n            }\n        )\n        pl_module.val_roc = ROC(thresholds=20, **metric_kwargs)\n        pl_module.val_prc = PrecisionRecallCurve(thresholds=20, **metric_kwargs)\n        pl_module.val_cmx = ConfusionMatrix(normalize=\"true\", **metric_kwargs)\n        added_metrics += list(pl_module.val_metrics.keys())\n        added_metrics += [f\"{self._val_prefix}/{m}\" for m in [\"roc\", \"prc\", \"cmx\"]]\n\n    # Test metrics and visualizations for the test stage\n    if stage == \"test\":\n        pl_module.test_metrics = metrics.clone(prefix=f\"{pl_module.test_set}/\")\n        pl_module.test_metrics.add_metrics(\n            {\n                \"AUROC\": AUROC(thresholds=20, **metric_kwargs),\n                \"AveragePrecision\": AveragePrecision(thresholds=20, **metric_kwargs),\n            }\n        )\n        pl_module.test_roc = ROC(thresholds=20, **metric_kwargs)\n        pl_module.test_prc = PrecisionRecallCurve(thresholds=20, **metric_kwargs)\n        pl_module.test_cmx = ConfusionMatrix(normalize=\"true\", **metric_kwargs)\n\n        # Instance Metrics\n        instance_metric_kwargs = {\"validate_args\": False, \"ignore_index\": 2, \"matching_threshold\": 0.3}\n        pl_module.test_metrics.add_metrics(\n            {\n                \"InstanceAccuracy\": BinaryInstanceAccuracy(**instance_metric_kwargs),\n                \"InstancePrecision\": BinaryInstancePrecision(**instance_metric_kwargs),\n                \"InstanceRecall\": BinaryInstanceRecall(**instance_metric_kwargs),\n                \"InstanceF1Score\": BinaryInstanceF1Score(**instance_metric_kwargs),\n                \"InstanceAveragePrecision\": BinaryInstanceAveragePrecision(thresholds=20, **instance_metric_kwargs),\n            }\n        )\n        boundary_metric_kwargs = {\"validate_args\": False, \"ignore_index\": 2}\n        pl_module.test_metrics.add_metrics(\n            {\n                \"InstanceBoundaryIoU\": BinaryBoundaryIoU(**boundary_metric_kwargs),\n            }\n        )\n        pl_module.test_instance_prc = BinaryInstancePrecisionRecallCurve(thresholds=20, **instance_metric_kwargs)\n        pl_module.test_instance_cmx = BinaryInstanceConfusionMatrix(normalize=True, **instance_metric_kwargs)\n\n        added_metrics += list(pl_module.test_metrics.keys())\n        added_metrics += [f\"{self.test_set}/{m}\" for m in [\"roc\", \"prc\", \"cmx\", \"instance_prc\", \"instance_cmx\"]]\n\n    # Log the added metrics\n    sep = \"\\n\\t- \"\n    logger.debug(f\"Added metrics:{sep + sep.join(added_metrics)}\")\n</code></pre>"},{"location":"reference/darts_segmentation/training/BinarySegmentationMetrics/#darts_segmentation.training.BinarySegmentationMetrics.teardown","title":"teardown","text":"<pre><code>teardown(\n    trainer: lightning.Trainer,\n    pl_module: lightning.LightningModule,\n    stage: darts_segmentation.training.callbacks.Stage,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/callbacks.py</code> <pre><code>def teardown(self, trainer: Trainer, pl_module: LightningModule, stage: Stage):  # noqa: D102\n    # Delete the references to the trainer and pl_module\n    del self.trainer\n    del self.pl_module\n    del self.stage\n\n    # No need to delete anything if we are in the predict stage\n    if stage == \"predict\":\n        return\n\n    if stage == \"fit\":\n        del pl_module.train_metrics\n\n    if stage == \"fit\" or stage == \"validate\":\n        del pl_module.val_metrics\n        del pl_module.val_roc\n        del pl_module.val_prc\n        del pl_module.val_cmx\n\n    if stage == \"test\":\n        del pl_module.test_metrics\n        del pl_module.test_roc\n        del pl_module.test_prc\n        del pl_module.test_cmx\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/","title":"darts_segmentation.training.DartsDataModule","text":"<p>               Bases: <code>lightning.LightningDataModule</code></p> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: Path,\n    batch_size: int,\n    fold: int = 0,  # Not used for test\n    augment: bool = True,  # Not used for test\n    num_workers: int = 0,\n    in_memory: bool = False,\n):\n    super().__init__()\n    self.save_hyperparameters()\n    self.data_dir = data_dir\n    self.batch_size = batch_size\n    self.fold = fold\n    self.augment = augment\n    self.num_workers = num_workers\n    self.in_memory = in_memory\n\n    data_dir = Path(data_dir)\n\n    store = zarr.storage.DirectoryStore(data_dir)\n    zroot = zarr.group(store=store)\n    self.nsamples = len(zroot[\"x\"])\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.augment","title":"augment  <code>instance-attribute</code>","text":"<pre><code>augment = darts_segmentation.training.data.DartsDataModule(\n    augment\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = (\n    darts_segmentation.training.data.DartsDataModule(\n        batch_size\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.data_dir","title":"data_dir  <code>instance-attribute</code>","text":"<pre><code>data_dir = darts_segmentation.training.data.DartsDataModule(\n    data_dir\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.fold","title":"fold  <code>instance-attribute</code>","text":"<pre><code>fold = darts_segmentation.training.data.DartsDataModule(\n    fold\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.in_memory","title":"in_memory  <code>instance-attribute</code>","text":"<pre><code>in_memory = (\n    darts_segmentation.training.data.DartsDataModule(\n        in_memory\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.nsamples","title":"nsamples  <code>instance-attribute</code>","text":"<pre><code>nsamples = len(zroot['x'])\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.num_workers","title":"num_workers  <code>instance-attribute</code>","text":"<pre><code>num_workers = (\n    darts_segmentation.training.data.DartsDataModule(\n        num_workers\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.setup","title":"setup","text":"<pre><code>setup(\n    stage: typing.Literal[\n        \"fit\", \"validate\", \"test\", \"predict\"\n    ]\n    | None = None,\n)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def setup(self, stage: Literal[\"fit\", \"validate\", \"test\", \"predict\"] | None = None):\n    if stage in [\"fit\", \"validate\"]:\n        kf = KFold(n_splits=5)\n        train_idx, val_idx = list(kf.split(range(self.nsamples)))[self.fold]\n\n        dsclass = DartsDatasetInMemory if self.in_memory else DartsDatasetZarr\n        self.train = dsclass(self.data_dir, self.augment, train_idx)\n        self.val = dsclass(self.data_dir, False, val_idx)\n    if stage == \"test\":\n        dsclass = DartsDatasetInMemory if self.in_memory else DartsDatasetZarr\n        self.test = dsclass(self.data_dir, False)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.test_dataloader","title":"test_dataloader","text":"<pre><code>test_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def test_dataloader(self):\n    return DataLoader(self.test, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def train_dataloader(self):\n    return DataLoader(self.train, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataModule/#darts_segmentation.training.DartsDataModule.val_dataloader","title":"val_dataloader","text":"<pre><code>val_dataloader()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def val_dataloader(self):\n    return DataLoader(self.val, batch_size=self.batch_size, num_workers=self.num_workers)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataset/","title":"darts_segmentation.training.DartsDataset","text":"<p>               Bases: <code>torch.utils.data.Dataset</code></p> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(self, data_dir: Path | str, augment: bool, indices: list[int] | None = None):\n    if isinstance(data_dir, str):\n        data_dir = Path(data_dir)\n\n    self.x_files = sorted((data_dir / \"x\").glob(\"*.pt\"))\n    self.y_files = sorted((data_dir / \"y\").glob(\"*.pt\"))\n    assert len(self.x_files) == len(self.y_files), (\n        f\"Dataset corrupted! Got {len(self.x_files)=} and {len(self.y_files)=}!\"\n    )\n    if indices is not None:\n        self.x_files = [self.x_files[i] for i in indices]\n        self.y_files = [self.y_files[i] for i in indices]\n\n    self.transform = (\n        A.Compose(\n            [\n                A.HorizontalFlip(),\n                A.VerticalFlip(),\n                A.RandomRotate90(),\n                # A.Blur(),\n                A.RandomBrightnessContrast(),\n                A.MultiplicativeNoise(per_channel=True, elementwise=True),\n                # ToTensorV2(),\n            ]\n        )\n        if augment\n        else None\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataset/#darts_segmentation.training.DartsDataset.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = (\n    albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(),\n            albumentations.VerticalFlip(),\n            albumentations.RandomRotate90(),\n            albumentations.RandomBrightnessContrast(),\n            albumentations.MultiplicativeNoise(\n                per_channel=True, elementwise=True\n            ),\n        ]\n    )\n    if darts_segmentation.training.data.DartsDataset(\n        augment\n    )\n    else None\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataset/#darts_segmentation.training.DartsDataset.x_files","title":"x_files  <code>instance-attribute</code>","text":"<pre><code>x_files = sorted(\n    darts_segmentation.training.data.DartsDataset(data_dir)\n    / \"x\".glob(\"*.pt\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataset/#darts_segmentation.training.DartsDataset.y_files","title":"y_files  <code>instance-attribute</code>","text":"<pre><code>y_files = sorted(\n    darts_segmentation.training.data.DartsDataset(data_dir)\n    / \"y\".glob(\"*.pt\")\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataset/#darts_segmentation.training.DartsDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    xfile = self.x_files[idx]\n    yfile = self.y_files[idx]\n    assert xfile.stem == yfile.stem, f\"Dataset corrupted! Files must have the same name, but got {xfile=} {yfile=}!\"\n\n    x = torch.load(xfile).numpy()\n    y = torch.load(yfile).int().numpy()\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDataset/#darts_segmentation.training.DartsDataset.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.x_files)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetInMemory/","title":"darts_segmentation.training.DartsDatasetInMemory","text":"<p>               Bases: <code>torch.utils.data.Dataset</code></p> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(self, data_dir: Path | str, augment: bool, indices: list[int] | None = None):\n    if isinstance(data_dir, str):\n        data_dir = Path(data_dir)\n\n    x_files = sorted((data_dir / \"x\").glob(\"*.pt\"))\n    y_files = sorted((data_dir / \"y\").glob(\"*.pt\"))\n    assert len(x_files) == len(y_files), f\"Dataset corrupted! Got {len(x_files)=} and {len(y_files)=}!\"\n    if indices is not None:\n        x_files = [x_files[i] for i in indices]\n        y_files = [y_files[i] for i in indices]\n\n    self.x = []\n    self.y = []\n    for xfile, yfile in zip(x_files, y_files):\n        assert xfile.stem == yfile.stem, (\n            f\"Dataset corrupted! Files must have the same name, but got {xfile=} {yfile=}!\"\n        )\n        x = torch.load(xfile).numpy()\n        y = torch.load(yfile).int().numpy()\n        self.x.append(x)\n        self.y.append(y)\n\n    self.transform = (\n        A.Compose(\n            [\n                A.HorizontalFlip(),\n                A.VerticalFlip(),\n                A.RandomRotate90(),\n                # A.Blur(),\n                A.RandomBrightnessContrast(),\n                A.MultiplicativeNoise(per_channel=True, elementwise=True),\n                # ToTensorV2(),\n            ]\n        )\n        if augment\n        else None\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetInMemory/#darts_segmentation.training.DartsDatasetInMemory.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = (\n    albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(),\n            albumentations.VerticalFlip(),\n            albumentations.RandomRotate90(),\n            albumentations.RandomBrightnessContrast(),\n            albumentations.MultiplicativeNoise(\n                per_channel=True, elementwise=True\n            ),\n        ]\n    )\n    if darts_segmentation.training.data.DartsDatasetInMemory(\n        augment\n    )\n    else None\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetInMemory/#darts_segmentation.training.DartsDatasetInMemory.x","title":"x  <code>instance-attribute</code>","text":"<pre><code>x = []\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetInMemory/#darts_segmentation.training.DartsDatasetInMemory.y","title":"y  <code>instance-attribute</code>","text":"<pre><code>y = []\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetInMemory/#darts_segmentation.training.DartsDatasetInMemory.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    x = self.x[idx]\n    y = self.y[idx]\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetInMemory/#darts_segmentation.training.DartsDatasetInMemory.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.x)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetZarr/","title":"darts_segmentation.training.DartsDatasetZarr","text":"<p>               Bases: <code>torch.utils.data.Dataset</code></p> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __init__(self, data_dir: Path | str, augment: bool, indices: list[int] | None = None):\n    if isinstance(data_dir, str):\n        data_dir = Path(data_dir)\n\n    store = zarr.storage.LocalStore(data_dir)\n    self.zroot = zarr.group(store=store)\n\n    assert \"x\" in self.zroot and \"y\" in self.zroot, (\n        f\"Dataset corrupted! {self.zroot.info=} must contain 'x' or 'y' arrays!\"\n    )\n\n    self.indices = indices if indices is not None else list(range(self.zroot[\"x\"].shape[0]))\n\n    self.transform = (\n        A.Compose(\n            [\n                A.HorizontalFlip(),\n                A.VerticalFlip(),\n                A.RandomRotate90(),\n                # A.Blur(),\n                A.RandomBrightnessContrast(),\n                A.MultiplicativeNoise(per_channel=True, elementwise=True),\n                # ToTensorV2(),\n            ]\n        )\n        if augment\n        else None\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetZarr/#darts_segmentation.training.DartsDatasetZarr.indices","title":"indices  <code>instance-attribute</code>","text":"<pre><code>indices = (\n    darts_segmentation.training.data.DartsDatasetZarr(\n        indices\n    )\n    if darts_segmentation.training.data.DartsDatasetZarr(\n        indices\n    )\n    is not None\n    else list(\n        range(\n            darts_segmentation.training.data.DartsDatasetZarr(\n                self\n            )\n            .zroot[\"x\"]\n            .shape[0]\n        )\n    )\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetZarr/#darts_segmentation.training.DartsDatasetZarr.transform","title":"transform  <code>instance-attribute</code>","text":"<pre><code>transform = (\n    albumentations.Compose(\n        [\n            albumentations.HorizontalFlip(),\n            albumentations.VerticalFlip(),\n            albumentations.RandomRotate90(),\n            albumentations.RandomBrightnessContrast(),\n            albumentations.MultiplicativeNoise(\n                per_channel=True, elementwise=True\n            ),\n        ]\n    )\n    if darts_segmentation.training.data.DartsDatasetZarr(\n        augment\n    )\n    else None\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetZarr/#darts_segmentation.training.DartsDatasetZarr.zroot","title":"zroot  <code>instance-attribute</code>","text":"<pre><code>zroot = zarr.group(store=store)\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetZarr/#darts_segmentation.training.DartsDatasetZarr.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __getitem__(self, idx):\n    i = self.indices[idx]\n\n    x = self.zroot[\"x\"][i]\n    y = self.zroot[\"y\"][i]\n\n    # Apply augmentations\n    if self.transform is not None:\n        augmented = self.transform(image=x.transpose(1, 2, 0), mask=y)\n        x = augmented[\"image\"].transpose(2, 0, 1)\n        y = augmented[\"mask\"]\n\n    return x, y\n</code></pre>"},{"location":"reference/darts_segmentation/training/DartsDatasetZarr/#darts_segmentation.training.DartsDatasetZarr.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/data.py</code> <pre><code>def __len__(self):\n    return len(self.indices)\n</code></pre>"},{"location":"reference/darts_segmentation/training/SMPSegmenter/","title":"darts_segmentation.training.SMPSegmenter","text":"<p>               Bases: <code>lightning.LightningModule</code></p> <p>Lightning module for training a segmentation model using the segmentation_models_pytorch library.</p> <p>Initialize the SMPSegmenter.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>darts_segmentation.segment.SMPSegmenterConfig</code>)           \u2013            <p>Configuration for the segmentation model.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>1e-05</code> )           \u2013            <p>Initial learning rate. Defaults to 1e-5.</p> </li> <li> <code>gamma</code>               (<code>float</code>, default:                   <code>0.9</code> )           \u2013            <p>Multiplicative factor of learning rate decay. Defaults to 0.9.</p> </li> <li> <code>focal_loss_alpha</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Weight factor to balance positive and negative samples. Alpha must be in [0...1] range, high values will give more weight to positive class. None will not weight samples. Defaults to None.</p> </li> <li> <code>focal_loss_gamma</code>               (<code>float</code>, default:                   <code>2.0</code> )           \u2013            <p>Focal loss power factor. Defaults to 2.0.</p> </li> <li> <code>kwargs</code>               (<code>dict[str, typing.Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments which should be saved to the hyperparameter file.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def __init__(\n    self,\n    config: SMPSegmenterConfig,\n    learning_rate: float = 1e-5,\n    gamma: float = 0.9,\n    focal_loss_alpha: float | None = None,\n    focal_loss_gamma: float = 2.0,\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Initialize the SMPSegmenter.\n\n    Args:\n        config (SMPSegmenterConfig): Configuration for the segmentation model.\n        learning_rate (float, optional): Initial learning rate. Defaults to 1e-5.\n        gamma (float, optional): Multiplicative factor of learning rate decay. Defaults to 0.9.\n        focal_loss_alpha (float, optional): Weight factor to balance positive and negative samples.\n            Alpha must be in [0...1] range, high values will give more weight to positive class.\n            None will not weight samples. Defaults to None.\n        focal_loss_gamma (float, optional): Focal loss power factor. Defaults to 2.0.\n        kwargs (dict[str, Any]): Additional keyword arguments which should be saved to the hyperparameter file.\n\n    \"\"\"\n    super().__init__()\n\n    # This saves config, learning_rate and gamma under self.hparams\n    self.save_hyperparameters(ignore=[\"test_set\", \"val_set\"])\n    self.model = smp.create_model(**config[\"model\"], activation=\"sigmoid\")\n\n    # Assumes that the training preparation was done with setting invalid pixels in the mask to 2\n    self.loss_fn = smp.losses.FocalLoss(\n        mode=\"binary\", alpha=focal_loss_alpha, gamma=focal_loss_gamma, ignore_index=2\n    )\n</code></pre>"},{"location":"reference/darts_segmentation/training/SMPSegmenter/#darts_segmentation.training.SMPSegmenter.loss_fn","title":"loss_fn  <code>instance-attribute</code>","text":"<pre><code>loss_fn = segmentation_models_pytorch.losses.FocalLoss(\n    mode=\"binary\",\n    alpha=darts_segmentation.training.module.SMPSegmenter(\n        focal_loss_alpha\n    ),\n    gamma=darts_segmentation.training.module.SMPSegmenter(\n        focal_loss_gamma\n    ),\n    ignore_index=2,\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/SMPSegmenter/#darts_segmentation.training.SMPSegmenter.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = segmentation_models_pytorch.create_model(\n    **darts_segmentation.training.module.SMPSegmenter(\n        config\n    )[\"model\"],\n    activation=\"sigmoid\",\n)\n</code></pre>"},{"location":"reference/darts_segmentation/training/SMPSegmenter/#darts_segmentation.training.SMPSegmenter.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def __repr__(self):  # noqa: D105\n    return f\"SMPSegmenter({self.hparams['config']['model']})\"\n</code></pre>"},{"location":"reference/darts_segmentation/training/SMPSegmenter/#darts_segmentation.training.SMPSegmenter.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def configure_optimizers(self):  # noqa: D102\n    optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=self.hparams.gamma)\n    return [optimizer], [scheduler]\n</code></pre>"},{"location":"reference/darts_segmentation/training/SMPSegmenter/#darts_segmentation.training.SMPSegmenter.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end()\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def on_train_epoch_end(self):  # noqa: D102\n    self.log(\"learning_rate\", self.lr_schedulers().get_last_lr()[0])\n</code></pre>"},{"location":"reference/darts_segmentation/training/SMPSegmenter/#darts_segmentation.training.SMPSegmenter.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def test_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/SMPSegmenter/#darts_segmentation.training.SMPSegmenter.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def training_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/SMPSegmenter/#darts_segmentation.training.SMPSegmenter.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx)\n</code></pre> Source code in <code>darts-segmentation/src/darts_segmentation/training/module.py</code> <pre><code>def validation_step(self, batch, batch_idx):  # noqa: D102\n    x, y = batch\n    y_hat = self.model(x).squeeze(1)\n    loss = self.loss_fn(y_hat, y.long())\n    return {\n        \"loss\": loss,\n        \"y_hat\": y_hat,\n    }\n</code></pre>"},{"location":"reference/darts_segmentation/training/create_training_patches/","title":"darts_segmentation.training.create_training_patches","text":"<p>Create training patches from a tile and labels.</p> <p>Parameters:</p> <ul> <li> <code>tile</code>               (<code>xarray.Dataset</code>)           \u2013            <p>The input tile, containing preprocessed, harmonized data.</p> </li> <li> <code>labels</code>               (<code>geopandas.GeoDataFrame</code>)           \u2013            <p>The labels to be used for training.</p> </li> <li> <code>bands</code>               (<code>list[str]</code>)           \u2013            <p>The bands to be used for training. Must be present in the tile.</p> </li> <li> <code>norm_factors</code>               (<code>dict[str, float]</code>)           \u2013            <p>The normalization factors for the bands.</p> </li> <li> <code>patch_size</code>               (<code>int</code>)           \u2013            <p>The size of the patches.</p> </li> <li> <code>overlap</code>               (<code>int</code>)           \u2013            <p>The size of the overlap.</p> </li> <li> <code>exclude_nopositive</code>               (<code>bool</code>)           \u2013            <p>Whether to exclude patches where the labels do not contain positives.</p> </li> <li> <code>exclude_nan</code>               (<code>bool</code>)           \u2013            <p>Whether to exclude patches where the input data has nan values.</p> </li> <li> <code>device</code>               (<code>typing.Literal['cuda', 'cpu'] | int</code>)           \u2013            <p>The device to use for the erosion.</p> </li> <li> <code>mask_erosion_size</code>               (<code>int</code>)           \u2013            <p>The size of the disk to use for erosion.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>collections.abc.Generator[tuple[torch.tensor, torch.tensor]]</code>           \u2013            <p>Generator[tuple[torch.tensor, torch.tensor]]: A tuple containing the input and the labels as pytorch tensors. The input has the format (C, H, W), the labels (H, W).</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If a band is not found in the preprocessed data.</p> </li> </ul> Source code in <code>darts-segmentation/src/darts_segmentation/training/prepare_training.py</code> <pre><code>def create_training_patches(\n    tile: xr.Dataset,\n    labels: gpd.GeoDataFrame,\n    bands: list[str],\n    norm_factors: dict[str, float],\n    patch_size: int,\n    overlap: int,\n    exclude_nopositive: bool,\n    exclude_nan: bool,\n    device: Literal[\"cuda\", \"cpu\"] | int,\n    mask_erosion_size: int,\n) -&gt; Generator[tuple[torch.tensor, torch.tensor]]:\n    \"\"\"Create training patches from a tile and labels.\n\n    Args:\n        tile (xr.Dataset): The input tile, containing preprocessed, harmonized data.\n        labels (gpd.GeoDataFrame): The labels to be used for training.\n        bands (list[str]): The bands to be used for training. Must be present in the tile.\n        norm_factors (dict[str, float]): The normalization factors for the bands.\n        patch_size (int): The size of the patches.\n        overlap (int): The size of the overlap.\n        exclude_nopositive (bool): Whether to exclude patches where the labels do not contain positives.\n        exclude_nan (bool): Whether to exclude patches where the input data has nan values.\n        device (Literal[\"cuda\", \"cpu\"] | int): The device to use for the erosion.\n        mask_erosion_size (int): The size of the disk to use for erosion.\n\n    Yields:\n        Generator[tuple[torch.tensor, torch.tensor]]: A tuple containing the input and the labels as pytorch tensors.\n            The input has the format (C, H, W), the labels (H, W).\n\n    Raises:\n        ValueError: If a band is not found in the preprocessed data.\n\n    \"\"\"\n    if len(labels) == 0 and exclude_nopositive:\n        logger.warning(\"No labels found in the labels GeoDataFrame. Skipping.\")\n        return\n\n    # Rasterize the labels\n    if len(labels) &gt; 0:\n        labels_rasterized = 1 - make_geocube(labels, measurements=[\"id\"], like=tile).id.isnull()\n    else:\n        labels_rasterized = xr.zeros_like(tile[\"valid_data_mask\"])\n\n    # Filter out the nodata values (class 2 -&gt; invalid data)\n    mask = erode_mask(tile[\"valid_data_mask\"], mask_erosion_size, device)\n    mask = tile[\"valid_data_mask\"]\n    labels_rasterized = xr.where(mask, labels_rasterized, 2)\n\n    # Normalize the bands and clip the values\n    for band in bands:\n        if band not in tile:\n            raise ValueError(f\"Band '{band}' not found in the preprocessed data.\")\n        with xr.set_options(keep_attrs=True):\n            tile[band] = tile[band] * norm_factors[band]\n            tile[band] = tile[band].clip(0, 1)\n\n    # Replace invalid values with nan (used for nan check later on)\n    tile = xr.where(tile[\"valid_data_mask\"], tile, float(\"nan\"))\n\n    # Convert to dataaray and select the bands (bands are now in specified order)\n    tile = tile.to_dataarray(dim=\"band\").sel(band=bands)\n\n    # Transpose to (C, H, W)\n    tile = tile.transpose(\"band\", \"y\", \"x\")\n    labels_rasterized = labels_rasterized.transpose(\"y\", \"x\")\n\n    # Convert to tensor\n    tensor_tile = torch.tensor(tile.values).float()\n    tensor_labels = torch.tensor(labels_rasterized.values).float()\n\n    assert tensor_tile.dim() == 3, f\"Expects tensor_tile to has shape (C, H, W), got {tensor_tile.shape}\"\n    assert tensor_labels.dim() == 2, f\"Expects tensor_labels to has shape (H, W), got {tensor_labels.shape}\"\n\n    # Create patches\n    tensor_patches = create_patches(tensor_tile.unsqueeze(0), patch_size, overlap)\n    tensor_patches = tensor_patches.reshape(-1, len(bands), patch_size, patch_size)\n    tensor_labels = create_patches(tensor_labels.unsqueeze(0).unsqueeze(0), patch_size, overlap)\n    tensor_labels = tensor_labels.reshape(-1, patch_size, patch_size)\n\n    # Turn the patches into a list of tuples\n    n_patches = tensor_patches.shape[0]\n    for i in range(n_patches):\n        x = tensor_patches[i]\n        y = tensor_labels[i]\n\n        if exclude_nopositive and not (y == 1).any():\n            continue\n\n        if exclude_nan and torch.isnan(x).any():\n            continue\n\n        # Skip where there are less than 10% visible pixel\n        if ((y != 2).sum() / y.numel()) &lt; 0.1:\n            continue\n\n        # Skip patches where everything is nan\n        if torch.isnan(x).all():\n            continue\n\n        # Convert all nan values to 0\n        x[torch.isnan(x)] = 0\n\n        logger.debug(f\"Yielding patch {i} with\\n\\t{x=}\\n\\t{y=}\")\n        yield x, y\n</code></pre>"},{"location":"reference/darts_utils/","title":"darts_utils","text":"<p>Utility functions for the DARTS dataset.</p> <p>Attributes:</p> <ul> <li> <code>__version__</code>           \u2013            </li> </ul>"},{"location":"reference/darts_utils/#darts_utils.__version__","title":"__version__  <code>module-attribute</code>","text":"<pre><code>__version__ = importlib.metadata.version('darts-nextgen')\n</code></pre>"},{"location":"reference/darts_utils/cuda/","title":"darts_utils.cuda","text":"<p>Utility functions around cuda, e.g. memory management.</p> <p>Functions:</p> <ul> <li> <code>free_cupy</code>             \u2013              <p>Free the CUDA memory of cupy.</p> </li> <li> <code>free_torch</code>             \u2013              <p>Free the CUDA memory of pytorch.</p> </li> </ul> <ul> <li>free_cupy</li> <li>free_torch</li> </ul>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.free_cupy","title":"free_cupy","text":"<pre><code>free_cupy()\n</code></pre> <p>Free the CUDA memory of cupy.</p> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def free_cupy():\n    \"\"\"Free the CUDA memory of cupy.\"\"\"\n    try:\n        import cupy as cp\n    except ImportError:\n        cp = None\n\n    if cp is not None:\n        gc.collect()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/darts_utils/cuda/#darts_utils.cuda.free_torch","title":"free_torch","text":"<pre><code>free_torch()\n</code></pre> <p>Free the CUDA memory of pytorch.</p> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def free_torch():\n    \"\"\"Free the CUDA memory of pytorch.\"\"\"\n    import torch\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/darts_utils/cuda/free_cupy/","title":"darts_utils.cuda.free_cupy","text":"<p>Free the CUDA memory of cupy.</p> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def free_cupy():\n    \"\"\"Free the CUDA memory of cupy.\"\"\"\n    try:\n        import cupy as cp\n    except ImportError:\n        cp = None\n\n    if cp is not None:\n        gc.collect()\n        cp.get_default_memory_pool().free_all_blocks()\n        cp.get_default_pinned_memory_pool().free_all_blocks()\n</code></pre>"},{"location":"reference/darts_utils/cuda/free_torch/","title":"darts_utils.cuda.free_torch","text":"<p>Free the CUDA memory of pytorch.</p> Source code in <code>darts-utils/src/darts_utils/cuda.py</code> <pre><code>def free_torch():\n    \"\"\"Free the CUDA memory of pytorch.\"\"\"\n    import torch\n\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/darts_utils/rich/","title":"darts_utils.rich","text":"<p>A singleton class to manage rich progress bars for the application.</p> <p>Classes:</p> <ul> <li> <code>RichManagerSingleton</code>           \u2013            <p>A singleton class to manage rich progress bars for the application.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>RichManager</code>           \u2013            </li> </ul> <ul> <li>RichManagerSingleton</li> </ul>"},{"location":"reference/darts_utils/rich/#darts_utils.rich.RichManager","title":"RichManager  <code>module-attribute</code>","text":"<pre><code>RichManager = darts_utils.rich.RichManagerSingleton()\n</code></pre>"},{"location":"reference/darts_utils/rich/#darts_utils.rich.RichManagerSingleton","title":"RichManagerSingleton","text":"<pre><code>RichManagerSingleton()\n</code></pre> <p>A singleton class to manage rich progress bars for the application.</p> <p>Initialize the RichProgressManager.</p> <p>Methods:</p> <ul> <li> <code>__del__</code>             \u2013              <p>Exit the RichProgressManager.</p> </li> <li> <code>__new__</code>             \u2013              <p>Create a new instance of the RichProgressManager if it does not exist yet.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>console</code>           \u2013            </li> </ul> Source code in <code>darts-utils/src/darts_utils/rich.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the RichProgressManager.\"\"\"\n    self.console = rich.console.Console()\n</code></pre>"},{"location":"reference/darts_utils/rich/#darts_utils.rich.RichManagerSingleton.console","title":"console  <code>instance-attribute</code>","text":"<pre><code>console = rich.console.Console()\n</code></pre>"},{"location":"reference/darts_utils/rich/#darts_utils.rich.RichManagerSingleton.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Exit the RichProgressManager.</p> Source code in <code>darts-utils/src/darts_utils/rich.py</code> <pre><code>def __del__(self):\n    \"\"\"Exit the RichProgressManager.\"\"\"\n</code></pre>"},{"location":"reference/darts_utils/rich/#darts_utils.rich.RichManagerSingleton.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create a new instance of the RichProgressManager if it does not exist yet.</p> Source code in <code>darts-utils/src/darts_utils/rich.py</code> <pre><code>def __new__(cls):\n    \"\"\"Create a new instance of the RichProgressManager if it does not exist yet.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n\n    return cls._instance\n</code></pre>"},{"location":"reference/darts_utils/rich/RichManagerSingleton/","title":"darts_utils.rich.RichManagerSingleton","text":"<p>A singleton class to manage rich progress bars for the application.</p> <p>Initialize the RichProgressManager.</p> Source code in <code>darts-utils/src/darts_utils/rich.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the RichProgressManager.\"\"\"\n    self.console = rich.console.Console()\n</code></pre>"},{"location":"reference/darts_utils/rich/RichManagerSingleton/#darts_utils.rich.RichManagerSingleton._instance","title":"_instance  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>_instance = None\n</code></pre>"},{"location":"reference/darts_utils/rich/RichManagerSingleton/#darts_utils.rich.RichManagerSingleton.console","title":"console  <code>instance-attribute</code>","text":"<pre><code>console = rich.console.Console()\n</code></pre>"},{"location":"reference/darts_utils/rich/RichManagerSingleton/#darts_utils.rich.RichManagerSingleton.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Exit the RichProgressManager.</p> Source code in <code>darts-utils/src/darts_utils/rich.py</code> <pre><code>def __del__(self):\n    \"\"\"Exit the RichProgressManager.\"\"\"\n</code></pre>"},{"location":"reference/darts_utils/rich/RichManagerSingleton/#darts_utils.rich.RichManagerSingleton.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create a new instance of the RichProgressManager if it does not exist yet.</p> Source code in <code>darts-utils/src/darts_utils/rich.py</code> <pre><code>def __new__(cls):\n    \"\"\"Create a new instance of the RichProgressManager if it does not exist yet.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__new__(cls)\n\n    return cls._instance\n</code></pre>"}]}